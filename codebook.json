{
   "Input Plausibility": {
    "description": "Failures induced by unrealistic or semantically invalid inputs may trigger incorrect model behavior, but they fall outside realistic  threat models. For instance, a stop sign that is slightly brightened or rotated remains visually plausible within the driving domain, whereas one corrupted by heavy random noise or visually incoherent textures does not. Similarly, speech containing mild background hiss remains intelligible and natural, while severely distorted or robotic utterances fall outside realistic speech conditions. This metric evaluates how such plausibility constraints are incorporated into the fuzzing process, and whether discovered failures are induced by inputs that remain realistic within the task domain. In DNN fuzzing, fault-revealing inputs are typically produced through iterative, input-centric mutation of seed inputs over extended fuzzing campaigns. While individual mutation steps may preserve perceptual or semantic plausibility, their cumulative effect can progressively introduce visible or audible artifacts, eventually yielding inputs that no longer resemble realistic instances from the task domain. Without mechanisms to control perceptual and semantic drift across successive fuzzing iterations, fuzzing may surface failures that are triggered only by implausible inputs, reducing their relevance under realistic adversarial threat models. To evaluate DNN fuzzing papers on this metric, we examine how mutation design and enforcement mechanisms enable or limit the preservation of input plausibility, including whether plausibility is treated as an end-to-end constraint across successive mutation steps or enforced only locally at individual mutations. In addition, we consider whether the fuzzing paper reports any explicit assessment of input plausibility for the final fault-triggering inputs, either through human study or quantitative, task-specific measures such as LPIPS and SSIM for visual similarity to the original input, PESQ and STOI for speech quality and intelligibility, or Perplexity for linguistic fluency.",
     "values": {
      "High": "Enforces input plausibility as an end-to-end constraint throughout the fuzzing process and provides explicit validation that final fault-inducing inputs remain perceptually or semantically realistic (e.g., via human evaluation or task-specific quantitative measures).",
      "Medium": "Enforces input plausibility locally at individual mutation steps (e.g., norm bounds or semantics-preserving transformations) or validates the plausibility of final fault-inducing inputs, but does not enforce end-to-end plausibility across fuzzing iterations.",
      "Low": "Does not enforce or assess input plausibility, allowing unrealistic or semantically invalid inputs to induce failures."
    }
  },
  "Attack Transferability": {
    "description": "Fault-inducing inputs crafted on a surrogate model often transfer across other models performing the same task, enabling substitute-model attacks against unseen targets without internal access. Such transferable attacks reveal shared vulnerability patterns across model implementations, rather than failures specific to a single model. We include the attack transferability metric to assess whether DNN fuzzing papers enable the discovery of failures that transfer across models. To evaluate DNN fuzzing papers on this metric, we examine whether fault-inducing inputs generated through fuzzing on one model are reused to test other models performing the same task, and whether the fuzzing design incorporates mechanisms that support the discovery of transferable security failures across models.",
    "values": {
      "High": "Fault-inducing inputs generated through fuzzing on one model are reused to test transferability to other models performing the same task, and the fuzzing design incorporates explicit mechanisms to support the discovery of transferable security failures across models.",
      "Medium": "Fault-inducing inputs generated through fuzzing on one model are reused to test transferability to other models, but the fuzzing design is not explicitly aimed at discovering transferable attacks.",
      "Low": "Does not demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models performing the same task."
    }
  }
}