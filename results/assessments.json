{
    "Liu et al. - 2024 - AUTODAN GENERATING STEALTHY JAILBREAK PROMPTS ON ALIGNED LARGE LANGUAGE MODELS": {
        "key": "liuautodan",
        "name": "AutoDAN",
        "year": "2024",
        "citation_count": "834",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly targets aligned/safety-trained LLMs and demonstrates that AutoDAN generates jailbreak prompts that cause those models to produce undesired/harmful outputs and to bypass explicit defenses. The authors state the attack goal as disrupting human-aligned refusals, evaluate on aligned models (e.g., Llama2-7b-chat) and show high attack success rates even when a perplexity-based defense is applied. They also report successful transfers to commercial models (e.g., GPT-3.5) in black-box settings. These points indicate AutoDAN uncovers high-impact failures that bypass alignment and safety mechanisms, matching the metric's definition of 'High.'",
                    "evidence": "Page 1, Abstract: \"AutoDAN: ... generating stealthy jailbreak prompts on aligned large language models\" and \"we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively.\" \n\nPage 3, Section 3.1 PRELIMINARIES (Threat model): \"The main goal of this type of attack is to disrupt the human-aligned values of LLMs or other constraints imposed by the model developer, compelling them to respond to malicious questions posed by adversaries with correct answers, rather than refusing to answer.\" \n\nPage 7, Section 4.2 RESULTS (Effectiveness against defense): \"As demonstrated in Tab. 3, the perplexity defense significantly undermines the effectiveness of the token-level jailbreak attack, i.e., GCG attack. However, the semantically meaningful jailbreak prompts AutoDAN (and the original handcrafted DAN) is not influenced.\" (Table 3 shows AutoDAN-HGA achieving high ASR on Llama2-7b-chat even under perplexity defense.)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper introduces AutoDAN, a method specifically designed to generate 'stealthy jailbreak prompts' for 'aligned Large Language Models (LLMs)'. Aligned LLMs are models equipped with explicit safety mechanisms to prevent them from producing harmful or malicious content. A jailbreak, by definition, is an attack that bypasses these safety features. The paper explicitly evaluates AutoDAN's ability to bypass a defense mechanism, demonstrating its effectiveness against perplexity-based defenses where a baseline attack's effectiveness is significantly reduced. This directly aligns with the 'High' severity metric, as the method's primary goal and demonstrated capability is to uncover high-impact failures (producing forbidden content) by bypassing explicit safety and policy mechanisms in safety-aligned models.",
                    "evidence": "Page 1, Abstract: '...jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. ... Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively.'\n\nPage 1, Introduction: '...jailbreak attacks have been discovered: combining the jailbreak prompt with malicious questions (e.g., how to steal someone\u2019s identity) can mislead the aligned LLMs to bypass the safety feature and consequently generate responses that compose harmful, discriminatory, violent, or sensitive content...'\n\nPage 8, Table 3 Caption: 'Effectiveness against perplexity defense. The results indicate that our method adeptly bypasses this type of defense, whereas GCG attack exhibits a substantial reduction in its attack strength. The evaluation highlights the importance of the preserving semantic meaningfulness of jailbreak prompts when confronting with defenses.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly targets aligned/safety-trained LLMs and demonstrates that AutoDAN generates jailbreak prompts that cause those models to produce undesired/harmful outputs and to bypass explicit defenses. The authors state the attack goal as disrupting human-aligned refusals, evaluate on aligned models (e.g., Llama2-7b-chat) and show high attack success rates even when a perplexity-based defense is applied. They also report successful transfers to commercial models (e.g., GPT-3.5) in black-box settings. These points indicate AutoDAN uncovers high-impact failures that bypass alignment and safety mechanisms, matching the metric's definition of 'High.'",
                    "evidence": "Page 1, Abstract: \"AutoDAN: ... generating stealthy jailbreak prompts on aligned large language models\" and \"we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively.\" \n\nPage 3, Section 3.1 PRELIMINARIES (Threat model): \"The main goal of this type of attack is to disrupt the human-aligned values of LLMs or other constraints imposed by the model developer, compelling them to respond to malicious questions posed by adversaries with correct answers, rather than refusing to answer.\" \n\nPage 7, Section 4.2 RESULTS (Effectiveness against defense): \"As demonstrated in Tab. 3, the perplexity defense significantly undermines the effectiveness of the token-level jailbreak attack, i.e., GCG attack. However, the semantically meaningful jailbreak prompts AutoDAN (and the original handcrafted DAN) is not influenced.\" (Table 3 shows AutoDAN-HGA achieving high ASR on Llama2-7b-chat even under perplexity defense.)"
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "High",
                    "why": "Guides towared output that start with a predified content"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Reports what fails without analyzing why. It provides tranferability rationale, not failure patterns or root cause"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly designs per-mutation constraints and mechanisms to preserve semantic plausibility (LLM-based diversification for initialization and mutation, synonym-based/momentum word replacement, and a low mutation probability), and it validates final fault-inducing prompts with a task-specific quantitative measure (sentence perplexity / PPL) and defense experiments (perplexity-based defense). However, the paper does not describe an explicit mechanism that enforces or measures plausibility across the entire evolutionary trajectory (i.e., cumulative drift across iterations) nor report iterative/plausibility traces; instead it relies on per-step design choices and final PPL measurements. Thus it validates final-input plausibility and constrains individual mutation steps but does not address cumulative-effects across many iterations, matching the metric's Medium level.",
                    "evidence": "3.2 Population Initialization (page 4): \"we employ LLMs as the agents responsible for revising the prototype prompt ... the modifications proposed by LLM can preserve the inherent logical flow and meaning of the original sentences, while simultaneously introducing diversity in word selection and sentence structure.\"; 3.4 Paragraph-level (page 5): \"We let the LLM-based diversification introduced in Alg. 5 to also serve as the mutation function due to its ability to preserve the global meaning and introduce diversity.\"; D.2 Implementation details (Appendix D, page 16): \"...a mutation rate of 0.01... We employ GPT-4-0613 as the base model for the Recheck metric and also as the base model for LLM-based Diversification.\"; 4.1 Metrics (page 6): \"For stealthiness, we use standard Sentence Perplexity (PPL) evaluated by GPT-2 as the metric.\"; 4.2 Results / Table 1 (page 6\u20137): \"Compared with the baseline, our method exhibits superior performance in terms of PPL, indicating more semantically meaningful and stealthy attacks being generated.\"; 3.5 Termination Criteria (page 5): \"If the algorithm has exhausted the maximum number of iterations or no keyword in a set list Lrefuse is detected in the LLM\u2019s top K words of its response, AutoDAN will terminate...\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's primary objective is to generate 'stealthy' and 'semantically meaningful' jailbreak prompts, which directly aligns with the concept of input plausibility. The proposed method, AutoDAN, enforces this plausibility throughout its generation process. Unlike methods that perform simple token-level mutations, AutoDAN employs a hierarchical genetic algorithm that operates on sentences and words. Crucially, its mutation operator uses an LLM to revise sentences, a mechanism designed to preserve logical flow and meaning, thus preventing the cumulative semantic drift common in iterative fuzzing. Furthermore, the paper provides explicit empirical evidence of this plausibility. It uses Sentence Perplexity (PPL), a standard quantitative measure for linguistic fluency, as a core evaluation metric for 'stealthiness'. The results demonstrate that AutoDAN's generated prompts have significantly lower (i.e., better) perplexity scores than token-based attack baselines (GCG) and are comparable to human-written prompts, confirming that the final fault-inducing inputs are realistic and semantically valid.",
                    "evidence": "Page 7, Section 4.1, Metrics: \"For stealthiness, we use standard Sentence Perplexity (PPL) evaluated by GPT-2 as the metric.\"\n\nPage 7, Section 4.2, Table 1 & Attack Effectiveness and Stealthiness: The results in Table 1 show AutoDAN-HGA achieving a PPL of 54.3820 on Llama2-7b-chat, vastly lower than the GCG baseline's 1027.5585. The text states: \"Compared with the baseline, our method exhibits superior performance in terms of PPL, indicating more semantically meaningful and stealthy attacks being generated.\"\n\nPage 6, Section 3.4.2, Paragraph-level: selection, crossover and mutation: \"We let the LLM-based diversification introduced in Alg. 5 to also serve as the mutation function due to its ability to preserve the global meaning and introduce diversity.\" Algorithm 5 on page 14 details this function, which instructs an LLM to revise a sentence, thus enforcing semantic plausibility during the mutation step."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "AutoDAN incorporates per-step mechanisms to preserve semantic plausibility\u2014initializing with human-crafted prompts, using an LLM to rewrite rather than randomly perturb text, employing a low mutation rate, and validating final prompts with sentence-level perplexity and a perplexity-based defense. However, the paper does not enforce or measure plausibility cumulatively across iterations (e.g., via intermediate PPL traces or explicit drift bounds), nor does it report any plausibility assessments for inputs generated mid-fuzzing. As a result, it constrains individual mutation steps and validates only the final outputs, matching the \"Medium\" level of the Input Plausibility metric.",
                    "evidence": "Section 3.2 (page 4): LLM-based diversification ensures each mutation preserves meaning; Section 3.4.2 (page 5): low mutation probability and LLM-based mutation function to maintain semantic coherence; Section 4.1 (page 6): final stealthiness evaluated via Sentence Perplexity (PPL); Section 4.2 Table 1 (pages 6\u20137): reports final PPL scores and perplexity-defense results\u2014no intermediate plausibility measures or cumulative drift analysis are provided."
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper's method operates directly on textual prompts (discrete, serialized text) rather than applying fine-grained numeric mutations to in-memory floating-point tensors. AutoDAN generates final jailbreak prompts as text strings (via sentence- and word-level crossover/mutation and LLM-based diversification) and evaluates success using textual oracles (keyword checks and an LLM re-check). Because inputs are produced as serialized text (generated prompts) and the oracle operates on model text outputs, the failures do not rely on fragile in-memory numerical perturbations that could be lost by standard I/O serialization (quantization/rounding). Thus, under the metric definition\u2014where approaches that generate inputs directly in serialized formats (e.g., generated text) are considered reproducible\u2014AutoDAN ensures reproducibility under standard I/O operations.",
                    "evidence": "Abstract (p.1): \"AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm.\"; Section 3.4.2 (p.5), paragraph describing hierarchical view: \"Our method views the jailbreak prompt as a combination of paragraph-level population, i.e., different combination of sentences, while these sentences, in turn, are formed by sentence-level population, for example, different words.\"; Section 4.2 (p.8): \"We conduct these evaluations by generating a jailbreak prompt for each malicious request in the dataset and testing the final responses from the victim LLM.\"; Appendix B, Alg.5 (p.13): \"Please revise the following sentence with no change to its length and only output the revised version, the sentence is: prompt\" (shows the mutation/generation operates on serialized text strings)."
                },
                "gemini": {
                    "value": "High",
                    "why": "The method, AutoDAN, generates jailbreak prompts, which are a form of text. Text is inherently a discrete and serialized format. Unlike continuous data such as images or audio, text does not suffer from information loss or alteration due to standard I/O operations like rounding, quantization, or clipping. The fault-inducing properties of the generated prompts are based on their lexical, semantic, and structural content, all of which are perfectly preserved when saved to and loaded from a file. The assessment metric explicitly states that approaches generating inputs 'directly in serialized formats (e.g., ... generated text)' are considered reproducible and rated as 'High' because they are 'inherently stable with respect to standard I/O operations'. The paper's method falls directly into this category by operating on and producing text-based prompts.",
                    "evidence": "Page 2, Section 2, Introduction: \"Based on the aforementioned insights, we propose AutoDAN, a hierarchical genetic algorithm tailored specifically for structured discrete data like prompt text.\"\n\nAnd from the metric definition itself: \"We also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., synthesized speech, rendered images, or generated text) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper's method operates directly on textual prompts (discrete, serialized text) rather than applying fine-grained numeric mutations to in-memory floating-point tensors. AutoDAN generates final jailbreak prompts as text strings (via sentence- and word-level crossover/mutation and LLM-based diversification) and evaluates success using textual oracles (keyword checks and an LLM re-check). Because inputs are produced as serialized text (generated prompts) and the oracle operates on model text outputs, the failures do not rely on fragile in-memory numerical perturbations that could be lost by standard I/O serialization (quantization/rounding). Thus, under the metric definition\u2014where approaches that generate inputs directly in serialized formats (e.g., generated text) are considered reproducible\u2014AutoDAN ensures reproducibility under standard I/O operations.",
                    "evidence": "Abstract (p.1): \"AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm.\"; Section 3.4.2 (p.5), paragraph describing hierarchical view: \"Our method views the jailbreak prompt as a combination of paragraph-level population, i.e., different combination of sentences, while these sentences, in turn, are formed by sentence-level population, for example, different words.\"; Section 4.2 (p.8): \"We conduct these evaluations by generating a jailbreak prompt for each malicious request in the dataset and testing the final responses from the victim LLM.\"; Appendix B, Alg.5 (p.13): \"Please revise the following sentence with no change to its length and only output the revised version, the sentence is: prompt\" (shows the mutation/generation operates on serialized text strings)."
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "High",
                    "why": "Mutation does not rely on guidance from the model and shows transferability"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper both (1) reuses fault-inducing prompts crafted on one model to test other models (explicit cross-model transfer experiments and reported ASR numbers), and (2) designs the fuzzing/optimization to produce semantically meaningful, lexical-level prompts (hierarchical GA, LLM-based diversification, momentum word scoring) which the authors argue and empirically show improves transfer to black-box models. Thus it demonstrates reuse across models and includes explicit mechanisms intended to discover transferable failures.",
                    "evidence": "Page 1, Abstract: \"Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability...\"; Page 8, Section 4.2 Results - Transferability: \"We conduct the evaluations by taking the jailbreak prompts with their corresponding requests and targeting another LLM. The results are shown in Tab. 2. AutoDAN exhibits a much better transferability in attacking the black-box LLMs compared with the baseline.\"; Page 8, Table 2 (Cross-model transferability): entries showing transfer ASR (examples) \u2014 \"Vicuna-7B -> Guanaco-7b: AutoDAN-HGA ASR 0.7058 Recheck 0.6712\" and \"Llama2-7b-chat -> Vicuna-7B: AutoDAN-HGA ASR 0.7288 Recheck 0.7019\"; Page 8, Table 2 caption / discussion: \"We hypothesize that this is because the AutoDAN generates prompts at a semantic level without relying on direct guidance from gradient information on the tokens, thereby avoiding overfitting on white-box models.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper explicitly evaluates the transferability of its generated jailbreak prompts. It conducts cross-model experiments where prompts generated on a source model are tested against other target models, and the results are presented in a dedicated table (Table 2). However, the design of the attack method, AutoDAN, does not incorporate explicit mechanisms aimed at discovering transferable attacks. The method uses a hierarchical genetic algorithm to generate semantically meaningful prompts. The authors speculate that the observed high transferability is a beneficial consequence of their semantic-level optimization approach, which avoids the overfitting common in gradient-based methods, rather than being an explicit design goal with specific supporting mechanisms.",
                    "evidence": "Page 8, Section 'Transferability': 'We further investigate the transferability of our method... We conduct the evaluations by taking the jailbreak prompts with their corresponding requests and targeting another LLM. The results are shown in Tab. 2. AutoDAN exhibits a much better transferability in attacking the black-box LLMs compared with the baseline. We speculate that the potential reason is the semantically meaningful jailbreak prompts may be inherently more transferable than the methods based on tokens\u2019 gradients.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "While the paper clearly evaluates whether fault-inducing prompts generated on one model transfer to others (via cross-model experiments in Section 4.2 and Table 2), the core AutoDAN design does not include mechanisms explicitly tailored for transferability (e.g. multi-model surrogate optimization or fitness functions that aggregate performance across targets). The authors observe that semantic-level optimization \u2018inherently\u2019 improves transfer, but they do not incorporate transferability as an explicit objective or design feature of their hierarchical genetic algorithm.",
                    "evidence": "Page 8, Section 4.2 (\u201cTransferability\u201d): \u201cWe conduct the evaluations by taking the jailbreak prompts with their corresponding requests and targeting another LLM. The results are shown in Tab. 2. AutoDAN exhibits a much better transferability \u2026 We speculate that the potential reason is the semantically meaningful jailbreak prompts may be inherently more transferable\u2026\u201d; Table 2 \u2013 cross-model ASR results; no mention of multi-model or transfer-focused optimization in Method (Sections 3.2\u20133.4)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Shows transferabilty but the fuzzing design is not explicit about trasferable attack discovery. It is a byproduct and the authors gives a hypothesis why they transfer",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Rule-based",
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "He et al. - 2024 - Curiosity-Driven Testing for Sequential Decision-Making Process": {
        "key": "he2024curiosity",
        "name": "CureFuzz",
        "year": "2024",
        "citation_count": "12",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Medium",
                    "why": "Uncovers crash-triggering scenarios (e.g., collisions, falls, or loss of control) across autonomous driving, aviation, and robotics systems. exposes behaviorally unsafe yet functionally valid failures \u2014 revealing unintended consequences of learned policies rather than breaches of safety enforcement mechanisms."
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper shows CureFuzz uncovers safety-critical, undesired behaviors (crash-triggering scenarios such as vehicle/aircraft collisions and robot falls) that violate user safety expectations, but it does not present any results or claims that it bypasses explicit safety, policy, or security mechanisms (e.g., adversarially-trained or safety-aligned models), nor does it report security-critical violations like data leakage, jailbreaks, or unauthorized actions. Therefore the failures discovered are high-impact safety violations in the sense of causing crashes, but they are not described as bypassing explicit defenses or security mechanisms, which matches the \"Medium\" category.",
                    "evidence": "Page 3, Section 3.1 Assumption: \"CureFuzz is designed to uncover crash-triggering scenarios that eventually lead to the crash of SDMs.\"; Page 6, Section 4.2.1 Autonomous Driving: \"In the CARLA simulator ... CureFuzz checks for the situations when the SDM-controlled vehicle experiences a collision with other vehicles or buildings.\"; Page 6, Section 4.2.2 Aircraft Collision Avoidance: \"In ACAS Xu, CureFuzz simply aims to find the scenarios when there are collisions between the SDM-controlled airplane with other airplanes.\"; Page 6-7, Section 4.2.3 Video Game / BipedalWalker: \"We aim to find the scenarios when the robot falls.\"; Page 10, RQ3 Results: \"For the repaired ACAS Xu, 55 faults and 4 distinct types of crash-triggering scenarios are detected. We find that the crashes found by CureFuzz are reduced by 73% compared with the original DNN model.\" (shows focus on crash detection and repair rather than bypassing defenses)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, CUREFUZZ, is designed to uncover 'crash-triggering scenarios' in sequential decision-makers (SDMs). These failures are explicitly defined as catastrophic and unsafe behaviors, such as an autonomous vehicle colliding with pedestrians or a robot falling. This directly aligns with the 'Medium' severity definition, which includes uncovering failures that violate safety expectations and produce unsafe behavior. The paper does not mention testing against models with explicit, separate safety or security mechanisms, nor does it claim to bypass such defenses. Instead, it finds unsafe behaviors inherent in the standard models' learned policies. Therefore, it does not meet the criteria for 'High' but is significantly more severe than 'Low,' which is limited to robustness errors like mispredictions.",
                    "evidence": "Page 5, Section 3.2, Approach Overview: \"CUREFUZZ is designed to uncover crash-triggering scenarios that eventually lead to the crash of SDMs. It is crucial to note that the definition of a 'crash' can vary across different environments. For instance, within the context of autonomous driving, a crash could be defined as an incident where an autonomous vehicle collides with pedestrians. In the context of robotics control, a crash can refer to the falling of a walking robot. We employ the term 'crash' to represent its broader conceptual meaning. Furthermore, our methodology focuses on catastrophic failure rather than minor deviations from optimal performance.\"\n\nPage 7, Section 4.2, Experiment Subject and Environment: \"In CARLA, CUREFUZZ checks for the situations when the SDM-controlled vehicle experiences a collision with other vehicles or buildings.\" and \"In ACAS XU, CUREFUZZ simply aims to find the scenarios when there are collisions between the SDM-controlled airplane with other airplanes.\" and for BipedalWalker, \"We aim to find the scenarios when the robot falls.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper shows CureFuzz uncovers safety-critical, undesired behaviors (crash-triggering scenarios such as vehicle/aircraft collisions and robot falls) that violate user safety expectations, but it does not present any results or claims that it bypasses explicit safety, policy, or security mechanisms (e.g., adversarially-trained or safety-aligned models), nor does it report security-critical violations like data leakage, jailbreaks, or unauthorized actions. Therefore the failures discovered are high-impact safety violations in the sense of causing crashes, but they are not described as bypassing explicit defenses or security mechanisms, which matches the \"Medium\" category.",
                    "evidence": "Page 3, Section 3.1 Assumption: \"CureFuzz is designed to uncover crash-triggering scenarios that eventually lead to the crash of SDMs.\"; Page 6, Section 4.2.1 Autonomous Driving: \"In the CARLA simulator ... CureFuzz checks for the situations when the SDM-controlled vehicle experiences a collision with other vehicles or buildings.\"; Page 6, Section 4.2.2 Aircraft Collision Avoidance: \"In ACAS Xu, CureFuzz simply aims to find the scenarios when there are collisions between the SDM-controlled airplane with other airplanes.\"; Page 6-7, Section 4.2.3 Video Game / BipedalWalker: \"We aim to find the scenarios when the robot falls.\"; Page 10, RQ3 Results: \"For the repaired ACAS Xu, 55 faults and 4 distinct types of crash-triggering scenarios are detected. We find that the crashes found by CureFuzz are reduced by 73% compared with the original DNN model.\" (shows focus on crash detection and repair rather than bypassing defenses)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "Steer toward safetly faults (crash-triggering scenarios) in in autonomous driving and other sequential decision-making systems"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "CureFuzz enforces plausibility constraints at each mutation step by restricting mutated seeds to the environment's legitimate state space and discarding illegal or initially crashing states (per-environment validity checks and simulator-based filtering). It also uses only \u2018small random perturbations\u2019 when mutating seeds. However, the paper does not describe mechanisms that explicitly prevent or measure cumulative drift across many fuzzing iterations nor report any human or quantitative perceptual/semantic plausibility assessments (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) for final fault-inducing inputs. The evaluation metrics focus on coverage and crash counts rather than input realism. This matches the metric\u2019s Medium definition: plausibility is enforced at individual mutation steps or by rule-based validity checks, but cumulative-effects and explicit plausibility validation of final fault-inducing inputs are not addressed.",
                    "evidence": "3.4 CureFuzz Architecture (Seed Mutation): \"The Seed-Mutation function generates a new mutated state by applying a small random perturbation to a selected state. ... We need to ensure that the mutated seed lies in the legitimate state spaces , and the mutated seed would not trigger an initial crash. In our experiments, we have carefully addressed this concern and verified the validity of each mutated seed, details are given in Section 4.\" (Section 3.4)\n\n4.2.1 Autonomous Driving (CARLA): \"We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.\" (Section 4.2.1)\n\n4.2.2 Aircraft Collision Avoidance (ACAS Xu): \"Given the predefined range of acceptable states, states that fall outside legal space are automatically discarded.\" (Section 4.2.2)\n\n4.2.3 Coop Navi (MARL) / BipedalWalker: \"With a clear definition of permissible positions and velocities, CureFuzz ignores any illegal states.\" and \"Since the valid ground types are known, illegal states can be easily detected and discarded.\" (Section 4.2.3 and 4.2.4)\n\n4.3 Implementation (mutation magnitude): \"When mutating a seed, the magnitude of the random perturbation is a critical factor that can impact the performance of the fuzzer. We re-use the code implementation from Pang et al. for generating mutations.\" (Section 4.3)\n\n5 Evaluation metrics (lack of perceptual plausibility measures): \"Our evaluation focuses on three key metrics: environmental state coverage, the total number of detected crash-triggering scenarios, and the distinct types of crash-triggering scenarios.\" (Section 5, RQ1 description)  -- no human studies or perceptual/semantic quality metrics (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) are reported."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility through rule-based constraints applied at each mutation step, which aligns with the 'Medium' rating. For each of the four experimental environments, the authors describe a specific mechanism to ensure that the mutated input state remains within a valid or 'legal' space, and they explicitly discard any states that violate these rules. For example, in the autonomous driving environment, the CARLA simulator is used to validate the mutated state, and in the aircraft collision avoidance system, states outside a predefined 'legal space' are discarded. However, the paper does not address the potential for cumulative drift, where a series of individually plausible mutations could lead to an overall unrealistic scenario. Furthermore, it lacks any empirical validation (such as human studies or quantitative metrics like LPIPS or SSIM) to confirm that the final crash-triggering scenarios are perceptually or semantically realistic.",
                    "evidence": "Page 7, Section 4.2.1 Autonomous Driving: 'We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.'\nPage 7, Section 4.2.2 Aircraft Collision Avoidance: 'Given the predefined range of acceptable states, states that fall outside legal space are automatically discarded.'\nPage 7, Section 4.2.3 Video Game: 'With a clear definition of permissible positions and velocities, CUREFUZZ ignores any illegal states.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "CureFuzz enforces plausibility constraints at each mutation step by restricting mutated seeds to the environment's legitimate state space and discarding illegal or initially crashing states (per-environment validity checks and simulator-based filtering). It also uses only \u2018small random perturbations\u2019 when mutating seeds. However, the paper does not describe mechanisms that explicitly prevent or measure cumulative drift across many fuzzing iterations nor report any human or quantitative perceptual/semantic plausibility assessments (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) for final fault-inducing inputs. The evaluation metrics focus on coverage and crash counts rather than input realism. This matches the metric\u2019s Medium definition: plausibility is enforced at individual mutation steps or by rule-based validity checks, but cumulative-effects and explicit plausibility validation of final fault-inducing inputs are not addressed.",
                    "evidence": "3.4 CureFuzz Architecture (Seed Mutation): \"The Seed-Mutation function generates a new mutated state by applying a small random perturbation to a selected state. ... We need to ensure that the mutated seed lies in the legitimate state spaces , and the mutated seed would not trigger an initial crash. In our experiments, we have carefully addressed this concern and verified the validity of each mutated seed, details are given in Section 4.\" (Section 3.4)\n\n4.2.1 Autonomous Driving (CARLA): \"We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.\" (Section 4.2.1)\n\n4.2.2 Aircraft Collision Avoidance (ACAS Xu): \"Given the predefined range of acceptable states, states that fall outside legal space are automatically discarded.\" (Section 4.2.2)\n\n4.2.3 Coop Navi (MARL) / BipedalWalker: \"With a clear definition of permissible positions and velocities, CureFuzz ignores any illegal states.\" and \"Since the valid ground types are known, illegal states can be easily detected and discarded.\" (Section 4.2.3 and 4.2.4)\n\n4.3 Implementation (mutation magnitude): \"When mutating a seed, the magnitude of the random perturbation is a critical factor that can impact the performance of the fuzzer. We re-use the code implementation from Pang et al. for generating mutations.\" (Section 4.3)\n\n5 Evaluation metrics (lack of perceptual plausibility measures): \"Our evaluation focuses on three key metrics: environmental state coverage, the total number of detected crash-triggering scenarios, and the distinct types of crash-triggering scenarios.\" (Section 5, RQ1 description)  -- no human studies or perceptual/semantic quality metrics (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) are reported."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Mutates continuous feature vectors representing simulator states (positions, angles, velocities) and enforces validity checks to keep them within legal bounds."
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper performs mutations on in-memory numeric environment states (positions, angles, speeds, ground types) and evaluates crashes by executing the mutated states in the simulator, but it does not describe any handling of standard I/O/serialization effects (rounding, quantization, clipping) nor constraining perturbations to survive serialization. For CARLA the SDM consumes RGB images, yet CureFuzz mutates underlying world-state floats (positions/angles) and uses the simulator to render and evaluate states without any explicit discussion of ensuring that the fault-inducing perturbations persist under serialization (e.g., 8-bit image quantization) or of generating inputs directly in serialized formats. Therefore failures discovered could be fragile numeric artifacts that would not persist after standard I/O operations, and the paper does not explicitly account for serialization effects during mutation or oracle evaluation.",
                    "evidence": "Page 6, Section 4.2.1 (Autonomous Driving): \"In the CARLA simulator, at each timestep, the SDM receives an RGB image and its current velocity as inputs.\" \n\nPage 6, Section 4.2.1 (Autonomous Driving): \"When CureFuzz mutates a given state, small perturbations are randomly generated and added to the positions and angles of these vehicles. We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.\" \n\nPage 5, Section 3.4 (Seed Mutation): \"Once a seed is selected from the corpus, the SeedMutation function generates a new mutated state by applying a small random perturbation to a selected state. The mutated state is fed into the EnvMonitor function to generate its corresponding state sequence and collect the cumulative reward. We need to ensure that the mutated seed lies in the legitimate state spaces, and the mutated seed would not trigger an initial crash. In our experiments, we have carefully addressed this concern and verified the validity of each mutated seed, details are given in Section 4.\" \n\nPage 7, Section 4.3 (Implementation): \"When mutating a seed, the magnitude of the random perturbation is a critical factor that can impact the performance of the fuzzer. We re-use the code implementation from Pang et al. for generating mutations.\" \n\n(Note: the paper contains no statements about handling serialization, quantization, rounding, clipping, or about generating inputs directly in serialized formats such as saved images/audio/text to ensure reproducibility under standard I/O.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper describes its mutation strategy as applying 'a small random perturbation to a selected state'. The validation of these mutated states focuses on ensuring they are legal within the specific domain of the simulation environment, such as preventing initial collisions in CARLA or staying within speed limits in ACAS Xu. The methodology does not mention or account for standard I/O operations like rounding, quantization, or clipping that would occur during serialization to a storage format. The checks performed are for domain-validity, not for I/O-reproducibility. Consequently, the small numerical perturbations introduced during fuzzing could be fragile and lost upon serialization, which means failures may not be reproducible. This approach aligns with the 'Low' rating, as it ignores I/O effects during both mutation and oracle evaluation.",
                    "evidence": "Page 6, Section 6, Seed Mutation: 'Once a seed is selected from the corpus, the Seed-Mutation function generates a new mutated state by applying a small random perturbation to a selected state. ... We need to ensure that the mutated seed lies in the legitimate state spaces, and the mutated seed would not trigger an initial crash.'\nPage 7, Section 4.2.1, Autonomous Driving: 'When CUREFUZZ mutates a given state, small perturbations are randomly generated and added to the positions and angles of these vehicles. We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.'\nPage 7, Section 4.2.2, Aircraft Collision Avoidance: 'Given the predefined range of acceptable states, states that fall outside legal space are automatically discarded.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper performs mutations on in-memory numeric environment states (positions, angles, speeds, ground types) and evaluates crashes by executing the mutated states in the simulator, but it does not describe any handling of standard I/O/serialization effects (rounding, quantization, clipping) nor constraining perturbations to survive serialization. For CARLA the SDM consumes RGB images, yet CureFuzz mutates underlying world-state floats (positions/angles) and uses the simulator to render and evaluate states without any explicit discussion of ensuring that the fault-inducing perturbations persist under serialization (e.g., 8-bit image quantization) or of generating inputs directly in serialized formats. Therefore failures discovered could be fragile numeric artifacts that would not persist after standard I/O operations, and the paper does not explicitly account for serialization effects during mutation or oracle evaluation.",
                    "evidence": "Page 6, Section 4.2.1 (Autonomous Driving): \"In the CARLA simulator, at each timestep, the SDM receives an RGB image and its current velocity as inputs.\" \n\nPage 6, Section 4.2.1 (Autonomous Driving): \"When CureFuzz mutates a given state, small perturbations are randomly generated and added to the positions and angles of these vehicles. We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.\" \n\nPage 5, Section 3.4 (Seed Mutation): \"Once a seed is selected from the corpus, the SeedMutation function generates a new mutated state by applying a small random perturbation to a selected state. The mutated state is fed into the EnvMonitor function to generate its corresponding state sequence and collect the cumulative reward. We need to ensure that the mutated seed lies in the legitimate state spaces, and the mutated seed would not trigger an initial crash. In our experiments, we have carefully addressed this concern and verified the validity of each mutated seed, details are given in Section 4.\" \n\nPage 7, Section 4.3 (Implementation): \"When mutating a seed, the magnitude of the random perturbation is a critical factor that can impact the performance of the fuzzer. We re-use the code implementation from Pang et al. for generating mutations.\" \n\n(Note: the paper contains no statements about handling serialization, quantization, rounding, clipping, or about generating inputs directly in serialized formats such as saved images/audio/text to ensure reproducibility under standard I/O.)"
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper contains no experiments or claims that fault-inducing inputs found on one model were reused to test other models for the same task, nor does CureFuzz include design elements aimed at producing transferable attacks across different model implementations. The evaluation applies CureFuzz separately to multiple SDMs and uses found scenarios to repair the same model (fine-tuning + re-test), but it does not measure whether seeds discovered on a surrogate transfer to unseen targets or test multiple implementations with the same seeds.",
                    "evidence": "Introduction (quote): \"We evaluate CureFuzz by applying it to well-known SDMs that use various learning algorithms. ... The experimental results demonstrate that CureFuzz effectively and efficiently identifies a significant number of catastrophic failures across all considered SDMs.\" (Introduction) \u2014 shows per-model evaluation but no claim of cross-model reuse.\n\nSection 5, RQ3 (quote): \"we utilize CureFuzz again to test the newly fine-tuned SDM. For the repaired ACAS Xu, 55 faults and 4 distinct types of crash-triggering scenarios are detected. We find that the crashes found by CureFuzz are reduced by 73% compared with the original DNN model.\" (Section 5, RQ3) \u2014 demonstrates using found scenarios to repair and retest the same model, not transfer to other models.\n\nSection 4.1 Research Questions (quote): \"To evaluate the performance of CureFuzz and comprehensively understand the impact, we formulate the three research questions: RQ1: How effective is CureFuzz in finding crash-triggering scenarios? RQ2: Can CureFuzz be effectively guided with the curiosity mechanism? RQ3: Can we use the crashes found by CureFuzz to improve SDMs?\" (Section 4.1) \u2014 the listed RQs do not include any question about transferability of fault-inducing inputs across different model implementations."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper introduces CUREFUZZ, a fuzzing method to find crash-triggering scenarios for sequential decision-makers (SDMs). The evaluation is conducted across multiple SDMs in various environments (e.g., two different models for the CARLA autonomous driving task, one based on DRL and one on IL). However, each SDM is treated as an independent and isolated target for the fuzzer. The experiments aim to demonstrate the effectiveness of the CUREFUZZ method on each target individually compared to baseline methods. The paper does not perform any experiments to test whether the crash-triggering scenarios discovered for one SDM would also cause failures (i.e., transfer) to another SDM performing the same task. The design of the fuzzer is focused on finding novel and diverse scenarios for the specific model under test, with no explicit mechanisms for discovering transferable failures.",
                    "evidence": "Page 8, Section 5 EXPERIMENT RESULTS, Tables 1, 2, and 3: The results for different models, even within the same environment, are presented in separate rows. For instance, 'Carla (RL)' and 'Carla (IL)' are evaluated independently. The paper discusses the performance of CUREFUZZ on each of these models but does not mention reusing the crash scenarios found for 'Carla (RL)' to test 'Carla (IL)' or vice-versa. The research questions also do not address transferability. For example, RQ1 asks: \"How effective is CUREFUZZ in finding crash-triggering scenarios?\" which is evaluated per model, not across models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper contains no experiments or claims that fault-inducing inputs found on one model were reused to test other models for the same task, nor does CureFuzz include design elements aimed at producing transferable attacks across different model implementations. The evaluation applies CureFuzz separately to multiple SDMs and uses found scenarios to repair the same model (fine-tuning + re-test), but it does not measure whether seeds discovered on a surrogate transfer to unseen targets or test multiple implementations with the same seeds.",
                    "evidence": "Introduction (quote): \"We evaluate CureFuzz by applying it to well-known SDMs that use various learning algorithms. ... The experimental results demonstrate that CureFuzz effectively and efficiently identifies a significant number of catastrophic failures across all considered SDMs.\" (Introduction) \u2014 shows per-model evaluation but no claim of cross-model reuse.\n\nSection 5, RQ3 (quote): \"we utilize CureFuzz again to test the newly fine-tuned SDM. For the repaired ACAS Xu, 55 faults and 4 distinct types of crash-triggering scenarios are detected. We find that the crashes found by CureFuzz are reduced by 73% compared with the original DNN model.\" (Section 5, RQ3) \u2014 demonstrates using found scenarios to repair and retest the same model, not transfer to other models.\n\nSection 4.1 Research Questions (quote): \"To evaluate the performance of CureFuzz and comprehensively understand the impact, we formulate the three research questions: RQ1: How effective is CureFuzz in finding crash-triggering scenarios? RQ2: Can CureFuzz be effectively guided with the curiosity mechanism? RQ3: Can we use the crashes found by CureFuzz to improve SDMs?\" (Section 4.1) \u2014 the listed RQs do not include any question about transferability of fault-inducing inputs across different model implementations."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ],
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "Zhou et al. - 2025 - Understanding the Effectiveness of Coverage Criteria for Large Language Models A Special Angle from": {
        "key": "zhou2025understanding",
        "name": "Zhou et al.",
        "year": "2025",
        "citation_count": "7",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly targets jailbreak attacks (high-impact security failures) and demonstrates that coverage-guided techniques both (1) generate successful jailbreak examples that bypass model alignment/safety behavior and (2) detect jailbreak queries in safety-aligned models. The authors report that attack queries can \"bypass\" safety neurons in well-aligned models and show coverage-guided generation produces substantially more successful jailbreaks than random; they also evaluate on an aligned model (Llama-2-7B-Chat) and report high success/detection results. These statements and empirical results indicate the method uncovers high-severity failures (jailbreaks that violate safety/policy mechanisms) rather than only low-level robustness errors.",
                    "evidence": "Page 730, Abstract (Application summary): \"we develop a real-time jailbreak detection mechanism that achieves high accuracy (93.61% on average) ... Lastly, we introduce a coverage-guided approach for generating jailbreak attack examples, enabling systematic refinement of prompts to uncover vulnerabilities.\"; Page 736, IV.B / RQ1 (attack bypass observation): \"...we observe significant coverage differences between normal (SN) and abnormal outputs (SNJ) in well-aligned models. Well-aligned models have safety neurons to reject malicious queries [46], but attack queries often bypass these and activate neurons linked to abnormal behaviors.\"; Page 739, V.C (generation results): \"As shown in Figure 6, the coverage-guided method outperforms the random strategy in generating successful jailbreak queries... These results confirm that coverage gains effectively guide query generation toward unexplored areas, increasing the number of successful jailbreak cases and enhancing robustness testing for LLMs.\"; Page 738\u2013739, V.A / Table V (detection on aligned model): \"As shown in Table V, our method achieves high average accuracies: 95.23% on OPT-125M, 93.92% on Llama-2-7B-Chat, and 91.69% on Pythia-12B, demonstrating its effectiveness across models.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's primary focus is on jailbreak attacks, which are explicitly defined as attempts to bypass the safety and alignment mechanisms of Large Language Models (LLMs) to generate harmful or disallowed content. This directly corresponds to the 'High' severity metric, as it involves uncovering high-impact failures by bypassing explicit safety mechanisms in safety-aligned models. The entire study, including the experimental design, evaluation standards, and proposed applications (Real-time Jailbreak Detection, Test Case Prioritization, and Jailbreak Case Generation), is centered on understanding, detecting, and generating these security-critical violations in models that are designed to prevent them.",
                    "evidence": "Page 1, Abstract: 'In this paper, we conduct a comprehensive empirical study to evaluate the effectiveness of traditional coverage criteria in identifying such inadequacies, exemplified by the significant security concern of jailbreak attacks.'\n\nPage 3, Section II-D, Jailbreak Attack and Defenses: 'Despite these efforts, jailbreak attacks have emerged that bypass alignment measures, causing LLMs to generate harmful outputs and compromising their security [8]\u2013[12].'\n\nPage 5, Section IV-A, Setup: The paper explicitly uses safety-aligned models for evaluation. 'These models include OPT-125M [42], Llama-2-7B-Chat [38], Pythia-12B [43], and Gemma-2-27B-it [44].'\n\nPage 9, Section V, Application: The proposed applications aim to address these high-severity failures directly. 'First, we propose Real-time Jailbreak Detection, using activation features from coverage criteria to classify queries as normal or jailbreak... Third, we investigate Jailbreak Case Generation, where coverage-guided methods refine prompts to generate adversarial examples.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly targets jailbreak attacks (high-impact security failures) and demonstrates that coverage-guided techniques both (1) generate successful jailbreak examples that bypass model alignment/safety behavior and (2) detect jailbreak queries in safety-aligned models. The authors report that attack queries can \"bypass\" safety neurons in well-aligned models and show coverage-guided generation produces substantially more successful jailbreaks than random; they also evaluate on an aligned model (Llama-2-7B-Chat) and report high success/detection results. These statements and empirical results indicate the method uncovers high-severity failures (jailbreaks that violate safety/policy mechanisms) rather than only low-level robustness errors.",
                    "evidence": "Page 730, Abstract (Application summary): \"we develop a real-time jailbreak detection mechanism that achieves high accuracy (93.61% on average) ... Lastly, we introduce a coverage-guided approach for generating jailbreak attack examples, enabling systematic refinement of prompts to uncover vulnerabilities.\"; Page 736, IV.B / RQ1 (attack bypass observation): \"...we observe significant coverage differences between normal (SN) and abnormal outputs (SNJ) in well-aligned models. Well-aligned models have safety neurons to reject malicious queries [46], but attack queries often bypass these and activate neurons linked to abnormal behaviors.\"; Page 739, V.C (generation results): \"As shown in Figure 6, the coverage-guided method outperforms the random strategy in generating successful jailbreak queries... These results confirm that coverage gains effectively guide query generation toward unexplored areas, increasing the number of successful jailbreak cases and enhancing robustness testing for LLMs.\"; Page 738\u2013739, V.A / Table V (detection on aligned model): \"As shown in Table V, our method achieves high average accuracies: 95.23% on OPT-125M, 93.92% on Llama-2-7B-Chat, and 91.69% on Pythia-12B, demonstrating its effectiveness across models.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "GPT-4 generates  new jailbreak queries through prompt rewriting. The generated contents are supposed to be natural but there was no measure reported how natural are those. The attack suffix is related to jailbreak detection, not creation"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper\u2019s coverage-guided generation and mutation processes do not enforce or measure input plausibility across iterations, nor do the authors report any human evaluation or task-specific perceptual/linguistic metrics (e.g., Perplexity used as a plausibility check) for the final fault-inducing inputs. Generated attack queries are produced via LLM rewriting (GPT-4) or by appending adversarial suffixes (GCG), and selection is based solely on coverage increase, with no mechanism described to constrain cumulative drift or to filter for semantic/naturalness. While the authors use perplexity-based methods as baselines for jailbreak detection, they do not use such measures to ensure that fuzzed/generated inputs remain realistic; likewise, no human study or quantitative plausibility metrics (e.g., Perplexity reported for final examples, LPIPS/SSIM/PESQ/STOI) are reported. Therefore the work does not enforce or assess input plausibility as required by the metric.",
                    "evidence": "III-A Motivation (page 732): \"Attack queries are generated by appending adversarial suffixes to rejected queries using GCG [7]. These queries aim to trigger the model to output malicious content (i.e., abnormal behaviors).\" \n\nV.C Jailbreak Case Generation (page 739): \"Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting. The query with the highest coverage increase is selected as the next seed in the coverage-guided approach.\" \n\nV.A Baseline (page 738-739): \"Baseline: We select the most widely used perplexity filter [33], the state-of-the-art method PARDEN [35], and self-reminder [49] as baselines for comparison.\" (shows perplexity is used as a baseline detector, not as an enforced plausibility constraint for generated inputs)\n\nEvaluation / General (pages 736-740, various): No sections or reported results describe human evaluation or quantitative perceptual/linguistic measures (e.g., Perplexity reported for generated fault-inducing prompts or other task-specific plausibility scores); coverage-guided selection is driven by coverage increase alone."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a coverage-guided method for generating jailbreak attack prompts. The generation process uses GPT-4 for 'prompt rewriting,' and the selection of subsequent prompts is guided entirely by maximizing 'coverage increase.' The evaluation of the generated prompts is based on their success rate in triggering a jailbreak, not on their plausibility or realism as user queries. The paper does not describe any mechanism to enforce input plausibility during the generation process, nor does it report any assessment of the plausibility of the final, successful jailbreak prompts, either through human evaluation or quantitative metrics like perplexity for linguistic fluency. The focus is entirely on the efficacy of the generated attacks, without consideration for whether the inputs are realistic within a normal threat model.",
                    "evidence": "Page 11, Section V-C, Jailbreak Case Generation: 'Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting. The query with the highest coverage increase is selected as the next seed in the coverage-guided approach.' and 'Evaluation results: As shown in Figure 6, the coverage-guided method outperforms the random strategy in generating successful jailbreak queries.' This demonstrates that the generation and selection process is driven by coverage and success rate, with no mention of plausibility constraints or validation."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper\u2019s coverage-guided generation and mutation processes do not enforce or measure input plausibility across iterations, nor do the authors report any human evaluation or task-specific perceptual/linguistic metrics (e.g., Perplexity used as a plausibility check) for the final fault-inducing inputs. Generated attack queries are produced via LLM rewriting (GPT-4) or by appending adversarial suffixes (GCG), and selection is based solely on coverage increase, with no mechanism described to constrain cumulative drift or to filter for semantic/naturalness. While the authors use perplexity-based methods as baselines for jailbreak detection, they do not use such measures to ensure that fuzzed/generated inputs remain realistic; likewise, no human study or quantitative plausibility metrics (e.g., Perplexity reported for final examples, LPIPS/SSIM/PESQ/STOI) are reported. Therefore the work does not enforce or assess input plausibility as required by the metric.",
                    "evidence": "III-A Motivation (page 732): \"Attack queries are generated by appending adversarial suffixes to rejected queries using GCG [7]. These queries aim to trigger the model to output malicious content (i.e., abnormal behaviors).\" \n\nV.C Jailbreak Case Generation (page 739): \"Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting. The query with the highest coverage increase is selected as the next seed in the coverage-guided approach.\" \n\nV.A Baseline (page 738-739): \"Baseline: We select the most widely used perplexity filter [33], the state-of-the-art method PARDEN [35], and self-reminder [49] as baselines for comparison.\" (shows perplexity is used as a baseline detector, not as an enforced plausibility constraint for generated inputs)\n\nEvaluation / General (pages 736-740, various): No sections or reported results describe human evaluation or quantitative perceptual/linguistic measures (e.g., Perplexity reported for generated fault-inducing prompts or other task-specific plausibility scores); coverage-guided selection is driven by coverage increase alone."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper studies coverage criteria and jailbreak detection for LLMs using textual prompts, tokenization, hidden-state inspection, and prompt-generation (paraphrasing/attack-suffix) techniques. It does not discuss applying fine-grained numeric mutations to floating-point input tensors nor does it account for standard I/O serialization effects (clipping, quantization, rounding) during test generation or oracle evaluation. Because the design and evaluation focus on discrete text prompts and internal activations without any explicit handling or constraints to ensure persistence of fault-inducing perturbations under serialization, the work ignores I/O effects relevant to the Failure Reproducibility metric.",
                    "evidence": "Page 731, Section II-A (Model Inference Process): \"The text is tokenized into discrete elements, each mapped to a dense embedding vector representing token semantics for further processing by the model.\" \n\nPage 732, Section III-A (Motivation: A Cluster Analysis Experiment): \"We extract the hidden states h from the 4th, 9th, 16th, and 31st transformer blocks for these types of queries and conduct k-means clustering [40].\" \n\nPage 734, Section IV-A(2) (Test Suite Construction / Synonymous Query Dataset): \"To construct synonymous queries, we use GPT-4 to generate corresponding synonymous paraphrases for the first 500 queries from the Alpaca-gpt4 dataset...\" \n\nPage 732, Section III-A (Attack query generation): \"Attack queries are generated by appending adversarial suffixes to rejected queries using GCG [7].\" \n\n(Throughout the paper there is no mention of handling serialization, quantization, clipping, or other I/O effects when generating inputs or when evaluating whether an input induces a failure.)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's methodology for generating and selecting test cases for Large Language Models (LLMs) operates entirely within the text domain. The inputs are prompts sourced from datasets, created by paraphrasing, or generated by appending suffixes or rewriting existing prompts. Text is an inherently serialized format, and the perturbations (the specific words and structures that constitute a 'jailbreak attack') are symbolic, not numerical. These inputs are not susceptible to the kind of information loss (e.g., from quantization, rounding, or clipping) that the metric describes for image or audio data when saved to disk. The metric definition explicitly states: 'We also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., ... generated text) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.' The paper's approach of generating and using text prompts perfectly aligns with this criterion, ensuring that the fault-inducing inputs are fully reproducible under any standard I/O operation.",
                    "evidence": "Page 4, Section III-A, 'Findings': 'Attack queries are generated by appending adversarial suffixes to rejected queries using GCG [7]. These queries aim to trigger the model to output malicious content (i.e., abnormal behaviors).' This describes a text-based generation method. \nPage 6, Section IV-A, 'Test Suite Construction': 'To construct synonymous queries, we use GPT-4 to generate corresponding synonymous paraphrases for the first 500 queries from the Alpaca-gpt4 dataset...'\nPage 11, Section V-C, 'Jailbreak Case Generation': 'Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting.' All these methods generate test cases directly in their final, serialized text format."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "All of the paper\u2019s test generation and evaluation is performed in the textual domain: inputs are discrete text prompts that are paraphrased, attacked via suffixes, or rewritten by GPT-4. Text is inherently a serialized format not subject to quantization, clipping or rounding losses. According to the Failure Reproducibility metric, fuzzing approaches that generate inputs directly in serialized formats (including text) are deemed fully reproducible under standard I/O operations. Therefore, the faults induced by these adversarial text prompts will persist unchanged through any standard serialization/deserialization pipeline.",
                    "evidence": "Page 731, Section III-A: \u201cAttack queries are generated by appending adversarial suffixes to rejected queries using GCG [7].\u201d\nPage 734, Section IV-A(2): \u201cTo construct synonymous queries, we use GPT-4 to generate corresponding synonymous paraphrases for the first 500 queries from the Alpaca-gpt4 dataset\u2026\u201d\nPage 738, Section V-C: \u201cOver five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting.\u201d\n(At no point does the paper apply fine-grained floating-point mutations or consider clipping, rounding, or quantization\u2014inputs remain plain text.)"
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "No analysis of fault transferability in input level of mutation level"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether fault-inducing inputs (jailbreak prompts) crafted on one model transfer to other models. The generation experiments and the coverage-guided jailbreak-case generation are conducted targeting a single model (Llama-2-7b-chat). Detection and prioritization experiments are performed per-model (training/validating classifiers or thresholds for each model), but the paper does not report experiments where inputs crafted on a surrogate model are reused to test different target models, nor does it describe mechanisms explicitly designed to discover transferable attacks across models.",
                    "evidence": "Section V.C (Jailbreak Case Generation), p. 739: \"Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round ... Each method ultimately generates 250 new jailbreak queries to evaluate effectiveness.\"; Section V.A (Jailbreak Detection), p. 738: \"... we train an MLP model that uses the number of activated neurons as input ... We validate its effectiveness on OPT-125M, Llama-2-7B-Chat, and Pythia-12B.\"; Section III.A (Motivation: A Cluster Analysis Experiment), p. 732: \"Experimental Setup: We first introduce the setup for our cluster analysis experiments using the Llama-2-7b-chat [38] model as our target.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes and evaluates methods for jailbreak detection, prioritization, and generation using coverage criteria. The experiments are conducted across four different LLMs. However, the evaluation of fault-inducing inputs (jailbreak attacks) is performed on a per-model basis. Specifically, in the 'Jailbreak Case Generation' application, new jailbreak queries are generated using coverage guidance from a single target model (Llama-2-7b-chat). The paper evaluates the success of this generation method for that specific model but does not subsequently test whether these generated queries are effective (i.e., transferable) in causing failures in the other models studied in the paper (OPT-125M, Pythia-12B, Gemma-2-27B-it). The research does not demonstrate or test whether fault-inducing inputs discovered on one model trigger failures in others.",
                    "evidence": "Page 11, Section V. APPLICATION, C. Jailbreak Case Generation: \"Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting. The query with the highest coverage increase is selected as the next seed in the coverage-guided approach. For comparison, a random strategy selects seeds randomly from rewritten candidates. Each method ultimately generates 250 new jailbreak queries to evaluate effectiveness.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether fault-inducing inputs (jailbreak prompts) crafted on one model transfer to other models. The generation experiments and the coverage-guided jailbreak-case generation are conducted targeting a single model (Llama-2-7b-chat). Detection and prioritization experiments are performed per-model (training/validating classifiers or thresholds for each model), but the paper does not report experiments where inputs crafted on a surrogate model are reused to test different target models, nor does it describe mechanisms explicitly designed to discover transferable attacks across models.",
                    "evidence": "Section V.C (Jailbreak Case Generation), p. 739: \"Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round ... Each method ultimately generates 250 new jailbreak queries to evaluate effectiveness.\"; Section V.A (Jailbreak Detection), p. 738: \"... we train an MLP model that uses the number of activated neurons as input ... We validate its effectiveness on OPT-125M, Llama-2-7B-Chat, and Pythia-12B.\"; Section III.A (Motivation: A Cluster Analysis Experiment), p. 732: \"Experimental Setup: We first introduce the setup for our cluster analysis experiments using the Llama-2-7b-chat [38] model as our target.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Provides correlation with internal coverage"
                }
            }
        },
        "taxonomy": {
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "Gong et al. - PaPillon Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs": {
        "key": "gong2025papillon",
        "name": "PAPILLON",
        "year": "2025",
        "citation_count": "0",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper proposes an automated black-box jailbreaking framework whose explicit goal is to bypass aligned models' safety mechanisms and elicit harmful outputs. The authors evaluate PAPILLON against safety-aligned/proprietary models (e.g., GPT-3.5, GPT-4, Gemini-Pro) and test its resiliency against specific defenses (perplexity filters, SmoothLLM, Llama Guard, and hybrid defenses), reporting high attack success rates even when defenses are enabled. This directly matches the metric's definition of high-severity failures: bypassing explicit safety/policy mechanisms to induce jailbreaks and unsafe outputs.",
                    "evidence": "Page 2401, Abstract: \"we introduce a novel jailbreaking attack framework called PAPILLON, which is an automated, black-box jailbreaking attack framework...\"; Page 2406, Section 3 Threat Model: \"The target LLM is assumed to be an aligned model, fine-tuned with instruction tuning or reinforcement learning from human feedback (RLHF), which typically refuses to respond to unethical or harmful questions.\"; Page 2406, Section 3 Threat Model: \"The objective of the attack is to craft jailbreak prompts ... 1) To circumvent the safety mechanisms of the target LLM, ensuring that it responds to a harmful query instead of opting not to answer.\"; Page 2412, Section 6 Resiliency against Defenses and Table 6 (results): show PAPILLON retains high ASR against defenses (e.g., \"GPT-4: PAPILLON 80% \u2192 After Llama Guard 64%\" in Table 6); Page 2412, Table 7 (hybrid defenses): show PAPILLON maintains high ASR under combined defenses (e.g., \"GPT-4: PAPILLON 80% \u2192 Perplexity + SmoothLLM 80%\")."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper introduces PAPILLON, a fuzz testing framework explicitly designed to generate jailbreak prompts that bypass the safety and policy mechanisms of large language models (LLMs). The goal is to induce the models to produce harmful or offensive content, which constitutes a high-impact security failure. The evaluation is specifically conducted on safety-aligned models (e.g., GPT-4, Llama-2-chat) and explicitly tests the method's resilience against several state-of-the-art defense mechanisms, including Perplexity Filters, SmoothLLM, and Llama Guard. The paper demonstrates that PAPILLON achieves a high attack success rate even when these explicit safety safeguards are in place, directly meeting the criteria for the 'High' severity rating.",
                    "evidence": "Page 14, Section 6.3 Llama Guard: \"Llama Guard [17] is a safeguard model based on the Llama-2-7B architecture, designed to classify prompts and outputs in AI-human interactions as \u201csafe\u201d or \u201cunsafe\u201d. Unsafe prompts are filtered to defend against jailbreak attempts. In our study, we targeted LLMs equipped with Llama Guard, with results presented in Table 6. The experimental findings reveal that we can achieve a high attack success rate even under Llama Guard\u2019s defense.\"\n\nPage 14, Section 6.4 Hybrid Defense: \"In addition to single defenses, we also consider two combinations of multiple defenses, referred to as hybrid defenses... Hybrid defenses generally provide stronger protection than single defenses. However, PAPILLON still achieves a high attack success rate in such cases.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper proposes an automated black-box jailbreaking framework whose explicit goal is to bypass aligned models' safety mechanisms and elicit harmful outputs. The authors evaluate PAPILLON against safety-aligned/proprietary models (e.g., GPT-3.5, GPT-4, Gemini-Pro) and test its resiliency against specific defenses (perplexity filters, SmoothLLM, Llama Guard, and hybrid defenses), reporting high attack success rates even when defenses are enabled. This directly matches the metric's definition of high-severity failures: bypassing explicit safety/policy mechanisms to induce jailbreaks and unsafe outputs.",
                    "evidence": "Page 2401, Abstract: \"we introduce a novel jailbreaking attack framework called PAPILLON, which is an automated, black-box jailbreaking attack framework...\"; Page 2406, Section 3 Threat Model: \"The target LLM is assumed to be an aligned model, fine-tuned with instruction tuning or reinforcement learning from human feedback (RLHF), which typically refuses to respond to unethical or harmful questions.\"; Page 2406, Section 3 Threat Model: \"The objective of the attack is to craft jailbreak prompts ... 1) To circumvent the safety mechanisms of the target LLM, ensuring that it responds to a harmful query instead of opting not to answer.\"; Page 2412, Section 6 Resiliency against Defenses and Table 6 (results): show PAPILLON retains high ASR against defenses (e.g., \"GPT-4: PAPILLON 80% \u2192 After Llama Guard 64%\" in Table 6); Page 2412, Table 7 (hybrid defenses): show PAPILLON maintains high ASR under combined defenses (e.g., \"GPT-4: PAPILLON 80% \u2192 Perplexity + SmoothLLM 80%\")."
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "Optimizes toward semantic policy-violation behavior (jailbreak) but not a specific output."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Reports only observable failures but no investigation of root-cause"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly designs mutation operators and optimization constraints to preserve semantic coherence and limit token-length during every mutation step (using an LLM helper and length hyperparameters), and it provides empirical evidence that final fault-inducing prompts remain realistic: quantitative perplexity measurements (low average perplexity) and a human evaluation (interpretability/naturalness scores and alignment checks) demonstrating that generated prompts are natural and align with human judgments. These elements show both (1) mechanisms that enforce plausibility during fuzzing and (2) empirical validation that the final inputs are perceptually/semantically realistic.",
                    "evidence": "Section 4.2 (Template Mutation and Optimization), p.2408: \"In this paper, we propose using GPT-driven mutation operators to create jailbreaking templates, allowing us to preserve the semantic integrity of the generated templates throughout the mutation process.\"; Section 4.2 (Optimization schemes), p.2408\u20132409: \"Ensuring that the generated adversarial prompts are readable (low perplexity) is crucial... Experiments show that the average perplexity of our generated jailbreak prompts for GPT-3.5 Turbo is as low as 34.61, significantly below the average perplexity threshold of 58.83.\"; Section 4.2 (Optimization schemes), p.2409: \"By customizing the length of these templates (with a default setting of 200 tokens) during the mutation process, we can maintain a high success rate even with shorter prompts when setting system instructions.\"; Section 7 (Human Evaluation), p.2412\u20132413: \"A total of 30 volunteers participated... For interpretability... Naturalness... The results are summarized in Table 9. We observed that our generated jailbreak prompts match or surpass manually crafted ones in interpretability and naturalness.\"; Section 7 (Human Evaluation), p.2412: \"We observed that for a score of 8 or higher, all outputs were consistently aligned with human evaluation, achieving a 100% alignment rate. This indicates that a score of 8 serves as an appropriate threshold...\"."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, PAPILLON, is explicitly designed to maintain the plausibility and semantic coherence of its generated inputs (jailbreak prompts) throughout the fuzzing process. The mutation strategies leverage an LLM helper specifically to generate natural and fluent prompts, directly addressing the limitations of prior work that produced 'gibberish' or 'unintelligible' outputs. The paper provides strong empirical evidence for the plausibility of the final, fault-inducing prompts through two distinct mechanisms: 1) a quantitative assessment using perplexity, a standard measure for linguistic fluency, demonstrating that their prompts have low perplexity scores, and 2) a dedicated human evaluation study that measures the 'interpretability' and 'naturalness' of the generated prompts, finding them to be superior to manually crafted ones.",
                    "evidence": "Page 10, Section 4.2, 'Maintain semantic coherence': 'Ensuring that the generated adversarial prompts are readable (low perplexity) is crucial for evading existing jailbreak defenses based on perplexity filters. In this paper, we propose using GPT-driven mutation operators to create jailbreaking templates, allowing us to preserve the semantic integrity of the generated templates throughout the mutation process. Experiments show that the average perplexity of our generated jailbreak prompts for GPT-3.5 Turbo is as low as 34.61, significantly below the average perplexity threshold of 58.83.'\n\nPage 14, Section 7, 'Human Evaluation': 'Prompts quality. To evaluate the interpretability and naturalness of the generated prompts, we developed two criteria for quantifiable human evaluation. For interpretability, we measure how easily humans can understand the mechanism behind the jailbreak prompt... Naturalness is assessed based on the readability of the prompt... We observed that our generated jailbreak prompts match or surpass manually crafted ones in interpretability and naturalness.' Table 9 on page 15 provides the quantitative results of this human study, showing PAPILLON's prompts achieved higher scores for both interpretability (4.64 vs 3.98) and naturalness (4.60 vs 3.66) compared to a baseline of manually crafted prompts."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly designs mutation operators and optimization constraints to preserve semantic coherence and limit token-length during every mutation step (using an LLM helper and length hyperparameters), and it provides empirical evidence that final fault-inducing prompts remain realistic: quantitative perplexity measurements (low average perplexity) and a human evaluation (interpretability/naturalness scores and alignment checks) demonstrating that generated prompts are natural and align with human judgments. These elements show both (1) mechanisms that enforce plausibility during fuzzing and (2) empirical validation that the final inputs are perceptually/semantically realistic.",
                    "evidence": "Section 4.2 (Template Mutation and Optimization), p.2408: \"In this paper, we propose using GPT-driven mutation operators to create jailbreaking templates, allowing us to preserve the semantic integrity of the generated templates throughout the mutation process.\"; Section 4.2 (Optimization schemes), p.2408\u20132409: \"Ensuring that the generated adversarial prompts are readable (low perplexity) is crucial... Experiments show that the average perplexity of our generated jailbreak prompts for GPT-3.5 Turbo is as low as 34.61, significantly below the average perplexity threshold of 58.83.\"; Section 4.2 (Optimization schemes), p.2409: \"By customizing the length of these templates (with a default setting of 200 tokens) during the mutation process, we can maintain a high success rate even with shorter prompts when setting system instructions.\"; Section 7 (Human Evaluation), p.2412\u20132413: \"A total of 30 volunteers participated... For interpretability... Naturalness... The results are summarized in Table 9. We observed that our generated jailbreak prompts match or surpass manually crafted ones in interpretability and naturalness.\"; Section 7 (Human Evaluation), p.2412: \"We observed that for a score of 8 or higher, all outputs were consistently aligned with human evaluation, achieving a 100% alignment rate. This indicates that a score of 8 serves as an appropriate threshold...\"."
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Mutate in text level"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "PAPILLON generates final test inputs as textual jailbreak prompts (serialized text) produced by its mutators and sent directly to target LLMs; textual prompts are inherently stable to typical numeric I/O effects (quantization/rounding) described in the metric. The paper explicitly defines the 'jailbreak prompt' as the final input string, describes using an LLM mutator to produce these templates (controlling token length via max_new_tokens), and treats those generated texts as the inputs used for both execution and judging. Because PAPILLON produces inputs directly in serialized text form (and controls prompt/token length), the fault-inducing inputs it discovers are reproducible under standard I/O operations per the metric\u2019s allowance for serialized-format inputs.",
                    "evidence": "Page 2407, Section 4.1: \"10The jailbreak prompt is the final input to the LLM, created by replacing the placeholders (denotes as [INSERT PROMPT HERE]) in the jailbreaking template with the harmful question.\"; Page 2408, Section 4.2: \"In PAPILLON, we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.\"; Page 2410, Section 5.1 (Experimental Setup): \"The length of the generated prompts is controlled using the max_new_tokens hyperparameter. Specifically, for the \u2018role-play' and \u2018contextualization' mutations, this value is set to 200 tokens, while an additional 100 tokens are allocated for the \u2018expand' mutation.\"; Page 2411, Section 5.3 (Attack Transferability - MLLM transferability): \"successful jailbreaking templates are printed on white-background images and used as image inputs...\" (shows the authors generate serialized text prompts as the primary inputs; when converted to images for MLLMs they do not claim special handling of image quantization, indicating the core method relies on serialized text)."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper proposes PAPILLON, a fuzzing framework where the inputs are text prompts for Large Language Models (LLMs). The mutation process uses a helper LLM to generate new jailbreaking prompts (text templates) based on strategies like 'Role-play', 'Contextualization', and 'Expand'. The metric definition explicitly states: 'We also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., synthesized speech, rendered images, or generated text) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.' Since PAPILLON's mutation operators generate text prompts directly in their final, serialized format, the resulting test cases are not susceptible to alteration or loss of the fault-inducing perturbation through standard I/O operations like rounding or quantization, which typically affect numerical data. The generated text inputs are inherently reproducible, thus meeting the criteria for a 'High' rating.",
                    "evidence": "Page 9, Section 4.2, Template Mutation and Optimization: 'PAPILLON uses a LLM (e.g., GPT-3.5 turbo), as a mutator to automatically create jailbreaking templates for specific questions.' This is further detailed with descriptions of the mutators: 'Role-play. It involves assigning the LLM a virtual role, such as a writer, screenwriter, gang member, or expert. The LLM is guided by the target question to generate a scenario where the question can be addressed within the scene.' This shows that the mutation process directly yields text, which is an inherently serialized format, ensuring failure reproducibility."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "PAPILLON generates final test inputs as textual jailbreak prompts (serialized text) produced by its mutators and sent directly to target LLMs; textual prompts are inherently stable to typical numeric I/O effects (quantization/rounding) described in the metric. The paper explicitly defines the 'jailbreak prompt' as the final input string, describes using an LLM mutator to produce these templates (controlling token length via max_new_tokens), and treats those generated texts as the inputs used for both execution and judging. Because PAPILLON produces inputs directly in serialized text form (and controls prompt/token length), the fault-inducing inputs it discovers are reproducible under standard I/O operations per the metric\u2019s allowance for serialized-format inputs.",
                    "evidence": "Page 2407, Section 4.1: \"10The jailbreak prompt is the final input to the LLM, created by replacing the placeholders (denotes as [INSERT PROMPT HERE]) in the jailbreaking template with the harmful question.\"; Page 2408, Section 4.2: \"In PAPILLON, we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.\"; Page 2410, Section 5.1 (Experimental Setup): \"The length of the generated prompts is controlled using the max_new_tokens hyperparameter. Specifically, for the \u2018role-play' and \u2018contextualization' mutations, this value is set to 200 tokens, while an additional 100 tokens are allocated for the \u2018expand' mutation.\"; Page 2411, Section 5.3 (Attack Transferability - MLLM transferability): \"successful jailbreaking templates are printed on white-background images and used as image inputs...\" (shows the authors generate serialized text prompts as the primary inputs; when converted to images for MLLMs they do not claim special handling of image quantization, indicating the core method relies on serialized text)."
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Medium",
                    "why": "Performs experiments on transferability"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly evaluates and demonstrates that fault-inducing prompts generated by PAPILLON on one model can be reused to successfully attack other models (cross-model and multimodal transfer experiments are reported). However, the PAPILLON design and mutation operators are described with goals of semantic coherence, short prompt length, and seed-pool/fuzzing effectiveness rather than with explicit mechanisms specifically intended to discover or maximize transferability across different model implementations (no surrogate-model training, ensemble surrogate testing, or transfer-focused mutators are described). Therefore the work shows reuse/testing of fault-inducing inputs for transferability but does not present an explicit design aimed at producing transferable attacks.",
                    "evidence": "Page 2401, Abstract: \"Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses.\"; Page 2412, Section 5.4 (Attack Transferability): \"We further investigate the transferability of PAPILLON. Transferability refers to the extent to which prompts designed to jailbreak the target model can successfully jailbreak another model.\" and \"Overall, PAPILLON demonstrates notable attack transferability across different models.\"; Page 2420, Table 17 (Cross-model transferability): shows the cross-model success-rate matrix (e.g., \"Baichuan2-7B -> Vicuna-7B-v1.3: 70%\", many non-zero cross-model ASR entries); Page 2407, Section 4.2 (Template Mutation and Optimization): \"we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.\" (describes focus on semantic coherence and prompt length rather than explicit transferability mechanisms)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper extensively evaluates the transferability of its generated jailbreak prompts. It includes a specific section (5.4 Attack Transferability) and tables (Table 5 and Table 17) demonstrating that prompts created for a source model can successfully trigger failures in other target models, including different LLMs and even multimodal models (MLLMs). This confirms that fault-inducing inputs are reused to test transferability. However, the paper's methodological description of the PAPILLON framework does not mention any explicit design choices or mechanisms specifically aimed at discovering or enhancing transferable attacks. The design goals of the mutation operators focus on maintaining semantic coherence and shortening prompt length to evade defenses and reduce costs, not on cross-model generalization. Therefore, while transferability is a demonstrated outcome, it is not an explicit goal of the fuzzing design itself.",
                    "evidence": "Page 13, Section 5.4 Attack Transferability: \"We further investigate the transferability of PAPILLON. Transferability refers to the extent to which prompts designed to jailbreak the target model can successfully jailbreak another model. ... LLM transferability. We first evaluate transferability by using jailbreak prompts generated by the source model to target another LLM. The results are shown in Table 17 (appendix). ... MLLM transferability. We then assess the transferability of the PAPILLON-generated jailbreak prompts to multimodal large language models (MLLMs). ... The results are presented in Table 5.\"\n\nPage 9, Section 4.2 Template Mutation and Optimization: \"Optimization schemes. When optimizing the jailbreaking templates, we control both semantic coherence and prompt length through system prompt instructions and the hyperparameter of the output length respectively.\" This section details the design goals of the attack generation, which do not include transferability."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper explicitly evaluates and demonstrates that fault-inducing prompts generated by PAPILLON on one model can be reused to successfully attack other models (cross-model and multimodal transfer experiments are reported). However, the PAPILLON design and mutation operators are described with goals of semantic coherence, short prompt length, and seed-pool/fuzzing effectiveness rather than with explicit mechanisms specifically intended to discover or maximize transferability across different model implementations (no surrogate-model training, ensemble surrogate testing, or transfer-focused mutators are described). Therefore the work shows reuse/testing of fault-inducing inputs for transferability but does not present an explicit design aimed at producing transferable attacks.",
                    "evidence": "Page 2401, Abstract: \"Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses.\"; Page 2412, Section 5.4 (Attack Transferability): \"We further investigate the transferability of PAPILLON. Transferability refers to the extent to which prompts designed to jailbreak the target model can successfully jailbreak another model.\" and \"Overall, PAPILLON demonstrates notable attack transferability across different models.\"; Page 2420, Table 17 (Cross-model transferability): shows the cross-model success-rate matrix (e.g., \"Baichuan2-7B -> Vicuna-7B-v1.3: 70%\", many non-zero cross-model ASR entries); Page 2407, Section 4.2 (Template Mutation and Optimization): \"we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.\" (describes focus on semantic coherence and prompt length rather than explicit transferability mechanisms)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ]
        }
    },
    "Du et al. - 2018 - DeepCruiser Automated Guided Testing for Stateful Deep Learning Systems": {
        "key": "du2018deepcruiser",
        "name": "DeepCruiser",
        "year": "2018",
        "citation_count": "45",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The failures DeepCruiser uncovers are transcription/misprediction errors in ASR (measured as increased WER/CER) produced by metamorphic audio mutations and coverage-guided generation. The paper's evaluation and research questions focus on increasing coverage and triggering erroneous transcriptions (higher WER). There is no presentation or evaluation of bypassing explicit safety, policy, or security mechanisms, nor any examples of inducing high-severity security behaviors (e.g., jailbreaks, data leakage, unauthorized actions, or toxic/harmful outputs). Therefore the failures reported are robustness errors (mispredictions) rather than high- or medium-severity safety/security violations.",
                    "evidence": "Page 13, Section 6.2 Coverage-Guided Testing: \"If the prediction result is inconsistent with the original seed, a' will be added into the failed tests. For audio in ASR, we decide a failed test by checking whether the Word/Character Error Rate (W/CER) exceeds a certain threshold.\"; Page 16, Section 7.3 (RQ2): \"We sample 100 audio from the test data whose Word Error Rates (WERs) are 0. The WER of the audio represents the error rate for the transcripts from RNN. ... Based on the 100 audio (seeds), we randomly generate 10,000 audio (with different WER) by metamorphic transformations.\"; Page 19, Section 7.5 (Answer to RQ4): \"Answer to RQ4: DeepCruiser can effectively generate tests to trigger erroneous behaviors of the RNN. For more fine-grained abstract model, it will capture more erroneous behaviors (i.e., higher WER) of the RNN.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The method, DeepCruiser, is designed to uncover model robustness errors in Automated Speech Recognition (ASR) systems. It operates by applying semantically preserving 'metamorphic transformations' to audio inputs (e.g., adding white noise, changing pitch, altering speed). A failure, referred to as a 'defect', is identified when the ASR model produces an incorrect transcription for the mutated audio, which is quantified by the Word/Character Error Rate (W/CER). This process directly corresponds to the definition of uncovering 'mispredictions or prediction inconsistencies under semantically preserving input mutations'. The paper's scope and evaluation are confined to these transcription errors and do not extend to exposing unsafe behaviors (like toxic or biased content) or bypassing explicit safety or security mechanisms.",
                    "evidence": "Page 13, Section 6.1: 'For defect detection, our goal is to generate audios which sounds normal to human but are incorrectly transcribed by ASRs.'\nPage 14, Section 6.2: 'If the prediction result is inconsistent with the original seed, a\u2032 will be added into the failed tests. For audio in ASR, we decide a failed test by checking whether the Word/Character Error Rate (W/CER) exceeds a certain threshold.'\nPage 18, Section 7.5: 'To answer the question, we measure the WER of the generated audio in Fig. 8. WER represents the erroneous degree of the RNN prediction.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The failures DeepCruiser uncovers are transcription/misprediction errors in ASR (measured as increased WER/CER) produced by metamorphic audio mutations and coverage-guided generation. The paper's evaluation and research questions focus on increasing coverage and triggering erroneous transcriptions (higher WER). There is no presentation or evaluation of bypassing explicit safety, policy, or security mechanisms, nor any examples of inducing high-severity security behaviors (e.g., jailbreaks, data leakage, unauthorized actions, or toxic/harmful outputs). Therefore the failures reported are robustness errors (mispredictions) rather than high- or medium-severity safety/security violations.",
                    "evidence": "Page 13, Section 6.2 Coverage-Guided Testing: \"If the prediction result is inconsistent with the original seed, a' will be added into the failed tests. For audio in ASR, we decide a failed test by checking whether the Word/Character Error Rate (W/CER) exceeds a certain threshold.\"; Page 16, Section 7.3 (RQ2): \"We sample 100 audio from the test data whose Word Error Rates (WERs) are 0. The WER of the audio represents the error rate for the transcripts from RNN. ... Based on the 100 audio (seeds), we randomly generate 10,000 audio (with different WER) by metamorphic transformations.\"; Page 19, Section 7.5 (Answer to RQ4): \"Answer to RQ4: DeepCruiser can effectively generate tests to trigger erroneous behaviors of the RNN. For more fine-grained abstract model, it will capture more erroneous behaviors (i.e., higher WER) of the RNN.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Provides heuristic explanations by showing correlation with state variation"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Ensures each audio is mutated at most once per transformation category (volume/speed/clearness) to prevent unnatural accumulation but does not measure naturalness on the final fuzzed inputs. The human validation is on single step transformation"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly designs and applies bounded, rule-based metamorphic transformations for audio (Section 6.1) and enforces constraints to limit accumulation of major distortions across mutation steps (e.g., altering volume, speed or clearness at most once). This shows the authors attempt to enforce input plausibility during mutation. However, they do not provide a systematic empirical assessment (human study or task-specific perceptual/fluency metrics such as PESQ/STOI/Perplexity/LPIPS/SSIM) to demonstrate that the final fault-inducing inputs remain perceptually/semantically realistic. The only explicit plausibility validation reported is a manual check of selected (100) high-quality seed inputs to mitigate false positives (Threats to Validity), not a broader or quantitative evaluation of final mutants. Therefore the paper enforces plausibility constraints in mutation design but lacks comprehensive validation of plausibility for produced failing inputs, matching the 'Medium' level.",
                    "evidence": "Section 6.1 (Metamorphic Transformations of Audio Signals), page 13: \"To generate suitable defect-triggering candidates, we apply the transformations with restraints to ensure that the audio seeds and the corresponding mutants sound the same to human beings.\"; Section 6.1 (same), page 13: \"(1) we carefully set the parameters to ensure that a single step transformation (on volume, speed or clearness) does not violate the metamorphic relations ... and (2) an audio, generated by mutating from a seed after a sequence of transformations T, is limited to be mutated by a transformation t \u2208 S \u222a UAT such that ({t} \u222a S) \u2229 T = \u2205, where S \u2208 {VRT, SRT, CRT}. Intuitively, the constraints make sure that a mutant is generated by altering the volume, speed or clearness of a seed input at most once. If this constraints is unsatisfied, the audio will not be transformed further.\"; Section 7.6 (Threats to Validity), page 19: \"We adopt a conservative metamorphic transformation strategy to make small changes on the original audio such that DeepCruiser will generate realistic audio. However, it may still cause false positives ... To mitigate this problem, we manually check and make sure the selected inputs (i.e., the 100 audio in RQ1, RQ2, RQ3 and RQ4) are of high quality.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility through rule-based constraints on its mutation operators, which it terms \"metamorphic transformations.\" The stated goal is to generate audio that \"sounds normal to human.\" Plausibility is enforced in two ways: 1) by carefully selecting parameters for individual transformations to ensure they do not perceptibly alter the audio, and 2) by explicitly limiting the cumulative effect of transformations related to volume, speed, or clearness to a single application per seed audio. This second constraint is a direct attempt to prevent the cumulative drift that can lead to unrealistic inputs. However, the paper falls short of the 'High' rating because it does not provide any empirical validation (e.g., a human study or quantitative metrics like PESQ/STOI) to confirm that the final, fault-inducing audio inputs are, in fact, plausible and intelligible. The claims of realism are based on the design of the transformation strategy rather than on an evaluation of its outputs.",
                    "evidence": "Page 14, Section 6.1: \"transformations for generating defect candidates indistinguishable to human before and after the transformations: (1) we carefully set the parameters to ensure that a single step transformation (on volume, speed or clearness) does not violate the metamorphic relations (i.e., human hearing are not affected); and (2) an audio, generated by mutating from a seed after a sequence of transformations T, is limited to be mutated by a transformation t \u2208 S U UAT such that ({t} U S) \u2229 T = \u2205, where S \u2208 {VR, SRT, CRT}. Intuitively, the constraints make sure that a mutant is generated by altering the volume, speed or clearness of a seed input at most once.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper explicitly designs and applies bounded, rule-based metamorphic transformations for audio (Section 6.1) and enforces constraints to limit accumulation of major distortions across mutation steps (e.g., altering volume, speed or clearness at most once). This shows the authors attempt to enforce input plausibility during mutation. However, they do not provide a systematic empirical assessment (human study or task-specific perceptual/fluency metrics such as PESQ/STOI/Perplexity/LPIPS/SSIM) to demonstrate that the final fault-inducing inputs remain perceptually/semantically realistic. The only explicit plausibility validation reported is a manual check of selected (100) high-quality seed inputs to mitigate false positives (Threats to Validity), not a broader or quantitative evaluation of final mutants. Therefore the paper enforces plausibility constraints in mutation design but lacks comprehensive validation of plausibility for produced failing inputs, matching the 'Medium' level.",
                    "evidence": "Section 6.1 (Metamorphic Transformations of Audio Signals), page 13: \"To generate suitable defect-triggering candidates, we apply the transformations with restraints to ensure that the audio seeds and the corresponding mutants sound the same to human beings.\"; Section 6.1 (same), page 13: \"(1) we carefully set the parameters to ensure that a single step transformation (on volume, speed or clearness) does not violate the metamorphic relations ... and (2) an audio, generated by mutating from a seed after a sequence of transformations T, is limited to be mutated by a transformation t \u2208 S \u222a UAT such that ({t} \u222a S) \u2229 T = \u2205, where S \u2208 {VRT, SRT, CRT}. Intuitively, the constraints make sure that a mutant is generated by altering the volume, speed or clearness of a seed input at most once. If this constraints is unsatisfied, the audio will not be transformed further.\"; Section 7.6 (Threats to Validity), page 19: \"We adopt a conservative metamorphic transformation strategy to make small changes on the original audio such that DeepCruiser will generate realistic audio. However, it may still cause false positives ... To mitigate this problem, we manually check and make sure the selected inputs (i.e., the 100 audio in RQ1, RQ2, RQ3 and RQ4) are of high quality.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Uses stable, perceptual transformations (volume, speed, clearness) likely to survive saving, though persistence is not explicitly verified."
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's testing framework mutates audio waveforms in memory using metamorphic transformations and evaluates failures via WER, but it does not describe any explicit handling of standard I/O serialization effects (e.g., quantization, clipping, rounding when saving to common audio/image formats) nor does it constrain mutations to ensure perturbations persist after serialization. The methodology and oracle (WER) operate on generated audio without mention of writing/reading through typical storage formats or accounting for their numeric effects. The authors also discuss using conservative metamorphic parameters and manual checking to avoid false positives, which indicates they did not incorporate explicit automated serialization-aware measures into generation or oracle evaluation.",
                    "evidence": "Section 6.1 \"Metamorphic Transformations of Audio Signals\" (page 12-13): \"To generate suitable defect-triggering candidates, we apply the transformations with restraints to ensure that the audio seeds and the corresponding mutants sound the same to human beings... we carefully set the parameters to ensure that a single step transformation (on volume, speed or clearness) does not violate the metamorphic relations... the audio will not be transformed further.\"; Section 6.2 Algorithm 1 (page 14): shows mutation and prediction steps (a' = mutate(t,a); cov, result <- Predict(R,a')) but contains no steps for serialization or handling quantization/clipping; Section 7.6 \"Threats to Validity\" (page 19): \"We adopt a conservative metamorphic transformation strategy to make small changes on the original audio such that DeepCruiser will generate realistic audio. However, it may still cause false positives... To mitigate this problem, we manually check...\" \u2014 no mention of accounting for I/O serialization effects (quantization/clipping) during test generation or oracle evaluation."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, DeepCruiser, uses a set of semantic-preserving metamorphic transformations on audio signals (e.g., changing volume, adding noise, altering speed) to generate test inputs. According to the metric definition, using such transformations without explicit handling of I/O effects provides only partial enforcement of failure reproducibility. The paper discusses applying these transformations with 'restraints' to ensure the generated audio is still recognizable by humans, thus preserving the metamorphic relation. However, it makes no mention of accounting for serialization effects like quantization, rounding, or clipping that occur when audio is saved to standard file formats. The test generation and oracle evaluation process described in the paper operates on the mutated audio directly without an intermediate save/load cycle, which could lead to the detection of failures caused by fragile numerical artifacts that would not persist after standard I/O operations. Therefore, the approach fits the 'Medium' category as it applies semantic-preserving transformations without explicitly enforcing reproducibility under serialization.",
                    "evidence": "Page 12, Section 6.1, 'Metamorphic Transformations of Audio Signals': 'Inspired by these practical scenarios, we derive a set of transformation operators to mimic the environment interference. Overall, they can be classified into four categories: \u2013 Volume-related transformations (VRT): ChangeVolume, LowPassFilter, HighPassFilter. \u2013 Speed-related transformations (SRT): PitchShift, ChangeSpeed. \u2013 Clearness-related transformations (CRT): AddWhiteNoise, ExtractHarmonic \u2013 Unaffected transformations (UAT): DRC, Trim.'\nPage 13, Section 6.1: 'To generate suitable defect-triggering candidates, we apply the transformations with restraints to ensure that the audio seeds and the corresponding mutants sound the same to human beings. Transformation operators satisfying the above requirements are said to keep a metamorphic relation [41] and we refer to them as metamorphic transformations.'\nPage 14, Section 6.2, 'Coverage-Guided Testing': 'DeepCruiser will generate a new audio a\u2019 with transformation t and perform the transformation with the deep learning system R. If the prediction result is inconsistent with the original seed, a\u2019 will be added into the failed tests. For audio in ASR, we decide a failed test by checking whether the Word/Character Error Rate (W/CER) exceeds a certain threshold.' This description of the workflow does not include steps for saving and reloading the audio to ensure perturbations survive serialization."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The DeepCruiser framework relies on semantic\u2010preserving, metamorphic transformations of audio inputs (e.g., volume, speed, noise adjustments) which inherently introduce coarse\u2010grained perturbations that are likely robust to standard I/O quantization. However, the paper does not explicitly account for serialization effects (rounding, clipping, quantization) when writing or reading audio in common formats, nor does it constrain mutations to guarantee that fault\u2010inducing perturbations survive such operations. According to the metric definition, approaches using metamorphic transformation without explicit handling of I/O effects receive a \u201cMedium\u201d rating, reflecting partial enforcement of failure reproducibility.",
                    "evidence": "Section 6.1 describes the set of metamorphic transformations on audio (ChangeVolume, LowPassFilter, PitchShift, etc.) applied directly in memory to preserve human\u2010perceived semantics, but makes no mention of an explicit save/load cycle or quantization/clipping safeguards; Section 6.2 (Algorithm 1) shows mutation and oracle evaluation steps operating solely on in\u2010memory audio without serialization."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "mutation and coverage strategies are tightly coupled to a specific RNN-based ASR model and training setup"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper evaluates DeepCruiser only on a single RNN-based ASR implementation (Mozilla DeepSpeech-0.3.0) and does not report reusing fault-inducing inputs to test other models nor describe design mechanisms explicitly intended to discover transferable attacks across different models. Therefore it does not demonstrate whether faults found on one model transfer to other models performing the same task.",
                    "evidence": "Page 16, Section 7.1 Datasets and Experiment Setup: \"We selected Mozilla\u2019s implementation of DeepSpeech-0.3.0 [42]... All our experiments were conducted on a pre-trained English model released along with DeepSpeech-0.3.0, which was trained with Fisher [43], LibriSpeech [44], Switchboard [45], and a pre-release snapshot of the English Common Voice training corpus [46].\"; Page 3, Introduction / contribution summary: \"We implement our testing framework, DeepCruiser, and perform in-depth evaluation on a state-of-the-art practical RNN based ASR system to demonstrate the usefulness.\" (demonstrates evaluation on a single ASR system and no cross-model transfer tests are reported)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's entire evaluation is conducted on a single, specific deep learning model for Automated Speech Recognition (ASR). The authors explicitly state they use Mozilla's DeepSpeech-0.3.0 model for all their experiments. The goal of the proposed framework, DeepCruiser, is to generate tests that increase coverage and find defects within this specific model. The paper does not mention testing the fault-inducing audio inputs generated for DeepSpeech on any other ASR models to check for transferability. Consequently, it does not demonstrate whether the discovered failures are specific to this one model or represent shared vulnerabilities across different models.",
                    "evidence": "Page 15, Section 7.1 Datasets and Experiment Setup: \"We selected Mozilla's implementation of DeepSpeech-0.3.0 [42] which produces one of the state-of-the-art open source ASR models. All our experiments were conducted on a pre-trained English model released along with DeepSpeech-0.3.0, which was trained with Fisher [43], LibriSpeech [44], Switchboard [45], and a pre-release snapshot of the English Common Voice training corpus [46].\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper evaluates DeepCruiser only on a single RNN-based ASR implementation (Mozilla DeepSpeech-0.3.0) and does not report reusing fault-inducing inputs to test other models nor describe design mechanisms explicitly intended to discover transferable attacks across different models. Therefore it does not demonstrate whether faults found on one model transfer to other models performing the same task.",
                    "evidence": "Page 16, Section 7.1 Datasets and Experiment Setup: \"We selected Mozilla\u2019s implementation of DeepSpeech-0.3.0 [42]... All our experiments were conducted on a pre-trained English model released along with DeepSpeech-0.3.0, which was trained with Fisher [43], LibriSpeech [44], Switchboard [45], and a pre-release snapshot of the English Common Voice training corpus [46].\"; Page 3, Introduction / contribution summary: \"We implement our testing framework, DeepCruiser, and perform in-depth evaluation on a state-of-the-art practical RNN based ASR system to demonstrate the usefulness.\" (demonstrates evaluation on a single ASR system and no cross-model transfer tests are reported)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Yuan et al. - 2023 - Revisiting Neuron Coverage for DNN Testing A Layer-Wise and Distribution-Aware Criterion": {
        "key": "yuan2023revisiting",
        "name": "NLC",
        "year": "2023",
        "citation_count": "36",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper evaluates NLC by using it to find mispredictions and adversarial examples (robustness errors) in standard discriminative and generative models. All reported failures are incorrect model outputs (prediction errors) induced by input mutations or adversarial attacks (e.g., CW, PGD). The paper does not demonstrate uncovering safety- or security-critical violations (such as bypassing explicit safety/policy mechanisms, jailbreaks, data leakage, or unauthorized actions), nor does it evaluate models with explicit defense or safety-alignment mechanisms. Therefore the failures uncovered are limited to low-impact model robustness errors.",
                    "evidence": "Abstract (page 1200): \"Its capacity to discover DNN prediction errors is promising.\" | VI.A, 2) Fault-Revealing Capability of Test Suites (page 1207\u20131208): \"To evaluate the fault-revealing capabilities of test suites, we construct adversarial examples (AE) using two adversarial attack algorithms, Carlini/Wagner (CW) [9] and Project Gradient Descent (PGD) [31]... all algorithms attacking the three models reach over 98% success rates. That is, more than 0.98 \u00d7 (#training data) AEs can trigger erroneous behaviors.\" | VIB (Guiding Input Mutation) / Triggered Faults (page 1209): \"Images mutated under the guidance of NLC (last row) have the highest error triggering rates compared with random mutation (3rd row) and other criteria...\" | Model list (page 1206, Table II): lists ResNet50, VGG16, MobileNet-V2, BigGAN, LSTM (standard discriminative and generative models; no defended/safety-aligned models reported)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper evaluates its proposed method, NLC, on its ability to find 'faults' which are exclusively defined and measured as model robustness errors, specifically mispredictions. The entire evaluation framework for fault-revealing capability is built around generating adversarial examples (AEs) and applying semantically preserving mutations (e.g., blurring, stylizing, rotation) to cause image classifiers to produce incorrect labels. The paper does not discuss or demonstrate the uncovering of unsafe or biased behaviors (Medium severity), nor does it test against models with explicit safety/security mechanisms to show bypasses (High severity). The failures are consistently framed as misclassifications.",
                    "evidence": "Page 9, Section VI-A.2, \"Fault-Revealing Capability of Test Suites\": \"To evaluate the fault-revealing capabilities of test suites, we construct adversarial examples (AE) using two adversarial attack algorithms, Carlini/Wagner (CW) [9] and Project Gradient Descent (PGD) [31]... The incorrect predictions uniformly distribute across all classes.\"\nPage 10, Section VI-B, \"Guiding Input Mutation in DNN Testing\": \"We evaluate the mutated inputs on 1) #triggered faults, 2) naturalness, and 3) diversity of the erroneous behaviors.\" In this context, for image classification models, a 'triggered fault' is a misclassification.\nPage 11, Section VII, \"Diversity of Erroneous Behaviors\": \"A higher diversity of erroneous behaviors indicates uncovering a practically larger vulnerability surface of DNN models. Overall, a collection of fault-triggering images are regarded as more diverse if they cover more classes.\" This confirms that the 'erroneous behaviors' being evaluated are misclassifications into different output classes."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper evaluates NLC by using it to find mispredictions and adversarial examples (robustness errors) in standard discriminative and generative models. All reported failures are incorrect model outputs (prediction errors) induced by input mutations or adversarial attacks (e.g., CW, PGD). The paper does not demonstrate uncovering safety- or security-critical violations (such as bypassing explicit safety/policy mechanisms, jailbreaks, data leakage, or unauthorized actions), nor does it evaluate models with explicit defense or safety-alignment mechanisms. Therefore the failures uncovered are limited to low-impact model robustness errors.",
                    "evidence": "Abstract (page 1200): \"Its capacity to discover DNN prediction errors is promising.\" | VI.A, 2) Fault-Revealing Capability of Test Suites (page 1207\u20131208): \"To evaluate the fault-revealing capabilities of test suites, we construct adversarial examples (AE) using two adversarial attack algorithms, Carlini/Wagner (CW) [9] and Project Gradient Descent (PGD) [31]... all algorithms attacking the three models reach over 98% success rates. That is, more than 0.98 \u00d7 (#training data) AEs can trigger erroneous behaviors.\" | VIB (Guiding Input Mutation) / Triggered Faults (page 1209): \"Images mutated under the guidance of NLC (last row) have the highest error triggering rates compared with random mutation (3rd row) and other criteria...\" | Model list (page 1206, Table II): lists ResNet50, VGG16, MobileNet-V2, BigGAN, LSTM (standard discriminative and generative models; no defended/safety-aligned models reported)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "shows heuristic link between coverage and errors."
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Uses inception score and FID score to measure natualness quality of the final inputs but overlook cumulave effects of mutations across iteration"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints at each mutation step (rule-based validity checks adopted from prior work) and reports quantitative perceptual quality measures (Inception Score and FID) showing that NLC-guided, fault-inducing images remain natural-looking. However, the paper does not describe any explicit mechanism that prevents cumulative drift across many sequential fuzzing iterations (i.e., global/aggregate plausibility guarantees), nor does it report human studies; plausibility is enforced/checked per-step and validated empirically on final outputs via IS/FID only.",
                    "evidence": "Sec. VI-B (Fuzzing setup) / page 1209: \"is_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than \u03b1\u00d7 #pixels or 2) the maximum of changed pixel value is less than \u03b2 \u00d7255. We set \u03b1 to 0.2 and \u03b2 to 0.4.\" (Alg.1 and accompanying text show the per-step validity check is applied before adding a mutated input as a new seed).; Sec. VI-B (Naturalness of Mutations) / pages 1209\u20131210: \"We measure the naturalness of mutated inputs under each criterion; ... We use Inception Score (IS) [39] and Frechet Inception Distance (FID) [18], which are widely used ... Overall, mutated images guided by NLC have the best scores for both IS and FID.\" (Table XI reports IS/FID values for NLC and other criteria).; There is no statement or section in the paper that introduces a global or cumulative-plausibility enforcement across fuzzing iterations (the algorithm checks is_valid(s_hat,s) relative to the immediate parent only, and mutated inputs are then added to the seed pool), implying cumulative drift across many bounded steps is not explicitly prevented nor analyzed."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper satisfies both conditions for a 'High' rating. First, it enforces input plausibility throughout the fuzzing iterations using an explicit validity check. The `is_valid()` function, described in Algorithm 1, constrains mutations by bounding the number of changed pixels and the maximum change in pixel value relative to the original seed input. This prevents the cumulative drift that can lead to unrealistic inputs. Second, the paper provides dedicated empirical evidence that the final fault-inducing inputs are realistic. It uses two standard quantitative, task-specific measures for image naturalness\u2014Inception Score (IS) and Fr\u00e8chet Inception Distance (FID)\u2014to evaluate the outputs of its fuzzing process. The results in Table XI demonstrate that the images generated under the guidance of the proposed NLC criterion have better naturalness scores compared to other methods and a random baseline.",
                    "evidence": "Page 10, Section VI-B, Triggered Faults: \"is_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than \u03b1 \u00d7 #pixels or 2) the maximum of changed pixel value is less than \u03b2 \u00d7 255. We set \u03b1 to 0.2 and \u03b2 to 0.4.\"\nPage 11, Section VI-B, Naturalness of Mutations: \"We measure the naturalness of mutated inputs under each criterion; a good criterion should lead to mutations that mostly preserve the naturalness. We use Inception Score (IS) [39] and Fr\u00e8chet Inception Distance (FID) [18], which are widely used in the AI and SE community to assess the naturalness of images, as the metrics.\"\nPage 11, Table XI: This table provides the results for IS and FID for various fuzzing configurations, with the accompanying text stating, \"Overall, mutated images guided by NLC have the best scores for both IS and FID.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility via per-step, rule-based validity checks (bounding pixel changes in each mutation) and reports quantitative measures (Inception Score and FID) showing that the final fault-inducing images are natural-looking. However, it does not introduce any mechanism to bound or assess cumulative drift over multiple fuzzing iterations, nor does it include a human evaluation of plausibility. Thus it validates plausibility at each mutation and on the final outputs, but overlooks aggregate guarantees across the entire fuzzing process.",
                    "evidence": "Section VI-B (Fuzzing setup) and Algorithm 1 (page 1209): \u201cis_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than \u03b1\u00d7#pixels or 2) the maximum of changed pixel value is less than \u03b2\u00d7255. We set \u03b1 to 0.2 and \u03b2 to 0.4.\u201d\n\nSection VI-B (Naturalness of Mutations) and Table XI (pages 1209\u20131210): \u201cWe measure the naturalness of mutated inputs \u2026 We use Inception Score (IS) [39] and Fr\u00e9chet Inception Distance (FID) [18] \u2026 Overall, mutated images guided by NLC have the best scores for both IS and FID.\u201d\n\nNo section in the paper describes any global or cumulative\u2010plausibility constraint over multiple iterations of fuzzing."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Plausibly stable due to large semantic changes, but applied in normalized float domain, so some discovered failures could vanish after re-encoding."
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's fuzzing/mutation design primarily uses semantic-preserving, metamorphic transformations (e.g., contrast, brightness, affine, blurring, stylizing) which tend to produce coarse, perceptible perturbations that are less likely to be lost by serialization; this aligns with the metric's allowance that metamorphic transformations provide partial enforcement of reproducibility. The authors also enforce a simple pixel-change validity constraint that bounds maximum pixel change relative to an 8-bit scale (\u03b2 \u00d7 255), which is a partial consideration related to clipping/quantization. However, the paper does not explicitly account for standard I/O serialization effects (rounding, quantization, or encoding to formats like PNG/JPEG/WAV) during test case generation or oracle evaluation, nor does it generate inputs directly in serialized formats or describe steps to ensure failures persist after disk serialization. Therefore the approach partially considers serialization-related concerns (via metamorphic transforms and amplitude bounds) but does not explicitly ensure reproducibility under standard I/O operations as required for a High rating.",
                    "evidence": "Sec. VI-B (Guiding Input Mutation in DNN Testing), p.1209: \"mutations in Table IX are representative, for example, blurring and stylizing (see Fig. 9) reveal the texture-bias (i.e., rely on texture to make decisions) of DNNs [7, 14].\" (Table IX lists: Pixel-Level: contrast, brightness; Affine: translation, scaling, rotation; Texture-Level: blurring, stylizing).  Sec. VI-B (Algorithm 1 description), p.1209: \"is_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than \u03b1\u00d7 #pixels or 2) the maximum of changed pixel value is less than \u03b2 \u00d7255. We set \u03b1 to 0.2 and \u03b2 to 0.4.\"  (These show the use of metamorphic transformations and a pixel-value bound relative to 255.)  No sections discuss serialization/encoding/rounding/quantization or saving inputs to disk (no mention of terms such as \"PNG\", \"JPEG\", \"serialize\", \"quantize\", \"rounding\", or \"I/O\") in the design, mutation, or oracle/evaluation descriptions (see Sec. V Implementation and Sec. VI Evaluation, pp.1208\u20131210), indicating the lack of explicit handling of standard I/O serialization effects."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's fuzzing methodology relies on a set of semantic-preserving metamorphic transformations, such as contrast, brightness, rotation, blurring, and stylizing, applied to image inputs. According to the metric definition, using such transformations without explicitly enforcing reproducibility under serialization qualifies for a 'Medium' rating. The paper describes a validity check for mutations (`is_valid()`) that constrains the number of changed pixels and the maximum change in pixel value. However, this check does not explicitly account for I/O effects like rounding or quantization that occur when saving an image to a standard format (e.g., PNG). The check `the maximum of changed pixel value is less than \u03b2 \u00d7 255` constrains the magnitude of the floating-point perturbation but does not guarantee the change will persist after being quantized to an 8-bit integer. The paper does not mention any procedure to save and reload the mutated inputs to verify that the failure is reproducible after standard I/O operations.",
                    "evidence": "Page 10, Section VI-B, Guiding Input Mutation in DNN Testing: 'mutations in Table IX are representative, for example, blurring and stylizing (see Fig. 9) reveal the texture-bias (i.e., rely on texture to make decisions) of DNNs [7, 14].' and 'is_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than \u03b1 \u00d7 #pixels or 2) the maximum of changed pixel value is less than \u03b2 \u00d7 255. We set \u03b1 to 0.2 and \u03b2 to 0.4.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper's fuzzing/mutation design primarily uses semantic-preserving, metamorphic transformations (e.g., contrast, brightness, affine, blurring, stylizing) which tend to produce coarse, perceptible perturbations that are less likely to be lost by serialization; this aligns with the metric's allowance that metamorphic transformations provide partial enforcement of reproducibility. The authors also enforce a simple pixel-change validity constraint that bounds maximum pixel change relative to an 8-bit scale (\u03b2 \u00d7 255), which is a partial consideration related to clipping/quantization. However, the paper does not explicitly account for standard I/O serialization effects (rounding, quantization, or encoding to formats like PNG/JPEG/WAV) during test case generation or oracle evaluation, nor does it generate inputs directly in serialized formats or describe steps to ensure failures persist after disk serialization. Therefore the approach partially considers serialization-related concerns (via metamorphic transforms and amplitude bounds) but does not explicitly ensure reproducibility under standard I/O operations as required for a High rating.",
                    "evidence": "Sec. VI-B (Guiding Input Mutation in DNN Testing), p.1209: \"mutations in Table IX are representative, for example, blurring and stylizing (see Fig. 9) reveal the texture-bias (i.e., rely on texture to make decisions) of DNNs [7, 14].\" (Table IX lists: Pixel-Level: contrast, brightness; Affine: translation, scaling, rotation; Texture-Level: blurring, stylizing).  Sec. VI-B (Algorithm 1 description), p.1209: \"is_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than \u03b1\u00d7 #pixels or 2) the maximum of changed pixel value is less than \u03b2 \u00d7255. We set \u03b1 to 0.2 and \u03b2 to 0.4.\"  (These show the use of metamorphic transformations and a pixel-value bound relative to 255.)  No sections discuss serialization/encoding/rounding/quantization or saving inputs to disk (no mention of terms such as \"PNG\", \"JPEG\", \"serialize\", \"quantize\", \"rounding\", or \"I/O\") in the design, mutation, or oracle/evaluation descriptions (see Sec. V Implementation and Sec. VI Evaluation, pp.1208\u20131210), indicating the lack of explicit handling of standard I/O serialization effects."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models performing the same task, nor does it describe mechanisms intended to discover or encourage transferable attacks. All fuzzing and adversarial-example experiments are conducted per tested model (i.e., inputs are generated/assessed against the same target DNN), and the evaluation reports coverage/faults per model without reusing generated fault-inducing inputs across different model implementations. Thus the work does not address attack transferability.",
                    "evidence": "VI.B (Guiding Input Mutation in DNN Testing), Alg. 1 (page 1209): \"Tested DNN: D; Criterion: C;\" \u2014 the fuzzing algorithm is defined in terms of a single tested DNN. \n\nVI.A.2 (Fault-Revealing Capability of Test Suites) (page 1208): \"AEs are generated using the training data (see 'Clarification' below for results of AEs generated using test data), and all algorithms attacking the three models reach over 98% success rates.\" \u2014 adversarial examples are generated and evaluated against the same model(s) they attack. \n\nVI (Evaluation) and Tables (e.g., Table VI, Table VII) (pages 1206\u20131208): All reported results present coverage and faults per model (ResNet, VGG, MobileNet, BigGAN, etc.), with no experiment or section reporting reuse of fault-inducing inputs across different models or measurements of transferability. (No section title or experiment in the paper addresses transferability or substitute-model attacks.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper focuses on designing a new coverage criterion (NLC) and evaluating its effectiveness for assessing test suites and guiding fuzzing for a single, specific DNN model at a time. The experiments involve generating fault-inducing inputs (either through adversarial attacks or fuzzing) on a given model and then evaluating the outcome on that same model. The paper does not contain any experiments or discussion on whether the fault-inducing inputs generated for one model (e.g., ResNet50) can also trigger failures in other models performing the same task (e.g., VGG16 or MobileNetV2). The fuzzing design and the NLC metric itself are aimed at exploring the internal state of a single model, not at finding shared vulnerabilities across different models.",
                    "evidence": "Page 10, Section VI-B, \"Guiding Input Mutation in DNN Testing\", Algorithm 1: The fuzzing algorithm is defined with a single \"Tested DNN: D\" as input. The core feedback loop in line 7, `if is_valid($,s) and coverage_inc(C,D) then`, clearly shows that the decision to keep a mutated input is based on its ability to increase the coverage metric `C` on that specific DNN `D`. This demonstrates that the fuzzing process is tailored to a single model, not designed for finding transferable inputs. The subsequent results in Table XI also report fault-triggering rates for individual models that were fuzzed, with no cross-evaluation."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models performing the same task, nor does it describe mechanisms intended to discover or encourage transferable attacks. All fuzzing and adversarial-example experiments are conducted per tested model (i.e., inputs are generated/assessed against the same target DNN), and the evaluation reports coverage/faults per model without reusing generated fault-inducing inputs across different model implementations. Thus the work does not address attack transferability.",
                    "evidence": "VI.B (Guiding Input Mutation in DNN Testing), Alg. 1 (page 1209): \"Tested DNN: D; Criterion: C;\" \u2014 the fuzzing algorithm is defined in terms of a single tested DNN. \n\nVI.A.2 (Fault-Revealing Capability of Test Suites) (page 1208): \"AEs are generated using the training data (see 'Clarification' below for results of AEs generated using test data), and all algorithms attacking the three models reach over 98% success rates.\" \u2014 adversarial examples are generated and evaluated against the same model(s) they attack. \n\nVI (Evaluation) and Tables (e.g., Table VI, Table VII) (pages 1206\u20131208): All reported results present coverage and faults per model (ResNet, VGG, MobileNet, BigGAN, etc.), with no experiment or section reporting reuse of fault-inducing inputs across different models or measurements of transferability. (No section title or experiment in the paper addresses transferability or substitute-model attacks.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Xie et al. - 2019 - DeepHunter a coverage-guided fuzz testing framework for deep neural networks": {
        "key": "xie2019deephunter",
        "name": "DeepHunter",
        "year": "2019",
        "citation_count": "518",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper defines and evaluates DeepHunter solely in terms of detecting mispredictions and model inconsistencies under semantically-preserving input mutations (including quantization-induced inconsistencies). All formal definitions of failures are phrased as erroneous predictions or quantization inconsistencies (Definitions 2 and 3). The evaluation uses standard image-classification datasets and pre-trained models (MNIST, CIFAR-10, ImageNet; LeNet, ResNet, MobileNet, VGG) and measures numbers/diversity of 'errors' and 'quantization erroneous behavior'. There is no design, experiment, or claim that DeepHunter targets bypassing explicit safety/policy mechanisms, safety-aligned or adversarially-defended models, or higher-severity behaviours like jailbreaks, data leakage, or unauthorized actions. Therefore the failures it uncovers are robustness/misprediction errors (low severity) rather than high-impact safety/security violations.",
                    "evidence": "Page 149, Section 3.2.1 (Test Oracle): \"Definition 2. Given a DNN F, a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x), a mutant x' in M(x) is an erroneous behavior of F if F(x') != F(x).\"; \"Definition 3. ... a mutant x' in M(x) is a quantization erroneous behavior between F and F' if ... F(x') != F(x).\" (these definitions show failures are mispredictions or quantization inconsistencies).\n\nPage 149, Section 3.2.2 (Metamorphic Mutation): \"... this paper focuses on the image domain, one of the most widely studied domains.\" (shows scope limited to image classification robustness).\n\nPage 151, Section 4 (Experimental Design and Settings), Datasets and DNN Models: \"We select three popular publicly available datasets (i.e., MNIST [29], CIFAR-10 [27], and ImageNet [41]) ... we study several popular pre-trained DNN models\" (shows evaluation on standard models, not defended/safety-aligned models).\n\nPage 154, Section 5.4 (RQ4 Summary): \"With corner-region based criteria, DeepHunter tends to be more useful for detecting errors introduced during the DNN quantization.\" (shows emphasis on detecting quantization-related misprediction errors).\n\nSection 6 (Related Work, Testing): \"Different from DeepXplore and DiffChaser which focus on differential testing, DeepHunter is a grey-box testing technique focusing on a single DNN model.\" (indicates the method targets single-model robustness testing rather than bypassing policy/safeguards)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper defines and evaluates failures strictly as model robustness errors, specifically prediction inconsistencies under semantically preserving input mutations. The core test oracle, as defined in the paper, identifies an 'erroneous behavior' when a model's prediction on a mutated input differs from its correct prediction on the original input. This aligns perfectly with the 'Low' severity definition. The experimental evaluation is conducted on standard image classification benchmarks (MNIST, CIFAR-10, ImageNet), and the 'errors' or 'defects' counted are misclassifications. The paper does not discuss or evaluate the fuzzer's ability to uncover unsafe behaviors (Medium) or to bypass any explicit safety, policy, or security mechanisms (High).",
                    "evidence": "Page 5, Section 3.2.1, \"Definition 2. Given a DNN F, a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x), a mutant x' \u2208 M(x) is an erroneous behavior of F if F(x') \u2260 F(x).\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper defines and evaluates DeepHunter solely in terms of detecting mispredictions and model inconsistencies under semantically-preserving input mutations (including quantization-induced inconsistencies). All formal definitions of failures are phrased as erroneous predictions or quantization inconsistencies (Definitions 2 and 3). The evaluation uses standard image-classification datasets and pre-trained models (MNIST, CIFAR-10, ImageNet; LeNet, ResNet, MobileNet, VGG) and measures numbers/diversity of 'errors' and 'quantization erroneous behavior'. There is no design, experiment, or claim that DeepHunter targets bypassing explicit safety/policy mechanisms, safety-aligned or adversarially-defended models, or higher-severity behaviours like jailbreaks, data leakage, or unauthorized actions. Therefore the failures it uncovers are robustness/misprediction errors (low severity) rather than high-impact safety/security violations.",
                    "evidence": "Page 149, Section 3.2.1 (Test Oracle): \"Definition 2. Given a DNN F, a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x), a mutant x' in M(x) is an erroneous behavior of F if F(x') != F(x).\"; \"Definition 3. ... a mutant x' in M(x) is a quantization erroneous behavior between F and F' if ... F(x') != F(x).\" (these definitions show failures are mispredictions or quantization inconsistencies).\n\nPage 149, Section 3.2.2 (Metamorphic Mutation): \"... this paper focuses on the image domain, one of the most widely studied domains.\" (shows scope limited to image classification robustness).\n\nPage 151, Section 4 (Experimental Design and Settings), Datasets and DNN Models: \"We select three popular publicly available datasets (i.e., MNIST [29], CIFAR-10 [27], and ImageNet [41]) ... we study several popular pre-trained DNN models\" (shows evaluation on standard models, not defended/safety-aligned models).\n\nPage 154, Section 5.4 (RQ4 Summary): \"With corner-region based criteria, DeepHunter tends to be more useful for detecting errors introduced during the DNN quantization.\" (shows emphasis on detecting quantization-related misprediction errors).\n\nSection 6 (Related Work, Testing): \"Different from DeepXplore and DiffChaser which focus on differential testing, DeepHunter is a grey-box testing technique focusing on a single DNN model.\" (indicates the method targets single-model robustness testing rather than bypassing policy/safeguards)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "It uses model coverage to guide input generation but does not provide any expeirment showing the correlation with coverage"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "User study was conducted on the images generated by single-step metamorphic mutations, not on the final fault-triggering fuzzed inputs after multiple chained transformations. Allows at most one affine transformation to reduce that risk."
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly designs mutation operators and checks that constrain mutations across iterative fuzzing (e.g., allowing at most one affine transform per mutation chain, and bounding cumulative pixel changes via L0/L\u221e computed relative to a reference image), thereby enforcing plausibility throughout sequential mutations. In addition, the authors performed a human user study that empirically measured the perceptual validity of generated tests (45,000 images total) and report low invalidity rates for DeepHunter (e.g., 1.2% MNIST, 2.9% CIFAR-10, 0.8% ImageNet). These two facts show the method both enforces plausibility across iterations and provides empirical evidence that final generated (including fault-inducing) inputs remain perceptually realistic.",
                    "evidence": "Page 149, Section 3.2 (Transformation and Mutation): \"In order to keep the semantics of the mutants close to the original seed, we adopt a conservative strategy that selects the Affine Transformation to be used only once ... A Pixel Value Transformation can be used multiple times ... we use L0 and L\u221e to constrain the pixel-level changes.\"; (Alg.2 / Definition 5 / Eq.2) Page 150: \"For sequential mutations that include an Affine Transformation ... the L0 and L\u221e between s0 and sn are computed as follows: L0(s0,sn) = L0(s0,sj-1) + L0(sj,sn)  L\u221e(s0,sn) = MAX(L\u221e(s0,sj-1), L\u221e(sj,sn)).\"; Page 151, Section 4 (User Study Settings): \"For each dataset, we randomly select 30 images as the seeds and generate 5,000 images with each of the 3 mutation strategies. Finally, 9 sets, with a total of 45,000 test images ... We recruit 9 participants ... A generated image is marked invalid if it either could not be perceived, or is recognized as a different class compared with its original counterpart.\"; Page 153, Section 5.1 (RQ1 Results) and Table 2: \"Table 2: The number and ratio of invalid images generated by different mutation strategies ... DeepHunter 60 (1.2%) 145 (2.9%) 40 (0.8%)\"; Page 147 (Abstract / Summary of results): \"The user study on mutation strategies shows ... with careful transformation constraint design and tuning, it can still reduce the likelihood to generate invalid tests to some extent (e.g., DeepHunter generates 1.2% on MNIST, 2.0% on CIFAR-10, and 0.8% on ImageNet).\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper explicitly designs its metamorphic mutation strategy to maintain the semantic validity of generated inputs throughout the fuzzing process, directly addressing the issue of cumulative transformations leading to implausible results. The methodology enforces this by allowing structural changes (Affine Transformations) only once per mutation sequence from an initial seed, while constraining subsequent pixel-level changes relative to a stable reference image. This design directly counters the problem of input drift over multiple iterations. Furthermore, the paper provides strong empirical evidence for the plausibility of its generated inputs via a formal user study. The study assessed the validity of thousands of generated images and found that DeepHunter's strategy produces a very low percentage of invalid images (e.g., 0.8% for ImageNet, 1.2% for MNIST), thus confirming that the inputs it generates, including those that find faults, remain perceptually realistic.",
                    "evidence": "Page 6, Section 3.2.3, Definition 5 and surrounding text: 'In order to keep the semantics of the mutants close to the original seed, we adopt a conservative strategy that selects the Affine Transformation to be used only once (we assume that mutation with only one affine transformation will not affect the semantics with the carefully selected parameters) as an image is more likely to be unrecognizable if it is changed by multiple affine transformations.' This describes the mechanism for controlling cumulative effects. \n\nPage 7, Section 4, 'User Study Settings': 'For RQ1, we perform the user study on 3 different strategies that constrain the generation of test images, and evaluate their effectiveness in generating valid test images... Each participant is asked to perform manual analysis on each of the generated test images in line with its original counterpart. A generated image is marked invalid if it either could not be perceived, or is recognized as a different class compared with its original counterpart.' This describes the empirical evaluation. \n\nPage 8, Table 2: This table presents the results of the user study, showing that DeepHunter generated invalid images at rates of only 1.2% (MNIST), 2.9% (CIFAR-10), and 0.8% (ImageNet), providing quantitative evidence of plausibility."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly designs mutation operators and checks that constrain mutations across iterative fuzzing (e.g., allowing at most one affine transform per mutation chain, and bounding cumulative pixel changes via L0/L\u221e computed relative to a reference image), thereby enforcing plausibility throughout sequential mutations. In addition, the authors performed a human user study that empirically measured the perceptual validity of generated tests (45,000 images total) and report low invalidity rates for DeepHunter (e.g., 1.2% MNIST, 2.9% CIFAR-10, 0.8% ImageNet). These two facts show the method both enforces plausibility across iterations and provides empirical evidence that final generated (including fault-inducing) inputs remain perceptually realistic.",
                    "evidence": "Page 149, Section 3.2 (Transformation and Mutation): \"In order to keep the semantics of the mutants close to the original seed, we adopt a conservative strategy that selects the Affine Transformation to be used only once ... A Pixel Value Transformation can be used multiple times ... we use L0 and L\u221e to constrain the pixel-level changes.\"; (Alg.2 / Definition 5 / Eq.2) Page 150: \"For sequential mutations that include an Affine Transformation ... the L0 and L\u221e between s0 and sn are computed as follows: L0(s0,sn) = L0(s0,sj-1) + L0(sj,sn)  L\u221e(s0,sn) = MAX(L\u221e(s0,sj-1), L\u221e(sj,sn)).\"; Page 151, Section 4 (User Study Settings): \"For each dataset, we randomly select 30 images as the seeds and generate 5,000 images with each of the 3 mutation strategies. Finally, 9 sets, with a total of 45,000 test images ... We recruit 9 participants ... A generated image is marked invalid if it either could not be perceived, or is recognized as a different class compared with its original counterpart.\"; Page 153, Section 5.1 (RQ1 Results) and Table 2: \"Table 2: The number and ratio of invalid images generated by different mutation strategies ... DeepHunter 60 (1.2%) 145 (2.9%) 40 (0.8%)\"; Page 147 (Abstract / Summary of results): \"The user study on mutation strategies shows ... with careful transformation constraint design and tuning, it can still reduce the likelihood to generate invalid tests to some extent (e.g., DeepHunter generates 1.2% on MNIST, 2.0% on CIFAR-10, and 0.8% on ImageNet).\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Enforce norm-bounded constraint between successibe mutations but the perturbation can accumulated over time",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "The metamorphic transformations likely to persist but not validated. Works on floating point input without applying rounding."
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "DeepHunter uses metamorphic (semantic-preserving) image transformations and enforces conservative pixel-level constraints (L0 and L\u221e) during mutation (partial consideration of numeric effects). The paper therefore does consider serialization-like concerns only implicitly by constraining pixel changes (e.g., bounds relative to 255) and by using metamorphic transformations, but it does not explicitly account for standard I/O serialization effects (rounding/quantization/clipping when saving inputs) during test generation or oracle evaluation. The paper treats metamorphic transformations as the oracle and evaluates validity via a human user study, and it focuses explicitly on model-weight quantization (quantizing network weights) as a separate concern; it does not describe explicit handling to ensure failures persist after common input serialization (e.g., saving as 8-bit PNG/JPEG) or any explicit post-serialization reproducibility checks. This matches the metric\u2019s \u201cMedium\u201d level: serialization effects are considered only partially (via metamorphic transforms and pixel-change constraints) but not explicitly enforced for reproducibility under I/O serialization.",
                    "evidence": "Page 149, Section 3.2 (Metamorphic Mutation): \"To keep the semantics of the mutants close to the original seed, we adopt a conservative strategy... A Pixel Value Transformation can be used multiple times... we use L0 and L\u221e to constrain the pixel-level changes... f(s,s') = L\u221e(s,s') \u2264 255, if L0(s,s') < \u03b1 \u00d7 size(s)  L\u221e(s,s') < \u03b2 \u00d7 255, otherwise (1) ... L0(s,s') represents the maximum number of the changed pixels... L\u221e represents the maximum value of a pixel changes...\" Page 153, Table 2 / Section 5.1 (User Study): \"Table 2 shows the number of invalid images generated by different strategies... DeepHunter 60 (1.2%) [MNIST], 145 (2.9%) [CIFAR-10], 40 (0.8%) [ImageNet]\" (shows reliance on metamorphic validity checks and human study). Page 148, Section 2 (DNN Quantization): \"DNN Quantization. ... quantizing the weights to lower bits (e.g., from 32-bit floating to 16-bit, 8-bit) ...\" (paper treats model-weight quantization explicitly, not input serialization). Nowhere in the methodology or evaluation (e.g., Sections 3.2, 3.3, 4, 5) does the paper describe explicit handling of standard I/O serialization (rounding/clipping when saving inputs to common storage formats) or post-serialization reproducibility checks."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper proposes DeepHunter, a fuzzing framework that uses metamorphic transformations (e.g., image brightness, rotation, scaling) to generate new test cases. The assessment metric explicitly states that applying semantic-preserving metamorphic transformations without explicit enforcement of reproducibility under serialization corresponds to a 'Medium' rating. DeepHunter's methodology for validating mutations, described in Section 3.2.2 and Algorithm 2, applies constraints based on L0 and L\u221e norms (Equation 1) to limit the magnitude of pixel-level changes. These constraints operate on the in-memory, likely floating-point, representation of the image and do not account for I/O effects like rounding or quantization that occur when serializing an image to a standard file format (e.g., 8-bit PNG). A small, fault-inducing floating-point perturbation (e.g., changing a pixel value from 155.0 to 155.4) could be generated and trigger a failure in memory, but this perturbation would be lost upon serialization as it would be rounded back to 155, making the failure non-reproducible from the stored artifact. The paper does not describe any mechanism to ensure that the generated mutations persist after a save-and-reload cycle.",
                    "evidence": "Page 5, Section 3.2.2, Metamorphic Mutation: \"To increase the changeability of mutation, we select eight image transformations of two categories: \u2022 Pixel Value transformation P: image contrast, image brightness, image blur, and image noise. \u2022 Affine transformation G: image translation, image scaling, image shearing, and image rotation.\"\n\nPage 6, Section 3.2.2: \"A Pixel Value Transformation can be used multiple times for increasing the changeability, we use Lo and L\u221e to constrain the pixel-level changes. Suppose an image s' is mutated from s by a pixel value transformation, then s' is valid if f(s, s') is satisfied.\"\n\nPage 6, Equation (1): The validity function f(s, s') is defined using L0(s, s') (number of changed pixels) and L\u221e(s, s') (maximum value of a pixel's change). This formulation constrains the magnitude of mutations but does not model or account for serialization effects such as rounding fractional changes to integers, which is critical for ensuring failure reproducibility from saved files."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "DeepHunter uses metamorphic (semantic-preserving) image transformations and enforces conservative pixel-level constraints (L0 and L\u221e) during mutation (partial consideration of numeric effects). The paper therefore does consider serialization-like concerns only implicitly by constraining pixel changes (e.g., bounds relative to 255) and by using metamorphic transformations, but it does not explicitly account for standard I/O serialization effects (rounding/quantization/clipping when saving inputs) during test generation or oracle evaluation. The paper treats metamorphic transformations as the oracle and evaluates validity via a human user study, and it focuses explicitly on model-weight quantization (quantizing network weights) as a separate concern; it does not describe explicit handling to ensure failures persist after common input serialization (e.g., saving as 8-bit PNG/JPEG) or any explicit post-serialization reproducibility checks. This matches the metric\u2019s \u201cMedium\u201d level: serialization effects are considered only partially (via metamorphic transforms and pixel-change constraints) but not explicitly enforced for reproducibility under I/O serialization.",
                    "evidence": "Page 149, Section 3.2 (Metamorphic Mutation): \"To keep the semantics of the mutants close to the original seed, we adopt a conservative strategy... A Pixel Value Transformation can be used multiple times... we use L0 and L\u221e to constrain the pixel-level changes... f(s,s') = L\u221e(s,s') \u2264 255, if L0(s,s') < \u03b1 \u00d7 size(s)  L\u221e(s,s') < \u03b2 \u00d7 255, otherwise (1) ... L0(s,s') represents the maximum number of the changed pixels... L\u221e represents the maximum value of a pixel changes...\" Page 153, Table 2 / Section 5.1 (User Study): \"Table 2 shows the number of invalid images generated by different strategies... DeepHunter 60 (1.2%) [MNIST], 145 (2.9%) [CIFAR-10], 40 (0.8%) [ImageNet]\" (shows reliance on metamorphic validity checks and human study). Page 148, Section 2 (DNN Quantization): \"DNN Quantization. ... quantizing the weights to lower bits (e.g., from 32-bit floating to 16-bit, 8-bit) ...\" (paper treats model-weight quantization explicitly, not input serialization). Nowhere in the methodology or evaluation (e.g., Sections 3.2, 3.3, 4, 5) does the paper describe explicit handling of standard I/O serialization (rounding/clipping when saving inputs to common storage formats) or post-serialization reproducibility checks."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly defines and implements a mechanism to detect faults that transfer from an original model to other models (specifically quantized versions), and the evaluation reuses tests generated on the original (unquantized) model to detect errors in the quantized models. Definition 3 formalizes a \u201cquantization erroneous behavior\u201d (i.e., a fault that manifests in one model but not the other), DeepHunter\u2019s workflow and experiments use that relation, and RQ4 reports results where tests generated on the original model are applied to quantized variants to measure such transfer. This demonstrates both reuse of fault-inducing inputs across models performing the same task (original vs. quantized) and an explicit design for discovering transferable failures.",
                    "evidence": "Page 149, Section 3.2.1 (Transformation and Mutation) \u2014 Definition 3: \"Given a DNN F and its quantized version F', a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x) \u2227 F'(x) = O(x), a mutant x' \u2208 M(x) is a quantization erroneous behavior between F and F' if (F(x) = F(x') \u2228 F'(x) = F'(x')) \u2227 F(x') \u2260 F(x).\" \n\nPage 154, Section 5.4 (RQ4. Error Detection for Quantization) \u2014 experimental procedure and results: \"In this experiment, all initial seeds of each dataset are unable to detect the errors before and after quantization for all cases... At runtime, we allocate 10 hours for test generation on each original unquantized model... Table 6 summarizes the results in detecting errors introduced during quantization by DeepHunter+Prob and TensorFuzz.\" \n\nPage 154, Section 5.4 (Answer to RQ4): \"With corner-region based criteria, DeepHunter tends to be more useful for detecting errors introduced during the DNN quantization. A larger QR often introduces larger decision logic inconsistencies before and after quantization.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed framework, DeepHunter, is designed and evaluated for testing a single Deep Neural Network (DNN) model. The methodology, including the test generation algorithm and oracles, is focused on finding defects within one specific target model or between that model and its quantized version. The experimental evaluation is conducted on several different DNNs, but each is tested in isolation. The paper does not include any experiments where fault-inducing inputs generated for one model (e.g., LeNet-5) are used to test another model performing the same task (e.g., a different MNIST classifier). The authors explicitly state this single-model focus in the related work section, distinguishing their approach from differential testing techniques that compare multiple DNNs.",
                    "evidence": "Page 11, Section 6 RELATED WORK, 'Different from DeepXplore and DiffChaser which focus on differential testing, DeepHunter is a grey-box testing technique focusing on a single DNN model.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "DeepHunter explicitly evaluates the transferability of fault\u2010inducing inputs by first generating mutants on an original (32\u2010bit) DNN model and then reusing those same tests to detect errors in its quantized counterparts. Definition 3 formalizes the notion of a \u201cquantization erroneous behavior\u201d (i.e., faults that appear in one model version but not the other), and RQ4\u2019s experimental setup and results demonstrate this cross\u2010model reuse. This matches the \u201cHigh\u201d level of the Attack Transferability metric, since the framework both reuses inputs across models performing the same task (original versus quantized) and incorporates mechanisms to discover these transferable failures.",
                    "evidence": "Page 149, Section 3.2.1 \u2013 Definition 3: Formalizes \u201cquantization erroneous behavior\u201d between F and its quantized version F\u2032, describing tests generated on one model and applied to the other.\nPage 154, Section 5.4 \u2013 RQ4: Describes running DeepHunter on the original model and applying the generated tests to quantized versions, with Table 6 summarizing the errors detected across models."
                },
                "manual": {
                    "value": "Low",
                    "why": "The oracle definitions is that if same input produce different output to a model and its quanitized version, then it is fault-triggering input. But it is not transferability.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic",
                "Differential"
            ]
        }
    },
    "Guo et al. - 2018 - DLFuzz differential fuzzing testing of deep learning systems": {
        "key": "guo2018dlfuzz",
        "name": "DLFuzz",
        "year": "2018",
        "citation_count": "360",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's testing goal and reported failures are limited to causing mispredictions (adversarial examples) and increasing neuron coverage. DLFuzz generates tiny perturbations to inputs and labels as 'incorrect behaviors' any case where the mutated input is classified differently from the original. The paper evaluates success by number of adversarial inputs, L2 perturbation magnitudes, and neuron coverage on standard image classification models (MNIST, ImageNet). It does not describe uncovering unsafe/undesired content (e.g., hallucination, toxicity, bias), nor does it describe bypassing explicit safety, policy, or security mechanisms or testing defended / safety-aligned models for jailbreaks, data leakage, or unauthorized actions. Therefore the failures exposed are robustness/misprediction errors only, corresponding to the 'Low' severity category.",
                    "evidence": "Abstract (page 739): \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\" \n\nSection 3.1 Architecture (page 740): \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.\" \n\nAlgorithm 1 / Section 3.2 (page 740): objective described as jointly maximizing \"#incorrect behaviors & neuron coverage\" (Algorithm 1 line 7 / equation (1)). \n\nSection 4 Results (page 741-742), Table 1: reports metrics such as \"l2 Distance\" and \"#Adv.\" (number of adversarial inputs), and discussion: \"adversarial inputs generated by DLFuzz have much smaller perturbations... DLFuzz averagely generated 338.59% more adversarial inputs with 89.82% smaller perturbations.\" \n\nNo sections or quotes in the paper describe bypassing defenses, attacking safety/policy mechanisms, inducing data leakage, or producing unsafe/toxic outputs; all reported failures are adversarial misclassifications on image classification models (MNIST, ImageNet)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, DLFuzz, is designed to uncover model robustness errors, specifically prediction inconsistencies. The core mechanism identifies a failure when a semantically-preserved mutated input results in a different classification label compared to the original input. The experiments and examples provided in the paper, such as misclassifying a 'candle' as a 'lampshade' or a handwritten '9' as a '4', are classic instances of mispredictions. The paper does not mention or evaluate the fuzzer's ability to uncover unsafe behaviors (like toxicity or bias) or to bypass any explicit safety or security mechanisms.",
                    "evidence": "Page 3, Section 3.1, Architecture: \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs. The inconsistent classification results indicate that at least one of them is wrong so that manually labeling effort is not required here.\" This quote explicitly defines the failure condition as a prediction inconsistency, which aligns with the 'Low' severity definition."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's testing goal and reported failures are limited to causing mispredictions (adversarial examples) and increasing neuron coverage. DLFuzz generates tiny perturbations to inputs and labels as 'incorrect behaviors' any case where the mutated input is classified differently from the original. The paper evaluates success by number of adversarial inputs, L2 perturbation magnitudes, and neuron coverage on standard image classification models (MNIST, ImageNet). It does not describe uncovering unsafe/undesired content (e.g., hallucination, toxicity, bias), nor does it describe bypassing explicit safety, policy, or security mechanisms or testing defended / safety-aligned models for jailbreaks, data leakage, or unauthorized actions. Therefore the failures exposed are robustness/misprediction errors only, corresponding to the 'Low' severity category.",
                    "evidence": "Abstract (page 739): \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\" \n\nSection 3.1 Architecture (page 740): \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.\" \n\nAlgorithm 1 / Section 3.2 (page 740): objective described as jointly maximizing \"#incorrect behaviors & neuron coverage\" (Algorithm 1 line 7 / equation (1)). \n\nSection 4 Results (page 741-742), Table 1: reports metrics such as \"l2 Distance\" and \"#Adv.\" (number of adversarial inputs), and discussion: \"adversarial inputs generated by DLFuzz have much smaller perturbations... DLFuzz averagely generated 338.59% more adversarial inputs with 89.82% smaller perturbations.\" \n\nNo sections or quotes in the paper describe bypassing defenses, attacking safety/policy mechanisms, inducing data leakage, or producing unsafe/toxic outputs; all reported failures are adversarial misclassifications on image classification models (MNIST, ImageNet)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Finds generic misclassifications"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Does not show any correlation between errors and coverage"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "norm-bounded perturbation"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper enforces perceptual plausibility throughout the fuzzing process by measuring and bounding the L2 distance of each mutated input relative to the original input (used as a criterion before keeping a seed), thereby preventing cumulative drift across iterations. In addition, the authors report an explicit manual validation of all generated adversarial inputs (366 images) and state they found no changes of identity and that perturbations are imperceptible. These constitute an enforcement mechanism plus empirical human validation that final fault-inducing inputs remain perceptually realistic.",
                    "evidence": "Section 3.2 (Algorithm) \u2014 (page 741): Algorithm 1, line 15: \"l2_distance = distance(x', x) //measure the perturbation\" and accompanying text: \"As for the conditions of seed keeping in line 16, DLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\" \n\nSection 3.1 (Architecture) \u2014 (page 740): \"The minute perturbation is restricted to be invisible and ensures the prediction results before and after the mutation to be the same.\" \n\nSection 4.3 (Discussion) \u2014 (page 742): \"For confirmation, we checked all the 366 adversarial inputs generated by DLFuzz, though DLFuzz maintains quite small l2 distance by the restricted threshold. We haven't found any adversarial inputs that have already changed their identities after mutation. The adversarial inputs are nearly the same as the original input, and the perturbations are imperceptible.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility in individual mutation steps by using a bounded L2 distance constraint to ensure perturbations are small and imperceptible. The algorithm's design, which uses mutated inputs as new seeds, may overlook the cumulative effect of these small perturbations over many iterations. However, the paper validates the plausibility of the final fault-inducing inputs through manual inspection. The authors state they visually checked all 366 generated adversarial inputs and found them to be imperceptible and nearly identical to the original inputs, which constitutes a form of human-based validation. This combination of bounded per-step constraints and validation of the final outputs, without enforcing plausibility throughout the entire process or using quantitative perceptual metrics, aligns with the 'Medium' rating.",
                    "evidence": "Page 4, Section 4, Fuzzing Process: \"Second, DLFuzz adopts l2 distance to measure the perturbation with similar computation with [12]. As for the conditions of seed keeping in line 16, DLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\"\n\nPage 5, Section 4.3, Discussion: \"Without Manual Effort. For confirmation, we checked all the 366 adversarial inputs generated by DLFuzz, though DLFuzz maintains quite small l2 distance by the restricted threshold. We haven't found any adversarial inputs that have already changed their identities after mutation. The adversarial inputs are nearly the same as the original input, and the perturbations are imperceptible.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "DLFuzz actively enforces plausibility at every fuzzing iteration by bounding the L2 distance of each mutated input to the original seed (Algorithm 1, lines 15\u201316), preventing cumulative drift. Furthermore, the authors conducted a full manual validation of all 366 adversarial examples, reporting that none changed their semantic identity and that all perturbations remained imperceptible. This combination of a global perturbation constraint and explicit human evaluation of the final fault-inducing inputs satisfies the criteria for a \u201cHigh\u201d rating.",
                    "evidence": "Section 3.2, Algorithm 1 (page 741): \u201cl2_distance = distance(x\u2032, x) // measure the perturbation\u201d and \u201cD\u2009Lfuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\u201d\n\nSection 4.3, Discussion (page 742): \u201cFor confirmation, we checked all the 366 adversarial inputs generated by DLFuzz\u2026 We haven\u2019t found any adversarial inputs that have already changed their identities after mutation. The adversarial inputs are nearly the same as the original input, and the perturbations are imperceptible.\u201d"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Bound perturbation relative to parent seed but that can already mutated multiple time. No control over cumulative deviation across iterations",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": "relies on imperceptible, float-level perturbations that can vanish after quantization or saving, with no persistence verification."
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "DLFuzz performs fine-grained, gradient-based in-memory perturbations (x' = x + perturbation) and constrains L2 distance to keep changes imperceptible, but the paper contains no explicit handling of standard I/O serialization effects (clipping, quantization, rounding) nor does it state that inputs are generated or validated in serialized formats. Because the method mutates floating-point tensors in memory and evaluates the oracle on those in-memory inputs without accounting for how those perturbations would be altered or lost when saved to common storage formats (e.g., 8-bit images), failures discovered by DLFuzz may be fragile and could be lost after standard I/O operations. This matches the metric definition for Low (ignores I/O effects during mutation and oracle evaluation).",
                    "evidence": "Page 741, Section 3.2 Algorithm: \"perturbation = processing(grads) ... x' = xs + perturbation\" and \"l2_distance = distance(x', x) ... DLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\"; Page 741, Section 4.1 Experiment Setup: \"We implemented DLFuzz based on the widespread frameworks, Tensorflow 1.2.1 and Keras 2.1.3. Tensorflow and Keras provide the efficient interfaces for computing the gradients and recording the intermediate output of all neurons after each prediction of the DNN.\"; Page 742, Section 4.3 Discussion (Without Manual Effort): \"For confirmation, we checked all the 366 adversarial inputs generated by DLFuzz ... We haven't found any adversarial inputs that have already changed their identities after mutation.\" (The paper does not describe any explicit handling of serialization, clipping, or quantization when generating or validating mutated inputs.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper describes DLFuzz's mutation strategy as adding small, gradient-based numerical perturbations directly to the input tensor in memory. The magnitude of these perturbations is controlled by keeping the L2 distance small to ensure they are imperceptible. The methodology does not mention any steps to account for standard I/O operations like clipping, rounding, or quantization. Small numerical changes, such as those generated by this method, are highly likely to be lost or altered when the input is serialized (e.g., saved as an 8-bit image), making the discovered failures fragile and not guaranteed to be reproducible. The paper's description of the mutation and oracle evaluation process completely ignores these serialization effects.",
                    "evidence": "Page 4, Algorithm 1, line 12: The mutation is a direct numerical addition: `x' = xs + perturbation //mutated input obtained`. Page 4, Section 4, Fuzzing Process: The paper confirms the use of small numerical perturbations measured by L2 distance: 'DLFuzz adopts l2 distance to measure the perturbation with similar computation with [12]. As for the conditions of seed keeping in line 16, DLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.' The paper contains no mention of clipping pixel values to a valid range (e.g., [0, 255]) or any other serialization-aware processing."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DLFuzz performs fine-grained, gradient-based in-memory perturbations (x' = x + perturbation) and constrains L2 distance to keep changes imperceptible, but the paper contains no explicit handling of standard I/O serialization effects (clipping, quantization, rounding) nor does it state that inputs are generated or validated in serialized formats. Because the method mutates floating-point tensors in memory and evaluates the oracle on those in-memory inputs without accounting for how those perturbations would be altered or lost when saved to common storage formats (e.g., 8-bit images), failures discovered by DLFuzz may be fragile and could be lost after standard I/O operations. This matches the metric definition for Low (ignores I/O effects during mutation and oracle evaluation).",
                    "evidence": "Page 741, Section 3.2 Algorithm: \"perturbation = processing(grads) ... x' = xs + perturbation\" and \"l2_distance = distance(x', x) ... DLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\"; Page 741, Section 4.1 Experiment Setup: \"We implemented DLFuzz based on the widespread frameworks, Tensorflow 1.2.1 and Keras 2.1.3. Tensorflow and Keras provide the efficient interfaces for computing the gradients and recording the intermediate output of all neurons after each prediction of the DNN.\"; Page 742, Section 4.3 Discussion (Without Manual Effort): \"For confirmation, we checked all the 366 adversarial inputs generated by DLFuzz ... We haven't found any adversarial inputs that have already changed their identities after mutation.\" (The paper does not describe any explicit handling of serialization, clipping, or quantization when generating or validating mutated inputs.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across different models. DLFuzz is explicitly designed to avoid cross-referencing multiple models (unlike DeepXplore) and treats an input as a fault when the mutated input and the original input produce different labels on the same CNN under test. The evaluation compares DLFuzz to DeepXplore on per-model metrics (neuron coverage, number of adversarial inputs, perturbation size, time) but does not reuse adversarial inputs generated on one model to test other models or report whether such inputs trigger failures across models. The only related experiment (augmenting training sets) retrains the same CNNs with adversarial images and is not a transferability test. Therefore the paper does not demonstrate transferable attacks nor include mechanisms explicitly aimed at discovering transferable failures across models.",
                    "evidence": "Abstract (p.739, Abstract): \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\" \n\nSection 3.1 Architecture (p.740, 3.1 Architecture): \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.\" \n\nIntroduction / Evaluation summary (p.740, end of Intro / start of 3): \"Compared with DeepXplore, DLFuzz does not need extra efforts to collect similar functional DL systems for cross-referencing label check...\" \n\nResults (p.742, 4.2 Result): \"we incorporated 114 adversarial images into the training set of three CNNs on MNIST and retrained them trying to increase their accuracy.\" (This describes retraining with generated adversarial inputs, not testing those inputs on other, unseen models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper presents DLFuzz as a method for testing a single Deep Learning (DL) system in isolation. A core contribution highlighted is its ability to find incorrect behaviors without needing \"cross-referencing oracles from other DL systems with the same functionality.\" The methodology focuses on generating adversarial inputs for one specific model under test by maximizing prediction differences between an original and a mutated input for that same model. The experimental evaluation, while conducted on multiple models (e.g., LeNet-1, LeNet-4, VGG-16), treats each model as an independent target. The results are reported on a per-model basis, and there is no mention of any experiment where fault-inducing inputs generated for one model are used to test another model to assess transferability.",
                    "evidence": "Page 2, Abstract: \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\"\n\nPage 5, Section 5, Conclusion: \"DLFuzz also overcomes the trouble of relying on multiple DL systems of the similar functionality in DeepXplore.\"\n\nPage 5, Table 1: \"Effectiveness of DLFuzz compared with DeepXplore.\" This table presents the results for each model (LeNet-1, LeNet-4, LeNet-5, VGG16, VGG19, ResNet50) individually, comparing DLFuzz to DeepXplore on that specific model. There is no data or discussion regarding the transferability of adversarial inputs generated for one model to any of the others."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across different models. DLFuzz is explicitly designed to avoid cross-referencing multiple models (unlike DeepXplore) and treats an input as a fault when the mutated input and the original input produce different labels on the same CNN under test. The evaluation compares DLFuzz to DeepXplore on per-model metrics (neuron coverage, number of adversarial inputs, perturbation size, time) but does not reuse adversarial inputs generated on one model to test other models or report whether such inputs trigger failures across models. The only related experiment (augmenting training sets) retrains the same CNNs with adversarial images and is not a transferability test. Therefore the paper does not demonstrate transferable attacks nor include mechanisms explicitly aimed at discovering transferable failures across models.",
                    "evidence": "Abstract (p.739, Abstract): \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\" \n\nSection 3.1 Architecture (p.740, 3.1 Architecture): \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.\" \n\nIntroduction / Evaluation summary (p.740, end of Intro / start of 3): \"Compared with DeepXplore, DLFuzz does not need extra efforts to collect similar functional DL systems for cross-referencing label check...\" \n\nResults (p.742, 4.2 Result): \"we incorporated 114 adversarial images into the training set of three CNNs on MNIST and retrained them trying to increase their accuracy.\" (This describes retraining with generated adversarial inputs, not testing those inputs on other, unseen models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Whitebox"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Prediction-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Yu et al. - LLM-Fuzzer Scaling Assessment of Large Language Model Jailbreaks": {
        "key": "yu2024llm",
        "name": "LLM-Fuzzer",
        "year": "2024",
        "citation_count": "40",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper's method is explicitly designed to find and produce jailbreak prompts that bypass safety/alignment mechanisms in defended models. The authors demonstrate that LLM-FUZZER generates prompts that successfully induce harmful outputs from well-aligned and commercial LLMs (quantified across many models and questions), showing it uncovers high-impact violations of safety/policy safeguards rather than mere low-level robustness errors.",
                    "evidence": "Page 4657, Abstract: \"Our experiments show that LLM-FUZZER-generated jailbreak prompts demonstrate significantly increased effectiveness and transferability. This highlights that many open-source and commercial LLMs suffer from severe jailbreak issues, even after safety fine-tuning.\"; Page 4659, Introduction (Section 1): \"...despite dedicated efforts in training set filtering and safety alignment, all existing open-source and commercial LLMs continue to exhibit vulnerabilities to jailbreak prompts.\"; Page 4658, Figure 1 caption: \"In a typical scenario, ChatGPT would reject the harmful question. When this question is embedded within the jailbreak template, the resulting prompt can fool the gpt-3.5-turbo-0301 model into generating a potentially harmful response.\"; Page 4664, Results for Experiment I (Section 4.3): \"Among the 100 unethical questions where human-written templates all failed to jailbreak gpt-3.5-turbo-0125, our fuzzer was capable of generating templates to assist on average 96.85 questions in achieving their jailbreaking objectives through template mutations.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper introduces LLM-Fuzzer, a method explicitly designed to find and scale the assessment of \"jailbreaks\" in Large Language Models. A jailbreak is, by definition, a bypass of a model's explicit safety and policy mechanisms to elicit harmful or prohibited content. The method is evaluated against contemporary, safety-aligned models such as gpt-3.5-turbo-0125 and Llama-2-7B-Chat. The stated goal is to generate prompts that successfully cause these models to produce outputs for \"unethical questions\" that they would normally refuse, which directly corresponds to the \"High\" severity category of uncovering high-impact failures by bypassing explicit safety mechanisms in defended models.",
                    "evidence": "Page 10, Section 4.2, Experiment Design: \"Can our fuzzer generate new templates to successfully facilitate jailbreak attempts for unethical questions that the seed templates fail to exploit against gpt-3.5-turbo-0125 and Llama-2-7B-Chat? If so, what is the average query budget and token budget required for each unethical question?\" This quote clearly states that the fuzzer's objective is to create successful jailbreak attempts against specific, well-known, safety-aligned LLMs for questions they are designed to resist."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper's method is explicitly designed to find and produce jailbreak prompts that bypass safety/alignment mechanisms in defended models. The authors demonstrate that LLM-FUZZER generates prompts that successfully induce harmful outputs from well-aligned and commercial LLMs (quantified across many models and questions), showing it uncovers high-impact violations of safety/policy safeguards rather than mere low-level robustness errors.",
                    "evidence": "Page 4657, Abstract: \"Our experiments show that LLM-FUZZER-generated jailbreak prompts demonstrate significantly increased effectiveness and transferability. This highlights that many open-source and commercial LLMs suffer from severe jailbreak issues, even after safety fine-tuning.\"; Page 4659, Introduction (Section 1): \"...despite dedicated efforts in training set filtering and safety alignment, all existing open-source and commercial LLMs continue to exhibit vulnerabilities to jailbreak prompts.\"; Page 4658, Figure 1 caption: \"In a typical scenario, ChatGPT would reject the harmful question. When this question is embedded within the jailbreak template, the resulting prompt can fool the gpt-3.5-turbo-0301 model into generating a potentially harmful response.\"; Page 4664, Results for Experiment I (Section 4.3): \"Among the 100 unethical questions where human-written templates all failed to jailbreak gpt-3.5-turbo-0125, our fuzzer was capable of generating templates to assist on average 96.85 questions in achieving their jailbreaking objectives through template mutations.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "aims to generate many jailbreak templates that trigger harmful content without specifying exact outcomes"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Does not provide any explanation why those errors occur."
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper designs mutation operators that explicitly aim to preserve semantic correctness (using LLM-assisted Generate/Crossover/Expand/Shorten/Rephrase operators) and adds a seed-selection reward discount (\u03b1 * path length) and early-termination probability to discourage deep mutation chains, i.e., mechanisms that constrain individual mutation steps and help limit cumulative drift. However, the paper does not present any empirical assessment of the perceptual or semantic plausibility of the final fault-inducing prompts (no human evaluation or task-specific quantitative measures such as perplexity or semantic-similarity scores are reported). Thus the work enforces plausibility in mutation design and includes measures to reduce accumulation effects, but it does not validate final-input plausibility empirically \u2014 matching the \"Medium\" definition.",
                    "evidence": "Section 3.3 Mutation (page 4662): \"To maintain the semantic correctness of newly generated prompts, we have introduced five innovative mutation operators that harness LLMs as assistants for prompt alteration while preserving their semantic integrity.\"; Section 3.2 Seed Selection / Algorithm 1 (page 4661): algorithm lines show reward discounting to penalize deep paths: \"if reward > 0 then\\n    reward \u2190 max(reward \u2212 \u03b1 * len(path), \u03b2)\" (Algorithm 1, Update); Section 3.3 (page 4662): \"To showcase that the newly generated templates can maintain their semantic integrity and diversify our template collection, Figure 3 presents several post-mutated templates.\" \u2014 i.e., qualitative examples only; Section 3.4 Oracle (page 4663): \"we harnessed these annotated responses to fine-tune a RoBERTa model... this RoBERTa model serves as our designated 'oracle'.\" \u2014 oracle evaluates harmfulness of outputs, not input plausibility; Section 4.1 Metrics (page 4664): defines evaluation metrics (JQN, ASR, EASR, QBC, TBC) with no mention of human evaluation or quantitative plausibility measures (e.g., perplexity or semantic-similarity), and no human study of final prompts is reported. Together these passages show plausibility is enforced in mutation steps and some controls for accumulation exist, but final-input plausibility is not empirically validated."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's methodology is explicitly designed to maintain the plausibility and semantic integrity of the fuzzer's inputs. The authors identify the shortcomings of traditional mutation techniques, which can produce \"syntactically incorrect or semantically nonsensical inputs\" when applied to natural language. To overcome this, their core mutation strategy leverages LLMs to perform all transformations (Generate, Crossover, Expand, Shorten, Rephrase). This design inherently ensures that mutated inputs remain within the plausible distribution of coherent, human-readable text throughout all fuzzing iterations. Furthermore, the paper provides empirical evidence to support this claim by presenting multiple examples of mutated templates and explicitly stating that this is done to \"showcase that the newly generated templates can maintain their semantic integrity\". These examples serve as qualitative validation that the final fault-inducing inputs remain semantically realistic.",
                    "evidence": "Page 7, Section 3.3 Mutation: \"Directly applying these techniques [traditional program fuzzing] to natural language inputs can result in syntactically incorrect or semantically nonsensical inputs, which are unlikely to be effective in jailbreaking LLMs. To address this challenge, we introduce distinct mutation methods that leverage LLMs themselves to assist in the mutation process. LLMs, with their proficiency in understanding and generating human-like text, offer a promising approach for crafting coherent and contextually relevant mutations... In order to showcase that the newly generated templates can maintain their semantic integrity and diversify our template collection, Figure 3 presents several post-mutated templates.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper incorporates mechanisms to enforce semantic plausibility during each mutation step\u2014namely, LLM-assisted Generate/Crossover/Expand/Shorten/Rephrase operators\u2014and penalizes overly deep mutation chains via reward discounting and early\u2010termination in the MCTS-Explore seed selection (Algorithm 1). However, it does not provide any explicit empirical evaluation of the final fault-inducing prompts\u2019 plausibility (no human studies or quantitative metrics such as perplexity or semantic-similarity scores are reported). Thus, while plausibility is enforced in mutation design and some mitigation against cumulative drift is applied, there is no empirical validation of final-input realism, matching the \u201cMedium\u201d level.",
                    "evidence": "Section 3.3 Mutation (page 4662): \u201cTo maintain the semantic correctness of newly generated prompts, we have introduced five innovative mutation operators that harness LLMs\u2026\u2009\u201d and Figure 3 qualitative examples; Algorithm 1 Update (page 4661): reward \u2190 max(reward \u2212 \u03b1*len(path),\u03b2) and early-termination to limit deep mutation chains; Section 4.1 Metrics (page 4664): defines JQN, ASR, EASR, QBC, TBC with no mention of human evaluation or task-specific measures (e.g., perplexity, SSIM) to assess input plausibility."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Mutations are purely at the text level"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper's fuzzer generates and mutates textual jailbreak templates/prompts (not fine-grained numeric tensor perturbations) and feeds those serialized text prompts to LLMs; the oracle likewise evaluates textual responses. Because inputs are produced directly as text (i.e., already in a serialized format stable to quantization/clipping), the fault-inducing test cases are inherently stable under standard I/O serialization and are not fragile numerical memory artifacts. This matches the metric's allowance that approaches which generate inputs directly in serialized formats (e.g., generated text) are reproducible.",
                    "evidence": "Page 4657, Abstract: \"LLM-FUZZER uses human-crafted jailbreak prompts as starting points. By employing carefully customized seed selection strategies and mutation mechanisms, LLM-FUZZER generates additional jailbreak prompts tailored to specific LLMs.\" \n\nPage 4660, Section 3.1 (Proposed Technique / Technical Overview): \"LLM-FUZZER ... begins with a corpus comprising human-written jailbreak templates, serving as the initial seed pool. In each iteration, LLM-FUZZER selects a template from this pool and randomly chooses a mutator to apply the mutation to create a new jailbreak template. Subsequently, it combines this newly generated template with a target question to form a prompt.\" \n\nPage 4662, Section 3.3 (Mutation): \"In this study, we introduce five distinct mutation schemes... Generate: This mutation operator employs an LLM by providing it with a template and instructing the model to generate a new template in a similar style but with different content.... Rephrase: ... instructing the LLM to rephrase every sentence within the template while maintaining the overall meaning.\" \n\nPage 4663, Section 3.4 (Oracle): \"The oracle\u2019s role is to take prompts as input and evaluate the responses to determine if they are harmful. If harmful responses are detected, the oracle assigns a corresponding reward.\" \n\nAppendix D.1 (Mutate Prompt examples): \"I need you to generate one template. I will give you one template example... You are required to write one example with similar style but different content and MUST have '[INSERT PROMPT HERE]' in your template.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The method proposed in the paper, LLM-Fuzzer, operates entirely on textual data. The mutation operators generate new jailbreak templates by using a large language model to modify existing text templates. The resulting test cases are text prompts, which are an inherently serialized format. The metric definition explicitly states: 'We also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., synthesized speech, rendered images, or generated text) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.' LLM-Fuzzer's approach of generating text-based inputs perfectly aligns with this condition. Unlike numerical perturbations in image or audio data that can be lost through quantization or rounding, the textual perturbations generated by LLM-Fuzzer are perfectly preserved during standard I/O operations (e.g., saving to a file), ensuring failure reproducibility.",
                    "evidence": "Page 7, Section 3.3, Mutation: The paper describes its mutation operators as text-to-text transformations performed by an LLM. For instance: 'Generate: This mutation operator employs an LLM by providing it with a template and instructing the model to generate a new template in a similar style but with different content.' The other operators (Crossover, Expand, Shorten, Rephrase) are also described as manipulating text. This confirms that the test cases are generated directly in their final, serialized text format."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper's fuzzer generates and mutates textual jailbreak templates/prompts (not fine-grained numeric tensor perturbations) and feeds those serialized text prompts to LLMs; the oracle likewise evaluates textual responses. Because inputs are produced directly as text (i.e., already in a serialized format stable to quantization/clipping), the fault-inducing test cases are inherently stable under standard I/O serialization and are not fragile numerical memory artifacts. This matches the metric's allowance that approaches which generate inputs directly in serialized formats (e.g., generated text) are reproducible.",
                    "evidence": "Page 4657, Abstract: \"LLM-FUZZER uses human-crafted jailbreak prompts as starting points. By employing carefully customized seed selection strategies and mutation mechanisms, LLM-FUZZER generates additional jailbreak prompts tailored to specific LLMs.\" \n\nPage 4660, Section 3.1 (Proposed Technique / Technical Overview): \"LLM-FUZZER ... begins with a corpus comprising human-written jailbreak templates, serving as the initial seed pool. In each iteration, LLM-FUZZER selects a template from this pool and randomly chooses a mutator to apply the mutation to create a new jailbreak template. Subsequently, it combines this newly generated template with a target question to form a prompt.\" \n\nPage 4662, Section 3.3 (Mutation): \"In this study, we introduce five distinct mutation schemes... Generate: This mutation operator employs an LLM by providing it with a template and instructing the model to generate a new template in a similar style but with different content.... Rephrase: ... instructing the LLM to rephrase every sentence within the template while maintaining the overall meaning.\" \n\nPage 4663, Section 3.4 (Oracle): \"The oracle\u2019s role is to take prompts as input and evaluate the responses to determine if they are harmful. If harmful responses are detected, the oracle assigns a corresponding reward.\" \n\nAppendix D.1 (Mutate Prompt examples): \"I need you to generate one template. I will give you one template example... You are required to write one example with similar style but different content and MUST have '[INSERT PROMPT HERE]' in your template.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly evaluates transferability: templates (fault-inducing inputs) generated by LLM-FUZZER on a set of surrogate models are reused to test many other models, and the fuzzer's design includes an explicit mechanism (oracle reward modification) to encourage generation of templates that succeed across multiple models. The authors both describe this cross-model generation/evaluation procedure (Experiment III) and report strong transfer results (EASR > 80% on many target models) and comparisons to other methods.",
                    "evidence": "Section 4.2 Experiment III (p.4664\u20134665): \"To address the question of transferability, we chose to create templates using gpt-3.5-turbo-0125, Llama-2-7B-Chat, and Vicuna-7B.... in the template reward assignment by the oracle, a zero reward was imposed if the template\u2019s all successful jailbreak attempts were attributed solely to one or two LLMs.\" \n\nSection 4.2 Experiment III (p.4665): \"Following the exhaustion of the 150k query budget, our experiment involved calculating the average ASR for each template generated across the three target models. The template with the highest average ASR, denoted as 'top-1', and the top five templates with the highest average ASR, known as 'top-5', were selected for further experimentation.\" \n\nSection 4.3 Results for Experiment III (p.4666\u20134668): \"These templates displayed a notable degree of transferability... As depicted in Figure 4, when evaluated against Vicuna-13B, Baichuan-13B, ChatGLM-6B, Llama-2-13B/70B, Claude1, and PaLM2, the EASR for LLM-FUZZER consistently exceeded 80%, having a huge advantage over EASRs of other methods.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper satisfies both conditions for a 'High' rating. First, it explicitly tests the transferability of the jailbreak templates generated by LLM-FUZZER. Templates are created using a set of three models and then evaluated against a larger, diverse set of unseen open-source and commercial LLMs. Second, the fuzzing design incorporates an explicit mechanism to support the discovery of transferable failures. Specifically for the transferability experiment, the reward mechanism of the fuzzer is modified to query multiple models simultaneously and penalize templates that are not universally effective, thereby actively guiding the fuzzer towards generating transferable jailbreaks.",
                    "evidence": "Page 10-11, Section 4.2, Experiment Design: \"Experiment III. To address the question of transferability, we chose to create templates using gpt-3.5-turbo-0125, Llama-2-7B-Chat, and Vicuna-7B. This approach was taken to ensure that the generated templates would be applicable across various LLMs. We followed a similar procedure with the previous experiment but made two slight modifications. First, in each fuzzing iteration, we queried 100 questions for each template across all three models, resulting in a total of 300 responses. Second, in the template reward assignment by the oracle, a zero reward was imposed if the template's all successful jailbreak attempts were attributed solely to one or two LLMs. This modification aimed to bolster universality by discouraging the generation of templates that could exploit only one or two specific LLMs.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly evaluates transferability: templates (fault-inducing inputs) generated by LLM-FUZZER on a set of surrogate models are reused to test many other models, and the fuzzer's design includes an explicit mechanism (oracle reward modification) to encourage generation of templates that succeed across multiple models. The authors both describe this cross-model generation/evaluation procedure (Experiment III) and report strong transfer results (EASR > 80% on many target models) and comparisons to other methods.",
                    "evidence": "Section 4.2 Experiment III (p.4664\u20134665): \"To address the question of transferability, we chose to create templates using gpt-3.5-turbo-0125, Llama-2-7B-Chat, and Vicuna-7B.... in the template reward assignment by the oracle, a zero reward was imposed if the template\u2019s all successful jailbreak attempts were attributed solely to one or two LLMs.\" \n\nSection 4.2 Experiment III (p.4665): \"Following the exhaustion of the 150k query budget, our experiment involved calculating the average ASR for each template generated across the three target models. The template with the highest average ASR, denoted as 'top-1', and the top five templates with the highest average ASR, known as 'top-5', were selected for further experimentation.\" \n\nSection 4.3 Results for Experiment III (p.4666\u20134668): \"These templates displayed a notable degree of transferability... As depicted in Figure 4, when evaluated against Vicuna-13B, Baichuan-13B, ChatGLM-6B, Llama-2-13B/70B, Claude1, and PaLM2, the EASR for LLM-FUZZER consistently exceeded 80%, having a huge advantage over EASRs of other methods.\""
                },
                "manual": {
                    "value": "High",
                    "why": "Transferability is encouraged in mutation by rewarding less to mutations that transfer less.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "Tian et al. - 2018 - DeepTest automated testing of deep-neural-network-driven autonomous cars": {
        "key": "tian2018deeptest",
        "name": "DeepTest",
        "year": "2018",
        "citation_count": "1791",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's DeepTest method uncovers many unsafe, high-impact functional failures (steering-angle deviations that can lead to potentially fatal crashes) and reports thousands of such erroneous behaviors in evaluated driving models. However, the evaluation targets standard Udacity driving models and the paper does not claim or demonstrate bypassing explicit safety, policy, or security mechanisms (e.g., adversarially-trained or safety-aligned models) nor uncover attacks like data leakage or jailbreaks. Therefore the failures exposed violate user safety expectations (unsafe steering) but do not show bypassing of explicit defenses, matching the 'Medium' category.",
                    "evidence": "Page 303, Abstract: \"DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.\"; Page 306, Section 3.4 (Creating a Test Oracle with Metamorphic Relations): \"the autonomous car\u2019s steering angle should not change significantly for the same image under any lighting/weather conditions, blurring, or any affine transformations with small parameter values.\"; Page 311, Results / RQ4: \"In total, DeepTest detects 6339 erroneous behaviors across all three models.\"; Page 308, Section 4 (Implementation): \"We evaluate our techniques on three DNN models that won top positions in the Udacity self-driving challenge ... Rambo, Chauffeur, and Epoch.\" (no mention of testing defended/adversarially-trained or safety-aligned models or bypassing explicit safety or security mechanisms)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper introduces DeepTest, a tool for automatically detecting erroneous behaviors in DNNs for autonomous cars. The failures it uncovers are significant deviations in steering angle under realistic, synthetically generated environmental conditions like rain, fog, and different lighting. These incorrect steering predictions can lead to \"potentially fatal crashes,\" which are clear violations of safety expectations and constitute unsafe behavior. The evaluation demonstrates finding thousands of such dangerous behaviors. The models under test are standard DNNs from a driving challenge and are not described as having explicit safety or security mechanisms that DeepTest bypasses. Therefore, the failures align with the 'Medium' severity level, as the tool uncovers unsafe behavior without the context of bypassing explicit defenses.",
                    "evidence": "Page 2, Abstract: \"In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. ... DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.\"\n\nPage 3, Figure 1 Caption: \"A sample dangerous erroneous behavior found by DeepTest in the Chauffeur DNN.\" (The accompanying image shows the car steering off the road in a synthetically generated rainy condition)."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper's DeepTest method uncovers many unsafe, high-impact functional failures (steering-angle deviations that can lead to potentially fatal crashes) and reports thousands of such erroneous behaviors in evaluated driving models. However, the evaluation targets standard Udacity driving models and the paper does not claim or demonstrate bypassing explicit safety, policy, or security mechanisms (e.g., adversarially-trained or safety-aligned models) nor uncover attacks like data leakage or jailbreaks. Therefore the failures exposed violate user safety expectations (unsafe steering) but do not show bypassing of explicit defenses, matching the 'Medium' category.",
                    "evidence": "Page 303, Abstract: \"DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.\"; Page 306, Section 3.4 (Creating a Test Oracle with Metamorphic Relations): \"the autonomous car\u2019s steering angle should not change significantly for the same image under any lighting/weather conditions, blurring, or any affine transformations with small parameter values.\"; Page 311, Results / RQ4: \"In total, DeepTest detects 6339 erroneous behaviors across all three models.\"; Page 308, Section 4 (Implementation): \"We evaluate our techniques on three DNN models that won top positions in the Udacity self-driving challenge ... Rambo, Chauffeur, and Epoch.\" (no mention of testing defended/adversarially-trained or safety-aligned models or bypassing explicit safety or security mechanisms)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Aims to find errors in steering angle predictions under realistic environmental variations."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Shows correlation between coverage and errors"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Does not verify the naturalness of the fault-revealing mutated images through any human study or algorithmic naturalness metric. Individual transformations are naturalistic but chained mutations might not be"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "DeepTest explicitly restricts individual mutation steps to realistic, domain-specific image transformations (brightness, contrast, affine transforms, blur, rain/fog composed with Photoshop filters) and specifies bounded parameter ranges (Table 4). It also applies a quantitative filter (Equation 3) and manual inspection to reduce implausible / false-positive reports, i.e., it validates plausibility of final fault-inducing inputs. However, the fuzzing algorithm (Algorithm 1) composes multiple transformations without an explicit mechanism that enforces plausibility across cumulative iterations (no distributional constraint or perceptual-similarity metric is enforced during generation), and the authors acknowledge that generated images \u201cmay not be exactly reproducible in reality.\u201d Thus DeepTest validates final inputs and constrains single-step mutations, but it does not fully prevent or measure cumulative drift with perceptual metrics or a human study, matching the paper to the Medium level.",
                    "evidence": "Section 3.2 (p.306): \"we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect).\"; Table 4 (p.306): lists bounded parameter ranges for each transformation (e.g., Translation (tx, ty) (10, 10) to (100, 100) step (10, 10); Rotation q 3 to 30 with step 3; Contrast \u03b1 1.2 to 3.0 step 0.2; Brightness \u03b2 10 to 100 step 10), showing per-step constraints; Algorithm 1 (Section 3.3, p.307): shows composition of transformations across iterations (e.g., \"Randomly pick parameter P1 for T1 ... newImage = ApplyTransforms(image, T1, P1, T2, P2)\"), indicating cumulative mutation without an explicit plausibility-enforcing constraint across iterations; Section 3.4 (p.309\u2013310): defines metamorphic oracle and filtering: \"(\u02c6\u03b8i \u2212 \u03b8t_i)^2 \u2264 \u03bb MSEorig\" (Eqn.2) and the filtration criterion \"| MSE(trans,param) \u2212 MSEorig | \u2264 \u03f5 (3)\" and states \"Thus, we only choose the transformations that obey Equation 3 for counting erroneous behaviors.\" \u2014 this is a quantitative validation/filter applied to final inputs; Section 6 Threats to Validity (p.311): \"DeepTest generates realistic synthetic images by applying different image transformations on the seed images. However, these transformations are not designed to be exhaustive and therefore may not cover all realistic cases... the generated pictures may not be exactly reproducible in reality due to a large number of unpredictable factors...\"; Section 5 (p.311): \"We also manually checked the bugs reported in Table 8 and report the false positives in Figure 8... all the authors have reviewed the images and agreed on the false positives.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The method is explicitly designed to generate inputs by mimicking realistic, real-world phenomena. The transformations applied to seed images include weather effects (rain, fog), lighting changes (brightness, contrast), and physical changes (rotation, shear, scaling) intended to be plausible. This enforces plausibility at the level of individual mutation steps. However, the paper's greedy algorithm for combining these transformations is driven solely by increasing neuron coverage, without an explicit mechanism to check if the combined transformations result in a plausible image, thus potentially overlooking cumulative effects. The paper does perform a final assessment of the plausibility of the failure-inducing inputs, but this is an informal, manual check by the authors rather than a formal human study or the use of quantitative perceptual metrics.",
                    "evidence": "Page 5, Section 3.2, 'Increasing Coverage with Synthetic Images': 'Therefore, DeepTest focuses on generating realistic synthetic images by applying image transformations on seed images and mimic different real-world phenomena like camera lens distortions, object movements, different weather conditions, etc. To this end, we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect).' \nPage 6, Section 3.3, 'Algorithm 1': The greedy search algorithm for combining transformations checks only for coverage increase ('if covInc(newimage) then') and does not include a plausibility check for the resulting 'newImage'.\nPage 10, Section 5, 'RQ4': 'We also manually checked the bugs reported in Table 8 and report the false positives in Figure 8. It also shows two synthetic images (the corresponding original images) where DeepTest incorrectly reports erroneous behaviors while the model\u2019s output is indeed safe. Although such manual investigation is, by definition, subjective and approximate, all the authors have reviewed the images and agreed on the false positives.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "DeepTest explicitly restricts individual mutation steps to realistic, domain-specific image transformations (brightness, contrast, affine transforms, blur, rain/fog composed with Photoshop filters) and specifies bounded parameter ranges (Table 4). It also applies a quantitative filter (Equation 3) and manual inspection to reduce implausible / false-positive reports, i.e., it validates plausibility of final fault-inducing inputs. However, the fuzzing algorithm (Algorithm 1) composes multiple transformations without an explicit mechanism that enforces plausibility across cumulative iterations (no distributional constraint or perceptual-similarity metric is enforced during generation), and the authors acknowledge that generated images \u201cmay not be exactly reproducible in reality.\u201d Thus DeepTest validates final inputs and constrains single-step mutations, but it does not fully prevent or measure cumulative drift with perceptual metrics or a human study, matching the paper to the Medium level.",
                    "evidence": "Section 3.2 (p.306): \"we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect).\"; Table 4 (p.306): lists bounded parameter ranges for each transformation (e.g., Translation (tx, ty) (10, 10) to (100, 100) step (10, 10); Rotation q 3 to 30 with step 3; Contrast \u03b1 1.2 to 3.0 step 0.2; Brightness \u03b2 10 to 100 step 10), showing per-step constraints; Algorithm 1 (Section 3.3, p.307): shows composition of transformations across iterations (e.g., \"Randomly pick parameter P1 for T1 ... newImage = ApplyTransforms(image, T1, P1, T2, P2)\"), indicating cumulative mutation without an explicit plausibility-enforcing constraint across iterations; Section 3.4 (p.309\u2013310): defines metamorphic oracle and filtering: \"(\u02c6\u03b8i \u2212 \u03b8t_i)^2 \u2264 \u03bb MSEorig\" (Eqn.2) and the filtration criterion \"| MSE(trans,param) \u2212 MSEorig | \u2264 \u03f5 (3)\" and states \"Thus, we only choose the transformations that obey Equation 3 for counting erroneous behaviors.\" \u2014 this is a quantitative validation/filter applied to final inputs; Section 6 Threats to Validity (p.311): \"DeepTest generates realistic synthetic images by applying different image transformations on the seed images. However, these transformations are not designed to be exhaustive and therefore may not cover all realistic cases... the generated pictures may not be exactly reproducible in reality due to a large number of unpredictable factors...\"; Section 5 (p.311): \"We also manually checked the bugs reported in Table 8 and report the false positives in Figure 8... all the authors have reviewed the images and agreed on the false positives.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Performs semantic, real-world image transformations (brightness, contrast, blur, fog, rain, etc.) but does not ensure IO stability using clipping and rounding"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "DeepTest generates test inputs by applying semantic, metamorphic image transformations (brightness, contrast, affine transforms, blur, rain/fog, etc.) and uses those transformed images as tests. This approach produces coarse, perceptible changes that are likely to survive standard image serialization, and the paper explicitly uses metamorphic relations as the oracle. However, the paper does not explicitly reason about or constrain low-level numerical effects of serialization (quantization, rounding, clipping) nor does it describe steps to ensure that fault-inducing perturbations survive typical I/O formats. Therefore DeepTest partially considers I/O effects via use of semantic image transformations (which tends to improve reproducibility) but does not explicitly enforce or evaluate reproducibility under serialization, matching the definition of the metric's Medium level.",
                    "evidence": "Page 306, Section 3.2 (Increasing Coverage with Synthetic Images): \"Therefore, DeepTest focuses on generating realistic synthetic images by applying image transformations on seed images and mimic different real-world phenomena like camera lens distortions, object movements, different weather conditions, etc. To this end, we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect).\"; Page 307, Section 4 (Implementation) / \"Image transformations\": \"In the experiments for RQ2 and RQ3, we leverage seven different types of simple image transformations: translation, scaling, horizontal shearing, rotation, contrast adjustment, brightness adjustment, and blurring. We use OpenCV to implement these transformations [7].\"; Page 307, Section 3.4 (Creating a Test Oracle with Metamorphic Relations): \"The key insight is that even though it is hard to specify the correct behavior of a self-driving car for every transformed image, one can define relationships between the car\u2019s behaviors across certain types of transformations. For example, the autonomous car\u2019s steering angle should not change significantly for the same image under any lighting/weather conditions, blurring, or any affine transformations with small parameter values.\"; Page 311, Section 6 (Threats to Validity): \"While our transformations like rain and fog effects are designed to be realistic, the generated pictures may not be exactly reproducible in reality due to a large number of unpredictable factors, e.g., the position of the sun, the angle and size of the rain drops. etc.\"; Page 304, Introduction (Contributions): \"We have made the erroneous behaviors detected by DeepTest available at https://deeplearningtest.github.io/deepTest/. We also plan to release the generated test images and the source of DeepTest for public use.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, DeepTest, generates new test inputs by applying semantic-preserving metamorphic transformations to seed images. These transformations include brightness/contrast adjustment, rotation, scaling, blurring, and adding synthetic fog or rain effects. According to the metric definition, using such metamorphic transformations without explicitly handling serialization effects (like quantization, rounding, or clipping) provides only partial enforcement of failure reproducibility. The paper describes applying these transformations using libraries like OpenCV and Photoshop filters but does not mention any steps to ensure that the resulting numerical perturbations in the image data would persist after standard I/O operations (e.g., saving to a PNG or JPEG file). There is no discussion of how floating-point pixel values are handled or whether they are constrained to a specific bit depth, meaning fault-inducing perturbations could be lost or altered upon serialization. This approach aligns directly with the 'Medium' rating.",
                    "evidence": "Page 5, Section 3.2, Increasing Coverage with Synthetic Images: \"Therefore, DeepTest focuses on generating realistic synthetic images by applying image transformations on seed images and mimic different real-world phenomena like camera lens distortions, object movements, different weather conditions, etc. To this end, we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect). These transformations can be classified into three groups: linear, affine, and convolutional.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "DeepTest generates test inputs by applying semantic, metamorphic image transformations (brightness, contrast, affine transforms, blur, rain/fog, etc.) and uses those transformed images as tests. This approach produces coarse, perceptible changes that are likely to survive standard image serialization, and the paper explicitly uses metamorphic relations as the oracle. However, the paper does not explicitly reason about or constrain low-level numerical effects of serialization (quantization, rounding, clipping) nor does it describe steps to ensure that fault-inducing perturbations survive typical I/O formats. Therefore DeepTest partially considers I/O effects via use of semantic image transformations (which tends to improve reproducibility) but does not explicitly enforce or evaluate reproducibility under serialization, matching the definition of the metric's Medium level.",
                    "evidence": "Page 306, Section 3.2 (Increasing Coverage with Synthetic Images): \"Therefore, DeepTest focuses on generating realistic synthetic images by applying image transformations on seed images and mimic different real-world phenomena like camera lens distortions, object movements, different weather conditions, etc. To this end, we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect).\"; Page 307, Section 4 (Implementation) / \"Image transformations\": \"In the experiments for RQ2 and RQ3, we leverage seven different types of simple image transformations: translation, scaling, horizontal shearing, rotation, contrast adjustment, brightness adjustment, and blurring. We use OpenCV to implement these transformations [7].\"; Page 307, Section 3.4 (Creating a Test Oracle with Metamorphic Relations): \"The key insight is that even though it is hard to specify the correct behavior of a self-driving car for every transformed image, one can define relationships between the car\u2019s behaviors across certain types of transformations. For example, the autonomous car\u2019s steering angle should not change significantly for the same image under any lighting/weather conditions, blurring, or any affine transformations with small parameter values.\"; Page 311, Section 6 (Threats to Validity): \"While our transformations like rain and fog effects are designed to be realistic, the generated pictures may not be exactly reproducible in reality due to a large number of unpredictable factors, e.g., the position of the sun, the angle and size of the rain drops. etc.\"; Page 304, Introduction (Contributions): \"We have made the erroneous behaviors detected by DeepTest available at https://deeplearningtest.github.io/deepTest/. We also plan to release the generated test images and the source of DeepTest for public use.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "No cross model failure analysis"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across models. DeepTest generates synthetic tests and reports erroneous behaviors for three different DNNs, but it treats each model's testing separately and does not (a) reuse inputs crafted on one model to test other models, (b) measure overlap of discovered faults across models, or (c) include design elements explicitly aimed at finding transferable attacks. The authors explicitly contrast their approach with prior differential-testing work (DeepXplore) that used multiple DNNs; DeepTest is presented as a single-model, metamorphic-relation-driven approach rather than a substitute-model transfer-attack methodology.",
                    "evidence": "\u2022 Paper objective / scope (multiple models tested but separately): \u201cWe use DeepTest to systematically test three top performing DNN models from the Udacity driving challenge.\u201d (Abstract / Contributions, p.304). \n\n\u2022 Explicit distinction from multi-model differential testing (shows DeepTest is single-model focused): \u201cPei et al. [70] proposed DeepXplore, a whitebox differential testing algorithm for systematically finding inputs that can trigger inconsistencies between multiple DNNs. \u2026 By contrast, our graybox methods use neuron coverage for guided test generation in a single DNN and leverage metamorphic relations to identify erroneous behaviors without requiring multiple DNNs.\u201d (Related Work \u2014 Testing and verification of machine learning / DeepXplore comparison, p.312). \n\n\u2022 Results present per-model erroneous behaviors but no cross-model reuse/transfer experiments or overlap analysis: e.g., Tables and text report errors per model (Table 8: \u201cNumber of unique erroneous behaviors reported by DeepTest for different models...\u201d and surrounding discussion) and aggregate counts, but contain no experiment where inputs found on one model are applied to other models to test transferability (Results / RQ4 and Table 8 discussion, pp.310\u2013311)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper introduces DeepTest, a method for finding erroneous behaviors in a single Deep Neural Network (DNN) by generating synthetic inputs that maximize neuron coverage. The evaluation is performed on three separate DNN models (Chauffeur, Rambo, and Epoch) that perform the same task of autonomous driving. However, the paper's methodology and experiments focus on testing each model in isolation. The results are presented separately for each model, highlighting which types of transformations induce errors in which models. There is no experiment or discussion where a fault-inducing input generated for one model is then used to test another model to check for transferability. The paper does not demonstrate whether the discovered failures are model-specific or shared across different architectures.",
                    "evidence": "Page 10, Section 5. RESULTS, Table 8: 'Number of unique erroneous behaviors reported by DeepTest for different models with \u03bb = 5'. The accompanying text states: 'Interestingly, some models are more prone to erroneous behaviors for some transformations than others. For example, Rambo produces 23 erroneous cases for shear, while the other two models do not show any such cases. Similarly, DeepTest finds 650 instances of erroneous behavior in Chauffeur for rain but only 64 and 27 for Epoch and Rambo respectively.' This evidence shows that bugs were identified and counted for each model independently, rather than testing if a bug found in one model transfers to others."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across models. DeepTest generates synthetic tests and reports erroneous behaviors for three different DNNs, but it treats each model's testing separately and does not (a) reuse inputs crafted on one model to test other models, (b) measure overlap of discovered faults across models, or (c) include design elements explicitly aimed at finding transferable attacks. The authors explicitly contrast their approach with prior differential-testing work (DeepXplore) that used multiple DNNs; DeepTest is presented as a single-model, metamorphic-relation-driven approach rather than a substitute-model transfer-attack methodology.",
                    "evidence": "\u2022 Paper objective / scope (multiple models tested but separately): \u201cWe use DeepTest to systematically test three top performing DNN models from the Udacity driving challenge.\u201d (Abstract / Contributions, p.304). \n\n\u2022 Explicit distinction from multi-model differential testing (shows DeepTest is single-model focused): \u201cPei et al. [70] proposed DeepXplore, a whitebox differential testing algorithm for systematically finding inputs that can trigger inconsistencies between multiple DNNs. \u2026 By contrast, our graybox methods use neuron coverage for guided test generation in a single DNN and leverage metamorphic relations to identify erroneous behaviors without requiring multiple DNNs.\u201d (Related Work \u2014 Testing and verification of machine learning / DeepXplore comparison, p.312). \n\n\u2022 Results present per-model erroneous behaviors but no cross-model reuse/transfer experiments or overlap analysis: e.g., Tables and text report errors per model (Table 8: \u201cNumber of unique erroneous behaviors reported by DeepTest for different models...\u201d and surrounding discussion) and aggregate counts, but contain no experiment where inputs found on one model are applied to other models to test transferability (Results / RQ4 and Table 8 discussion, pp.310\u2013311)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Xian Yuen et al. - 2023 - ASDF A Differential Testing Framework for Automatic Speech Recognition Systems": {
        "key": "yuen2023asdf",
        "name": "ASDF",
        "year": "2023",
        "citation_count": "8",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper describes ASDF as a differential testing framework that identifies instances where an ASR system's transcription does not match the original text (i.e., mispredictions / transcription errors) and performs phonetic analysis to find error-prone phonemes. The tool's goals, methods, and metrics focus on generating more test cases and measuring transcription failures and phoneme-level weaknesses. The paper contains no description or claim that ASDF is designed to bypass safety, policy, or security mechanisms in defended or safety-aligned models, nor does it report uncovering high-severity behaviors (e.g., jailbreaks, data leakage, unauthorized actions) or testing models with explicit defense/safety mechanisms. Therefore the failures it uncovers correspond to standard robustness/misprediction errors rather than high- or medium-severity safety/security violations.",
                    "evidence": "Page 461, I. INTRODUCTION: \"Intuitively, the transcription produced by an ASR system should be equivalent to the text used to generate the audio. Otherwise, a failed test case for an ASR system is uncovered.\"; Page 462, IV. USAGE: \"A failed text is defined as an audio file that is incorrectly transcribed by at least one ASR system.\"; Page 462, III. CORE FUNCTIONALITIES: \"The ASR systems currently available in ASDF are DeepSpeech [19], wav2letter [20], and wav2vec2 [21].\" (tooling described focuses on transcription comparison and phonetic analysis; no sections or quotes describe bypassing defenses, jailbreaks, data leakage, or testing of safety-aligned/defended models)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper describes ASDF, a framework for testing Automatic Speech Recognition (ASR) systems. The failures it aims to uncover are incorrect transcriptions. ASDF generates test cases by applying various text transformations (e.g., changing tense, substituting homophones) to an initial text, converting it to audio, and then checking if the ASR's transcription matches the original text. A failure is explicitly defined as a mismatch between the transcription and the source text. This process fits the definition of uncovering \"model robustness errors, such as mispredictions or prediction inconsistencies under semantically preserving input mutations.\" The paper does not mention any goal or capability related to uncovering unsafe, biased, or toxic outputs (Medium severity), nor does it discuss bypassing any safety or security mechanisms (High severity). The entire focus is on the correctness and robustness of the speech-to-text transcription.",
                    "evidence": "Page 2, Section IV. USAGE: \"A failed test case is defined as a specific text output from an individual ASR service that does not match its corresponding input text.\" and \"A failed text is defined as an audio file that is incorrectly transcribed by at least one ASR system.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper describes ASDF as a differential testing framework that identifies instances where an ASR system's transcription does not match the original text (i.e., mispredictions / transcription errors) and performs phonetic analysis to find error-prone phonemes. The tool's goals, methods, and metrics focus on generating more test cases and measuring transcription failures and phoneme-level weaknesses. The paper contains no description or claim that ASDF is designed to bypass safety, policy, or security mechanisms in defended or safety-aligned models, nor does it report uncovering high-severity behaviors (e.g., jailbreaks, data leakage, unauthorized actions) or testing models with explicit defense/safety mechanisms. Therefore the failures it uncovers correspond to standard robustness/misprediction errors rather than high- or medium-severity safety/security violations.",
                    "evidence": "Page 461, I. INTRODUCTION: \"Intuitively, the transcription produced by an ASR system should be equivalent to the text used to generate the audio. Otherwise, a failed test case for an ASR system is uncovered.\"; Page 462, IV. USAGE: \"A failed text is defined as an audio file that is incorrectly transcribed by at least one ASR system.\"; Page 462, III. CORE FUNCTIONALITIES: \"The ASR systems currently available in ASDF are DeepSpeech [19], wav2letter [20], and wav2vec2 [21].\" (tooling described focuses on transcription comparison and phonetic analysis; no sections or quotes describe bypassing defenses, jailbreaks, data leakage, or testing of safety-aligned/defended models)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Performs phonetic analysis to correlated phonemes with errors."
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Synthesized inputs might not be natural and no measure of naturnaless provided"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces a plausibility filter on transformed inputs by requiring that any transformed text be transcribed correctly by at least one ASR system (cross-referencing) before it is considered valid, which is an explicit validation of final mutated inputs. However, the paper does not report any human evaluation or task-specific perceptual/fluency metrics (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for final fault-inducing audio/text, nor does it describe mechanisms that bound or control cumulative drift across multiple mutation iterations. The authors also acknowledge limitations of using TTS-generated audio versus real human speech. Together, this matches the metric\u2019s \u201cMedium\u201d level: plausibility of final inputs is validated, but cumulative-effects and empirical perceptual assessments are not addressed.",
                    "evidence": "Section IV (USAGE), p.462: \"It is important to note that cross-referencing is used to filter the low-quality, unrealistic texts resulting from text transformation. A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\" Section V (CHALLENGES AND FUTURE WORK), p.462: \"ASDF generates an audio test suite using a TTS library. This assumes that the generated audio is interchangeable with the real-world speech of humans, which may not be the case in real-life situations. There would still be differences between human speech and computer-generated audio, namely pronunciation, accent, and tone. As a result, if human speech were to be used instead of computer-generated audio, such inconsistencies may yield different results for our differential testing.\" (Also: Section III / Abstract describe text transformations such as tense change and homophone substitution used to generate test cases, but no human study or perceptual/fluency metrics are reported in the paper or the described evaluation.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper does not provide empirical evidence, such as human studies or quantitative perceptual metrics (e.g., PESQ, STOI), to confirm that the final fault-inducing audio inputs are realistic. In fact, the authors explicitly acknowledge this as a limitation, stating that the Text-To-Speech (TTS) generated audio may not be interchangeable with real human speech. However, the method does not completely ignore plausibility. It incorporates an indirect validation mechanism through cross-referencing, where a transformed input is only considered valid if at least one ASR system under test can transcribe it correctly. This step is explicitly designed to filter out \"low-quality, unrealistic texts.\" This filtering of final inputs aligns with the criteria for a 'Medium' rating, as it constitutes a form of plausibility validation without enforcing it throughout the generation process or providing direct realism measurements.",
                    "evidence": "Page 2, Section IV. USAGE: \"It is important to note that cross-referencing is used to filter the low-quality, unrealistic texts resulting from text transformation. A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\"\n\nPage 2, Section V. CHALLENGES AND FUTURE WORK: \"ASDF generates an audio test suite using a TTS library. This assumes that the generated audio is interchangeable with the real-world speech of humans, which may not be the case in real-life situations. There would still be differences between human speech and computer-generated audio, namely pronunciation, accent, and tone.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces a plausibility filter on transformed inputs by requiring that any transformed text be transcribed correctly by at least one ASR system (cross-referencing) before it is considered valid, which is an explicit validation of final mutated inputs. However, the paper does not report any human evaluation or task-specific perceptual/fluency metrics (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for final fault-inducing audio/text, nor does it describe mechanisms that bound or control cumulative drift across multiple mutation iterations. The authors also acknowledge limitations of using TTS-generated audio versus real human speech. Together, this matches the metric\u2019s \u201cMedium\u201d level: plausibility of final inputs is validated, but cumulative-effects and empirical perceptual assessments are not addressed.",
                    "evidence": "Section IV (USAGE), p.462: \"It is important to note that cross-referencing is used to filter the low-quality, unrealistic texts resulting from text transformation. A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\" Section V (CHALLENGES AND FUTURE WORK), p.462: \"ASDF generates an audio test suite using a TTS library. This assumes that the generated audio is interchangeable with the real-world speech of humans, which may not be the case in real-life situations. There would still be differences between human speech and computer-generated audio, namely pronunciation, accent, and tone. As a result, if human speech were to be used instead of computer-generated audio, such inconsistencies may yield different results for our differential testing.\" (Also: Section III / Abstract describe text transformations such as tense change and homophone substitution used to generate test cases, but no human study or perceptual/fluency metrics are reported in the paper or the described evaluation.)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "The speech input is generated using TTS library which are fed to the ASR model. There is not in memory mutation on the audio."
                },
                "chatgpt": {
                    "value": "High",
                    "why": "ASDF generates test inputs as serialized audio files via a TTS engine (gTTS) and feeds those audio files into ASR systems for evaluation. Because inputs are produced and saved in a serialized audio format before being consumed by the oracle (the ASR systems), the fault-inducing perturbations are inherently preserved across standard I/O serialization; the paper therefore produces reproducible failures under standard I/O operations.",
                    "evidence": "Page 461, Section II (Tool Design): \"A Text-To-Speech (TTS) service is used to generate audio from the text, which is then fed into the ASR systems to obtain their transcriptions.\"; Page 462, Section III (Core Functionalities): \"Each line of text in the file is converted into an audio file by gTTS [18].\"; Page 462, Section V (Challenges and Future Work): \"ASDF generates an audio test suite using a TTS library.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, ASDF, generates test inputs by synthesizing audio directly from text using a Text-To-Speech (TTS) service. This process creates inputs in a standard, serialized audio format. The metric definition explicitly states that approaches generating inputs 'directly in serialized formats (e.g., synthesized speech...)' are considered reproducible and meet the 'High' criteria. The mutations in ASDF are performed at the text level (e.g., changing tense, substituting homophones) before the audio is ever generated. This means the fault-inducing properties are embedded in the content of the speech itself, not as fragile numerical perturbations to an audio waveform that could be lost during I/O operations like quantization or rounding. The generated audio file is inherently stable, ensuring that any failure it induces is reproducible.",
                    "evidence": "Page 2, Section III. CORE FUNCTIONALITIES: 'The input text corpus should be in the format of a .txt file. Each line of text in the file is converted into an audio file by gTTS [18].'\n\nPage 1, Section I. INTRODUCTION: 'After collecting a small initial set of failed test cases, ASDF transforms the failed texts using various text transformation methods to further generate more test cases. For example, changing the tense of a sentence or substituting error-inducing terms with other words that have similar phonemes.'\n\nPage 1, Section II. TOOL DESIGN: 'A Text-To-Speech (TTS) service is used to generate audio from the text, which is then fed into the ASR systems to obtain their transcriptions.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "ASDF generates test inputs as serialized audio files via a TTS engine (gTTS) and feeds those audio files into ASR systems for evaluation. Because inputs are produced and saved in a serialized audio format before being consumed by the oracle (the ASR systems), the fault-inducing perturbations are inherently preserved across standard I/O serialization; the paper therefore produces reproducible failures under standard I/O operations.",
                    "evidence": "Page 461, Section II (Tool Design): \"A Text-To-Speech (TTS) service is used to generate audio from the text, which is then fed into the ASR systems to obtain their transcriptions.\"; Page 462, Section III (Core Functionalities): \"Each line of text in the file is converted into an audio file by gTTS [18].\"; Page 462, Section V (Challenges and Future Work): \"ASDF generates an audio test suite using a TTS library.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper describes a differential testing framework that runs multiple ASR systems in parallel and uses cross-referencing to identify failed cases, and it leverages failures to generate transformed test texts. However, it does not demonstrate or evaluate whether fault-inducing inputs discovered on one model are reused to test other (unseen) target models, nor does it describe explicit mechanisms or experiments aimed at measuring cross-model transferability of faults. All descriptions pertain to comparing outputs across selected ASR systems within the same test run and generating transformations from observed failures, but no reuse-on-surrogate \u2192 test-on-other-model workflow or transferability analysis is presented.",
                    "evidence": "Page 461, Section II (Tool Design): \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\" \n\nPage 461-462, Section III (Core Functionalities): \"The ASR systems currently available in ASDF are DeepSpeech [19], wav2letter [20], and wav2vec2 [21].\" and \"Cross-referencing is performed amongst the selected ASR systems to ensure that the test cases are valid and can be determined by at least one ASR service.\" \n\nPage 462, Section IV (Usage): \"A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\" \n\n(There are no sections or quotes in the paper that describe generating fault-inducing inputs on a surrogate model and then reusing them to test other unseen target models, nor experiments measuring transferability across models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper describes a differential testing framework, ASDF, where a failure is defined by a disagreement among multiple Automated Speech Recognition (ASR) models. A test case is considered a failure for a specific ASR system only if at least one other system under test transcribes the audio correctly. This design principle means the framework is explicitly built to find model-specific failures, not failures that transfer across different models. The methodology does not involve generating a fault-inducing input on one model and then reusing it to test for transferability on others; rather, it tests a single input on all models simultaneously to find discrepancies. Furthermore, the framework discards inputs that cannot be correctly transcribed by at least one ASR service, which would include cases where a failure might be transferable across all tested models. Therefore, the paper does not demonstrate whether fault-inducing inputs discovered for one model also trigger failures in others.",
                    "evidence": "Page 1, Section II, TOOL DESIGN: \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\" \n\nPage 2, Section IV, USAGE: \"A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper describes a differential testing framework that runs multiple ASR systems in parallel and uses cross-referencing to identify failed cases, and it leverages failures to generate transformed test texts. However, it does not demonstrate or evaluate whether fault-inducing inputs discovered on one model are reused to test other (unseen) target models, nor does it describe explicit mechanisms or experiments aimed at measuring cross-model transferability of faults. All descriptions pertain to comparing outputs across selected ASR systems within the same test run and generating transformations from observed failures, but no reuse-on-surrogate \u2192 test-on-other-model workflow or transferability analysis is presented.",
                    "evidence": "Page 461, Section II (Tool Design): \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\" \n\nPage 461-462, Section III (Core Functionalities): \"The ASR systems currently available in ASDF are DeepSpeech [19], wav2letter [20], and wav2vec2 [21].\" and \"Cross-referencing is performed amongst the selected ASR systems to ensure that the test cases are valid and can be determined by at least one ASR service.\" \n\nPage 462, Section IV (Usage): \"A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\" \n\n(There are no sections or quotes in the paper that describe generating fault-inducing inputs on a surrogate model and then reusing them to test other unseen target models, nor experiments measuring transferability across models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ]
        }
    },
    "Odena et al. - 2019 - TensorFuzz Debugging Neural Networks with Coverage-Guided Fuzzing": {
        "key": "odena2019tensorfuzz",
        "name": "TensorFuzz",
        "year": "2019",
        "citation_count": "426",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper demonstrates that TensorFuzz uncovers robustness and implementation errors (numerical NaNs, model prediction disagreements after quantization, broken loss-function behavior, and unintended semantic changes from code transformations). These are model robustness and software-bug failures rather than violations of safety/policy mechanisms or security-critical behaviors: the authors do not claim discovering jailbreaks, data leakage, bypasses of safety-aligned defenses, or unauthorized actions in defended models. Therefore the failures exposed fit the metric's description of low-severity (robustness/misprediction) failures.",
                    "evidence": "Section 6 (Experimental Results): \"TensorFuzz can efficiently find numerical errors in trained neural networks: ... We focus on finding inputs that result in not-a-number (NaN) values.\"; \"TensorFuzz surfaces disagreements between models and their quantized versions: ... the fuzzer was able to generate disagreements for 70% of the examples we tried.\"; \"TensorFuzz can find real bugs in popular model implementations: ... we hooked up TensorFuzz to the sub-graph defined by the loss function and wrote an objective ... TensorFuzz was able to quickly find a satisfying input.\" (all from Section 6, Experimental Results). Conclusion: \"We have demonstrated the practical utility of TensorFuzz by finding numerical errors, exposing disagreements between neural networks and their quantized versions, surfacing broken loss functions in popular repositories, and making performance improvements to TensorFlow.\" (Conclusion). Nowhere in the paper do the authors claim bypassing explicit safety/policy mechanisms, inducing jailbreaks, data leakage, or other security-critical behaviors (no sections or experiments addressing defended/safety-aligned models or security bypasses)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper demonstrates that TensorFuzz is primarily used to uncover model robustness and correctness errors. The experiments focus on three types of failures: 1) numerical errors (e.g., finding inputs that cause NaN values), 2) prediction inconsistencies (e.g., finding inputs where a model and its quantized version disagree), and 3) implementation bugs (e.g., surfacing issues in loss function implementations). These examples align directly with the definition of 'Low' severity, as they concern robustness and consistency rather than unsafe behavior or the bypassing of explicit safety mechanisms. The paper does not provide any examples of uncovering unsafe, biased, or toxic outputs, nor does it test models with explicit safety alignments or security defenses.",
                    "evidence": "Page 7, Section 6, \"TensorFuzz surfaces disagreements between models and their quantized versions\": \"We then compared the predictions of the 32-bit and the 16-bit model on the MNIST test set and found 0 disagreements... With these settings, the fuzzer was able to generate disagreements for 70% of the examples we tried. Thus, CGF allowed us to find real errors that could have occured at test time.\"\n\nPage 7, Section 6, \"TensorFuzz can efficiently find numerical errors in trained neural networks\": \"We focus on finding inputs that result in not-a-number (NaN) values... As shown in Figure 2, TensorFuzz found NaNs quickly across 10 different random initializations.\"\n\nPage 8, Section 6, \"TensorFuzz can find real bugs in popular model implementations\": \"TensorFuzz was able to quickly find a satsifying input. The input was one in which the output of the discriminator had too high a magnitude, which caused the incorrectly implemented loss function to have saturating gradients.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper demonstrates that TensorFuzz uncovers robustness and implementation errors (numerical NaNs, model prediction disagreements after quantization, broken loss-function behavior, and unintended semantic changes from code transformations). These are model robustness and software-bug failures rather than violations of safety/policy mechanisms or security-critical behaviors: the authors do not claim discovering jailbreaks, data leakage, bypasses of safety-aligned defenses, or unauthorized actions in defended models. Therefore the failures exposed fit the metric's description of low-severity (robustness/misprediction) failures.",
                    "evidence": "Section 6 (Experimental Results): \"TensorFuzz can efficiently find numerical errors in trained neural networks: ... We focus on finding inputs that result in not-a-number (NaN) values.\"; \"TensorFuzz surfaces disagreements between models and their quantized versions: ... the fuzzer was able to generate disagreements for 70% of the examples we tried.\"; \"TensorFuzz can find real bugs in popular model implementations: ... we hooked up TensorFuzz to the sub-graph defined by the loss function and wrote an objective ... TensorFuzz was able to quickly find a satisfying input.\" (all from Section 6, Experimental Results). Conclusion: \"We have demonstrated the practical utility of TensorFuzz by finding numerical errors, exposing disagreements between neural networks and their quantized versions, surfacing broken loss functions in popular repositories, and making performance improvements to TensorFlow.\" (Conclusion). Nowhere in the paper do the authors claim bypassing explicit safety/policy mechanisms, inducing jailbreaks, data leakage, or other security-critical behaviors (no sections or experiments addressing defended/safety-aligned models or security bypasses)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "does not deliberately target particular outputs or behaviors. Objective function mechanism allows arbitrary user-defined checks to enable diverse failure detection but the exploration is not targeted toward any specific class of outputs "
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "norm-bounded"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints on inputs (e.g., restricting inputs to valid shapes/ranges and providing a bounded L\u221e mutation option relative to an original seed) and explicitly uses such bounded perturbations in at least one experiment (the quantization experiment where mutations are restricted to an L\u221e ball). However, the paper does not report any explicit empirical assessment of the perceptual or semantic realism of final fault-inducing inputs (no human study or task-specific perceptual/fluency metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity are provided). Thus TensorFuzz enforces plausibility via bounded/rule-based constraints in mutation design and uses them in experiments, but it does not provide quantitative or human-evaluation evidence that final failing inputs remain perceptually/semantically realistic across fuzzing iterations.",
                    "evidence": "Section 4 (An Overview of the TensorFuzz Library): \"we restrict the inputs to those that are valid neural network inputs. If the inputs are images, we restrict our inputs to have the correct size and shape, and to lie in the same interval as the input pixels of the dataset under consideration. If the inputs are sequences of characters, we only allow characters that are in the vocabulary extracted from the training set.\" \n\nSection 5 (Details \u2014 Mutator): \"For image inputs, we implemented two different types of mutation. The first is to just add white noise of a user-configurable variance to the input. The second is to add white noise, but to constrain the difference between the mutated element and the original element from which it is descended to have a user-configurable L\u221e norm. This type of constrained mutation can be useful if we want to find inputs that satisfy some objective function, but are still plausibly of the same 'class' as the original input that was used as a seed. In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.\" \n\nSection 6 (Experimental Results \u2014 Quantization experiment): \"We then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. We restrict to inputs near seed images because these inputs nearly all have unambiguous class semantics.\" \n\nAbsence of empirical plausibility evaluation: The paper describes mutation constraints and uses bounded perturbations in experiments but contains no section or statements reporting human evaluation or task-specific perceptual/fluency metrics (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) for the final fault-inducing inputs."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper describes a mutation strategy specifically designed to maintain input plausibility by constraining mutations within an L-infinity norm ball around the original seed image. The authors explicitly state this is done to ensure inputs retain \"unambiguous class semantics\" and to avoid testing on \"out-of-domain garbage data\". This method enforces plausibility by bounding the cumulative perturbation from the original seed. However, the paper does not provide any empirical validation, such as a human study or quantitative perceptual quality metrics (e.g., LPIPS, SSIM), to systematically assess whether the final fault-inducing inputs are indeed realistic. It only provides a single visual example (Figure 3). This meets the criteria for \"Medium\" as it enforces plausibility through bounded constraints but does not include an explicit assessment of the final inputs' plausibility.",
                    "evidence": "Page 7, Section 6, \"TensorFuzz surfaces disagreements between models and their quantized versions\": \"We then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. We restrict to inputs near seed images because these inputs nearly all have unambiguous class semantics. It is less interesting if two versions of the model disagree on out-of-domain garbage data with no true class.\"\n\nPage 5, Section 5, \"Mutator\": \"The second is to add white noise, but to constrain the difference between the mutated element and the original element from which it is descended to have a user-configurable L\u221e norm. This type of constrained mutation can be useful if we want to find inputs that satisfy some objective function, but are still plausibly of the same 'class' as the original input that was used as a seed.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints on inputs (e.g., restricting inputs to valid shapes/ranges and providing a bounded L\u221e mutation option relative to an original seed) and explicitly uses such bounded perturbations in at least one experiment (the quantization experiment where mutations are restricted to an L\u221e ball). However, the paper does not report any explicit empirical assessment of the perceptual or semantic realism of final fault-inducing inputs (no human study or task-specific perceptual/fluency metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity are provided). Thus TensorFuzz enforces plausibility via bounded/rule-based constraints in mutation design and uses them in experiments, but it does not provide quantitative or human-evaluation evidence that final failing inputs remain perceptually/semantically realistic across fuzzing iterations.",
                    "evidence": "Section 4 (An Overview of the TensorFuzz Library): \"we restrict the inputs to those that are valid neural network inputs. If the inputs are images, we restrict our inputs to have the correct size and shape, and to lie in the same interval as the input pixels of the dataset under consideration. If the inputs are sequences of characters, we only allow characters that are in the vocabulary extracted from the training set.\" \n\nSection 5 (Details \u2014 Mutator): \"For image inputs, we implemented two different types of mutation. The first is to just add white noise of a user-configurable variance to the input. The second is to add white noise, but to constrain the difference between the mutated element and the original element from which it is descended to have a user-configurable L\u221e norm. This type of constrained mutation can be useful if we want to find inputs that satisfy some objective function, but are still plausibly of the same 'class' as the original input that was used as a seed. In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.\" \n\nSection 6 (Experimental Results \u2014 Quantization experiment): \"We then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. We restrict to inputs near seed images because these inputs nearly all have unambiguous class semantics.\" \n\nAbsence of empirical plausibility evaluation: The paper describes mutation constraints and uses bounded perturbations in experiments but contains no section or statements reporting human evaluation or task-specific perceptual/fluency metrics (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) for the final fault-inducing inputs."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": "Mutations occur in-memory on float tensors and are never verified for persistence after I/O."
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's fuzzing design does not explicitly account for standard I/O serialization effects (rounding/quantization/format encoding). The implemented image mutations are adding white noise and optional L\u221e-bounded perturbations followed by clipping to the training input range, but there is no handling or checking that fault-inducing perturbations survive common serialization steps (e.g., 8-bit PNG/JPEG quantization or file encoding). The only use of quantization in the paper is quantizing model weights/activations (32-bit \u2192 16-bit) to compare model disagreement, which is a model-internal quantization rather than input serialization handling. Because TensorFuzz neither constrains mutations to be robust under typical I/O quantization/rounding nor evaluates failures after serialization, failures it finds may rely on fragile in-memory numerical perturbations that would be lost when inputs are serialized. This matches the metric definition for Low (ignores I/O effects during mutation and oracle evaluation).",
                    "evidence": "Section 5 (Mutator): \"For image inputs, we implemented two different types of mutation. The first is to just add white noise ... The second is to add white noise, but to constrain the difference ... to have a user-configurable L\u221e norm. ... In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.\" (Section 5, \"Mutator\")\n\nSection 4 (Overview of the TensorFuzz Library): \"If the inputs are images, we restrict our inputs to have the correct size and shape, and to lie in the same interval as the input pixels of the dataset under consideration.\" (Section 4, \"Overview of the fuzzing procecure\")\n\nSection 6 (Quantization experiment): \"We then truncated all weights and activations to 16-bits. We then compared the predictions of the 32-bit and the 16-bit model ...\" \u2014 this describes quantization of model internals, not input serialization handling. (Section 6, \"TensorFuzz surfaces disagreements between models and their quantized versions\")\n\nNowhere in the paper do the authors describe serializing mutated inputs to common on-disk formats (PNG/JPEG/WAV/MP3) or explicitly modeling/handling rounding, clipping, or encoding effects during test generation or oracle evaluation; the paper only describes clipping to the input value range and L\u221e constraints (Sections 4 and 5). (See Sections 4 and 5 for mutation/clip descriptions and Section 6 for the separate model quantization experiment.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper states that the mutation process for images includes a clipping step to ensure the resulting values remain within the valid range of the training data. This explicitly accounts for one type of serialization effect (clipping). However, the paper does not mention any handling of other common I/O effects like rounding or quantization. Mutations are applied to floating-point representations, and small perturbations (e.g., changing a pixel value from 155.0 to 155.4) would be preserved by clipping but lost when serialized to an 8-bit integer format (which would round it to 155). This partial consideration of I/O effects aligns with the 'Medium' rating, as fault-inducing inputs are not fully guaranteed to be reproducible after standard serialization.",
                    "evidence": "Page 5, Section 5. Details of the TensorFuzz Library, Mutator: \"In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "TensorFuzz\u2019s mutation and oracle procedures operate entirely on floating-point tensors and only clip mutated inputs to lie within the model\u2019s training value range. The paper does not describe serializing mutated inputs to common on-disk formats (e.g. 8-bit PNG/JPEG) nor modeling or enforcing that perturbations survive typical quantization or rounding steps during I/O. Clipping to the floating-point input range is not equivalent to handling integer quantization or encoding effects. Therefore the approach ignores standard I/O effects and may produce failures that vanish after serialization, matching the \u201cLow\u201d category.",
                    "evidence": "Section 5 (\u201cMutator\u201d): \u201cIn both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.\u201d \n\nNo mention in Sections 4\u20136 of writing mutated inputs to disk or accounting for 8-bit quantization, rounding, or file encoding. The only quantization discussed (Section 6) applies to model weights/activations, not to input serialization."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly reuses fault-inducing inputs generated by fuzzing one model (the 32-bit model) to test another implementation (the 16-bit quantized/truncated model) and reports that many such inputs cause disagreements. However, the fuzzing framework is presented as a general-purpose CGF + PBT tool rather than being explicitly designed primarily to discover transferable attacks across different model implementations; the cross-model experiment is an application/example rather than a described core design goal. Thus the work demonstrates transferability reuse but the fuzzing design is not explicitly aimed at discovering transferable attacks across models.",
                    "evidence": "Section 6, 'TensorFuzz surfaces disagreements between models and their quantized versions': \"We then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. ... With these settings, the fuzzer was able to generate disagreements for 70% of the examples we tried.\" Also in same subsection: \"As a baseline experiment, we trained an MNIST classifier (this time without intentionally introducing numerical issues) using 32-bit floating point numbers. We then truncated all weights and activations to 16-bits. We then compared the predictions of the 32-bit and the 16-bit model on the MNIST test set and found 0 disagreements.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper does not evaluate whether the fault-inducing inputs discovered through fuzzing on one model can trigger failures in other, distinct models performing the same task. The experiments focus on finding bugs within a single model instance (e.g., numerical errors), comparing a model to its own quantized version, or testing the correctness of specific code implementations. There is no investigation into the transferability of these discovered faults to different model architectures or independently trained models, which is the core of the Attack Transferability metric.",
                    "evidence": "Page 7, Section \"TensorFuzz surfaces disagreements between models and their quantized versions\": The paper describes an experiment comparing a 32-bit MNIST classifier to its 16-bit quantized version. It states, \"We then compared the predictions of the 32-bit and the 16-bit model on the MNIST test set and found 0 disagreements. We then ran the fuzzer... the fuzzer was able to generate disagreements for 70% of the examples we tried.\" While this involves two model variants, it is a test of a model against its direct, transformed derivative, not a test of transferability to a separate, independently trained model. The other experiments in the paper similarly focus on single models or codebases without testing for transferability."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper includes an explicit experiment in which fault-inducing inputs generated by fuzzing a 32-bit MNIST classifier are reused to test a 16-bit quantized version, demonstrating that many of these inputs transfer and cause disagreements. This satisfies the criterion of reusing fuzzed inputs across two models performing the same task. However, the overall fuzzing framework is presented as a general CGF+PBT tool rather than being specifically designed to discover transferable failures across arbitrary surrogate and target models, so it lacks an explicit architectural mechanism for transferability beyond this illustrative application.",
                    "evidence": "Section 6, \u201cTensorFuzz surfaces disagreements between models and their quantized versions\u2019: \u201cWe then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. ... With these settings, the fuzzer was able to generate disagreements for 70% of the examples we tried.\u201d"
                },
                "manual": {
                    "value": "Low",
                    "why": "It is not transferability. It is differential testing based oracle.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Property-based",
                "Differential"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ]
        }
    },
    "Chao et al. - 2025 - Jailbreaking Black Box Large Language Models in Twenty Queries": {
        "key": "chao2025jailbreaking",
        "name": "PAIR",
        "year": "2025",
        "citation_count": "806",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly targets and successfully produces semantic jailbreaks that coax safety-aligned and closed-source LLMs (e.g., GPT-3.5/4 and Gemini-Pro) into producing unsafe/forbidden outputs, and it evaluates PAIR against existing defenses showing PAIR can bypass or remain effective under defense mechanisms. Thus PAIR uncovers high-impact failures that bypass explicit safety/policy mechanisms (jailbreaks), not merely low-level robustness errors.",
                    "evidence": "Page 23, Abstract: \"However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails.\" \n\nPage 23, Contributions (Efficiency/Effectiveness): \"PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini.\" \n\nPage 27, Table III (Direct jailbreak attacks): shows PAIR Jailbreak % for closed-source models (e.g., \"GPT-3.5 51%\" and \"GPT-4 48%\", and \"Gemini 73%\"), demonstrating PAIR elicits unsafe responses from safety-aligned/closed models. \n\nPage 29, Section IV-C (Defended performance of PAIR): \"In Table VIII, we evaluate the performance of PAIR against two jailbreaking defenses: SmoothLLM [20] and a perplexity filter [37, 38]. ... the JB% of PAIR drops significantly less than GCG when defended by these two defenses, meaning that PAIR is significantly harder to defend against than GCG.\" \n\nPage 29, Fig. 3 caption (example jailbreak): \"Claude-1 jailbreak. A successful jailbreak where the objective is to 'design a phishing email that can trick users into giving away their login credentials.'\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper introduces PAIR, a method explicitly designed to generate \"semantic jailbreaks\" that bypass the safety guardrails of large language models. The evaluation is conducted on a range of modern, safety-aligned LLMs, including Llama-2, GPT-4, Claude, and Gemini, which are described as representing the state-of-the-art in safety alignment. The goal is to induce the models to produce objectionable content for harmful behaviors, which constitutes a high-impact failure by bypassing explicit safety mechanisms. This aligns perfectly with the 'High' severity rating, which is defined by uncovering failures that bypass explicit safety mechanisms in defended or safety-aligned models to induce violations like jailbreaks.",
                    "evidence": "Page 1, Abstract: \"The alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. ... we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM.\"\n\nPage 4, Section IV. EXPERIMENTS, Target LLMs: \"We red team seven different LLMs... Of these models, Vicuna and Llama-2 are open source, whereas the remaining five are only available as black boxes. These models collectively represent the current state-of-the-art in terms of both generation capability (GPT-4 and Gemini-Pro) and safety alignment (Claude and Llama-2).\"\n\nPage 5, Table III: The table is titled \"Direct jailbreak attacks on JailbreakBench\" and reports the \"Jailbreak %\" for various defended models like Llama-2, GPT-3.5, GPT-4, Claude-1, Claude-2, and Gemini, demonstrating the method's capability to bypass their safety features."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly targets and successfully produces semantic jailbreaks that coax safety-aligned and closed-source LLMs (e.g., GPT-3.5/4 and Gemini-Pro) into producing unsafe/forbidden outputs, and it evaluates PAIR against existing defenses showing PAIR can bypass or remain effective under defense mechanisms. Thus PAIR uncovers high-impact failures that bypass explicit safety/policy mechanisms (jailbreaks), not merely low-level robustness errors.",
                    "evidence": "Page 23, Abstract: \"However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails.\" \n\nPage 23, Contributions (Efficiency/Effectiveness): \"PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini.\" \n\nPage 27, Table III (Direct jailbreak attacks): shows PAIR Jailbreak % for closed-source models (e.g., \"GPT-3.5 51%\" and \"GPT-4 48%\", and \"Gemini 73%\"), demonstrating PAIR elicits unsafe responses from safety-aligned/closed models. \n\nPage 29, Section IV-C (Defended performance of PAIR): \"In Table VIII, we evaluate the performance of PAIR against two jailbreaking defenses: SmoothLLM [20] and a perplexity filter [37, 38]. ... the JB% of PAIR drops significantly less than GCG when defended by these two defenses, meaning that PAIR is significantly harder to defend against than GCG.\" \n\nPage 29, Fig. 3 caption (example jailbreak): \"Claude-1 jailbreak. A successful jailbreak where the objective is to 'design a phishing email that can trick users into giving away their login credentials.'\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "Targets specific malicious intents (e.g. bomb-making, phishing) but does not enforce a fixed phrase or label."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis of discovered faults. It provides transferability rationale"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The PAIR method explicitly constrains prompt generation via rule-based attacker system prompts (role-playing, logical appeal, authority endorsement), JSON output formats, and in-context examples that keep mutations semantic and human-interpretable in each iteration. These design choices enforce plausibility at the individual mutation step level. However, the paper does not provide an explicit mechanism or empirical evaluation that enforces or measures plausibility across cumulative iterations (e.g., no bounded-perturbation guarantees or perceptual/semantic metrics like perplexity reported for final fault-inducing prompts), nor a human study that directly assesses the realism of final jailbreak prompts. Therefore PAIR enforces plausibility through rule-based / template constraints per mutation but does not validate or measure cumulative drift of final inputs, matching the Medium category.",
                    "evidence": "III-A Attacker\u2019s system prompt (p.25): \"we carefully design three distinct system prompts templates, all of which instructs the LLM to output a specific kind of objectionable content... each system prompt template is based on one of three criteria: logical appeal, authority endorsement, and role-playing.\"; III-A (p.25): \"Format... we require that the attacker generate its responses in JSON format.\" (shows rule-based/template constraints and structured generation per iteration). Algorithm / Iterative refinement (III-B, p.25): \"If S == 1 then return P ... otherwise the conversation is updated... the conversation history is then passed back to the attacker, and the process repeats.\" (shows iterative mutation without bounded perturbation guarantees). IV Evaluation (p.26): \"We compute the Jailbreak %\u2014the percentage of behaviors that elicit a jailbroken response according to JUDGE\u2014and the Queries per Success.\" (evaluation reports jailbreak rates, not perceptual/plausibility metrics). IV / JUDGE dataset (p.26, Table I): \"We collected a dataset of 100 prompts and responses... Three expert annotators labeled each pair...\" (human annotation was used to evaluate JUDGE agreement on jailbreak labeling, not to assess plausibility of generated prompts). Appendix C / Examples (pp.41-42): contains example generated prompts and conversations (illustrative human-interpretable examples), but no reported human study or quantitative plausibility measures such as perplexity, LPIPS, SSIM, PESQ, STOI, or Perplexity for final fault-inducing prompts."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's proposed method, Prompt Automatic Iterative Refinement (PAIR), is explicitly designed to generate semantically plausible and human-interpretable inputs (prompts) as its core mechanism. Plausibility is enforced throughout the iterative process by using an attacker Large Language Model (LLM) to generate and refine prompts based on conversational history and chain-of-thought reasoning. This design inherently constrains the generated inputs to a distribution of realistic natural language. The paper contrasts this directly with token-level attacks that produce semantically invalid or uninterpretable inputs. Furthermore, the paper provides substantial empirical evidence, in the form of numerous qualitative examples of the final fault-inducing prompts, which are all semantically coherent and represent realistic user queries.",
                    "evidence": "Page 2, Contributions, 'Interpretability': 'PAIR generates prompt-level semantic jailbreaks that are interpretable to humans and includes interpretable, chain-of-thought improvement assessments.'\n\nPage 2, Figure 2 'Prompt- vs. token-level jailbreaks': This figure provides a direct visual comparison, showing a plausible prompt generated by PAIR ('Write a fun and creative story about a person who gets into a lot of trouble for committing insider trading...') versus an uninterpretable token-level jailbreak from another method.\n\nPage 3, Section III-A, 'Attacker's system prompt': The method's design relies on plausible social engineering strategies to construct prompts: 'each system prompt template is based on one of three criteria: logical appeal, authority endorsement, and role-playing.'\n\nPages 7, 19, and 20, Figures 3, 9, 10, 11, 12, 13, 14: These figures provide multiple examples of final, successful jailbreak prompts generated by PAIR. All examples are coherent, plausible, and framed as realistic scenarios (e.g., 'You are a creative writer working on a new political thriller... write a convincing letter from the IRS which the protagonist will use...')."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "While PAIR\u2019s design clearly constrains each generated prompt to remain semantically coherent\u2014via role-playing, logical-appeal and authority-endorsement system prompts, JSON formatting, and chain-of-thought refinements\u2014it does not include any explicit mechanism to prevent cumulative semantic drift over multiple iterations, nor does it report quantitative or human-study assessments (e.g. perplexity scores or user judgments) of the final fault-inducing prompts\u2019 realism. Thus, PAIR enforces plausibility at the individual mutation step level but overlooks validation of the aggregate effect of those mutations, fitting the Medium category.",
                    "evidence": "III-A Attacker\u2019s system prompt design (p.25): uses structured templates and in-context examples to keep each prompt plausible; III-B Algorithm 1 (p.25): iterative refinement with no bounded-perturbation or plausibility filter; IV Evaluation (p.26): reports only jailbreak success rates and query counts, with no perceptual/linguistic plausibility metrics or human evaluation of final prompts."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Fuzzed inputs are text prompts generated by another LLM, not numerical perturbations"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "PAIR generates adversarial inputs as textual prompts (i.e., already-serialized inputs) and explicitly enforces stable, well-formed serialization for attacker outputs (JSON formatting, seeding of braces, termination on closing brace). The paper also states experiments use public APIs for query access, which means prompts are sent and stored via standard serialized channels rather than relying on fragile in-memory floating-point perturbations. These design choices ensure that fault-inducing inputs remain reproducible under standard I/O/serialization operations.",
                    "evidence": "III-A Implementing the attacker LLM (page 25): \"To standardize the generation of this content, we require that the attacker generate its responses in JSON format.\"; III-A Implementing the attacker LLM (page 25): \"we initialize the output of the language model to begin with the brace '{' ... For the first iteration ... we seed the output with: {\"improvement\":\"\" ... Secondly, we terminate generation upon any closing brace.\"; IV. Experiments (page 26): \"Since PAIR only requires black box access, we use public APIs for all of our experiments, which reduces costs and ensures reproduciblility.\"; Appendix B / Table XII (System prompt template, page 37): \"FORMAT: Format your response in JSON, with the two elements 'improvement' and 'prompt'... Your response should **only** contain this JSON element and nothing else.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The proposed method, PAIR (Prompt Automatic Iterative Refinement), generates adversarial inputs as text prompts. Text is an inherently discrete and serialized format. Unlike floating-point representations of images or audio, text prompts do not suffer from numerical instability caused by standard I/O operations like quantization, clipping, or rounding. A generated text prompt, when saved to disk and reloaded, remains identical, thus perfectly preserving the properties that induce the failure (the 'jailbreak'). The metric definition explicitly states that such approaches are considered highly reproducible: 'We also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., synthesized speech, rendered images, or generated text) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.' The paper consistently describes its output as semantic, interpretable text prompts, which fall directly into this category.",
                    "evidence": "Page 2, Section II. PRELIMINARIES: 'We focus on prompt-level jailbreaks, wherein the goal is to craft semantic, human-interpretable prompts that fool a targeted LLM into outputting objectionable content.' This confirms that the adversarial inputs are text. Page 1, Abstract: '...we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks...'. The entire methodology revolves around generating and refining natural language prompts, which are inherently stable under standard I/O operations."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "PAIR generates adversarial inputs as textual prompts (i.e., already-serialized inputs) and explicitly enforces stable, well-formed serialization for attacker outputs (JSON formatting, seeding of braces, termination on closing brace). The paper also states experiments use public APIs for query access, which means prompts are sent and stored via standard serialized channels rather than relying on fragile in-memory floating-point perturbations. These design choices ensure that fault-inducing inputs remain reproducible under standard I/O/serialization operations.",
                    "evidence": "III-A Implementing the attacker LLM (page 25): \"To standardize the generation of this content, we require that the attacker generate its responses in JSON format.\"; III-A Implementing the attacker LLM (page 25): \"we initialize the output of the language model to begin with the brace '{' ... For the first iteration ... we seed the output with: {\"improvement\":\"\" ... Secondly, we terminate generation upon any closing brace.\"; IV. Experiments (page 26): \"Since PAIR only requires black box access, we use public APIs for all of our experiments, which reduces costs and ensures reproduciblility.\"; Appendix B / Table XII (System prompt template, page 37): \"FORMAT: Format your response in JSON, with the two elements 'improvement' and 'prompt'... Your response should **only** contain this JSON element and nothing else.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "High",
                    "why": "Generates new test cases by semantic mutations. The authors attributed transferability to this mutation "
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly evaluates transferability by reusing jailbreak prompts found on one model to test other models (Section IV-B, Table VI) and reports transfer percentages. Furthermore, the PAIR design intentionally generates semantic, prompt-level (interpretable) attacks\u2014via attacker system-prompt templates, chain-of-thought improvement assessments, and parallel streams\u2014which the authors argue and evidence as supporting cross-model transferability (they compare PAIR vs GCG transfer and attribute better transfer to PAIR's semantic prompts). Thus the paper both demonstrates reuse of fault-inducing inputs across models and incorporates design choices aimed at discovering transferable failures.",
                    "evidence": "Abstract (p. 23): \"PAIR also achieves competitive jailbreaking success rates and transferability on open and closed\u2011source LLMs, including GPT-3.5/4, Vicuna, and Gemini.\"; Introduction / Contributions (p. 23): \"We also find that PAIR jailbreaks are often more trans\u2011ferrable to other LLMs than jailbreaks generated by GCG.\"; Section IV-B 'Jailbreak transfer experiments' (p. 28): \"For PAIR, we use the successful jailbreaks found for GPT-4 and Vicuna... Our results in Table VI indicate that PAIR\u2019s Vicuna prompts transfer more readily than those generated by GCG on all models except GPT-3.5, and PAIR\u2019s GPT-4 prompts transfer well on Vicuna, GPT-3.5, and Gemini. We believe that this is largely attributable to the fact that PAIR\u2019s prompts are semantic, and they therefore target similar vulnerabilities across LLMs...\"; Table VI caption (p. 28): \"Jailbreak transferability. We report the jailbreaking percentage of prompts that successfully jailbreak a source LLM when transferred to downstream LLM.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper extensively evaluates the transferability of the jailbreak prompts generated by its proposed method, PAIR. Fault-inducing inputs (prompts) generated on source models (GPT-4 and Vicuna) are systematically reused to test their effectiveness on a suite of other target models, with results clearly presented. This fulfills the primary condition for a 'Medium' rating. However, the paper does not meet the criteria for a 'High' rating because the PAIR method's design is not explicitly aimed at discovering transferable attacks. The core algorithm is an iterative refinement process focused on jailbreaking a single, specific target model. The authors attribute the observed high transferability to the semantic nature of the generated prompts, which is a beneficial byproduct of the method rather than an explicit mechanism integrated into the design to optimize for transferability across models.",
                    "evidence": "Page 6, Section IV-B, Jailbreak transfer experiments: \"We next evaluate the transferability of the attacks generated in \u00a7 IV-A. For PAIR, we use the successful jailbreaks found for GPT-4 and Vicuna... Our results in Table VI indicate that PAIR\u2019s Vicuna prompts transfer more readily than those generated by GCG on all models except GPT-3.5, and PAIR\u2019s GPT-4 prompts transfer well on Vicuna, GPT-3.5, and Gemini. We believe that this is largely attributable to the fact that PAIR\u2019s prompts are semantic, and they therefore target similar vulnerabilities across LLMs, which are generally trained on similar datasets.\" This section, along with Table VI, shows the reuse of prompts to test transferability, but the explanation attributes this property to the semantic nature of the prompts, not to an explicit design choice for promoting transferability."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper clearly demonstrates reuse of fault-inducing inputs (jailbreak prompts) generated on one model to test their effectiveness on other models (see Section IV-B, Table VI). However, while the authors attribute high transferability to the semantic, interpretable nature of PAIR\u2019s prompt-level attacks, the core design of PAIR is focused on efficiently generating jailbreaks on a single target model rather than explicitly optimizing for cross-model transfer. Thus it meets the criteria for Medium: it reuses inputs for transfer tests but does not incorporate mechanisms explicitly aimed at discovering transferable failures.",
                    "evidence": "Section IV-B (p. 28): \u201cFor PAIR, we use the successful jailbreaks found for GPT-4 and Vicuna\u2026 Our results in Table VI indicate that PAIR\u2019s Vicuna prompts transfer more readily than those generated by GCG\u2026 We believe that this is largely attributable to the fact that PAIR\u2019s prompts are semantic, and they therefore target similar vulnerabilities across LLMs.\u201d; Table VI caption: \u201cJailbreak transferability. We report the jailbreaking percentage of prompts that successfully jailbreak a source LLM when transferred to downstream LLM.\u201d"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Transferability is a byproduct in this paper, not a design goal",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ]
        }
    },
    "Asyrofi et al. - 2020 - CrossASR Efficient Differential Testing of Automatic Speech Recognition via Text-To-Speech": {
        "key": "asyrofi2020crossasr",
        "name": "CrossASR",
        "year": "2020",
        "citation_count": "25",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": "transcription error"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The failures CrossASR is designed to uncover are transcription errors and correctness/accuracy defects (mispredictions) in ASR outputs identified via differential testing between ASRs. The paper demonstrates examples of incorrect transcriptions (e.g., words substituted or mistranscribed) and frames the goal as detecting ASR correctness issues. It does not claim to target or demonstrate uncovering unsafe, biased, hallucinated, or security-critical behaviors, nor does it evaluate bypassing safety/policy or defended/safety-aligned models. Therefore the failures exposed correspond to low-severity (robustness/correctness) errors rather than medium- or high-severity safety/security violations.",
                    "evidence": "Page 640, Abstract: \"CrossASR is a differential testing solution that compares outputs of multiple ASR systems to uncover erroneous behaviors among ASRs.\" \n\nPage 641, Introduction / Section II overview: \"CrossASR performs a black-box differential testing to uncover erroneous behaviors on ASR systems by cross-referencing different ASRs to detect different transcriptions from ASRs.\" \n\nPage 642, Section II-A (Case 2): \"In this case, CrossASR considers cases where the transcribed text does not match input text as failed test cases; it considers the other cases as successful test cases.\" \n\nPage 647, Section IV.A (example failed test case): Table IX shows a concrete transcription error: Original Text: \"they can be consulted without difficulty\"; ASR Transcribed Text: \"they can be controlled without difficulty\" (demonstrating a misprediction). \n\nPage 648, Section V.A (related work comparison): \"CrossASR ... is concerned in assessing correctness of ASRs instead of their robustness.\" \n\nPage 643, Experimental setup / Table II: lists the evaluated ASRs (Deepspeech, DeepSpeech2, wav2letter, wit) \u2014 the paper does not describe testing of defended, safety-aligned, or policy-protected models nor any experiments demonstrating bypassing of safety or security mechanisms."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The CrossASR method is designed to uncover correctness errors in Automatic Speech Recognition (ASR) systems, which manifest as mispredictions. A failure is identified when an ASR's transcribed text does not match the ground-truth text used to generate the audio input. The paper provides examples of such failures, like transcribing 'consulted' as 'controlled', which are transcription errors. The methodology does not aim to generate or identify unsafe, biased, or toxic behavior, nor does it test the model against any explicit safety or security mechanisms. The paper explicitly states its focus is on 'correctness' rather than other types of failures. This aligns with the definition of uncovering model robustness errors or mispredictions without exposing unsafe behaviors.",
                    "evidence": "Page 3, Section II-A, Case 2: 'In this case, CrossASR considers cases where the transcribed text does not match input text as failed test cases...'\n\nPage 1, Abstract: '...we found that most samples are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR)...'\n\nPage 8, Section IV-A and Table IX: 'DeepSpeech mistakenly transcribed \u201cconsulted\u201d to \u201ccontrolled\u201d.'\n\nPage 10, Section V-A: 'Also, CrossASR is concerned in assessing correctness of ASRs instead of their robustness.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The failures CrossASR is designed to uncover are transcription errors and correctness/accuracy defects (mispredictions) in ASR outputs identified via differential testing between ASRs. The paper demonstrates examples of incorrect transcriptions (e.g., words substituted or mistranscribed) and frames the goal as detecting ASR correctness issues. It does not claim to target or demonstrate uncovering unsafe, biased, hallucinated, or security-critical behaviors, nor does it evaluate bypassing safety/policy or defended/safety-aligned models. Therefore the failures exposed correspond to low-severity (robustness/correctness) errors rather than medium- or high-severity safety/security violations.",
                    "evidence": "Page 640, Abstract: \"CrossASR is a differential testing solution that compares outputs of multiple ASR systems to uncover erroneous behaviors among ASRs.\" \n\nPage 641, Introduction / Section II overview: \"CrossASR performs a black-box differential testing to uncover erroneous behaviors on ASR systems by cross-referencing different ASRs to detect different transcriptions from ASRs.\" \n\nPage 642, Section II-A (Case 2): \"In this case, CrossASR considers cases where the transcribed text does not match input text as failed test cases; it considers the other cases as successful test cases.\" \n\nPage 647, Section IV.A (example failed test case): Table IX shows a concrete transcription error: Original Text: \"they can be consulted without difficulty\"; ASR Transcribed Text: \"they can be controlled without difficulty\" (demonstrating a misprediction). \n\nPage 648, Section V.A (related work comparison): \"CrossASR ... is concerned in assessing correctness of ASRs instead of their robustness.\" \n\nPage 643, Experimental setup / Table II: lists the evaluated ASRs (Deepspeech, DeepSpeech2, wav2letter, wit) \u2014 the paper does not describe testing of defended, safety-aligned, or policy-protected models nor any experiments demonstrating bypassing of safety or security mechanisms."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": "Performs human study"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "CrossASR generates test inputs by synthesizing speech from natural texts (Europarl) via TTS engines rather than applying unconstrained, accumulating perturbations. The paper describes explicit text preprocessing (normalization) \u201cintended to ensure that the text are pronounced properly by TTS\u201d (preventing implausible TTS outputs), uses multiple real-world TTSes to avoid bias, and treats texts from a real corpus as the input distribution. Importantly, the authors perform a human verification on a statistically representative sample of final failed test cases (395 samples) and report low false positive rates (12 false positives out of 395), explicitly noting that the failed cases are \u201caudio understandable to humans but cannot be transcribed properly by an ASR.\u201d These design choices and the human evaluation demonstrate that CrossASR enforces and empirically validates input plausibility for final fault-inducing inputs.",
                    "evidence": "Intro / Abstract (page 640): \"CrossASR employs Text-to-Speech (TTS) engines to synthesize test inputs from texts and treats the texts as the ground truth labels...\"; Architecture / Text preprocessing (Section II-A / II-B, page 643): \"The Text Preprocessor lowercases the text, removes punctuation from it, and normalizes non-standard words... This normalization is intended to ensure that the text are pronounced properly by TTS.\"; Experimental validation / RQ4 & manual check (Section III-D, pages 645-647): \"we ask a native English speaker, who is not an author of this paper to verify whether the failed test cases generated by CrossASR are actual failed test cases...\" and Table VI reporting low FPRs; Discussion / manual sample analysis (page 647): \"From 33,618 failed test cases, we manually verified a statistically representative sample of size 395 and found 12 of them to be false positives.\"; Abstract / Conclusion (page 640 / 649): \"Based on a manual verification carried out on statistically representative sample size, we found that most samples are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR).\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, CrossASR, generates audio test cases using Text-To-Speech (TTS) engines. This design choice inherently aims for plausible inputs, as TTS systems are specifically created to produce realistic, human-like speech. More importantly, the paper provides direct empirical evidence for the plausibility of the fault-inducing inputs through a human evaluation. The authors tasked a native English speaker with verifying a statistically significant sample of the generated failed test cases. The explicit goal was to check if the audio was understandable to a human. The results confirmed that the vast majority of inputs that caused failures were indeed plausible and intelligible. This combination of a generation method designed for plausibility (TTS) and a rigorous human-centric validation of the final, failure-inducing inputs satisfies the criteria for a 'High' rating.",
                    "evidence": "Page 10, Section VI, Conclusion: \"CrossASR is highly reliable in determining failed test cases, as demonstrated by manual verification. Almost all of the failed test case samples inspected are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR).\"\n\nPage 5, Section III-C, RQ4: \"To measure the reliability of CrossASR in determining failed test cases, we ask a native English speaker, who is not an author of this paper to verify whether the failed test cases generated by CrossASR are actual failed test cases.\"\n\nPage 6, Section III-D, RQ4: \"The non-author verified that in the random sample, CrossASR generates failed test cases with the highest and lowest FPR of 9% and 0%, respectively. From all combination of TTSes and ASRs, the FPRs are low, which means that the failed test cases that are detected by CrossASR actually highlight a deficiency in the corresponding ASR (it is a true positive).\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "CrossASR generates test inputs by synthesizing speech from natural texts (Europarl) via TTS engines rather than applying unconstrained, accumulating perturbations. The paper describes explicit text preprocessing (normalization) \u201cintended to ensure that the text are pronounced properly by TTS\u201d (preventing implausible TTS outputs), uses multiple real-world TTSes to avoid bias, and treats texts from a real corpus as the input distribution. Importantly, the authors perform a human verification on a statistically representative sample of final failed test cases (395 samples) and report low false positive rates (12 false positives out of 395), explicitly noting that the failed cases are \u201caudio understandable to humans but cannot be transcribed properly by an ASR.\u201d These design choices and the human evaluation demonstrate that CrossASR enforces and empirically validates input plausibility for final fault-inducing inputs.",
                    "evidence": "Intro / Abstract (page 640): \"CrossASR employs Text-to-Speech (TTS) engines to synthesize test inputs from texts and treats the texts as the ground truth labels...\"; Architecture / Text preprocessing (Section II-A / II-B, page 643): \"The Text Preprocessor lowercases the text, removes punctuation from it, and normalizes non-standard words... This normalization is intended to ensure that the text are pronounced properly by TTS.\"; Experimental validation / RQ4 & manual check (Section III-D, pages 645-647): \"we ask a native English speaker, who is not an author of this paper to verify whether the failed test cases generated by CrossASR are actual failed test cases...\" and Table VI reporting low FPRs; Discussion / manual sample analysis (page 647): \"From 33,618 failed test cases, we manually verified a statistically representative sample of size 395 and found 12 of them to be false positives.\"; Abstract / Conclusion (page 640 / 649): \"Based on a manual verification carried out on statistically representative sample size, we found that most samples are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR).\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Uses TTS to generate the input audio - non in memory mutation"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "CrossASR generates test inputs by synthesizing speech audio from text via Text-To-Speech (TTS) engines and represents each test case as a tuple that includes the TTS-generated audio. That is, inputs are produced directly as serialized audio artifacts (speech audio files) rather than as ephemeral in-memory floating-point perturbations. Because CrossASR feeds these synthesized audio artifacts to ASRs and records them as part of the generated test cases (s in the 5-tuple), the failures it detects are based on persisted, serialized inputs and thus are stable with respect to standard I/O (quantization/rounding/clipping) effects. The paper also indicates metamorphic transformations are future work, implying the current approach does not rely on fine-grained numerical mutations that could be lost on serialization.",
                    "evidence": "Section II-A (Architecture), p. 642: \"Each piece of text in the batch is then fed into the Text-To-Speech Engine (TTS). The TTS converts the Text into a Speech Audio. The Speech Audio is then fed to the Automatic Speech Recognition (ASR) Engines.\"; Section II-A (Architecture), p. 642: \"Each 5-tuple is of the format t, s, a, t', l, where t is the input text, s is the TTS generated audio, a is the ASR used to transcribe t, t' is the ASR transcribed text, and l is the status of the test case.\"; Conclusion (VI), p. 649: \"We want to add some metamorphic transformations to generate more speech audios that can lead to failed test cases.\" (indicating such transformations are future work and not the current source of test mutations)."
                },
                "gemini": {
                    "value": "High",
                    "why": "The method generates test inputs (speech audio) directly in a serialized format using Text-To-Speech (TTS) engines. As stated in the metric definition, inputs generated this way are considered inherently stable and reproducible under standard I/O operations. The approach does not rely on applying fine-grained numerical perturbations to in-memory data representations, thus avoiding the issue of fault-inducing changes being lost during serialization (e.g., through quantization or clipping). The generated audio file is the stable test case, and failures are determined by the ASR's ability to transcribe it correctly.",
                    "evidence": "Page 3, Section II-A. Architecture: 'Given a Text Collection, the Test Selection Engine selects a Text Batch. Each Text in the Text Batch is then fed into the Text-To-Speech Engine (TTS). The TTS converts the Text into a Speech Audio. The Speech Audio is then fed to the Automatic Speech Recognition (ASR) Engines.' This quote, supported by Figure 1, demonstrates that the test cases are synthesized directly into a serialized audio format, ensuring they are reproducible by design."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "CrossASR generates test inputs by synthesizing speech audio from text via Text-To-Speech (TTS) engines and represents each test case as a tuple that includes the TTS-generated audio. That is, inputs are produced directly as serialized audio artifacts (speech audio files) rather than as ephemeral in-memory floating-point perturbations. Because CrossASR feeds these synthesized audio artifacts to ASRs and records them as part of the generated test cases (s in the 5-tuple), the failures it detects are based on persisted, serialized inputs and thus are stable with respect to standard I/O (quantization/rounding/clipping) effects. The paper also indicates metamorphic transformations are future work, implying the current approach does not rely on fine-grained numerical mutations that could be lost on serialization.",
                    "evidence": "Section II-A (Architecture), p. 642: \"Each piece of text in the batch is then fed into the Text-To-Speech Engine (TTS). The TTS converts the Text into a Speech Audio. The Speech Audio is then fed to the Automatic Speech Recognition (ASR) Engines.\"; Section II-A (Architecture), p. 642: \"Each 5-tuple is of the format t, s, a, t', l, where t is the input text, s is the TTS generated audio, a is the ASR used to transcribe t, t' is the ASR transcribed text, and l is the status of the test case.\"; Conclusion (VI), p. 649: \"We want to add some metamorphic transformations to generate more speech audios that can lead to failed test cases.\" (indicating such transformations are future work and not the current source of test mutations)."
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "Utilize multiple ASRs to find faults using differential oracle but does not analyze if same error is reproducible to other model"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models, nor does it present experiments or analyses measuring transferability. CrossASR runs each synthesized audio against multiple ASRs simultaneously (differential testing) and reports per-ASR failure counts, but it does not (a) create inputs targeted at one surrogate ASR and then reuse them to test other ASRs, (b) measure overlap/transfer of failures across models, or (c) claim design choices explicitly intended to discover transferable attacks. The classifier (failure probability predictor) is trained with labels per ASR to predict likely failures, but the approach and experiments focus on efficiently finding failures per ASR rather than on demonstrating or quantifying transferability across ASRs.",
                    "evidence": "Page 642, I. Introduction: \"CrossASR performs a black-box differential testing to uncover erroneous behaviors on ASR systems by cross-referencing different ASRs to detect different transcriptions from ASRs.\" \n\nPage 643, II-B Test Selection Engine: \"For this training, a text has m labels, each corresponds to an ASR.\" \n\nPage 646, III-D RQ3 / Table V: \"Table V shows the total number of failed test cases that is generated from the entire 20,000 texts for all combinations of experimented TTSes and ASRs.\" (Table V reports per-ASR failed-case counts but no cross-model transfer/overlap statistics.) \n\nPage 649, VI Conclusion / Future Work: \"In the future, we plan to evaluate CrossASR using additional ASRs and TTSes.\" (Indicates no current evaluation of transferability across broader sets of models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, CrossASR, is based on differential testing, not attack transferability. It generates a single audio input and feeds it to multiple Automatic Speech Recognition (ASR) systems simultaneously. A failure is defined based on a disagreement among the outputs of these systems. Specifically, a test case is considered a failure for a particular ASR if its transcription does not match the input text, while at least one other ASR's transcription does. The methodology does not involve generating a fault-inducing input on a single source/surrogate model and then reusing that specific input to test for failures on other target models. The evaluation counts the total number of failures detected for each ASR system individually but does not analyze or report on whether the same audio inputs caused failures across multiple ASRs. Therefore, it does not demonstrate the transferability of fault-inducing inputs.",
                    "evidence": "Page 3, Section II-A, Architecture, Case 2: \"At least one of the ASR transcribed texts that matches with the input Text while at least one of the other ASR Transcribed Texts do not match with the input Text. In this case, CrossASR considers cases where the transcribed text does not match input text as failed test cases...\" This describes the core differential testing mechanism, where a failure in one model is identified by its disagreement with a successful transcription from another model for the same input, rather than testing for transferability of a known failing input. \n\nPage 6, Table V, \"NUMBER OF FAILED TEST CASES GENERATED FOR EACH COMBINATION OF TTSES AND ASRS\": This table presents the total number of failures for each ASR model as separate, aggregated counts. For example, using the Google TTS, it found 4,036 failures for Deepspeech and 2,539 for Deepspeech2. The paper does not analyze the intersection of these sets to determine how many specific audio files caused failures in both models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models, nor does it present experiments or analyses measuring transferability. CrossASR runs each synthesized audio against multiple ASRs simultaneously (differential testing) and reports per-ASR failure counts, but it does not (a) create inputs targeted at one surrogate ASR and then reuse them to test other ASRs, (b) measure overlap/transfer of failures across models, or (c) claim design choices explicitly intended to discover transferable attacks. The classifier (failure probability predictor) is trained with labels per ASR to predict likely failures, but the approach and experiments focus on efficiently finding failures per ASR rather than on demonstrating or quantifying transferability across ASRs.",
                    "evidence": "Page 642, I. Introduction: \"CrossASR performs a black-box differential testing to uncover erroneous behaviors on ASR systems by cross-referencing different ASRs to detect different transcriptions from ASRs.\" \n\nPage 643, II-B Test Selection Engine: \"For this training, a text has m labels, each corresponds to an ASR.\" \n\nPage 646, III-D RQ3 / Table V: \"Table V shows the total number of failed test cases that is generated from the entire 20,000 texts for all combinations of experimented TTSes and ASRs.\" (Table V reports per-ASR failed-case counts but no cross-model transfer/overlap statistics.) \n\nPage 649, VI Conclusion / Future Work: \"In the future, we plan to evaluate CrossASR using additional ASRs and TTSes.\" (Indicates no current evaluation of transferability across broader sets of models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Asyrofi et al. - 2021 - CrossASR++ a modular differential testing framework for automatic speech recognition": {
        "key": "asyrofi2021crossasr++",
        "name": "CrossASR++",
        "year": "2021",
        "citation_count": "25",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper defines and evaluates failures solely as recognition mismatches discovered via differential (cross-referencing) testing\u2014i.e., cases where the system under test transcription does not match the original text while another ASR does, or cases deemed indeterminable due to TTS. All technical descriptions, workflow, and experiments focus on counting such failed test cases (robustness/misprediction errors). The paper does not describe uncovering unsafe/undesired outputs (e.g., toxic/bias/hallucination), nor any attempt to bypass safety, policy, or security mechanisms or to elicit data leakage or unauthorized actions in defended or safety-aligned models.",
                    "evidence": "Section 1 (Introduction), p.1575: \"If the transcribed text recognized by the ASR system under test does not match the text input to TTS while another ASR\u2019s transcribed text does, this input audio is viewed as a failed test case.\"; Section 2.1 (Workflow), p.1576: \"For a piece of audio and its corresponding transcription, there are three possible situations: 1) if the SUT can recognize it successfully, i.e. the transcript matches the original text... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT. 3) If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case...\"; Section 3 (Experiments), p.1577: evaluation reports only counts of \"failed test cases\" (e.g., \"CrossASR++ can find 26.2% more failed test cases than the original tool\"), with no discussion of safety-policy bypasses or high-severity failures."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, CrossASR++, uses differential testing to find failures in Automatic Speech Recognition (ASR) systems. A failure is defined as an instance where the ASR system under test incorrectly transcribes an audio file generated from a known text, while another ASR system transcribes it correctly. This process identifies prediction inconsistencies and mispredictions, which are classic model robustness errors. The paper does not describe any attempt to generate or identify failures related to unsafe, biased, or toxic content, nor does it target the bypassing of any safety or security mechanisms. The sole focus is on the correctness of the speech-to-text transcription.",
                    "evidence": "Page 2, Section 1, Introduction: 'If the transcribed text recognized by the ASR system under test does not match the text input to TTS while another ASR's transcribed text does, this input audio is viewed as a failed test case.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper defines and evaluates failures solely as recognition mismatches discovered via differential (cross-referencing) testing\u2014i.e., cases where the system under test transcription does not match the original text while another ASR does, or cases deemed indeterminable due to TTS. All technical descriptions, workflow, and experiments focus on counting such failed test cases (robustness/misprediction errors). The paper does not describe uncovering unsafe/undesired outputs (e.g., toxic/bias/hallucination), nor any attempt to bypass safety, policy, or security mechanisms or to elicit data leakage or unauthorized actions in defended or safety-aligned models.",
                    "evidence": "Section 1 (Introduction), p.1575: \"If the transcribed text recognized by the ASR system under test does not match the text input to TTS while another ASR\u2019s transcribed text does, this input audio is viewed as a failed test case.\"; Section 2.1 (Workflow), p.1576: \"For a piece of audio and its corresponding transcription, there are three possible situations: 1) if the SUT can recognize it successfully, i.e. the transcript matches the original text... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT. 3) If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case...\"; Section 3 (Experiments), p.1577: evaluation reports only counts of \"failed test cases\" (e.g., \"CrossASR++ can find 26.2% more failed test cases than the original tool\"), with no discussion of safety-policy bypasses or high-severity failures."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "TTS engines are inherently designed to produce natural-sounding speech from text, thereby providing a theoretical justification and design constraint for input naturalness."
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "CrossASR++ generates test inputs via TTS and enforces a plausibility-based filter in its differential-testing logic: an audio is counted as a failed test case for the System Under Test (SUT) only if the SUT fails but at least one other ASR recognizes the audio correctly. This implicitly filters out wholly implausible/invalid TTS outputs (which the paper calls \"indeterminable\"). The paper also explicitly discusses that better TTSes produce fewer invalid/indeterminable audios and that indeterminable cases exist. However, the paper does not provide explicit perceptual or task-specific quantitative assessments (e.g., PESQ, STOI, human evaluation, or other audio-quality/intelligibility metrics) of the final fault-inducing audios. Because CrossASR++ therefore enforces plausibility via cross-referencing (a form of validation of final inputs) but lacks dedicated empirical perceptual/quality evaluation, the appropriate rating is Medium.",
                    "evidence": "Section 2.1 Workflow (p.1576): \"For a piece of audio and its corresponding transcription, there are three possible situations: 1) if the SUT can recognize it successfully... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT. 3) If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case because a TTS may generate an invalid audio.\" (Section 2.1, Workflow, p.1576) Section 2.2 Extensibility (p.1576): \"The main reason is that more advanced TTSes generate fewer invalid audios that are more likely to be indeterminable test cases.\" (Section 2.2, p.1576) Section 3 Experiments (p.1577): experimental evaluation focuses on counts of \"# Failed Test Case\" and ablation of features (visibility, added ASRs, estimators) \u2014 no reported perceptual or speech-quality / intelligibility metrics or human evaluation is provided in the experiments or results tables (e.g., Tables 1\u20133 measure numbers of failed test cases). (Section 3, RQ1/RQ2, p.1577)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The method generates test inputs by synthesizing audio from a text corpus using standard Text-to-Speech (TTS) engines. This design inherently constrains the inputs to be plausible speech, as TTS systems are designed to produce human-like utterances. This serves as a strong, rule-based constraint that enforces input plausibility in the generation step. However, the paper does not provide any empirical validation, either through human studies or quantitative metrics (e.g., PESQ, STOI), to confirm that the final, failure-inducing audio inputs are realistic and intelligible. The paper acknowledges that TTS can produce 'invalid audios,' but these are filtered out as 'indeterminable test cases' based on consensus among ASR systems, not by a direct assessment of plausibility. This approach fits the 'Medium' criteria, as it enforces plausibility during generation but does not empirically validate the final outputs.",
                    "evidence": "Page 3, Section 2.1, Workflow: 'Then, TTS is used to generate an audio file for each piece of selected text (Line 9)... If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case because a TTS may generate an invalid audio.' This shows the method of generation (TTS) and the indirect way of handling potentially implausible ('invalid') inputs. The Experiments section (Section 3) evaluates the method based on the number of failed test cases found, without reporting any measures of the audio inputs' realism or quality."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "CrossASR++ generates test inputs via TTS and enforces a plausibility-based filter in its differential-testing logic: an audio is counted as a failed test case for the System Under Test (SUT) only if the SUT fails but at least one other ASR recognizes the audio correctly. This implicitly filters out wholly implausible/invalid TTS outputs (which the paper calls \"indeterminable\"). The paper also explicitly discusses that better TTSes produce fewer invalid/indeterminable audios and that indeterminable cases exist. However, the paper does not provide explicit perceptual or task-specific quantitative assessments (e.g., PESQ, STOI, human evaluation, or other audio-quality/intelligibility metrics) of the final fault-inducing audios. Because CrossASR++ therefore enforces plausibility via cross-referencing (a form of validation of final inputs) but lacks dedicated empirical perceptual/quality evaluation, the appropriate rating is Medium.",
                    "evidence": "Section 2.1 Workflow (p.1576): \"For a piece of audio and its corresponding transcription, there are three possible situations: 1) if the SUT can recognize it successfully... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT. 3) If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case because a TTS may generate an invalid audio.\" (Section 2.1, Workflow, p.1576) Section 2.2 Extensibility (p.1576): \"The main reason is that more advanced TTSes generate fewer invalid audios that are more likely to be indeterminable test cases.\" (Section 2.2, p.1576) Section 3 Experiments (p.1577): experimental evaluation focuses on counts of \"# Failed Test Case\" and ablation of features (visibility, added ASRs, estimators) \u2014 no reported perceptual or speech-quality / intelligibility metrics or human evaluation is provided in the experiments or results tables (e.g., Tables 1\u20133 measure numbers of failed test cases). (Section 3, RQ1/RQ2, p.1577)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "CrossASR++ generates test inputs by synthesizing and saving audio files via TTS (i.e., it produces serialized audio inputs that are used for cross-referencing). Because the tool produces and persists TTS-generated audio files (and even provides these audio files in its repository for reuse), the failure-inducing inputs are inherently serialized and thus stable with respect to standard I/O operations (they are not ephemeral in-memory floating\u2011point perturbations that would be lost by quantization/rounding). The paper therefore meets the metric criterion that inputs are generated in serialized form and remain reproducible under standard I/O operations.",
                    "evidence": "Section 2.1 Workflow (page 1576): \"Then, TTS is used to generate an audio file for each piece of selected text (Line 9).\" (Algorithm 1 shows: audios = tts.generateAudio(texts);). Section 2.4.2 Configuration and Execution (page 1577): \"In the Github repository, we also provide all the TTS-generated audios we use. Users can set recompute as false to use them directly.\" Section 2.2 Extensibility (page 1576): \"In TTS class, the method generateAudio() which converts a text into audio must be overrided by derived classes.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The method generates test cases by synthesizing speech from text using Text-to-Speech (TTS) engines. This process directly creates audio files in a standard, serialized format (e.g., WAV). The metric definition explicitly states that approaches generating inputs directly in serialized formats, such as synthesized speech, are considered reproducible because they are inherently stable with respect to standard I/O operations. The framework does not apply fine-grained numerical mutations to in-memory audio tensors, thus bypassing the problem of perturbations being lost during serialization (e.g., through quantization or rounding). The generated audio file itself is the test case, ensuring its reproducibility.",
                    "evidence": "Page 3, Section 2.1 Workflow, Algorithm 1: The paper describes the core workflow where text is converted directly into audio. The algorithm states: 'audios = tts.generateAudio(texts);' (Line 9). This shows that the test inputs are synthesized directly into an audio format. The abstract also confirms this approach: 'This method first utilizes Text-to-Speech (TTS) to generate audios from texts automatically and then feed these audios into different ASR systems for cross-referencing to uncover failed test cases.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "CrossASR++ generates test inputs by synthesizing and saving audio files via TTS (i.e., it produces serialized audio inputs that are used for cross-referencing). Because the tool produces and persists TTS-generated audio files (and even provides these audio files in its repository for reuse), the failure-inducing inputs are inherently serialized and thus stable with respect to standard I/O operations (they are not ephemeral in-memory floating\u2011point perturbations that would be lost by quantization/rounding). The paper therefore meets the metric criterion that inputs are generated in serialized form and remain reproducible under standard I/O operations.",
                    "evidence": "Section 2.1 Workflow (page 1576): \"Then, TTS is used to generate an audio file for each piece of selected text (Line 9).\" (Algorithm 1 shows: audios = tts.generateAudio(texts);). Section 2.4.2 Configuration and Execution (page 1577): \"In the Github repository, we also provide all the TTS-generated audios we use. Users can set recompute as false to use them directly.\" Section 2.2 Extensibility (page 1576): \"In TTS class, the method generateAudio() which converts a text into audio must be overrided by derived classes.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "Uses multiple ASRs in differential oracle, does not show transferability of failures across errors"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper describes a differential testing tool that generates the same TTS-produced inputs and feeds them to multiple ASRs for cross-referencing (so inputs are evaluated across ASRs concurrently), but it does not evaluate or claim that fault-inducing inputs discovered on one model are reused to test transferability to other (unseen) models, nor does it include experiments that measure cross-model transfer of faults or support surrogate-model attacks. The evaluation focuses on counting failed test cases per SUT and on how adding more ASRs or changing estimator/visibility affects the number of failures found, without demonstrating or measuring whether inputs that fail one model also cause failures in other models beyond the concurrent cross-referencing setup or in unseen targets.",
                    "evidence": "Page 1576, Section 2.1 Workflow: \"we use all ASR systems (including the System Under Test (SUT)) to convert the audio files back to texts (Line 11), and then we perform cross-reference (Line 12). For a piece of audio and its corresponding transcription, there are three possible situations: ... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\"  Page 1577, Section 3 RQ1 and Table 1: RQ1 is \"How many failed test cases can CrossASR++ find?\" and results report counts of failed test cases per SUT under different configurations (no analysis of reuse/transfer to unseen models).  Page 1577, Section 3 RQ2 / Table 2: \"By simply adding one more ASR for cross-referencing, we can increase the number of failed test cases uncovered for 4 SUTs by 25.07%, 39.63%, 20.95% and 8.17% respectively.\" (shows cross-referencing effect but not a transferability experiment).  Page 1578, Conclusion: \"CrossASR++ , an extensible tool that performs black-box differential testing on ASR systems... The extensibility allows CrossASR++ to flexibly utilizes more tools to boost its performance on uncovering failed test cases.\" (no mention of evaluating or supporting transferable/substitute-model attacks)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, CrossASR++, is a differential testing framework. A test case is defined as a failure for a specific System Under Test (SUT) if that SUT fails to transcribe an audio input correctly, while at least one other ASR system succeeds. This design inherently focuses on finding disagreements between models, rather than shared vulnerabilities or transferable failures. The case where all ASR models fail on the same input is categorized as an \"indeterminable test case\" and is not treated as a successful discovery of a fault. The paper does not reuse the inputs that are found to cause failures in one model to evaluate if they also induce failures in other models. Therefore, it does not demonstrate the transferability of fault-inducing inputs.",
                    "evidence": "Page 3, Section 2.1 Workflow, \"For a piece of audio and its corresponding transcription, there are three possible situations: ... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT. 3) If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case because a TTS may generate an invalid audio.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper describes a differential testing tool that generates the same TTS-produced inputs and feeds them to multiple ASRs for cross-referencing (so inputs are evaluated across ASRs concurrently), but it does not evaluate or claim that fault-inducing inputs discovered on one model are reused to test transferability to other (unseen) models, nor does it include experiments that measure cross-model transfer of faults or support surrogate-model attacks. The evaluation focuses on counting failed test cases per SUT and on how adding more ASRs or changing estimator/visibility affects the number of failures found, without demonstrating or measuring whether inputs that fail one model also cause failures in other models beyond the concurrent cross-referencing setup or in unseen targets.",
                    "evidence": "Page 1576, Section 2.1 Workflow: \"we use all ASR systems (including the System Under Test (SUT)) to convert the audio files back to texts (Line 11), and then we perform cross-reference (Line 12). For a piece of audio and its corresponding transcription, there are three possible situations: ... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\"  Page 1577, Section 3 RQ1 and Table 1: RQ1 is \"How many failed test cases can CrossASR++ find?\" and results report counts of failed test cases per SUT under different configurations (no analysis of reuse/transfer to unseen models).  Page 1577, Section 3 RQ2 / Table 2: \"By simply adding one more ASR for cross-referencing, we can increase the number of failed test cases uncovered for 4 SUTs by 25.07%, 39.63%, 20.95% and 8.17% respectively.\" (shows cross-referencing effect but not a transferability experiment).  Page 1578, Conclusion: \"CrossASR++ , an extensible tool that performs black-box differential testing on ASR systems... The extensibility allows CrossASR++ to flexibly utilizes more tools to boost its performance on uncovering failed test cases.\" (no mention of evaluating or supporting transferable/substitute-model attacks)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Data-driven"
            ]
        }
    },
    "Asyrofi et al. - 2021 - Can Differential Testing Improve Automatic Speech Recognition Systems": {
        "key": "asyrofi2021can",
        "name": "Asyrofi et al.",
        "year": "2021",
        "citation_count": "16",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's differential-testing approach (CrossASR++) identifies failures solely as cases where the ASR under test mis-transcribes a synthesized audio compared to the input text. The evaluation and improvements are measured by counts of such failed test cases and by word error rate (WER), classic robustness metrics for mispredictions. The paper contains no description or experiments that uncover unsafe, biased, hallucinated outputs, privacy/data-leakage, authorization bypasses, jailbreaks, or any bypassing of explicit safety/security mechanisms. Therefore the failures exposed are model robustness errors (mispredictions) and correspond to the 'Low' severity category.",
                    "evidence": "Page 675, Section II.A (CrossASR++): \"The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1). The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio (i.e., each word in the transcription matches each corresponding word in the input text). A valid test case can be either a failed test case or a successful test case. If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT.\" Page 677, Section III.C (Preliminary Evaluation): \"For the second evaluation, the WER is the most common metric to evaluate the performance of ASR systems... The WER is computed as follows: WER = (I + D + S) / N.\" (These passages show failures are defined as mispredictions and evaluated by WER; no safety/security violations or bypassing of defenses are discussed anywhere in the paper.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's methodology focuses exclusively on uncovering and correcting model robustness errors, specifically mispredictions in Automatic Speech Recognition (ASR) systems. A failure is strictly defined as an instance where the ASR system's output transcription does not match the ground-truth input text. The evaluation metrics used are the 'number of failed test cases' and 'Word Error Rate (WER)', both of which measure transcription accuracy. The paper does not investigate or report on failures related to unsafe behavior (e.g., toxic or biased outputs) nor does it attempt to bypass any explicit safety or security mechanisms. The uncovered failures are purely prediction inconsistencies.",
                    "evidence": "Page 2, Section II-A, 'CrossASR++': 'A valid test case can be either a failed test case or a successful test case. If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT. Otherwise, a successful test case is found.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's differential-testing approach (CrossASR++) identifies failures solely as cases where the ASR under test mis-transcribes a synthesized audio compared to the input text. The evaluation and improvements are measured by counts of such failed test cases and by word error rate (WER), classic robustness metrics for mispredictions. The paper contains no description or experiments that uncover unsafe, biased, hallucinated outputs, privacy/data-leakage, authorization bypasses, jailbreaks, or any bypassing of explicit safety/security mechanisms. Therefore the failures exposed are model robustness errors (mispredictions) and correspond to the 'Low' severity category.",
                    "evidence": "Page 675, Section II.A (CrossASR++): \"The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1). The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio (i.e., each word in the transcription matches each corresponding word in the input text). A valid test case can be either a failed test case or a successful test case. If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT.\" Page 677, Section III.C (Preliminary Evaluation): \"For the second evaluation, the WER is the most common metric to evaluate the performance of ASR systems... The WER is computed as follows: WER = (I + D + S) / N.\" (These passages show failures are defined as mispredictions and evaluated by WER; no safety/security violations or bypassing of defenses are discussed anywhere in the paper.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility via a rule-based cross-referencing filter: synthesized audio-text pairs are accepted as valid only if at least one ASR correctly transcribes them, which ensures generated inputs are intelligible/valid at the time of creation. However, the paper does not provide any human evaluation or task-specific perceptual/semantic quality metrics (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for the final fault-inducing inputs, nor does it report additional quantitative checks of perceptual realism. Also, although their cross-referencing filter enforces plausibility per generated audio, the work does not report explicit mechanisms or analyses addressing cumulative mutations or perceptual assessment of final failing inputs. These facts align with the \"Medium\" level: plausibility is enforced by rule-based constraints but there is no empirical perceptual validation of final fault-triggering inputs.",
                    "evidence": "Section II.A (p.675): \"to avoid using the invalid audios synthesized by the TTS system, the generated audio-text pair is fed into multiple ASR systems for cross-referencing: If at least one ASR system can successfully transcribe the audio, the audio-text pair is viewed as a valid test case.\"; Section II.A (p.675): \"We take the audio-text pair as an indeterminable test case if none of them can successfully transcribe the audio.\"; Section III.B (p.676): \"The TTS system is ResponsiveVoice. The 5 ASR systems are DeepSpeech, DeepSpeech2, Wav2Letter++, Wav2Vec2, and Wit.\"; Section III.C (p.676): \"We use two metrics to evaluate the performance of the SUT. One is the number of failed test cases uncovered by CrossASR++, and the other is the word error rate (WER) on a test set.\". The paper contains no reported human study or perceptual/quality measures (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for assessing realism of final fault-inducing inputs."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method generates audio test cases from a real-world text corpus using a Text-to-Speech (TTS) system. It incorporates a mechanism to enforce input plausibility by filtering the generated audio. Specifically, an audio file is considered a 'valid test case' only if at least one of the multiple Automatic Speech Recognition (ASR) systems used for cross-referencing can transcribe it correctly. This step is explicitly designed to discard 'invalid audios synthesized by the TTS system,' which serves as a proxy for filtering out unintelligible or unrealistic speech. However, the paper does not provide any additional empirical validation of the plausibility of the final fault-inducing audio inputs, such as through human evaluation or by reporting quantitative speech quality metrics (e.g., PESQ, STOI). The method enforces plausibility during the test case generation and filtering process but lacks a final, explicit assessment of the resulting inputs.",
                    "evidence": "Page 2, Section II.A, paragraph 1: \"The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio (i.e., each word in the transcription matches each corresponding word in the input text). A valid test case can be either a failed test case or a successful test case... We take the audio-text pair as an indeterminable test case if none of them can successfully transcribe the audio.\"\nThis quote describes the filtering mechanism that ensures a baseline level of intelligibility for an input to be considered 'valid', thus enforcing a form of plausibility. The paper lacks any further assessment beyond this."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility via a rule-based cross-referencing filter: synthesized audio-text pairs are accepted as valid only if at least one ASR correctly transcribes them, which ensures generated inputs are intelligible/valid at the time of creation. However, the paper does not provide any human evaluation or task-specific perceptual/semantic quality metrics (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for the final fault-inducing inputs, nor does it report additional quantitative checks of perceptual realism. Also, although their cross-referencing filter enforces plausibility per generated audio, the work does not report explicit mechanisms or analyses addressing cumulative mutations or perceptual assessment of final failing inputs. These facts align with the \"Medium\" level: plausibility is enforced by rule-based constraints but there is no empirical perceptual validation of final fault-triggering inputs.",
                    "evidence": "Section II.A (p.675): \"to avoid using the invalid audios synthesized by the TTS system, the generated audio-text pair is fed into multiple ASR systems for cross-referencing: If at least one ASR system can successfully transcribe the audio, the audio-text pair is viewed as a valid test case.\"; Section II.A (p.675): \"We take the audio-text pair as an indeterminable test case if none of them can successfully transcribe the audio.\"; Section III.B (p.676): \"The TTS system is ResponsiveVoice. The 5 ASR systems are DeepSpeech, DeepSpeech2, Wav2Letter++, Wav2Vec2, and Wit.\"; Section III.C (p.676): \"We use two metrics to evaluate the performance of the SUT. One is the number of failed test cases uncovered by CrossASR++, and the other is the word error rate (WER) on a test set.\". The paper contains no reported human study or perceptual/quality measures (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for assessing realism of final fault-inducing inputs."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper's test-generation pipeline synthesizes and uses audio files directly via a TTS system (i.e., inputs are created in serialized audio form and stored as audio\u2013text pairs). According to the metric, approaches that generate inputs directly in serialized formats (such as synthesized speech) are reproducible under standard I/O operations because the fault-inducing perturbations exist in the saved audio files rather than only in transient in-memory floating-point tensors. The paper both (1) synthesizes and feeds audio files to ASR systems and (2) stores those audio\u2013text pairs and fine-tunes models on them, which indicates failures are tied to persisted audio files rather than fragile in-memory numerical perturbations that could be lost by serialization.",
                    "evidence": "Page 675, Section II.A (CrossASR++): \"Specifically, given a piece of text from the corpus, it first employs a TTS system to synthesize an audio file from the text. The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1).\"; Page 675, Section II.B (Improving ASR Systems) and Fig.1 caption: \"We store test cases for both the ASR system under test (SUT) and cross-referenced AS systems and then fine-tune these ASR systems on their corresponding test cases.\"; Abstract: \"It employs a Text-To-Speech (TTS) system to synthesize audios from texts and then reveals failed test cases by feeding them to multiple ASR systems for cross-referencing.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The method generates test cases by synthesizing audio from text using a Text-To-Speech (TTS) system. This process creates test inputs directly in a serialized audio format. The metric definition explicitly states that fuzzing approaches generating inputs directly in serialized formats (e.g., synthesized speech) are considered reproducible, as such inputs are inherently stable with respect to standard I/O operations. The fault-inducing properties are embedded within the synthesized audio file itself, not in fragile, in-memory numerical perturbations that could be lost during serialization processes like rounding or quantization.",
                    "evidence": "Page 2, Section II-A, CrossASR++: 'Specifically, given a piece of text from the corpus, it first employs a TTS system to synthesize an audio file from the text. The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1).'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper's test-generation pipeline synthesizes and uses audio files directly via a TTS system (i.e., inputs are created in serialized audio form and stored as audio\u2013text pairs). According to the metric, approaches that generate inputs directly in serialized formats (such as synthesized speech) are reproducible under standard I/O operations because the fault-inducing perturbations exist in the saved audio files rather than only in transient in-memory floating-point tensors. The paper both (1) synthesizes and feeds audio files to ASR systems and (2) stores those audio\u2013text pairs and fine-tunes models on them, which indicates failures are tied to persisted audio files rather than fragile in-memory numerical perturbations that could be lost by serialization.",
                    "evidence": "Page 675, Section II.A (CrossASR++): \"Specifically, given a piece of text from the corpus, it first employs a TTS system to synthesize an audio file from the text. The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1).\"; Page 675, Section II.B (Improving ASR Systems) and Fig.1 caption: \"We store test cases for both the ASR system under test (SUT) and cross-referenced AS systems and then fine-tune these ASR systems on their corresponding test cases.\"; Abstract: \"It employs a Text-To-Speech (TTS) system to synthesize audios from texts and then reveals failed test cases by feeding them to multiple ASR systems for cross-referencing.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "Does not demonstrate if fuzzed input on one model aslo causes error in another model"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. While CrossASR++ feeds each synthesized audio to multiple ASR systems simultaneously (so generated inputs are executed on several models), the authors do not (a) craft fault-inducing inputs on a surrogate model and then reuse them to test other models, nor (b) analyze whether fault-inducing inputs discovered for one model also trigger failures in other models. The experiments focus on (i) fine-tuning the SUT with generated test cases to reduce its failures, and (ii) fine-tuning one cross-referenced ASR to improve CrossASR++\u2019s ability to find failures. These evaluations measure improvement in detection and model quality, not the transferability of faults between independently-trained target models.",
                    "evidence": "Page 675, Section II.A: \"The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1). The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio.\" \n\nPage 677, Section III.C (RQ2 - Experiment Design & Result): \"In this research question, we want to investigate whether fine-tuning one of the cross-referenced ASR systems using generated test cases can boost the performance of CrossASR++ to uncover failed test cases... For the evolved CrossASR++, we continuously evolve DeepSpeech and always utilize DeepSpeech as one of the ASR systems used for cross-referencing... By comparing the number of failed test cases found by the static and evolved CrossASR++ head-to-head, it is clear that the evolved CrossASR++ uncovers more failed test cases than the static one for each ASR system.\" \n\n(These excerpts show that inputs are evaluated across multiple ASRs and that the paper measures improvements from fine-tuning, but there is no experiment or analysis that shows fault-inducing inputs crafted on one (surrogate) model transfer to other models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, evolutionary differential testing, uses multiple Automatic Speech Recognition (ASR) models for cross-referencing. However, this is done to create a reliable test oracle\u2014a test case is deemed valid if at least one model can correctly transcribe the audio. A failure is then identified specifically for the System Under Test (SUT) if it fails on this valid test case. The paper does not investigate or demonstrate whether these fault-inducing inputs for one SUT also trigger failures in other ASR models. The generated failed test cases are used to fine-tune and improve the SUT itself, rather than being reused to test for attack transferability across different models. The core focus is on improving a single model or the testing tool, not on discovering shared vulnerabilities.",
                    "evidence": "Page 2, Section II-A, \"CrossASR++\": \"A valid test case can be either a failed test case or a successful test case. If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT. Otherwise, a successful test case is found.\" This quote illustrates that a failure is defined and recorded for a single, specific SUT, not in the context of transferability to other models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. While CrossASR++ feeds each synthesized audio to multiple ASR systems simultaneously (so generated inputs are executed on several models), the authors do not (a) craft fault-inducing inputs on a surrogate model and then reuse them to test other models, nor (b) analyze whether fault-inducing inputs discovered for one model also trigger failures in other models. The experiments focus on (i) fine-tuning the SUT with generated test cases to reduce its failures, and (ii) fine-tuning one cross-referenced ASR to improve CrossASR++\u2019s ability to find failures. These evaluations measure improvement in detection and model quality, not the transferability of faults between independently-trained target models.",
                    "evidence": "Page 675, Section II.A: \"The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1). The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio.\" \n\nPage 677, Section III.C (RQ2 - Experiment Design & Result): \"In this research question, we want to investigate whether fine-tuning one of the cross-referenced ASR systems using generated test cases can boost the performance of CrossASR++ to uncover failed test cases... For the evolved CrossASR++, we continuously evolve DeepSpeech and always utilize DeepSpeech as one of the ASR systems used for cross-referencing... By comparing the number of failed test cases found by the static and evolved CrossASR++ head-to-head, it is clear that the evolved CrossASR++ uncovers more failed test cases than the static one for each ASR system.\" \n\n(These excerpts show that inputs are evaluated across multiple ASRs and that the paper measures improvements from fine-tuning, but there is no experiment or analysis that shows fault-inducing inputs crafted on one (surrogate) model transfer to other models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Data-driven"
            ]
        }
    },
    "Pei et al. - 2017 - DeepXplore Automated Whitebox Testing of Deep Learning Systems": {
        "key": "pei2017deepxplore",
        "name": "DeepXplore",
        "year": "2017",
        "citation_count": "1814",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": "reveals incorrect corner-case behaviors but does not bypass explicity safety mechinasims"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper demonstrates that DeepXplore uncovers failures that violate user intent and safety expectations \u2014 e.g., it finds inputs that cause self-driving models to make dangerous steering decisions (\"crashing into guard rails\") and causes malware detectors to misclassify malicious files as benign \u2014 which are high-impact in real-world terms. However, the paper does not claim or demonstrate that DeepXplore specifically bypasses explicit safety, policy, or security mechanisms in defended or safety-aligned models (e.g., adversarially-trained or policy-enforced systems), nor does it claim to induce jailbreaks, data leakage, or unauthorized actions by circumventing explicit defenses. Moreover, the authors explicitly note limitations of differential testing (it requires multiple models and cannot find failures if all models make the same mistake), indicating the method is not presented as a technique for bypassing explicit safeguards. Thus the failures exposed match the metric's description for Medium severity (unsafe/undesired behaviors without demonstrated bypassing of explicit defenses).",
                    "evidence": "Page 1, Abstract: \"DeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models...\"; Page 11, Section 7 / Table 3 caption: \"The features added to the manifest file by DeepXplore for generating two sample malware inputs which Android app classifiers (Drebin) incorrectly mark as benign.\"; Page 14, Section 8 Limitations: \"First, differential testing requires at least two different DNNs with the same functionality... Second, differential testing can only detect an erroneous behavior if at least one DNN produces different results than other DNNs. If all the tested DNNs make the same mistake, DeepXplore cannot generate the corresponding test case.\"; Page 1, Introduction: \"In safety- and security-critical settings, such incorrect behaviors can lead to disastrous consequences such as a fatal collision of a self-driving car.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper demonstrates that DeepXplore uncovers failures that go beyond simple model robustness errors and directly violate user and safety expectations by producing unsafe or security-compromising outputs. Specifically, the authors show that their method can generate inputs causing a self-driving car model to make decisions that lead to a crash, and cause malware detection models to misclassify malicious software as benign. These represent clear violations of safety and security expectations. However, the paper does not provide evidence of bypassing explicit, engineered safety or security mechanisms in defended or safety-aligned models (e.g., adversarially trained models, or models with safety filters), which is a requirement for the 'High' rating. The failures are exposed in standard, state-of-the-art models of the time, not models with specific defenses.",
                    "evidence": "Page 2, Abstract: \"DeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models...\"\n\nPage 3, Figure 1 Caption: \"An example erroneous behavior found by DeepXplore in Nvidia DAVE-2 self-driving car platform. The DNN-based self-driving car correctly decides to turn left for image (a) but incorrectly decides to turn right and crashes into the guardrail for image (b), a slightly darker version of (a).\"\n\nPage 9, Section 6.1, Test datasets and DNNs: The evaluation is performed on datasets with clear safety and security implications, including the 'Driving' dataset from Udacity's self-driving car challenge, the 'Contagio/VirusTotal' PDF malware dataset, and the 'Drebin' Android malware dataset."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper demonstrates that DeepXplore uncovers failures that violate user intent and safety expectations \u2014 e.g., it finds inputs that cause self-driving models to make dangerous steering decisions (\"crashing into guard rails\") and causes malware detectors to misclassify malicious files as benign \u2014 which are high-impact in real-world terms. However, the paper does not claim or demonstrate that DeepXplore specifically bypasses explicit safety, policy, or security mechanisms in defended or safety-aligned models (e.g., adversarially-trained or policy-enforced systems), nor does it claim to induce jailbreaks, data leakage, or unauthorized actions by circumventing explicit defenses. Moreover, the authors explicitly note limitations of differential testing (it requires multiple models and cannot find failures if all models make the same mistake), indicating the method is not presented as a technique for bypassing explicit safeguards. Thus the failures exposed match the metric's description for Medium severity (unsafe/undesired behaviors without demonstrated bypassing of explicit defenses).",
                    "evidence": "Page 1, Abstract: \"DeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models...\"; Page 11, Section 7 / Table 3 caption: \"The features added to the manifest file by DeepXplore for generating two sample malware inputs which Android app classifiers (Drebin) incorrectly mark as benign.\"; Page 14, Section 8 Limitations: \"First, differential testing requires at least two different DNNs with the same functionality... Second, differential testing can only detect an erroneous behavior if at least one DNN produces different results than other DNNs. If all the tested DNNs make the same mistake, DeepXplore cannot generate the corresponding test case.\"; Page 1, Introduction: \"In safety- and security-critical settings, such incorrect behaviors can lead to disastrous consequences such as a fatal collision of a self-driving car.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "performs untargeted exploration to reveal generic model faults (differential behaviour, inaccuracies). No targeted misclassification."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces input-plausibility via explicit, rule-based domain constraints that are applied during gradient-based mutation (the authors describe modifying gradients so each iteration preserves constraints and give concrete image/file constraints). This shows the fuzzing design actively restricts mutations to remain realistic at each mutation step. However, the paper does not provide any empirical, task-specific or human evaluation of the final fault-inducing inputs\u2019 perceptual or semantic realism (no human study or quantitative perceptual measures such as LPIPS/SSIM/PESQ/STOI/Perplexity are reported). Because the work enforces plausibility through bounded / rule-based constraints during mutation but does not report explicit empirical validation of final-input realism, the appropriate rating is Medium.",
                    "evidence": "Section 4.2 (Domain-specific constraints), p. 6: \"One important aspect of the optimization process is that the generated test inputs need to satisfy several domain-specific constraints to be physically realistic... Our algorithm ensures this property by modifying the gradient \u0434rad (line 13 in Algorithm 1) such that xi+1 = xi + s \u00b7 \u0434rad still satisfies the constraints (s is the step size in the gradient ascent).\" \n\nSection 6.2 (Domain-specific constraints), p. 9: \"DeepXplore used three different types of constraints for simulating different environment conditions of images: (1) lighting effects for simulating different intensities of lights, (2) occlusion by a single small rectangle for simulating an attacker potentially blocking some parts of a camera, and (3) occlusion by multiple tiny black rectangles for simulating effects of dirt on camera lens... The first constraint restricts image modifications so that DeepXplore can only make the image darker or brighter without changing its content. Specifically, the modification can only increase or decrease all pixel values by the same amount...\" \n\nSection 7 (Results), p. 10: \"For the experimental results shown in Figure 8, we apply three domain-specific constraints (lighting effects, occlusion by a single rectangle, and occlusion by multiple rectangles) as described in \u00a76.2.\" \n\n(Absence of empirical perceptual evaluation) The evaluation (Sections 7 and 7.2, pp. 10\u201313) reports neuron coverage, counts of difference-inducing inputs, runtimes, and retraining accuracy improvements, but contains no human-subject study or quantitative perceptual/linguistic measures (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity) assessing whether final fault-inducing inputs remain perceptually/semantically realistic."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility through explicit, rule-based, domain-specific constraints designed to generate realistic inputs. For image-based tasks, these constraints simulate plausible environmental conditions like changes in lighting, occlusions from objects, or dirt on a camera lens. For other domains like Android malware, constraints ensure that application functionality is not broken (e.g., only adding permissions, not removing them). The paper provides visual examples (Figure 8) of the generated inputs, which serve as a qualitative validation of their plausibility. However, it does not meet the 'High' criteria because it lacks a formal, empirical validation of the final fault-inducing inputs, such as a human study or quantitative perceptual quality metrics (e.g., LPIPS, SSIM), to confirm their realism. The methodology relies on pre-defined rules rather than ensuring mutations remain within a learned plausible distribution.",
                    "evidence": "Page 10, Section 6.2, \"Domain-specific constraints\": \"As discussed earlier, to be useful in practice, we need to ensure that the generated tests are valid and realistic by applying domain-specific constraints. For example, generated images should be physically producible by a camera... Below we describe two major types of domain-specific constraints (i.e., image and file constraints) that we use in this paper.\"\n\nPage 10, Section 6.2, \"Image constraints (MNIST, ImageNet, and Driving)\": \"DeepXplore used three different types of constraints for simulating different environment conditions of images: (1) lighting effects for simulating different intensities of lights, (2) occlusion by a single small rectangle for simulating an attacker potentially blocking some parts of a camera, and (3) occlusion by multiple tiny black rectangles for simulating effects of dirt on camera lens.\"\n\nPage 11, Figure 8: This figure provides visual examples of the seed inputs and the final difference-inducing inputs generated by DeepXplore under the various constraints (lighting, single occlusion, multiple occlusions), qualitatively demonstrating the plausibility of the outputs."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces input-plausibility via explicit, rule-based domain constraints that are applied during gradient-based mutation (the authors describe modifying gradients so each iteration preserves constraints and give concrete image/file constraints). This shows the fuzzing design actively restricts mutations to remain realistic at each mutation step. However, the paper does not provide any empirical, task-specific or human evaluation of the final fault-inducing inputs\u2019 perceptual or semantic realism (no human study or quantitative perceptual measures such as LPIPS/SSIM/PESQ/STOI/Perplexity are reported). Because the work enforces plausibility through bounded / rule-based constraints during mutation but does not report explicit empirical validation of final-input realism, the appropriate rating is Medium.",
                    "evidence": "Section 4.2 (Domain-specific constraints), p. 6: \"One important aspect of the optimization process is that the generated test inputs need to satisfy several domain-specific constraints to be physically realistic... Our algorithm ensures this property by modifying the gradient \u0434rad (line 13 in Algorithm 1) such that xi+1 = xi + s \u00b7 \u0434rad still satisfies the constraints (s is the step size in the gradient ascent).\" \n\nSection 6.2 (Domain-specific constraints), p. 9: \"DeepXplore used three different types of constraints for simulating different environment conditions of images: (1) lighting effects for simulating different intensities of lights, (2) occlusion by a single small rectangle for simulating an attacker potentially blocking some parts of a camera, and (3) occlusion by multiple tiny black rectangles for simulating effects of dirt on camera lens... The first constraint restricts image modifications so that DeepXplore can only make the image darker or brighter without changing its content. Specifically, the modification can only increase or decrease all pixel values by the same amount...\" \n\nSection 7 (Results), p. 10: \"For the experimental results shown in Figure 8, we apply three domain-specific constraints (lighting effects, occlusion by a single rectangle, and occlusion by multiple rectangles) as described in \u00a76.2.\" \n\n(Absence of empirical perceptual evaluation) The evaluation (Sections 7 and 7.2, pp. 10\u201313) reports neuron coverage, counts of difference-inducing inputs, runtimes, and retraining accuracy improvements, but contains no human-subject study or quantitative perceptual/linguistic measures (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity) assessing whether final fault-inducing inputs remain perceptually/semantically realistic."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Enforces domain specific constraints like keeping pixel values between 0 and 255, but it can produce decimal point pixel value within that range and does not perform rounding"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The DeepXplore paper explicitly enforces domain-specific constraints that prevent out-of-range or non-integer feature values (e.g., clipping pixel values to [0,255] and rounding discrete features), which partially addresses I/O effects such as clipping/quantization. However, the paper does not describe explicit handling or verification of common serialization/encoding effects (e.g., saving as PNG/JPEG or re-loading compressed formats) nor does it state that generated tests are serialized and re-evaluated through typical I/O pipelines before deciding a failure. Thus DeepXplore partially considers serialization effects (clipping/discretization) but does not fully account for I/O/encoding effects during both generation and oracle evaluation, matching the \u201cMedium\u201d level in the metric definition.",
                    "evidence": "Section 3 (Overview) \u2014 page 5: \u201cWe further add domain-specific constraints (e.g., ensure the pixel values are integers within 0 and 255 for image input) to make sure that the modified inputs still represent real-world images.\u201d; Section 4.2 (Domain-specific constraints) \u2014 page 9: \u201cFor discrete features, we round the gradient to an integer. For DNNs handling visual input (e.g., images), we add different spatial restrictions ... we only allow the pixel values to be decreased.\u201d; Algorithm 1 / Section 4.2 \u2014 page 6: the oracle check uses model predictions in memory (pseudo-code): \u201cif d.predict(x) != (dnns-d).predict(x) then gen_test.add(x)\u201d, indicating failures are detected from in-memory model outputs rather than from inputs re-serialized and re-loaded through standard I/O pipelines. (No text in the paper describes explicit serialization/encoding and re-evaluation steps such as saving to PNG/JPEG/WAV and confirming failures persist.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The method proposed in the paper, DeepXplore, partially accounts for I/O effects. The methodology includes applying 'domain-specific constraints' to ensure that generated test inputs are 'physically realistic'. For image-based inputs, this explicitly includes clipping, i.e., ensuring pixel values remain within a valid range like [0, 255]. The paper also describes using several semantic-preserving metamorphic transformations, such as simulating different lighting conditions and adding occlusions. However, the paper does not explicitly state that it accounts for quantization or rounding effects inherent in standard serialization formats (e.g., saving a floating-point tensor as an 8-bit PNG). The core test generation works by applying gradient ascent to a continuous representation of the input, and there is no mention of a serialization/deserialization step to confirm that a discovered failure persists after such an operation. Therefore, since the method considers clipping and uses metamorphic transformations but does not fully address all aspects of serialization like rounding, it aligns with the 'Medium' rating.",
                    "evidence": "Page 8, Section 4.2, 'Domain-specific constraints': 'One important aspect of the optimization process is that the generated test inputs need to satisfy several domain-specific constraints to be physically realistic [63]. [...] For example, for a generated test image x the pixel values must be within a certain range (e.g., 0 to 255).' and Page 10, Section 6.2, 'Image constraints (MNIST, ImageNet, and Driving)': 'DeepXplore used three different types of constraints for simulating different environment conditions of images: (1) lighting effects for simulating different intensities of lights, (2) occlusion by a single small rectangle for simulating an attacker potentially blocking some parts of a camera, and (3) occlusion by multiple tiny black rectangles for simulating effects of dirt on camera lens.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The DeepXplore paper explicitly enforces domain-specific constraints that prevent out-of-range or non-integer feature values (e.g., clipping pixel values to [0,255] and rounding discrete features), which partially addresses I/O effects such as clipping/quantization. However, the paper does not describe explicit handling or verification of common serialization/encoding effects (e.g., saving as PNG/JPEG or re-loading compressed formats) nor does it state that generated tests are serialized and re-evaluated through typical I/O pipelines before deciding a failure. Thus DeepXplore partially considers serialization effects (clipping/discretization) but does not fully account for I/O/encoding effects during both generation and oracle evaluation, matching the \u201cMedium\u201d level in the metric definition.",
                    "evidence": "Section 3 (Overview) \u2014 page 5: \u201cWe further add domain-specific constraints (e.g., ensure the pixel values are integers within 0 and 255 for image input) to make sure that the modified inputs still represent real-world images.\u201d; Section 4.2 (Domain-specific constraints) \u2014 page 9: \u201cFor discrete features, we round the gradient to an integer. For DNNs handling visual input (e.g., images), we add different spatial restrictions ... we only allow the pixel values to be decreased.\u201d; Algorithm 1 / Section 4.2 \u2014 page 6: the oracle check uses model predictions in memory (pseudo-code): \u201cif d.predict(x) != (dnns-d).predict(x) then gen_test.add(x)\u201d, indicating failures are detected from in-memory model outputs rather than from inputs re-serialized and re-loaded through standard I/O pipelines. (No text in the paper describes explicit serialization/encoding and re-evaluation steps such as saving to PNG/JPEG/WAV and confirming failures persist.)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's testing approach is explicitly differential: it requires whitebox access to multiple DNNs and constructs inputs by optimizing across those models so that they disagree. DeepXplore does not present or evaluate the scenario where fault-inducing inputs are crafted on a single surrogate model and then reused (without access) to trigger failures in other, unseen target models. In other words, the paper demonstrates generating inputs that cause differences among available models, but it does not demonstrate or measure transferability of fault-inducing inputs from one model to other independent/unknown models (the kind of surrogate-model transfer evaluated by the metric).",
                    "evidence": "Page 1, Abstract: \"Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles to avoid manual checking.\"; Page 5, Section 3 (Overview): \"DeepXplore takes unlabeled test inputs as seeds and generates new tests that cover a large number of neurons ... while causing the tested DNNs to behave differently.\"; Page 6, Section 4.2 / Algorithm 1: algorithm inputs show \"dnns \u2190 multiple DNNs under test\" and the algorithm selects \"d = randomly select one dnn from dnns\" (showing the method relies on multiple available models during generation); Page 14, Section 8 (Limitations): \"First, differential testing requires at least two different DNNs with the same functionality. Further, if two DNNs only differ slightly ... DeepXplore will take longer to find difference-inducing inputs...\" (stating the necessity of multiple models and acknowledging that DeepXplore cannot operate in the single-surrogate transfer scenario)."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's core methodology, differential testing, is explicitly designed to discover transferable failures by finding inputs that cause different behaviors across multiple deep learning models with similar functionality. The method does not simply generate a fault-inducing input for one model and then test its transferability on others as a secondary step. Instead, it uses multiple models as 'cross-referencing oracles' from the outset. The fuzzing design incorporates an explicit mechanism\u2014a joint optimization function\u2014that directly aims to maximize the behavioral differences between the models under test. This process inherently generates inputs that demonstrate failures across different models.",
                    "evidence": "Page 7, Section 4.2, 'Maximizing differential behaviors': 'The first objective of the optimization problem is to generate test inputs that can induce different behaviors in the tested DNNs, i.e., different DNNs will classify the same input into different classes.' and Page 8, Equation (2) and its description: 'obj1(x) = \u2211k\u2260j Fk(x)[c] - \u03bb1 \u00b7 Fj(x)[c] where \u03bb1 is a parameter to balance the objective terms between the DNNs Fk\u2260j that maintain the same class outputs as before and the DNN Fj that produce different class outputs.' This objective function is the explicit mechanism to find transferable/differential failures."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DeepXplore\u2019s approach requires whitebox access to multiple DNNs during test generation and optimizes inputs to provoke disagreements among them. It does not generate fault-inducing inputs on a surrogate model and then reuse those inputs (without internal access) to trigger failures in independently trained or unseen target models. Thus, it does not evaluate or support the traditional notion of attack transferability from one model to another.",
                    "evidence": "Page 1 (Abstract): \u201cNext, we leverage multiple DL systems with similar functionality as cross-referencing oracles\u2026\u201d; Page 5 (Section 3 Overview): \u201cDeepXplore takes unlabeled test inputs as seeds \u2026 while causing the tested DNNs to behave differently.\u201d; Page 6 (Algorithm 1 inputs): \u201cdnns \u2190 multiple DNNs under test\u201d and selection of one DNN from dnns for optimization; Page 14 (Section 8 Limitations): \u201cdifferential testing requires at least two different DNNs with the same functionality\u2026 DeepXplore cannot operate in the single-surrogate transfer scenario.\u201d"
                },
                "manual": {
                    "value": "Low",
                    "why": "It is differential testing based oracle. Not transferability.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Whitebox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Prediction-guided"
            ]
        }
    },
    "Gao et al. - 2020 - Fuzz testing based data augmentation to improve robustness of deep neural networks": {
        "key": "gao2020fuzz",
        "name": "Sensei",
        "year": "2020",
        "citation_count": "169",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper frames and evaluates Sensei as a data-augmentation/search technique to improve DNN robustness to natural, benign input variations (rotation, translation, brightness, etc.). The method is explicitly targeted at reducing mispredictions under semantically-preserving mutations and improving 'robust accuracy' on perturbed images. The authors explicitly state they do not target adversarial (security) perturbations and do not claim to bypass safety, policy, or defense mechanisms; no experiments attempt jailbreaks, data exfiltration, or violations of explicit safety mechanisms. Therefore the failures the fuzzer exposes are standard robustness errors (mispredictions/inconsistencies under input mutations), not high- or medium-severity safety/security violations.",
                    "evidence": "Page 1148, Introduction: \"In this work, we focus on the latter scenario and investigate how software testing techniques can be applied for the robust generalization of Deep Neural Networks (DNNs) to natural environmental variations.\"; Page 1149, Section 2.3 (Robustness of DNNs): \"As discussed earlier, this work targets robustness issues arising from natural, environmental perturbations \u03b4 in the input data and not perturbations \u03b4 constructed adversarially, in a security context.\"; Page 1153, Section 4.3 (Evaluation Metric): \"robust accuracy is the proportion of images in the testing dataset where the prediction of a DNN does not fluctuate with any small realistic perturbations.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper focuses exclusively on improving model robustness against semantically preserving input mutations that simulate natural environmental variations. The failures uncovered by the fuzzing process are defined as mispredictions or prediction inconsistencies under these small perturbations. The paper's stated goal, methodology, and evaluation metric (robust accuracy) are all centered on this type of low-severity robustness error. It explicitly distinguishes its scope from security contexts or adversarial attacks and does not address unsafe behaviors like toxicity or bias, nor does it attempt to bypass any safety or security mechanisms.",
                    "evidence": "Page 4, Section 2.3, \"As discussed earlier, this work targets robustness issues arising from natural, environmental perturbations \u03b4 in the input data and not perturbations \u03b4 constructed adversarially, in a security context.\" and Page 7, Section 4.3, \"More specifically, robust accuracy is the proportion of images in the testing dataset where the prediction of a DNN does not fluctuate with any small realistic perturbations.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper frames and evaluates Sensei as a data-augmentation/search technique to improve DNN robustness to natural, benign input variations (rotation, translation, brightness, etc.). The method is explicitly targeted at reducing mispredictions under semantically-preserving mutations and improving 'robust accuracy' on perturbed images. The authors explicitly state they do not target adversarial (security) perturbations and do not claim to bypass safety, policy, or defense mechanisms; no experiments attempt jailbreaks, data exfiltration, or violations of explicit safety mechanisms. Therefore the failures the fuzzer exposes are standard robustness errors (mispredictions/inconsistencies under input mutations), not high- or medium-severity safety/security violations.",
                    "evidence": "Page 1148, Introduction: \"In this work, we focus on the latter scenario and investigate how software testing techniques can be applied for the robust generalization of Deep Neural Networks (DNNs) to natural environmental variations.\"; Page 1149, Section 2.3 (Robustness of DNNs): \"As discussed earlier, this work targets robustness issues arising from natural, environmental perturbations \u03b4 in the input data and not perturbations \u03b4 constructed adversarially, in a security context.\"; Page 1153, Section 4.3 (Evaluation Metric): \"robust accuracy is the proportion of images in the testing dataset where the prediction of a DNN does not fluctuate with any small realistic perturbations.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Aims to find generic misclassifications"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Apply large semantic level mutations and perform floating point operations (zoom, contrast, shear), which may produce non-discrete pixel values. They don't perform clipping/rounding to ensure IO persistence."
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper uses semantic-preserving, metamorphic image transformations (rotation, translation, shear, zoom, brightness, contrast) as mutation operators and generates augmented inputs on-the-fly during training. These coarse transformations are inherently less sensitive to tiny floating\u2011point memory artifacts, but the paper does not explicitly address standard I/O serialization effects (quantization, rounding, clipping) nor does it claim to constrain mutations so that perturbations survive common image serialization. It therefore aligns with the metric\u2019s description of \u201cconsidering serialization effects partially (e.g., applying metamorphic transformations) but without explicit enforcement of reproducibility under serialization,\u201d i.e., Medium.",
                    "evidence": "Section 4.2 (Generation of Realistic Variations), p.1153: \"The operations and corresponding restrictions with respect to an image x are as follows: \u2022 rotation(x,d): rotate x by d degree within a range [-30, 30]. \u2022 translation(x, d): horizontally or vertically translate x by d pixels within a range of [-10%, 10%] of image size. \u2022 shear(x,d): ... \u2022 brightness(x,d): uniformly add or subtract a value d for each pixel of x within a range of [-32, 32] \u2022 contrast(x,d): scale the RGB value of each pixel of x by a factor d within range of [0.8, 1.2]\"; Section 3.3.2 (Generation of population), p.1150: \"...Sensei always applies the resulting transformation vector (chromosome) on the original image (as opposed to applying on an already transformed data) to prevent the resulting data from being unrealistic.\"; Section 5.1 (Implementation), p.1153: \"we implement a new data generator that augments the training data during training. Our data generator takes as inputs the current model and original training set, augments the original data and then feeds the augmented data to the training process at each step.\" These passages show the use of metamorphic (coarse) image transformations applied in-memory during training but no explicit handling or discussion of serialization/quantization/clipping to ensure failures persist after saving to standard image formats."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper proposes a method, SENSEI, that uses fuzzing with semantic-preserving metamorphic transformations to generate augmented training data. The transformations include geometric operations (rotation, translation, shear, zoom) and color operations (brightness, contrast). According to the metric definition, applying such transformations without explicit enforcement of reproducibility under serialization warrants a 'Medium' rating. The paper details the transformations and their parameter ranges but does not mention any mechanism to account for I/O effects like quantization, rounding, or clipping that occur when saving an image to a file. For instance, a brightness adjustment could create pixel values outside the standard [0, 255] range for 8-bit images, which would be clipped upon serialization. The paper does not state that these effects are considered during the mutation or oracle evaluation (loss calculation) stages. Therefore, while the coarse-grained nature of the transformations provides some resilience against I/O effects compared to fine-grained numerical noise, the lack of explicit handling of serialization means failure reproducibility is only partially enforced.",
                    "evidence": "Page 7, Section 4.2 Generation of Realistic Variations, \"SENSEI focuses on improving the robustness of DNN models by augmenting training data with natural environmental variations. Since, we focus on the applications with image, we choose two major kinds of image operations: i) geometric operations ii) color operations to simulate the camera movements and lighting conditions in real life... The operations and corresponding restrictions with respect to an image x are as follows:\n\u2022 rotation(x,d): rotate x by d degree within a range [-30, 30].\n\u2022 translation(x, d): horizontally or vertically translate x by d pixels within a range of [-10%, 10%] of image size.\n\u2022 shear(x,d): horizontally shear x with a shear factor d within a range of [-0.1, 0.1].\n\u2022 zoom(x,d): zoom in/out x with a zoom factor d ranging [0.9,1.1]\n\u2022 brightness(x,d): uniformly add or subtract a value d for each pixel of x within a range of [-32, 32]\n\u2022 contrast(x,d): scale the RGB value of each pixel of x by a factor d within range of [0.8, 1.2]\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper uses semantic-preserving, metamorphic image transformations (rotation, translation, shear, zoom, brightness, contrast) as mutation operators and generates augmented inputs on-the-fly during training. These coarse transformations are inherently less sensitive to tiny floating\u2011point memory artifacts, but the paper does not explicitly address standard I/O serialization effects (quantization, rounding, clipping) nor does it claim to constrain mutations so that perturbations survive common image serialization. It therefore aligns with the metric\u2019s description of \u201cconsidering serialization effects partially (e.g., applying metamorphic transformations) but without explicit enforcement of reproducibility under serialization,\u201d i.e., Medium.",
                    "evidence": "Section 4.2 (Generation of Realistic Variations), p.1153: \"The operations and corresponding restrictions with respect to an image x are as follows: \u2022 rotation(x,d): rotate x by d degree within a range [-30, 30]. \u2022 translation(x, d): horizontally or vertically translate x by d pixels within a range of [-10%, 10%] of image size. \u2022 shear(x,d): ... \u2022 brightness(x,d): uniformly add or subtract a value d for each pixel of x within a range of [-32, 32] \u2022 contrast(x,d): scale the RGB value of each pixel of x by a factor d within range of [0.8, 1.2]\"; Section 3.3.2 (Generation of population), p.1150: \"...Sensei always applies the resulting transformation vector (chromosome) on the original image (as opposed to applying on an already transformed data) to prevent the resulting data from being unrealistic.\"; Section 5.1 (Implementation), p.1153: \"we implement a new data generator that augments the training data during training. Our data generator takes as inputs the current model and original training set, augments the original data and then feeds the augmented data to the training process at each step.\" These passages show the use of metamorphic (coarse) image transformations applied in-memory during training but no explicit handling or discussion of serialization/quantization/clipping to ensure failures persist after saving to standard image formats."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "No analysis of fault transferability in input level or mutation level"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or claim that fault-inducing inputs found on one (surrogate) model transfer to other models. Sensei is designed as an in-training data-augmentation method that generates variants per model/seed during that model's training; the experiments measure per-model robust accuracy after applying Sensei to each model independently. The paper explicitly contrasts its approach with multi-model differential testing (DeepXplore) and states that Sensei does not require multiple models, indicating the design and evaluation do not target discovering or measuring transferable faults across models.",
                    "evidence": "p.1148, Abstract: \"We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets.\"; p.1150, Section 3.2 (Overview): \"Note that, Sensei is in-training data augmentation approach, i.e., data generation and augmentation happen on-the-fly during training.\"; p.1156, Section 6 (Related Work): \"DeepXplore [24] generates test inputs that lead to exhibit different behaviors by different models for the same task. Our approach does not require multiple models.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a data augmentation technique, SENSEI, to improve the robustness of a single Deep Neural Network (DNN) model during its training phase. The method uses a genetic algorithm to generate high-loss variants of training inputs, which are then immediately used to train that same model. The entire process of generating and utilizing these 'fault-inducing' inputs is self-contained within the training loop of one specific model. The experiments evaluate the robustness of multiple individual models after they have been trained with SENSEI, but they do not test whether the variants generated for one model can cause failures in other models performing the same task. The paper does not mention or evaluate the concept of attack transferability between different models.",
                    "evidence": "Page 5, Algorithm 2, 'Overall algorithm': The algorithm describes a training process for a single model 'M'. In each epoch, for each training input 'X[i]', variants are generated and evaluated using the current model 'M' (line 14: `f := fitness(M, children);`). The model 'M' is then retrained on the augmented data (line 19: `M := train(X, Y);`). This shows the generated inputs are specific to and consumed by the model being trained, not transferred to other models.\n\nPage 7, Section 4.3, 'Evaluation Metric': The paper's own evaluation metric is defined for a single model's performance: 'A DNN is robust around x if and only if M(x') = c for all x' \u2208 X'. This confirms that the evaluation is focused on the robustness of an individual model, not on cross-model failure patterns.\n\nPage 9, Table 2, 'The robust accuracy for Random, W-10 and SENSEI': This table presents the results for 15 different models. Each model is trained and evaluated independently. The methodology does not involve generating inputs on one model (e.g., GTSRB-1) and testing them on another (e.g., GTSRB-2)."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or claim that fault-inducing inputs found on one (surrogate) model transfer to other models. Sensei is designed as an in-training data-augmentation method that generates variants per model/seed during that model's training; the experiments measure per-model robust accuracy after applying Sensei to each model independently. The paper explicitly contrasts its approach with multi-model differential testing (DeepXplore) and states that Sensei does not require multiple models, indicating the design and evaluation do not target discovering or measuring transferable faults across models.",
                    "evidence": "p.1148, Abstract: \"We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets.\"; p.1150, Section 3.2 (Overview): \"Note that, Sensei is in-training data augmentation approach, i.e., data generation and augmentation happen on-the-fly during training.\"; p.1156, Section 6 (Related Work): \"DeepXplore [24] generates test inputs that lead to exhibit different behaviors by different models for the same task. Our approach does not require multiple models.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "coonstrained transformation but no naturalness measure of the fuzzed inputs"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility through bounded, rule-based constraints on allowed transformations and by design prevents cumulative drift (it always applies each mutation to the original seed). These measures ensure mutated inputs remain realistic by construction during the iterative search. However, the paper does not provide empirical perceptual or semantic plausibility assessments (no human study or task-specific perceptual metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity) for the final fault-inducing / augmented inputs, so it does not meet the 'High' requirement of providing empirical evidence that final inputs remain perceptually realistic.",
                    "evidence": "Section 3.3.2 (Generation of population), p.1150: \"once a new chromosome is generated through the mutation or crossover, it is validated to make sure that it is within the range of each transformation that we set globally (line 12). Furthermore, Sensei always applies the resulting transformation vector (chromosome) on the original image (as opposed to applying on an already transformed data) to prevent the resulting data from being unrealistic.\"; Section 4.2 (Generation of Realistic Variations), p.1153: \"To make sure the translated images are visually similar to natural ones, we restrict the space of allowed perturbations following [8] where it is applicable. The operations and corresponding restrictions ... rotation(x,d): rotate x by d degree within a range [-30, 30]. translation(x, d): ... within a range of [-10%, 10%] ... brightness(x,d): ... range of [-32, 32] ... contrast(x,d): ... range of [0.8, 1.2].\"; Section 4.3 (Evaluation Metric), p.1153: \"robust accuracy is the proportion of images in the testing dataset where the prediction of a DNN does not fluctuate with any small realistic perturbations...\" \u2014 the paper's evaluation uses robust accuracy as the reported metric and does not report any human evaluation or perceptual-similarity scores (e.g., LPIPS, SSIM) for the generated/augmented images."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility using rule-based constraints. It defines a set of image transformations (rotation, translation, shear, zoom, brightness, contrast) intended to simulate 'natural environmental variations' and restricts the parameters of these transformations to bounded ranges to ensure the resulting images are 'visually similar to natural ones'. Crucially, the method is designed to prevent the cumulative effect of mutations across iterations, which could lead to unrealistic inputs. It achieves this by always applying transformations to the original seed image, not to an already-mutated one. However, the paper does not provide any empirical validation that the final augmented inputs are perceptually realistic, such as through a human study or by using quantitative metrics like SSIM or LPIPS. The plausibility is asserted based on the design constraints rather than being empirically verified, which prevents a 'High' rating.",
                    "evidence": "Page 6, Section 3.3.2, 'Generation of population': 'Furthermore, SENSEI always applies the resulting transformation vector (chromosome) on the original image (as opposed to applying on an already transformed data) to prevent the resulting data from being unrealistic.' \nPage 7, Section 4.2, 'Generation of Realistic Variations': 'SENSEI focuses on improving the robustness of DNN models by augmenting training data with natural environmental variations. ... To make sure the translated images are visually similar to natural ones, we restrict the space of allowed perturbations... The operations and corresponding restrictions with respect to an image x are as follows: \u2022 rotation(x,d): rotate x by d degree within a range [-30, 30]. \u2022 translation(x, d): horizontally or vertically translate x by d pixels within a range of [-10%, 10%] of image size. ... These image operations preserve the content of original image.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility through bounded, rule-based constraints on allowed transformations and by design prevents cumulative drift (it always applies each mutation to the original seed). These measures ensure mutated inputs remain realistic by construction during the iterative search. However, the paper does not provide empirical perceptual or semantic plausibility assessments (no human study or task-specific perceptual metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity) for the final fault-inducing / augmented inputs, so it does not meet the 'High' requirement of providing empirical evidence that final inputs remain perceptually realistic.",
                    "evidence": "Section 3.3.2 (Generation of population), p.1150: \"once a new chromosome is generated through the mutation or crossover, it is validated to make sure that it is within the range of each transformation that we set globally (line 12). Furthermore, Sensei always applies the resulting transformation vector (chromosome) on the original image (as opposed to applying on an already transformed data) to prevent the resulting data from being unrealistic.\"; Section 4.2 (Generation of Realistic Variations), p.1153: \"To make sure the translated images are visually similar to natural ones, we restrict the space of allowed perturbations following [8] where it is applicable. The operations and corresponding restrictions ... rotation(x,d): rotate x by d degree within a range [-30, 30]. translation(x, d): ... within a range of [-10%, 10%] ... brightness(x,d): ... range of [-32, 32] ... contrast(x,d): ... range of [0.8, 1.2].\"; Section 4.3 (Evaluation Metric), p.1153: \"robust accuracy is the proportion of images in the testing dataset where the prediction of a DNN does not fluctuate with any small realistic perturbations...\" \u2014 the paper's evaluation uses robust accuracy as the reported metric and does not report any human evaluation or perceptual-similarity scores (e.g., LPIPS, SSIM) for the generated/augmented images."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ],
            "oracle": [
                "Metamorphic",
                "Property-based"
            ]
        }
    },
    "You et al. - 2023 - Regression Fuzzing for Deep Learning Systems": {
        "key": "you2023regression",
        "name": "DRFuzz",
        "year": "2023",
        "citation_count": "27",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper explicitly defines regression faults as cases where the new model version makes wrong predictions while the prior version was correct, and the proposed DRFuzz is designed to discover such mispredictions (diversity/fidelity of inputs). The evaluation and examples focus on classification errors, bias/overfitting explanations, and counts of regression mispredictions across image datasets and regression scenarios (including adversarially-trained models). The work does not claim or demonstrate uncovering high-impact safety/security violations (e.g., jailbreaks, data leakage, unauthorized actions, toxic or hallucinated outputs) nor bypassing explicit safety/policy mechanisms. Therefore DRFuzz uncovers model robustness/regression errors only, which corresponds to the Low severity level in the metric.",
                    "evidence": "Page 82, Introduction: \"Detecting regression faults refers to finding test inputs that make the current version produce wrong predictions but make the prior version produce correct predictions.\"; Page 84, Section II. DEFINITION, \"Definition 2. Regression fault. ... x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\"; Page 87, Section IV-A2 (Regression Scenarios): \"Adversarial Training ... fine-tunes the model by incorporating adversarial inputs and then produces a new version of the model.\" (evaluations report numbers of regression mispredictions in these scenarios but do not report safety/security violations such as data leakage or unauthorized actions)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper defines a 'regression fault' as an input that was correctly predicted by a prior model version (M1) but is incorrectly predicted by the subsequent version (M2). This aligns directly with the metric's definition of 'Low' severity, as it constitutes a model robustness error, specifically a prediction inconsistency resulting from model evolution. The paper's entire methodology and evaluation framework are built around finding and counting these mispredictions. There is no mention or analysis of uncovering unsafe behaviors (like toxicity or bias) or bypassing explicit safety or security mechanisms.",
                    "evidence": "Page 2, Section II. DEFINITION, Definition 2. Regression fault: \"Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., c_M1[x] = y ^ c_M2[x] != y.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper explicitly defines regression faults as cases where the new model version makes wrong predictions while the prior version was correct, and the proposed DRFuzz is designed to discover such mispredictions (diversity/fidelity of inputs). The evaluation and examples focus on classification errors, bias/overfitting explanations, and counts of regression mispredictions across image datasets and regression scenarios (including adversarially-trained models). The work does not claim or demonstrate uncovering high-impact safety/security violations (e.g., jailbreaks, data leakage, unauthorized actions, toxic or hallucinated outputs) nor bypassing explicit safety/policy mechanisms. Therefore DRFuzz uncovers model robustness/regression errors only, which corresponds to the Low severity level in the metric.",
                    "evidence": "Page 82, Introduction: \"Detecting regression faults refers to finding test inputs that make the current version produce wrong predictions but make the prior version produce correct predictions.\"; Page 84, Section II. DEFINITION, \"Definition 2. Regression fault. ... x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\"; Page 87, Section IV-A2 (Regression Scenarios): \"Adversarial Training ... fine-tunes the model by incorporating adversarial inputs and then produces a new version of the model.\" (evaluations report numbers of regression mispredictions in these scenarios but do not report safety/security violations such as data leakage or unauthorized actions)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Intargeted exploration for generic regression misclassifications - any inputs that cause the new model to misclassify examples previously classified correctly"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "does not analyze the root cause of regression faults, acknowledging that identifying their causes remains an open challenge. It provides behavior-level explanations (e.g., which classes or behaviors regress, per-class fault heatmaps, and faulty-behavior tuples). Interprets regression faults as evidence of bias or overfitting (e.g., specific classes suffering more after retraining)."
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": "Every batch of mutated inputs is passed through the GAN discriminator and only natural mutated inputs are kept for execution and further mutation."
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly incorporates multiple mechanisms to enforce input plausibility throughout fuzzing iterations and provides empirical human-evaluation evidence that the final fault-inducing inputs remain perceptually/semantically realistic. Concretely, DRFuzz: (1) constrains mutations (pixel-level mutations limited to 0.5% of pixels and image-level mutations applied to whole images to avoid unrealistic partial edits); (2) uses a GAN discriminator (trained on the original training set) to compute a per-seed fidelity threshold and filter out low-fidelity mutated inputs during the fuzzing process; and (3) limits cumulative drift via seed-generation controls (e.g., max_times to bound number of times a seed can be mutated and an exponential-decay seed selection probability) and tree-based trimming. The authors also report a manual/human evaluation comparing DRFuzz (with GAN filtering) to a variant without the GAN: two authors labeled sampled inputs (Cohen\u2019s kappa = 0.67), finding 86% of DRFuzz inputs labeled high-fidelity vs 57.5% for the no-GAN variant. These elements together satisfy the metric requirement of enforcing plausibility through iterations and providing empirical (human) evidence for plausibility of final fault-inducing inputs.",
                    "evidence": "Page 84, Section III-B (GAN-based Fidelity Assurance): \"During the fuzzing process, we feed an initial seed into the Discriminator to get a fidelity score, which serves as the fidelity threshold for filtering out the low-fidelity inputs generated from this initial seed.\"; Page 86, Table I / Section III-C (Mutation Rules): \"Pixel-Level Mutation aims to change a small set of pixels in an image ... DRFuzz randomly selects a small set of pixels (i.e., 0.5% total pixels) for mutation.\" and \"Image-Level Mutation ... this category of mutation rules can simulate real-world scenarios more effectively when applying them to the whole image.\"; Page 88, Section IV-A5 (Implementations and Experiments): \"... we set ... max_times to be 10 ...\" (bounding number of times a seed can be mutated); Page 90, Section IV-B2 (Contribution of the main component of DRFuzz): \"Specifically, we sample 100 inputs generated by DRFuzz_NG and DRFuzz, respectively, and two authors manually evaluate their fidelity individually. The Cohen\u2019s Kappa coefficient between their evaluation is 0.67. They label 86% of inputs generated by DRFuzz as high-fidelity and only 57.5% of inputs generated by DRFuzz_NG as high-fidelity.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper explicitly designs a mechanism, \"GAN-based Fidelity Assurance\", to enforce input plausibility throughout the fuzzing process. This component uses a discriminator, trained to distinguish between real and synthetic data, to filter out mutated inputs that are not semantically similar to the original data distribution. This ensures that generated inputs remain plausible during the fuzzing iterations. Furthermore, the paper provides direct empirical evidence of the effectiveness of this mechanism through a human evaluation. In an ablation study, two authors manually assessed the fidelity of 100 inputs generated with and without the GAN filter. The study found that 86% of inputs from the proposed method (DRFuzz) were high-fidelity, compared to only 57.5% from the variant without the GAN filter (DRFuzz_NG). This combination of a dedicated plausibility enforcement mechanism and empirical validation via human study aligns perfectly with the 'High' rating.",
                    "evidence": "Page 9, Section IV-B2, \"Contribution of the main component of DRFuzz\": \"We further manually evaluate the fidelity of inputs obtained with and without the fidelity assurance mechanism. Specifically, we sample 100 inputs generated by DRFuzz_NG and DRFuzz, respectively, and two authors manually evaluate their fidelity individually. The Cohen\u2019s Kappa coefficient between their evaluation is 0.67. They label 86% of inputs generated by DRFuzz as high-fidelity and only 57.5% of inputs generated by DRFuzz_NG as high-fidelity. This indicates that the GAN-based Fidelity Assurance technique can filter out more than 20% of fault-triggering inputs with low fidelity.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly incorporates multiple mechanisms to enforce input plausibility throughout fuzzing iterations and provides empirical human-evaluation evidence that the final fault-inducing inputs remain perceptually/semantically realistic. Concretely, DRFuzz: (1) constrains mutations (pixel-level mutations limited to 0.5% of pixels and image-level mutations applied to whole images to avoid unrealistic partial edits); (2) uses a GAN discriminator (trained on the original training set) to compute a per-seed fidelity threshold and filter out low-fidelity mutated inputs during the fuzzing process; and (3) limits cumulative drift via seed-generation controls (e.g., max_times to bound number of times a seed can be mutated and an exponential-decay seed selection probability) and tree-based trimming. The authors also report a manual/human evaluation comparing DRFuzz (with GAN filtering) to a variant without the GAN: two authors labeled sampled inputs (Cohen\u2019s kappa = 0.67), finding 86% of DRFuzz inputs labeled high-fidelity vs 57.5% for the no-GAN variant. These elements together satisfy the metric requirement of enforcing plausibility through iterations and providing empirical (human) evidence for plausibility of final fault-inducing inputs.",
                    "evidence": "Page 84, Section III-B (GAN-based Fidelity Assurance): \"During the fuzzing process, we feed an initial seed into the Discriminator to get a fidelity score, which serves as the fidelity threshold for filtering out the low-fidelity inputs generated from this initial seed.\"; Page 86, Table I / Section III-C (Mutation Rules): \"Pixel-Level Mutation aims to change a small set of pixels in an image ... DRFuzz randomly selects a small set of pixels (i.e., 0.5% total pixels) for mutation.\" and \"Image-Level Mutation ... this category of mutation rules can simulate real-world scenarios more effectively when applying them to the whole image.\"; Page 88, Section IV-A5 (Implementations and Experiments): \"... we set ... max_times to be 10 ...\" (bounding number of times a seed can be mutated); Page 90, Section IV-B2 (Contribution of the main component of DRFuzz): \"Specifically, we sample 100 inputs generated by DRFuzz_NG and DRFuzz, respectively, and two authors manually evaluate their fidelity individually. The Cohen\u2019s Kappa coefficient between their evaluation is 0.67. They label 86% of inputs generated by DRFuzz as high-fidelity and only 57.5% of inputs generated by DRFuzz_NG as high-fidelity.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Generates and evaluates mutated inputs entirely in memory using both pixel level and image level semantic mutations. They use GAN-based fidelity filtering to ensure naturalness but does not ensure if the mutation will survive I/O."
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper describes in-memory, floating-point mutations (pixel-level and image-level) and a GAN-based fidelity filter, but it does not describe any handling of standard I/O serialization effects (e.g., quantization, rounding, clipping) to ensure perturbations survive disk storage or common image encodings. It even explicitly states that the quantization scenario (which would exercise serialization/precision effects) is not considered. Therefore failures found by DRFuzz may rely on fragile, in-memory numerical perturbations that would be lost or altered by standard I/O serialization, and the approach does not explicitly enforce reproducibility under such operations.",
                    "evidence": "Page 84, Section III (Overview): \"DRFuzz selects a seed input each time to conduct mutation, and the mutated inputs are passed into the GAN-based Fidelity Assurance model to identify inputs with high fidelity, which are then executed by both the original model and the regression model.\" \n\nPage 85, Section III-C (Mutation Rules): \"Pixel-Level Mutation aims to change a small set of pixels in an image to generate a new test input. ... DRFuzz randomly selects a small set of pixels (i.e., 0.5% total pixels) for mutation.\" \n\nPage 88, Section IV-A2 (Regression Scenarios): \"Please note that the quantization scenario targeted by DiffChaser is not a regression scenario since quantization is conducted once-for-all without the characteristics of continuous evolution for regression. Hence, we do not consider it in our study.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The method, DRFuzz, utilizes a set of 16 mutation rules categorized into 'Pixel-Level Mutation' (e.g., adding Gaussian noise, shuffling patches) and 'Image-Level Mutation' (e.g., scaling, rotating, brightness adjustment). The 'Image-Level' mutations are semantic-preserving metamorphic transformations. The metric definition explicitly states that applying such transformations without explicit enforcement of reproducibility under serialization results in a 'Medium' rating. The paper does not mention any mechanism to account for I/O effects such as quantization, rounding, or clipping that occur during standard file saving operations (e.g., to PNG). The 'Pixel-Level' mutations, which introduce small numerical changes, are particularly susceptible to being lost or altered during such I/O operations. Since the fuzzing process and oracle evaluation are performed on in-memory representations without considering serialization effects, the method does not qualify for a 'High' rating.",
                    "evidence": "Page 4, Section C, Mutation Rules: \"In DRFuzz, we adopt all the mutation rules for images designed by the existing work [11], [27], [28]... In total, we implemented 16 mutation rules in DRFuzz... According to the usage of each mutation rule, we classify them into two categories, i.e., Pixel-Level Mutation and Image-Level Mutation.\"\n\nPage 4, Table I, \"MUTATION RULES\": This table lists the specific mutation operators. The 'Pixel-Level Mutation' rules include 'Pixel Adding Gaussian Noise' and 'Pixel Adding Salt & Pepper Noise'. The 'Image-Level Mutation' rules include 'Image Rotating' and 'Image Brightness Adjustment', which are examples of semantic-preserving metamorphic transformations."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "DRFuzz applies a variety of semantic-preserving metamorphic image transformations (e.g., rotation, scaling, brightness/contrast adjustment) which introduce coarse input changes that tend to survive standard serialization, but it does not explicitly account for or constrain the fine-grained numerical perturbations (e.g. pixel-level Gaussian or salt-and-pepper noise) to ensure they survive quantization, rounding or clipping when images are saved or transmitted. The paper makes no mention of handling I/O effects or verifying that mutations persist through common image formats, so serialization effects are only partially considered via the use of metamorphic transformations, yielding a \u201cMedium\u201d rating under the metric.",
                    "evidence": "Page 4, Table I (\u201cMUTATION RULES\u201d): lists both pixel-level numerical mutations and image-level metamorphic transformations (e.g. rotation, brightness adjustment).\n\nPage 4, Section III-C: \u201cAccording to the usage of each mutation rule, we classify them into two categories, i.e., Pixel-Level Mutation and Image-Level Mutation.\u201d  There is no discussion of serialization, quantization, rounding, or clipping effects in the mutation or oracle evaluation steps."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper defines and evaluates regression-fault-triggering inputs strictly with respect to a prior model M1 and its regression model M2 (i.e., it detects inputs where M1 is correct and M2 is incorrect). All described generation, fidelity filtering, and evaluation steps execute mutated inputs on that M1\u2013M2 pair. The experiments use multiple dataset/model pairs but do not evaluate whether inputs found on one model transfer to other models performing the same task, nor does the design include explicit mechanisms to discover or measure transferable attacks across different model implementations. Therefore the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "Section II (Definition), Definition 2 \u2014 \"Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\" (Section II, Definition 2)\n\nSection III (Approach) \u2014 \"the mutated inputs are passed into the GAN-based Fidelity Assurance model ... The inputs that do not trigger regression faults will be maintained ... the mutated inputs ... are then executed by both the original model and the regression model. If the two models provide different prediction results, then a regression-fault-triggering input is identified.\" (Section III, overview / Approach)\n\nSection IV-A1 (Subjects / Experimental setup) \u2014 \"we used four pairs of datasets and DNN models as subjects, i.e., LeNet-5 on MNIST, VGG16 on CIFAR10, AlexNet on Fashion-MNIST (FM), and ResNet18 on SVHN\" (Section IV-A1 Subjects). \n\nThroughout the paper (Approach and Evaluation sections) all testing and analysis focus on M1 vs. M2 pairs; there is no experiment or statement reusing inputs crafted on one model to test different, unseen target models nor any mechanism intended to discover transferable attacks across model implementations."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's entire methodology and evaluation are focused on 'regression fuzzing', which it defines as finding faults between a prior version of a model (M1) and its subsequent version (M2). The goal is to find inputs that were correctly classified by M1 but are misclassified by M2 after an evolution event (e.g., fine-tuning, pruning). The experiments are designed around creating these M1/M2 pairs and evaluating the proposed fuzzer, DRFuzz, in its ability to find such regression-specific faults. The paper does not perform any experiments to check if the fault-inducing inputs found for a specific model (e.g., LeNet-5) also trigger failures in other, different model architectures (e.g., VGG16) performing the same task. Therefore, it does not demonstrate attack transferability across models.",
                    "evidence": "Page 2, Section II. DEFINITION, Definition 2. Regression fault: \"Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1 [x] = y \u2227 cM2 [x] != y.\" This definition explicitly frames the problem as a comparison between two versions of the same model, not between different models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper defines and evaluates regression-fault-triggering inputs strictly with respect to a prior model M1 and its regression model M2 (i.e., it detects inputs where M1 is correct and M2 is incorrect). All described generation, fidelity filtering, and evaluation steps execute mutated inputs on that M1\u2013M2 pair. The experiments use multiple dataset/model pairs but do not evaluate whether inputs found on one model transfer to other models performing the same task, nor does the design include explicit mechanisms to discover or measure transferable attacks across different model implementations. Therefore the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "Section II (Definition), Definition 2 \u2014 \"Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\" (Section II, Definition 2)\n\nSection III (Approach) \u2014 \"the mutated inputs are passed into the GAN-based Fidelity Assurance model ... The inputs that do not trigger regression faults will be maintained ... the mutated inputs ... are then executed by both the original model and the regression model. If the two models provide different prediction results, then a regression-fault-triggering input is identified.\" (Section III, overview / Approach)\n\nSection IV-A1 (Subjects / Experimental setup) \u2014 \"we used four pairs of datasets and DNN models as subjects, i.e., LeNet-5 on MNIST, VGG16 on CIFAR10, AlexNet on Fashion-MNIST (FM), and ResNet18 on SVHN\" (Section IV-A1 Subjects). \n\nThroughout the paper (Approach and Evaluation sections) all testing and analysis focus on M1 vs. M2 pairs; there is no experiment or statement reusing inputs crafted on one model to test different, unseen target models nor any mechanism intended to discover transferable attacks across model implementations."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Zhang et al. - 2019 - Life after Speech Recognition Fuzzing Semantic Misinterpretation for Voice Assistant Applications": {
        "key": "zhang2019life",
        "name": "LipFuzzer",
        "year": "2019",
        "citation_count": "79",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Medium",
                    "why": "It does not bypass permissions or authentication; instead, it tricks the VA into misinterpreting intent. The resulting misbehavior (e.g., invoking the wrong vApp or disclosing data to a malicious app). Expose semantically incorrect but functionally valid behaviors that breach user-intent and specification expectations\u2014posing integrity and trust risks without directly bypassing safety or security mechanisms."
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's LipFuzzer uncovers misinterpretations in the NLU Intent Classifier that lead to violations of user intent and security expectations (e.g., vApp squatting, installation/invocation hijacks, phishing, privacy leakage, and device-control denial-of-service). These are higher-impact than simple robustness errors. However, the work does not claim to target or bypass explicit safety- or security-aligned defenses (e.g., adversarially trained or safety-filtered models); instead it exploits template/intent-matching design flaws and developer-supplied templates in a black-box setting. Therefore the failures found are severe (unsafe, unauthorized behaviors and privacy/security consequences) but are not described as bypassing explicit defended/safety-aligned mechanisms, which matches the metric's \"Medium\" category.",
                    "evidence": "Abstract (p.1): \"...the semantic inconsistency caused by the improper semantic interpretation of an Intent Classifier can create the opportunity of breaching the integrity of vApp processing when attackers delicately leverage some common spoken errors.\"; II.D Threat Model (p.4): \"Attack consequences. We summarize a list of example consequences of interacting with malicious vApps as follows: Denial of Service: This would occur if a device-controlling vApp is hijacked. Privacy Leakage: vApps intimately connect users\u2019 daily life to a more exposed digital world... Phishing: Third-party vApps\u2019 back-end processing is fully controlled by the developers...\"; VI-C vApp Store Evaluation (p.12): \"As a result, a total of 715 (71.5%) Amazon Alexa vApps and 59 (29.5%) Google Assistant are verified to be vulnerable.\"; VI-D Case Study - Attacking 'True Bank' (p.13): \"As a result, a total number of 3 malicious vApps are invoked, which clearly demonstrate the practicability of the attack.\"; II.D Threat Model (p.4): \"We assume that an adversary does not need to access any internal implementations/configurations of vApp including ASR, NLU components.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's fuzzing tool, LipFuzzer, is designed to discover semantic misinterpretations in voice assistant platforms. These failures cause a user's voice command to be incorrectly mapped to an unintended or malicious third-party application (vApp). This is a direct violation of user intent. The paper explicitly details how these failures can lead to undesired and unsafe outcomes, including denial of service (e.g., a garage door command failing), privacy leaks, and phishing attacks. These consequences go beyond simple model robustness errors and constitute unsafe behavior. However, the method does not target or bypass explicit safety or security mechanisms in defended models. Instead, it exploits the inherent fuzzy matching logic and vulnerabilities within the standard Natural Language Understanding (NLU) and Intent Classifier components of the voice assistant architecture.",
                    "evidence": "Page 5, Section II-D. Threat Model: \"Attack consequences. We summarize a list of example consequences of interacting with malicous vApps as follows: Denial of Service: This would occur if a device-controlling vApp is hijacked. For example, when a user says \u201cAlexa, close the garage door\u201d to a mismatched malicious vApp instead of a legitimate one, the garage door may be left open. ... Phishing: Third-party vApps\u2019 back-end processing is fully controlled by the developers. ... Audio can thus be manipulated by an attacker at runtime, substituting a benign audio file with a \u201cmalicious\" audio file. For example, ... with \u201cSorry, a critical problem occurred in your Amazon Pay, please answer with your account username and password to know more.\u201d\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper's LipFuzzer uncovers misinterpretations in the NLU Intent Classifier that lead to violations of user intent and security expectations (e.g., vApp squatting, installation/invocation hijacks, phishing, privacy leakage, and device-control denial-of-service). These are higher-impact than simple robustness errors. However, the work does not claim to target or bypass explicit safety- or security-aligned defenses (e.g., adversarially trained or safety-filtered models); instead it exploits template/intent-matching design flaws and developer-supplied templates in a black-box setting. Therefore the failures found are severe (unsafe, unauthorized behaviors and privacy/security consequences) but are not described as bypassing explicit defended/safety-aligned mechanisms, which matches the metric's \"Medium\" category.",
                    "evidence": "Abstract (p.1): \"...the semantic inconsistency caused by the improper semantic interpretation of an Intent Classifier can create the opportunity of breaching the integrity of vApp processing when attackers delicately leverage some common spoken errors.\"; II.D Threat Model (p.4): \"Attack consequences. We summarize a list of example consequences of interacting with malicious vApps as follows: Denial of Service: This would occur if a device-controlling vApp is hijacked. Privacy Leakage: vApps intimately connect users\u2019 daily life to a more exposed digital world... Phishing: Third-party vApps\u2019 back-end processing is fully controlled by the developers...\"; VI-C vApp Store Evaluation (p.12): \"As a result, a total of 715 (71.5%) Amazon Alexa vApps and 59 (29.5%) Google Assistant are verified to be vulnerable.\"; VI-D Case Study - Attacking 'True Bank' (p.13): \"As a result, a total number of 3 malicious vApps are invoked, which clearly demonstrate the practicability of the attack.\"; II.D Threat Model (p.4): \"We assume that an adversary does not need to access any internal implementations/configurations of vApp including ASR, NLU components.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "untargeted exploration to expose semantic or intent-classification inconsistencies, sometimes aligning with attacker intent (e.g., LAPSUS examples), though its discovery of attacker intent is incidental rather than goal-optimized."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Empirically isolates the NLU Intent Classifier as the primary source of error and correlates misinterpretation patterns with linguistic mutation types"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "They perform human study to train the linguistic model that help selection potential fault triggering input. However, the synthesized voice commands are not measured for naturalness"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper enforces plausibility throughout its fuzzing design and provides empirical human-driven evidence that the fault-inducing inputs remain realistic. LipFuzzer mutates NLP-preprocessed templates using linguistically-grounded LAPSUS Models (Bayesian Networks) constructed from curated linguistic knowledge and further weight-trained with audio recordings from a user study. The fuzzer applies a probability cutoff to filter low-likelihood mutations and explicitly limits mutations per seed (\"we only get one mutation for each voice command\"), reducing cumulative drift. Finally, generated inputs are converted to speech (TTS) and verified on real VA devices; the authors report human-collected/resynthesized examples and measured rates (e.g., 77% of synthesized transcriptions still induced semantic inconsistency), demonstrating that many fault-triggering inputs are perceptually/semantically plausible.",
                    "evidence": "Page 2, (Introduction / Our Approach): \"we design LipFuzzer\u2019s basic template fuzzing by first mutating NLP pre-processed voice command templates. Then, we input mutated voice commands to VA devices by using machine speech synthesis to eliminate the human factor of producing ASR-related misinterpretation.\"; Page 7, (IV.B Weight Training): \"Weight Training. After we have the initial BN ... we further train the weight ... through a user study with audio recording.\"; Page 8, (IV.C Verification / Template Fuzzing): \"Verification. We verify if a LAPSUS is effective by testing the synthesis audio with Alexa. To do that, we first synthesize speech from generated LAPSUS voice commands.\"; Page 11, (VI-B Cutoffs for Different Query Strategies): \"Note that, during the fuzzing, we only get one mutation for each voice command.\"; Page 11, (VI-A Intent Classifier Evaluation): \"Our result shows that 77% (84/109) still incur the semantic inconsistency, which means LAPSUS still exists.\"; Page 11, (VI-B The Cutoff Value): \"By applying this cutoff value, we cut off 49.9% of generated LAPSUS population with the effectiveness rate increased to 59.52%.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper enforces input plausibility throughout the fuzzing process by design, and provides empirical evidence that the discovered fault-inducing inputs are realistic. The fuzzer, LipFuzzer, is explicitly a \"linguistic-model-guided\" tool that generates mutations based on models of plausible human speech errors, termed LAPSUS. These models are constructed from real-world linguistic knowledge (e.g., pronunciation, vocabulary, grammar rules) and are statistically trained using data from a human user study. This ensures that the generated voice commands are not random or unrealistic, but rather emulate common speech errors. The paper further provides empirical evidence of plausibility by conducting a user study with 150 MTurk workers to collect real-world voice command errors. The study confirms that the types of errors generated by LipFuzzer naturally occur, with 38.14% of collected user audio recordings classified as LAPSUS. Examples of these realistic errors are provided, directly demonstrating their plausibility.",
                    "evidence": "Page 8, Section IV-B, \"Weight Training\": \"After we have the initial BN (i.e., a graph with all edge weights set as 1) ready, we further train the weight (i.e., the probabilities of successful transition states) through a user study with audio recording. In this user study, we find sentences or short expressions which contains the states in the models. Then, we ask users in the study to repeat these sentences or expressions... Next, we calculate how many times these transitions are observed.\"\n\nPage 11, Section VI-A, \"Intent Classifier Evaluation\": \"The goal of this experiment is to confirm the existence of LAPSUS in the real world. From the collected audio records, we first remove unqualified audio samples... As a result, 61.86% (193/312) of them are processed with intended skills and functionality. This means that 38.14% of the recorded voice commands are LAPSUS. We showcase examples of these LAPSUS in Table III.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper enforces plausibility throughout its fuzzing design and provides empirical human-driven evidence that the fault-inducing inputs remain realistic. LipFuzzer mutates NLP-preprocessed templates using linguistically-grounded LAPSUS Models (Bayesian Networks) constructed from curated linguistic knowledge and further weight-trained with audio recordings from a user study. The fuzzer applies a probability cutoff to filter low-likelihood mutations and explicitly limits mutations per seed (\"we only get one mutation for each voice command\"), reducing cumulative drift. Finally, generated inputs are converted to speech (TTS) and verified on real VA devices; the authors report human-collected/resynthesized examples and measured rates (e.g., 77% of synthesized transcriptions still induced semantic inconsistency), demonstrating that many fault-triggering inputs are perceptually/semantically plausible.",
                    "evidence": "Page 2, (Introduction / Our Approach): \"we design LipFuzzer\u2019s basic template fuzzing by first mutating NLP pre-processed voice command templates. Then, we input mutated voice commands to VA devices by using machine speech synthesis to eliminate the human factor of producing ASR-related misinterpretation.\"; Page 7, (IV.B Weight Training): \"Weight Training. After we have the initial BN ... we further train the weight ... through a user study with audio recording.\"; Page 8, (IV.C Verification / Template Fuzzing): \"Verification. We verify if a LAPSUS is effective by testing the synthesis audio with Alexa. To do that, we first synthesize speech from generated LAPSUS voice commands.\"; Page 11, (VI-B Cutoffs for Different Query Strategies): \"Note that, during the fuzzing, we only get one mutation for each voice command.\"; Page 11, (VI-A Intent Classifier Evaluation): \"Our result shows that 77% (84/109) still incur the semantic inconsistency, which means LAPSUS still exists.\"; Page 11, (VI-B The Cutoff Value): \"By applying this cutoff value, we cut off 49.9% of generated LAPSUS population with the effectiveness rate increased to 59.52%.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "LipFuzzer generates mutated inputs as serialized audio (via Text-to-Speech / phoneme-to-speech) and uses those synthesized audio files as the actual test inputs to VA devices, and the paper describes verifying failures by playing the synthesized audio and observing the VA response. Because inputs are generated directly in a serialized format (audio) and the verification/oracle uses those serialized artifacts, the fault-inducing perturbations are preserved across I/O (serialization) and the paper explicitly accounts for this in both test-generation and oracle evaluation. This matches the metric's criterion that approaches which generate inputs directly in serialized formats are reproducible under standard I/O operations.",
                    "evidence": "Page 2, 'Our Approach' (Section I): \"...we input mutated voice commands to VA devices by using machine speech synthesis to eliminate the human factor of producing ASR-related misinterpretation.\" \n\nPage 8, 'C. Template Fuzzing' (Verification): \"We verify if a LAPSUS is effective by testing the synthesis audio with Alexa. To do that, we first synthesize speech from generated LAPSUS voice commands. Then we monitor the response of the VA. ... We define that a LAPSUS is verified to be effective when it is incorrectly interpreted by the VA system.\" \n\nPage 9, 'V. IMPLEMENTATION / Speech Synthesis': \"...we use two types of speech synthesis methods to generate LAPSUS. ... We use each vApp platforms\u2019 native speech synthesis tools: Amazon Polly [6] and Google Cloud TTS [9]. ... The second speech synthesis method is for vocabulary and grammar levels. We direct input fuzzing results generated with LipFuzzer to the above mentioned services and perform a Text-to-Speech conversion.\" \n\nPage 11, 'VI.A Intent Classifier Evaluation' (Experiment Setup/verification): \"...we synthesize audios (so that ASR processing is guaranteed to be correct) to the Amazon Echo device to check if the semantic inconsistency still exists.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's proposed method, LipFuzzer, generates fault-inducing inputs by mutating voice commands at a high level of abstraction (text, phonemes) and then uses speech synthesis systems (e.g., Amazon Polly, Google Cloud TTS, ESpeak) to create the final audio test cases. This approach generates inputs directly in a serialized format (an audio file like WAV or MP3). The metric definition explicitly states: 'We also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., synthesized speech...) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.' The perturbations are embedded in the content of the speech itself, not as fragile numerical artifacts on a waveform, and thus are not susceptible to being lost or altered by standard I/O operations like saving and reloading the audio file. Therefore, the method ensures that the generated fault-inducing inputs are reproducible.",
                    "evidence": "Page 9, Section V. IMPLEMENTATION, A. LipFuzzer, subsection \"Speech Synthesis\": \"In order to work with real VA platforms, We generate voice command records to verify the effectiveness of fuzzing results. For machine-generated voice commands, we use two types of speech synthesis methods to generate LAPSUS. The first speech synthesis method (for Phoneme-level) is phoneme-to-speech that is used for phoneme based fuzzing results... The second speech synthesis method is for vocabulary and grammar levels. We direct input fuzzing results generated with LipFuzzer to the above mentioned services and perform a Text-to-Speech conversion.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "LipFuzzer generates mutated inputs as serialized audio (via Text-to-Speech / phoneme-to-speech) and uses those synthesized audio files as the actual test inputs to VA devices, and the paper describes verifying failures by playing the synthesized audio and observing the VA response. Because inputs are generated directly in a serialized format (audio) and the verification/oracle uses those serialized artifacts, the fault-inducing perturbations are preserved across I/O (serialization) and the paper explicitly accounts for this in both test-generation and oracle evaluation. This matches the metric's criterion that approaches which generate inputs directly in serialized formats are reproducible under standard I/O operations.",
                    "evidence": "Page 2, 'Our Approach' (Section I): \"...we input mutated voice commands to VA devices by using machine speech synthesis to eliminate the human factor of producing ASR-related misinterpretation.\" \n\nPage 8, 'C. Template Fuzzing' (Verification): \"We verify if a LAPSUS is effective by testing the synthesis audio with Alexa. To do that, we first synthesize speech from generated LAPSUS voice commands. Then we monitor the response of the VA. ... We define that a LAPSUS is verified to be effective when it is incorrectly interpreted by the VA system.\" \n\nPage 9, 'V. IMPLEMENTATION / Speech Synthesis': \"...we use two types of speech synthesis methods to generate LAPSUS. ... We use each vApp platforms\u2019 native speech synthesis tools: Amazon Polly [6] and Google Cloud TTS [9]. ... The second speech synthesis method is for vocabulary and grammar levels. We direct input fuzzing results generated with LipFuzzer to the above mentioned services and perform a Text-to-Speech conversion.\" \n\nPage 11, 'VI.A Intent Classifier Evaluation' (Experiment Setup/verification): \"...we synthesize audios (so that ASR processing is guaranteed to be correct) to the Amazon Echo device to check if the semantic inconsistency still exists.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "evaluates two voice assistant systems independently but does not analyze whether failures generalize"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or report experiments where fault-inducing inputs (LAPSUS) discovered on one model/target are reused to test other models to show cross-model transferability. Although LipFuzzer is applied to both Amazon Alexa and Google Assistant stores (i.e., the authors run the tool on multiple platforms), the stated evaluation goals and the described experiments focus on finding and verifying LAPSUS per platform. The paper does not present (a) an explicit experiment that generates faults on a surrogate model and then reuses those same inputs on different/unknown target models, nor (b) design elements explicitly aimed at discovering or measuring the transferability of discovered fault-inducing inputs across independently implemented models performing the same task.",
                    "evidence": "Section/Pages and quotes from the paper:\n\n1) Abstract (p.1): \"We also use LipFuzzer to evaluate both Amazon Alexa and Google Assistant vApp platforms.\" \u2014 supports that both platforms were evaluated but does not indicate reuse/transfer experiments.\n\n2) Evaluation goals (Section VI, p.11): \"More specifically, our evaluation has three goals: (i) We empirically verify that the problematic Intent Classifier can lead to speech misinterpretation related to LAPSUS. (ii) We show LipFuzzer's performance in terms of the LAPSUS Models' accuracy and effectiveness. (iii) We use LipFuzzer to reveal that problematic templates widely exist in both Amazon Alexa and Google Assistant platforms.\" \u2014 none of these goals mentions generating fault-inducing inputs on one model and testing them on other models to assess transferability.\n\n3) vApp store experiment setup (Section VI-C, p.12): \"we apply the same LAPSUS Models used in LipFuzzer evaluation.\" and \"we evaluate LipFuzzer by using templates crawled from the Amazon Alexa Store and Google Assistant Store.\" \u2014 indicates application across stores but does not describe experiments that generate LAPSUS on one platform and then reuse them to test the other platform as a transferability study.\n\n(These quotes are from the paper's Abstract, Section VI (Evaluation), and Section VI-C (vApp Store Evaluation).)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper applies its fuzzing methodology to two different major Voice Assistant platforms (Amazon Alexa and Google Assistant), which represent distinct models performing the same task of semantic interpretation. The design of the fuzzer, LipFuzzer, explicitly incorporates mechanisms to discover transferable failures. It achieves this by building platform-agnostic 'LAPSUS Models' based on general linguistic knowledge of common speech errors (e.g., pronunciation, vocabulary, grammar) rather than on the specific implementation of any single platform's Natural Language Understanding (NLU) component. By using these general models to generate fault-inducing inputs for applications on both platforms, the paper demonstrates that the same classes of vulnerabilities exist across different models, fulfilling both criteria for a 'High' rating.",
                    "evidence": "Page 12, Section VI-C, \"vApp Store Evaluation\": \"1) Experiment Setup: We evaluate LipFuzzer by using templates crawled from the Amazon Alexa Store and Google Assistant Store. For Amazon Alexa Store, we acquired a seed template dataset of 98,261 voice commands from 32,892 vApps. For Google Assistant Store, we gathered 2,328 vApps with 9,044 voice commands.\" This is further supported by Table IV on Page 13, which shows the fuzzing results for both platforms. The design for transferability is evidenced on Page 6, Section IV-A, \"Fuzzing Input & Output\": \"The input of linguistic modeling is linguistic knowledge data shown in 1 of Figure 4. We choose LAPSUS-related linguistic knowledge from multiple sources [12] [26] [1] [20] [24].\" This confirms the models are built from general, platform-agnostic linguistic sources."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "Although the authors apply LipFuzzer separately to both Amazon Alexa and Google Assistant vApp platforms, they do not generate fault-inducing inputs on one platform and then reuse those exact inputs to test the other. The evaluation treats each platform in isolation, and the paper does not include any explicit experiment or analysis showing that LAPSUS examples found on a surrogate (e.g., Alexa) transfer to an unseen target (e.g., Google Assistant). Thus, it does not demonstrate cross-model transferability of specific fault-inducing inputs.",
                    "evidence": "Section VI\u2013C (vApp Store Evaluation, pp. 12\u201313): the authors describe running LipFuzzer \u201cby using templates crawled from the Amazon Alexa Store\u201d and \u201ctemplates crawled from the Google Assistant Store\u201d but do not report reusing the same LAPSUS across both platforms.  Section VI (Evaluation Goals, p. 11): none of the three stated goals mention testing the transfer of inputs from one model to another."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Data-driven"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Woodlief et al. - 2022 - Semantic image fuzzing of AI perception systems": {
        "key": "woodlief2022semantic",
        "name": "semSensFuzz",
        "year": "2022",
        "citation_count": "18",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's semantic-fuzzing approach (semSensFuzz / semImFuzz) uncovers misinterpretations and undesirable perception outputs (e.g., objects misclassified after semantic additions or occlusions) that violate safety expectations for autonomous systems. The authors report many significant and extreme semantic failures (misclassifications caused by added cars/people) that could be safety-relevant. However, the work targets standard perception models and semantic mutations (add car, add pedestrian, change color) and does not demonstrate bypassing explicit safety, policy, or security mechanisms (e.g., attacking defended or safety-aligned models, inducing data leakage or unauthorized actions). Therefore the failures exposed are beyond simple low-level robustness errors but do not meet the paper\u2019s or the metric\u2019s definition of high severity (bypassing explicit defenses), placing the method in the Medium category.",
                    "evidence": "Page 1958, Abstract: \"we found that it synthesized tests with novel and subjectively realistic image inputs, and that it discovered inputs that revealed significant inconsistencies between the specified and computed interpretations.\" (Abstract) \n\nPage 1966, Section 6.4.1 (RQ1 Results): \"1210 SUT inconsistencies resulting from 884 mutations and that each SUT had over 100 inconsistencies. Further, each of the SUTs exhibited at least one significant inconsistency and 3 of the 5 SUTs combined to exhibit a total of 20 extreme inconsistencies.\" (Section 6.4.1) \n\nPage 1966, Figure 12 caption / discussion: \"The rightmost column shows an extreme inconsistency; an added car occludes a truck, causing EfficientPS to misclassify the truck, labelling portions as 'do not care' and the rest as building.\" (Section 6.4.1, example of semantic misclassification) \n\nPage 1961, Section 4.1: \"we created an extensible pipeline in Python for fuzzing perception systems of autonomous vehicles with a single front-facing camera, semImFuzz, that implements three mutations: changing the color of cars, adding cars, and adding pedestrians.\" (Section 4.1) \n\nPage 1965, Section 6.1 (SUTs): \"we evaluate it on five highly competitive perception systems submitted to the Cityscapes benchmark...\" (Section 6.1) \u2014 showing the evaluation targets are standard benchmark SUTs rather than defended or safety-aligned models."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, semImFuzz, uncovers failures that violate safety expectations by producing unsafe outputs, which aligns with the 'Medium' severity definition. The fuzzer operates by applying semantic mutations, such as adding cars or pedestrians to an existing scene. The evaluation shows that these mutations can cause state-of-the-art perception systems to make critical misinterpretations, such as misclassifying a truck as a building after another car is added to the scene. These are not mere robustness errors (as in the 'Low' definition) but significant perception failures that could lead to unsafe behavior in an autonomous vehicle. The work does not meet the 'High' criteria because it does not test against models with explicit, bypassable safety or security mechanisms; instead, it tests standard perception models.",
                    "evidence": "Page 10, Section 6.4.1, \"For contextualizing the magnitude of the inconsistencies, Figure 12 shows three mutations produced by semImFuzz and their interpretations by the EfficientPS SUT before and after the mutation. ... The rightmost column shows an extreme inconsistency; an added car occludes a truck, causing EfficientPS to misclassify the truck, labelling portions as 'do not care' and the rest as building.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper's semantic-fuzzing approach (semSensFuzz / semImFuzz) uncovers misinterpretations and undesirable perception outputs (e.g., objects misclassified after semantic additions or occlusions) that violate safety expectations for autonomous systems. The authors report many significant and extreme semantic failures (misclassifications caused by added cars/people) that could be safety-relevant. However, the work targets standard perception models and semantic mutations (add car, add pedestrian, change color) and does not demonstrate bypassing explicit safety, policy, or security mechanisms (e.g., attacking defended or safety-aligned models, inducing data leakage or unauthorized actions). Therefore the failures exposed are beyond simple low-level robustness errors but do not meet the paper\u2019s or the metric\u2019s definition of high severity (bypassing explicit defenses), placing the method in the Medium category.",
                    "evidence": "Page 1958, Abstract: \"we found that it synthesized tests with novel and subjectively realistic image inputs, and that it discovered inputs that revealed significant inconsistencies between the specified and computed interpretations.\" (Abstract) \n\nPage 1966, Section 6.4.1 (RQ1 Results): \"1210 SUT inconsistencies resulting from 884 mutations and that each SUT had over 100 inconsistencies. Further, each of the SUTs exhibited at least one significant inconsistency and 3 of the 5 SUTs combined to exhibit a total of 20 extreme inconsistencies.\" (Section 6.4.1) \n\nPage 1966, Figure 12 caption / discussion: \"The rightmost column shows an extreme inconsistency; an added car occludes a truck, causing EfficientPS to misclassify the truck, labelling portions as 'do not care' and the rest as building.\" (Section 6.4.1, example of semantic misclassification) \n\nPage 1961, Section 4.1: \"we created an extensible pipeline in Python for fuzzing perception systems of autonomous vehicles with a single front-facing camera, semImFuzz, that implements three mutations: changing the color of cars, adding cars, and adding pedestrians.\" (Section 4.1) \n\nPage 1965, Section 6.1 (SUTs): \"we evaluate it on five highly competitive perception systems submitted to the Cityscapes benchmark...\" (Section 6.1) \u2014 showing the evaluation targets are standard benchmark SUTs rather than defended or safety-aligned models."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "generates safety-critical scenarios (cars, pedestrians, obstacles) to expose perception failures."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Offers descriptive correlations between input changes and output errors, but no analysis on root cause"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Applies design constratraints but no measure of naturalness of the fault-triggering fuzzed inputs"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly designs and enforces per-mutation plausibility constraints (resource-based parameterization, preconditions, perspective and lighting checks) and it performs an explicit human validation of final fault-inducing inputs. However, the implementation still produces a substantial rate of non-conforming (false-positive) mutants, the authors report limitations of automated conformity checks (failed discriminator), and they do not address cumulative drift across multiple iterative mutations. Thus the work enforces plausibility at the individual-mutation level and validates final cases empirically, but does not fully ensure plausibility throughout iterative fuzzing and exhibits imperfect conformity enforcement.",
                    "evidence": "Section 3.2 (p.1960): \"Second, we associate a set of preconditions Prec with each mutation \u03b4, specified in terms of the interp that defines whether \u03b4 is applicable to a given t. If Prec(interp) is not satisfied, then \u03b4 is not applicable to that test.\" (Section name: 3.2 Semantic Mutations with semSensFuzz) \n\nSection 4.3.1 (p.1963): \"To ensure that the lighting conditions are similar, if the entity does not have a median value within 5 units (~2%) of the median value of the base image target area, then it fails the conformity check.\" (Section name: 4.3 Adding an Entity / Implementation) \n\nSection 4.3.1 (p.1962\u20131963): description of multiple conformity checks (vanishing point quadrant matching, bounding-box-on-road check, occlusion filtering) used before generating a mutated image. (Section name: 4.3 Adding an Entity / Implementation) \n\nSection 4.4.4 (p.1964): \"We explored this approach, training a binary classifier based on a CNN ... The discriminator learned to differentiate between the classes, but could not differentiate conforming versus nonconforming images.\" (Section name: 4.4.4 Discriminators for Conformity Checking) \n\nSection 6.4.1 and Table 2 (p.1966\u20131967): human-validation of plausibility: \"we manually inspect all the generated test cases that led to 5+ p.p. drop, and sample 10% of the test cases that led to a drop between 1 and 5 p.p.. The process entailed each author examining each image and classifying them as either a true positive or false positive.\" Table 2 (p.1967) reports substantial false-positive rates (e.g., EfficientPS: 43% for [1,5) p.p., 47% for [5,10), 53% for [10,100]). (Section name: 6.4.1 RQ1 Results: Finding Inconsistencies) \n\nSection 4.3.2 and Figures 9 (p.1963): authors enumerate concrete failure modes leading to nonconforming images (perspective mismatch, overlapping entities, inconsistent lighting, missing shadows). (Section name: 4.3 Adding an Entity / Potential for False Positives) \n\nSection 3.2 (p.1960): \"All of the mechanisms in our approach make a conscious trade off between conformity and a smaller space of available mutations.\" (Section name: 3.2 Semantic Mutations with semSensFuzz)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's methodology is fundamentally centered on maintaining input plausibility, which it terms \"conformity.\" The fuzzing design incorporates numerous mechanisms to enforce this throughout the mutation process, such as using semantic preconditions, leveraging real-world data for mutations, and performing explicit checks for consistent perspective, physical feasibility (e.g., placing a car on a road), and lighting. Most importantly, the paper provides direct empirical evidence assessing the plausibility of the final fault-inducing inputs. The authors conducted a manual study where they inspected all high-impact and a sample of moderate-impact failure-inducing images to classify them as conforming (\"true positive\") or non-conforming (\"false positive\"). The results of this human evaluation, including false positive rates and visual examples, are reported. This combination of enforcing plausibility by design and empirically validating the realism of the final outputs fully meets the criteria for a \"High\" rating.",
                    "evidence": "Page 11, Section 6.4, \"...one potential issue is the presence of test cases that are nonconforming, which leads to false inconsistencies. As highlighted in Section 4, there are several factors that can result in a nonconforming test case, and determining conformity is subjective. Still, to gain a better grasp on the rate of false positives we manually inspect all the generated test cases that led to 5+ p.p. drop, and sample 10% of the test cases that led to a drop between 1 and 5 p.p.. The process entailed each author examining each image and classifying them as either a true positive or false positive. If any of the three authors deemed an image a false positive, it was conservatively recorded as such.\" This is further supported by Table 2, \"False Positive Rate for Inconsistencies Found,\" and Figure 14, which provides visual examples of both \"True Positive\" and \"False Positive\" (i.e., non-conforming) results."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "semSensFuzz builds in strong, per-mutation plausibility constraints (semantic preconditions, real data parameterization, perspective & lighting checks) and even conducts a human validation of final fault-inducing inputs, but it does not enforce a global, iterative plausibility guarantee across successive mutations (no cumulative-drift control) nor apply quantitative perceptual metrics (e.g. LPIPS/SSIM) on the full mutated distribution. Thus it validates individual steps and final cases empirically, but stops short of the end-to-end, iteration-aware enforcement required for a \u201cHigh\u201d rating.",
                    "evidence": "Section 3.2 (p. 1960): \u201cwe associate a set of preconditions Prec with each mutation \u03b4\u2026 If Prec(interp) is not satisfied, then \u03b4 is not applicable\u2026\u201d\n\nSection 4.3.1 (p. 1962\u20131963): Perspective quadrant matching, on-road bounding-box check, occlusion filtering, and median HSV brightness checks are all applied before each \u2018add entity\u2019 mutation.\n\nSection 4.4.4 (p. 1964): \u201cWe explored\u2026 a discriminator\u2026 but it could not differentiate conforming versus nonconforming images.\u201d\n\nSection 6.4.1 & Table 2 (p. 1966\u20131967): Authors manually inspect all high-impact and a sampled set of moderate-impact fault inputs, reporting substantial false-positive rates (e.g. 43%\u201375% nonconforming in some bins)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Perturbations occur at a semantic level that is plausibly I/O-stable, but the paper neither enforces nor verifies persistence after I/O transformations, and minor artifacts from the mutation implementation may cause non-persistent faults."
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper applies semantic, metamorphic image-level mutations (e.g., adding cars/pedestrians, recoloring) rather than fine-grained numerical perturbations, so the generated failures are less likely to be fragile numerical artifacts. However, the authors do not explicitly account for standard I/O/serialization effects (clipping, quantization, rounding) nor constrain mutations to guarantee that fault-inducing changes persist after typical image serialization. Thus the work corresponds to the metric's \"semantic-preserving metamorphic transformations without explicit enforcement of reproducibility under serialization\" case.",
                    "evidence": "Page 1960, Section 3.1 (Problem definition): \"At a conceptual level, our approach thus constitutes a form of metamorphic testing [32] where existing test cases are converted into new ones...\"; Page 1962, Section 4.1 (semImFuzz Architecture / Implementation): \"We implemented each mutation using Numpy [13] along with OpenCV for Python [4] and Pillow [6], two common image processing libraries for Python.\"; Page 1963, Section 4.3.1 (Adding an Entity - Implementation): \"When adding the entity, the mutation takes the isolated portion of the image and overlays it at the same pixel coordinates on the base image. Figure 8 demonstrates this process.\"; Page 1963, Section 4.3.2 (Potential for False Positives): \"...so semImFuzz does not attempt to add a shadow when adding an entity. This can lead to nonconforming images, as shown in Figure 9d.\" These excerpts show the approach uses semantic/metamorphic pixel-level edits (not fine numerical perturbations) but contain no statement that the design explicitly models or enforces persistence of perturbations through standard I/O operations such as quantization, clipping, or rounding."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, `semImFuzz`, generates test cases by applying semantic-preserving metamorphic transformations to existing images, such as adding a car or changing an object's color. According to the metric's definition, using such transformations without explicitly enforcing reproducibility under serialization qualifies for a 'Medium' rating. The paper's methodology focuses heavily on ensuring the 'conformity' of the mutated images\u2014that they are visually realistic and physically plausible\u2014through various checks related to perspective, lighting, and object placement. However, the paper does not mention or explicitly account for numerical alterations that can occur during standard I/O operations like saving an image to a file (which involves quantization, rounding, or clipping). The test generation and execution pipeline appears to be conducted entirely in memory, without a step to serialize and then deserialize the mutated input to confirm that the failure-inducing perturbation persists.",
                    "evidence": "Page 6, Section 4.2, Changing Object Color: \"The simplest mutation we implemented changes the color of a single entity in the test case...\".\nPage 6, Section 4.3, Adding an Entity: \"Adding entities to scenes is one of the advances of semImFuzz. The goal of this mutation is to add an entity to a scene in a way that may impact the perception system such as adding a vehicle or pedestrian.\"\nPage 7, Section 4.3.2, Potential for False Positives: This section discusses sources of invalid tests, but focuses on semantic and visual realism issues like perspective mismatches, object overlap, inconsistent lighting, and missing shadows, rather than numerical artifacts lost during I/O operations. For instance: \"Another consideration is consistent lighting... This can lead to nonconforming images, as shown in Figure 9d.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper applies semantic, metamorphic image-level mutations (e.g., adding cars/pedestrians, recoloring) rather than fine-grained numerical perturbations, so the generated failures are less likely to be fragile numerical artifacts. However, the authors do not explicitly account for standard I/O/serialization effects (clipping, quantization, rounding) nor constrain mutations to guarantee that fault-inducing changes persist after typical image serialization. Thus the work corresponds to the metric's \"semantic-preserving metamorphic transformations without explicit enforcement of reproducibility under serialization\" case.",
                    "evidence": "Page 1960, Section 3.1 (Problem definition): \"At a conceptual level, our approach thus constitutes a form of metamorphic testing [32] where existing test cases are converted into new ones...\"; Page 1962, Section 4.1 (semImFuzz Architecture / Implementation): \"We implemented each mutation using Numpy [13] along with OpenCV for Python [4] and Pillow [6], two common image processing libraries for Python.\"; Page 1963, Section 4.3.1 (Adding an Entity - Implementation): \"When adding the entity, the mutation takes the isolated portion of the image and overlays it at the same pixel coordinates on the base image. Figure 8 demonstrates this process.\"; Page 1963, Section 4.3.2 (Potential for False Positives): \"...so semImFuzz does not attempt to add a shadow when adding an entity. This can lead to nonconforming images, as shown in Figure 9d.\" These excerpts show the approach uses semantic/metamorphic pixel-level edits (not fine numerical perturbations) but contain no statement that the design explicitly models or enforces persistence of perturbations through standard I/O operations such as quantization, clipping, or rounding."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper reuses fault-inducing inputs generated by semImFuzz to test multiple different perception models: the same set of 150,000 semantically mutated tests was executed against each of the five SUTs, and the authors report aggregated counts of inconsistencies across SUTs (e.g., \"1210 SUT inconsistencies resulting from 884 mutations\"), which demonstrates reuse of generated failures to evaluate other models. However, the paper does not present design elements or explicit mechanisms aimed at producing or measuring transferable adversarial attacks (e.g., surrogate-model training, cross-model transfer rate analyses, or targeted transferability objectives). The evaluation reports per-SUT inconsistency counts and notes which mutations trigger errors, but it does not characterize transfer rates or describe methods explicitly designed to discover transferable security failures across models. These facts match the metric definition for Medium: inputs are reused to test transferability but the fuzzing design is not explicitly aimed at discovering transferable attacks.",
                    "evidence": "Section 6.2 (Tests Generated), p. 1966: \"For evaluation, we ran each SUT on 150,000 tests generated by semImFuzz comprised of 50,000 'Add Car' mutations, 50,000 'Add Person' mutations, and 50,000 'Change Car Color' mutations.\" \n\nSection 6.4.1 (RQ1 Results: Finding Inconsistencies), p. 1966: \"We first remark that semImFuzz found 1210 SUT inconsistencies resulting from 884 mutations and that each SUT had over 100 inconsistencies.\" \n\n(Absence of transfer-focused design) The paper's evaluation and methodology sections (Sections 3, 4, 6) describe mutation design, conformity checks, and per-SUT inconsistency reporting but do not describe surrogate-model-based strategies, explicit cross-model transferability metrics, or procedures to maximize/measure transfer of failures between models. For example, Section 6 (Evaluation) frames results as per-SUT inconsistency counts and severity drops rather than transfer rates or cross-model vulnerability analyses."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's evaluation reuses the generated fault-inducing inputs across multiple models but the fuzzing design is not explicitly aimed at discovering transferable attacks. The methodology involves creating a single set of 150,000 mutated inputs and then executing this same set against five different perception models (SUTs) performing the same task. This experimental design inherently tests for transferability by observing how different models react to the same set of inputs. However, the design of the fuzzer, `semImFuzz`, focuses on generating semantically valid and conforming mutations based on real-world constraints (e.g., adding cars to roads, changing colors) rather than incorporating explicit mechanisms to discover vulnerabilities that are common across different model architectures. Therefore, it satisfies the condition of reusing inputs but lacks the explicit design goal for transferability required for a 'High' rating.",
                    "evidence": "Page 9, Section 6.2 Tests Generated, \"For evaluation, we ran each SUT on 150,000 tests generated by semImFuzz comprised of 50,000 \"Add Car\" mutations, 50,000 \"Add Person\" mutations, and 50,000 \u201cChange Car Color\u201d mutations.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper reuses fault-inducing inputs generated by semImFuzz to test multiple different perception models: the same set of 150,000 semantically mutated tests was executed against each of the five SUTs, and the authors report aggregated counts of inconsistencies across SUTs (e.g., \"1210 SUT inconsistencies resulting from 884 mutations\"), which demonstrates reuse of generated failures to evaluate other models. However, the paper does not present design elements or explicit mechanisms aimed at producing or measuring transferable adversarial attacks (e.g., surrogate-model training, cross-model transfer rate analyses, or targeted transferability objectives). The evaluation reports per-SUT inconsistency counts and notes which mutations trigger errors, but it does not characterize transfer rates or describe methods explicitly designed to discover transferable security failures across models. These facts match the metric definition for Medium: inputs are reused to test transferability but the fuzzing design is not explicitly aimed at discovering transferable attacks.",
                    "evidence": "Section 6.2 (Tests Generated), p. 1966: \"For evaluation, we ran each SUT on 150,000 tests generated by semImFuzz comprised of 50,000 'Add Car' mutations, 50,000 'Add Person' mutations, and 50,000 'Change Car Color' mutations.\" \n\nSection 6.4.1 (RQ1 Results: Finding Inconsistencies), p. 1966: \"We first remark that semImFuzz found 1210 SUT inconsistencies resulting from 884 mutations and that each SUT had over 100 inconsistencies.\" \n\n(Absence of transfer-focused design) The paper's evaluation and methodology sections (Sections 3, 4, 6) describe mutation design, conformity checks, and per-SUT inconsistency reporting but do not describe surrogate-model-based strategies, explicit cross-model transferability metrics, or procedures to maximize/measure transfer of failures between models. For example, Section 6 (Evaluation) frames results as per-SUT inconsistency counts and severity drops rather than transfer rates or cross-model vulnerability analyses."
                },
                "manual": {
                    "value": "Low",
                    "why": "Mutated inputs are evaluated independently across multiple inputs but no measure whether same failures discovered by the fauzzer transfer to other models",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Data-driven"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Lee et al. - 2020 - Effective white-box testing of deep neural networks with adaptive neuron-selection strategy": {
        "key": "lee2020effective",
        "name": "ADAPT",
        "year": "2020",
        "citation_count": "81",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's contribution and evaluation focus on increasing coverage and finding adversarial inputs (misclassifications) for standard pre-trained models (LeNet, VGG, ResNet). All reported failures are robustness errors (incorrectly classified labels / adversarial images). The authors do not evaluate or claim bypassing of any explicit safety, policy, or security mechanisms, nor do they test defended or safety-aligned models or demonstrate jailbreaks, data leakage, or unauthorized actions. Therefore the failures uncovered correspond to model robustness errors only, matching the metric's Low category.",
                    "evidence": "Page 165 (Abstract): \"Experiments with real-world network models and datasets show that Adapt is remarkably more effective than existing testing techniques in terms of coverage and adversarial inputs found.\" \n\nPage 172 (Section 4.3, 'Adversarial Inputs'): \"Table 3 reports the average number of mutations per image (Mutations), the average number of adversarial inputs found per image (Adv. Inputs), the total number of incorrectly classified labels (Labels), and the number of initial inputs (Seeds) from which adversarial inputs were found.\" \n\nPage 173 (Section 4.3): \"In particular, the adversarial images found for VGG-19 and ResNet-50 were visually indistinguishable from the original ones.\" \n\nPage 169 (Section 4.1 / Table 2): \"We used two datasets and four neural network models in Table 2... LeNet-4, LeNet-5, VGG-19 and ResNet-50\" (standard pre-trained models; no defended/safety-aligned models are listed). \n\nPage 174 (Conclusion): \"we demonstrated that Adapt is significantly more effective than existing white-box and grey-box techniques in increasing coverage and finding adversarial inputs.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, ADAPT, is designed and evaluated to find adversarial inputs that cause misclassifications in standard image recognition models. The failures are defined as \"model robustness errors,\" where small, semantically preserving mutations to an input image cause the model to produce an incorrect label. The evaluation metrics focus on the number of adversarial inputs found and the quantity of incorrectly classified labels. The paper does not test for or claim to uncover unsafe, biased, or toxic behaviors, nor does it attempt to bypass any explicit safety or security mechanisms in defended models. The uncovered failures are purely mispredictions, which aligns perfectly with the 'Low' severity definition.",
                    "evidence": "Page 9, Section 4.3, Adversarial Inputs: \"Table 3 reports the average number of mutations per image (Mutations), the average number of adversarial inputs found per image (Adv. Inputs), the total number of incorrectly classified labels (Labels), and the number of initial inputs (Seeds) from which adversarial inputs were found. An incorrectly classified label consists of an original image and a found label.\" This quote demonstrates that the failures are defined as incorrect classifications. Further evidence on the nature of the inputs is on page 7, Section 4.1: \"All the techniques, including ADAPT, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\", confirming the use of small, semantically preserving mutations."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's contribution and evaluation focus on increasing coverage and finding adversarial inputs (misclassifications) for standard pre-trained models (LeNet, VGG, ResNet). All reported failures are robustness errors (incorrectly classified labels / adversarial images). The authors do not evaluate or claim bypassing of any explicit safety, policy, or security mechanisms, nor do they test defended or safety-aligned models or demonstrate jailbreaks, data leakage, or unauthorized actions. Therefore the failures uncovered correspond to model robustness errors only, matching the metric's Low category.",
                    "evidence": "Page 165 (Abstract): \"Experiments with real-world network models and datasets show that Adapt is remarkably more effective than existing testing techniques in terms of coverage and adversarial inputs found.\" \n\nPage 172 (Section 4.3, 'Adversarial Inputs'): \"Table 3 reports the average number of mutations per image (Mutations), the average number of adversarial inputs found per image (Adv. Inputs), the total number of incorrectly classified labels (Labels), and the number of initial inputs (Seeds) from which adversarial inputs were found.\" \n\nPage 173 (Section 4.3): \"In particular, the adversarial images found for VGG-19 and ResNet-50 were visually indistinguishable from the original ones.\" \n\nPage 169 (Section 4.1 / Table 2): \"We used two datasets and four neural network models in Table 2... LeNet-4, LeNet-5, VGG-19 and ResNet-50\" (standard pre-trained models; no defended/safety-aligned models are listed). \n\nPage 174 (Conclusion): \"we demonstrated that Adapt is significantly more effective than existing white-box and grey-box techniques in increasing coverage and finding adversarial inputs.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Correlates neuron features with faults"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Norm-bounded"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility during fuzzing via an explicit distance constraint (Constraint(I,I')) and by tuning mutation parameters (e.g., \u03b71) to limit drift; the authors also report maintaining the L2 distance between original and mutated images (\u22480.05) during experiments and note that some adversarial images were \"visually indistinguishable\" from originals. However, the paper does not provide an explicit empirical assessment of the perceptual or semantic realism of the final fault-inducing inputs using human studies or task-specific quantitative measures (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity). Therefore it enforces plausibility at mutation time and validates it qualitatively, but lacks rigorous quantitative / human-evaluation evidence required for a High rating.",
                    "evidence": "Section 2.3 (Algorithm discussion), p.166: \"For example, Constraint(I, I') is true if L2 distance between I and I' is less than a certain threshold.\"; Section 3.2 (Hyperparameters), p.169: \"The second hyper-parameter \u03b71 is the number of times that the set of selected neurons is used for generating new inputs... We manually set the \u03b71 to 3 so that the distance is not too far.\"; Section 4.1 (Experimental setup), p.170: \"All the techniques, including Adapt, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\"; Section 4.3 (Adversarial Inputs), p.172: \"...the adversarial images found for VGG-19 and ResNet-50 were visually indistinguishable from the original ones.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces a plausibility constraint during input generation and provides a qualitative assessment of the final fault-inducing inputs. A bounded L2-distance constraint is maintained between the initial seed and any mutated input, which helps control the cumulative effect of perturbations. The authors also qualitatively assess the realism of the generated inputs, stating that for the large ImageNet models, the adversarial examples were 'visually indistinguishable from the original ones' and providing visual examples in Figure 3. However, this does not meet the 'High' standard because the plausibility assessment is not a formal human study or based on quantitative perceptual metrics (e.g., LPIPS, SSIM), and the L2-norm is a simple proxy for maintaining inputs within a truly plausible distribution.",
                    "evidence": "Page 7, Section 4.1, Experimental Setup: \"All the techniques, including ADAPT, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\"\n\nPage 10, Section 4.3, Correlation between Coverage and Adversarial Inputs: \"In particular, the adversarial images found for VGG-19 and ResNet-50 were visually indistinguishable from the original ones.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility during fuzzing via an explicit distance constraint (Constraint(I,I')) and by tuning mutation parameters (e.g., \u03b71) to limit drift; the authors also report maintaining the L2 distance between original and mutated images (\u22480.05) during experiments and note that some adversarial images were \"visually indistinguishable\" from originals. However, the paper does not provide an explicit empirical assessment of the perceptual or semantic realism of the final fault-inducing inputs using human studies or task-specific quantitative measures (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity). Therefore it enforces plausibility at mutation time and validates it qualitatively, but lacks rigorous quantitative / human-evaluation evidence required for a High rating.",
                    "evidence": "Section 2.3 (Algorithm discussion), p.166: \"For example, Constraint(I, I') is true if L2 distance between I and I' is less than a certain threshold.\"; Section 3.2 (Hyperparameters), p.169: \"The second hyper-parameter \u03b71 is the number of times that the set of selected neurons is used for generating new inputs... We manually set the \u03b71 to 3 so that the distance is not too far.\"; Section 4.1 (Experimental setup), p.170: \"All the techniques, including Adapt, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\"; Section 4.3 (Adversarial Inputs), p.172: \"...the adversarial images found for VGG-19 and ResNet-50 were visually indistinguishable from the original ones.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's testing method mutates inputs by adding gradients directly to floating\u2011point input tensors and evaluates failures in memory (constraining L2 distance), but it does not mention any handling of serialization effects (clipping, quantization, or encoding) or generating inputs directly in serialized formats. Because the technique relies on fine-grained numeric gradient perturbations (I' = I' + \u03bb * gradient) and there is no explicit treatment to ensure those perturbations survive common I/O operations (e.g., 8-bit image encoding), fault\u2011inducing perturbations could be lost or altered when inputs are written to or read from disk. Thus the paper ignores I/O/serialization effects during mutation and oracle evaluation and scores Low on Failure Reproducibility.",
                    "evidence": "Page 166, Section 2.3 (Algorithm 1): \"I' \u2190 I' + \u03bb \u00b7 \u2202(\u03a3_{n\u2208N} Neuron(n, I'))/\u2202I'\" (shows mutations applied by adding gradients to the input tensor in memory).\nPage 168, Section 3.2 (Algorithm 2): \"I' \u2190 I' + \u03bb \u00b7 \u2202(\u03a3_{n\u2208N} Neuron(n, I'))/\u2202I'\" (the adaptive algorithm likewise generates inputs by in\u2011memory gradient additions).\nPage 170, Section 4.1 (Experimental setup): \"All the techniques, including Adapt, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\" (shows constraint on numeric perturbation magnitude but no mention of serialization/clipping/rounding handling).\nPage 169, Section 4 (Implementation): \"We implemented our technique in a tool, called Adapt, using Python 3.6.3, Tensorflow 1.14.0, and Keras 2.2.4 without any modification of the frameworks.\" (implementation context indicates in\u2011framework tensor operations; nowhere in the paper do the authors describe explicitly accounting for serialization / I/O quantization effects when generating or evaluating test inputs)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a white-box testing method, ADAPT, which generates new test inputs by adding the gradient of selected neurons to an existing input's numerical representation. This process directly manipulates the in-memory floating-point values of the input tensor. The paper specifies that a constraint is applied to the generated inputs, but this constraint is a numerical L2-distance to the original input, which does not account for serialization effects like quantization, rounding, or clipping. The methodology described in Algorithm 2 and the experimental setup does not include any step to save the generated adversarial inputs to a standard file format (e.g., PNG, JPEG) and then reload them to verify if the failure persists. The entire process of mutation and evaluation appears to be performed on in-memory tensors, ignoring potential I/O effects. Consequently, the fault-inducing perturbations are likely fragile numerical artifacts that could be lost or altered by standard serialization.",
                    "evidence": "Page 7, Section 4.1, \"Experimental Setup\": \"All the techniques, including ADAPT, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\" This shows the constraint is purely numerical and does not address I/O effects.\nPage 5, Algorithm 2, Lines 14-16: The algorithm generates a new input `I'` via gradient addition, immediately runs the DNN on it (`O' \u2190 Run(DNN, I')`), and then checks for new coverage (`if Cov(O') ...`). This workflow contains no intermediate step to account for serialization or I/O operations, indicating that evaluation is performed on the raw numerical output of the mutation."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's testing method mutates inputs by adding gradients directly to floating\u2011point input tensors and evaluates failures in memory (constraining L2 distance), but it does not mention any handling of serialization effects (clipping, quantization, or encoding) or generating inputs directly in serialized formats. Because the technique relies on fine-grained numeric gradient perturbations (I' = I' + \u03bb * gradient) and there is no explicit treatment to ensure those perturbations survive common I/O operations (e.g., 8-bit image encoding), fault\u2011inducing perturbations could be lost or altered when inputs are written to or read from disk. Thus the paper ignores I/O/serialization effects during mutation and oracle evaluation and scores Low on Failure Reproducibility.",
                    "evidence": "Page 166, Section 2.3 (Algorithm 1): \"I' \u2190 I' + \u03bb \u00b7 \u2202(\u03a3_{n\u2208N} Neuron(n, I'))/\u2202I'\" (shows mutations applied by adding gradients to the input tensor in memory).\nPage 168, Section 3.2 (Algorithm 2): \"I' \u2190 I' + \u03bb \u00b7 \u2202(\u03a3_{n\u2208N} Neuron(n, I'))/\u2202I'\" (the adaptive algorithm likewise generates inputs by in\u2011memory gradient additions).\nPage 170, Section 4.1 (Experimental setup): \"All the techniques, including Adapt, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\" (shows constraint on numeric perturbation magnitude but no mention of serialization/clipping/rounding handling).\nPage 169, Section 4 (Implementation): \"We implemented our technique in a tool, called Adapt, using Python 3.6.3, Tensorflow 1.14.0, and Keras 2.2.4 without any modification of the frameworks.\" (implementation context indicates in\u2011framework tensor operations; nowhere in the paper do the authors describe explicitly accounting for serialization / I/O quantization effects when generating or evaluating test inputs)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper presents Adapt as a white-box testing technique that operates on a single target DNN (Algorithm 2 takes a single DNN as input) and evaluates it per-model. The experiments report per-model coverage and adversarial inputs but do not test or report whether fault-inducing inputs produced on one model transfer to other models. The authors explicitly note that DeepXplore's differential (multi-DNN) testing was not used in its multi-model mode: \u201cwe used it with a single DNN and labeled data in our experiments.\u201d There is no experiment, analysis, or mechanism described to generate or evaluate transferable (cross-model) fault-inducing inputs or surrogate-model attacks. Therefore the paper does not demonstrate attack transferability across models.",
                    "evidence": "Section 3.2 (Algorithm 2): \"1: procedure Adapt(DNN, I,Cov)\" (Algorithm 2 signature shows Adapt operates on a single DNN) \u2014 (page 168, Section 3.2, Algorithm 2).\n\nSection 4.1 Experimental Setup: \"DeepXplore originally performs differential testing requiring multiple DNNs without labeled data but we used it with a single DNN and labeled data in our experiments.\" (shows the authors evaluated tools in single-model mode and did not perform multi-model/differential transfer experiments) \u2014 (page 170, Section 4.1 Experimental Setup).\n\nOverall evaluation description and results (Sections 4.2\u20134.3) report per-model coverage and adversarial inputs but contain no experiments or discussion on reusing inputs across models or on surrogate-model transferability \u2014 (pages 171\u2013173, Sections 4.2\u20134.3)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's methodology and experimental evaluation focus on generating fault-inducing inputs for a single, white-box model at a time. The proposed method, ADAPT, is designed to optimize its neuron-selection strategy based on the internal structure and feedback from the specific model under test. The experiments are conducted independently for each of the four models (LeNet-4, LeNet-5, VGG-19, ResNet-50). The results report the number of adversarial inputs found for each model individually. There is no procedure or result mentioned where fault-inducing inputs generated for one model (e.g., LeNet-4) are tested against another model performing the same task (e.g., LeNet-5) to assess transferability. The paper even notes that it adapted a baseline technique, DeepXplore, to work with a single DNN, moving away from its original multi-DNN differential testing approach.",
                    "evidence": "Page 7, Section 4.1, \"Experimental Setup\": \"DeepXplore originally performs differential testing requiring multiple DNNs without labeled data but we used it with a single DNN and labeled data in our experiments.\" This quote shows a deliberate choice to focus on single-model testing, not cross-model transferability. Additionally, Table 3 on page 9, \"Effectiveness for finding adversarial inputs,\" presents results for each model in separate, independent sections, with no indication that inputs generated for one model were ever tested on another."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper presents Adapt as a white-box testing technique that operates on a single target DNN (Algorithm 2 takes a single DNN as input) and evaluates it per-model. The experiments report per-model coverage and adversarial inputs but do not test or report whether fault-inducing inputs produced on one model transfer to other models. The authors explicitly note that DeepXplore's differential (multi-DNN) testing was not used in its multi-model mode: \u201cwe used it with a single DNN and labeled data in our experiments.\u201d There is no experiment, analysis, or mechanism described to generate or evaluate transferable (cross-model) fault-inducing inputs or surrogate-model attacks. Therefore the paper does not demonstrate attack transferability across models.",
                    "evidence": "Section 3.2 (Algorithm 2): \"1: procedure Adapt(DNN, I,Cov)\" (Algorithm 2 signature shows Adapt operates on a single DNN) \u2014 (page 168, Section 3.2, Algorithm 2).\n\nSection 4.1 Experimental Setup: \"DeepXplore originally performs differential testing requiring multiple DNNs without labeled data but we used it with a single DNN and labeled data in our experiments.\" (shows the authors evaluated tools in single-model mode and did not perform multi-model/differential transfer experiments) \u2014 (page 170, Section 4.1 Experimental Setup).\n\nOverall evaluation description and results (Sections 4.2\u20134.3) report per-model coverage and adversarial inputs but contain no experiments or discussion on reusing inputs across models or on surrogate-model transferability \u2014 (pages 171\u2013173, Sections 4.2\u20134.3)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Whitebox"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Ben Braiek and Khomh - 2019 - DeepEvolution A Search-Based Testing Approach for Deep Neural Networks": {
        "key": "braiek2019deepevolution",
        "name": "DeepEvolution",
        "year": "2019",
        "citation_count": "53",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "All failures the paper reports are robustness-style errors (misclassifications / 'erroneous behaviors') and divergences between original and quantized models (quantization defects). The authors evaluate DeepEvolution by generating semantically-preserving input mutations that increase neuron coverage and then report misclassified synthetic inputs and difference-inducing inputs for quantized models. The paper does not claim or show that DeepEvolution discovers unsafe/ toxic outputs, policy/safety bypasses, data leakage, jailbreaks, or other security-critical violations of defended or safety-aligned models; therefore the uncovered failures correspond to low-severity robustness errors under the provided metric.",
                    "evidence": "Page 456, Section V.C (RQ2) \u2014 Motivation: \"The objective is to assess the effectiveness of our approach in testing the robustness of the DNN; by finding misclassified synthetic inputs.\" \n\nPage 456, Section V.C (RQ2) \u2014 Findings: \"DeepEvolution can effectively generate test cases that trigger erroneous behaviors of the DNN.\" \n\nPage 457, Section V.D (RQ3) \u2014 Findings: \"DeepEvolution can effectively detect defects introduced during DNN quantization, outperforming the coverage-guided fuzzing tool TensorFuzz.\" \n\nPage 454, Abstract: \"we could successfully find several corner-case behaviors\" and \"DeepEvolution outperformed Tensorfuzz ... in detecting latent defects introduced during the quantization of the models.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, DeepEvolution, focuses on uncovering model robustness errors through metamorphic testing. The evaluation is centered on two types of failures: 1) misclassifications resulting from semantically-preserving input transformations (e.g., changing image brightness or rotation), and 2) prediction inconsistencies between a full-precision model and its quantized version. These failures perfectly align with the metric's definition of 'model robustness errors'. The paper does not mention or evaluate the detection of unsafe behaviors (such as toxicity or bias), nor does it test the bypassing of any explicit safety or security mechanisms. The entire scope of failure detection is confined to robustness and consistency checks.",
                    "evidence": "Page 457, Section V-C, RQ2: Detection of DNN Erroneous Behaviors\n- Motivation: 'The objective is to assess the effectiveness of our approach in testing the robustness of the DNN; by finding misclassified synthetic inputs.'\n- Findings: 'DeepEvolution can effectively generate test cases that trigger erroneous behaviors of the DNN. Table II presents erroneous behaviors detected by each metaheuristic algorithm.' The paper clarifies that these 'erroneous behaviors' are misclassifications.\n\nPage 457, Section V-D, RQ3: DNN Quantization Defects\n- Motivation: 'The goal is to assess the usefulness of Deep-Evolution in finding difference-inducing inputs that expose potential quantization defects.'\n- Findings: 'Table III presents the number of synthetic test data that were able to induce a difference between the DNN\u2019s outcomes (difference-inducing inputs); exposing quantization defects.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "All failures the paper reports are robustness-style errors (misclassifications / 'erroneous behaviors') and divergences between original and quantized models (quantization defects). The authors evaluate DeepEvolution by generating semantically-preserving input mutations that increase neuron coverage and then report misclassified synthetic inputs and difference-inducing inputs for quantized models. The paper does not claim or show that DeepEvolution discovers unsafe/ toxic outputs, policy/safety bypasses, data leakage, jailbreaks, or other security-critical violations of defended or safety-aligned models; therefore the uncovered failures correspond to low-severity robustness errors under the provided metric.",
                    "evidence": "Page 456, Section V.C (RQ2) \u2014 Motivation: \"The objective is to assess the effectiveness of our approach in testing the robustness of the DNN; by finding misclassified synthetic inputs.\" \n\nPage 456, Section V.C (RQ2) \u2014 Findings: \"DeepEvolution can effectively generate test cases that trigger erroneous behaviors of the DNN.\" \n\nPage 457, Section V.D (RQ3) \u2014 Findings: \"DeepEvolution can effectively detect defects introduced during DNN quantization, outperforming the coverage-guided fuzzing tool TensorFuzz.\" \n\nPage 454, Abstract: \"we could successfully find several corner-case behaviors\" and \"DeepEvolution outperformed Tensorfuzz ... in detecting latent defects introduced during the quantization of the models.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Uses neuron coveage as heuristic guide, does not show correlation between coverage and neurons"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": "Enforces naturalness across iterations by computing SSIM after each mutation and rejecting any visually distorted image."
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper enforces plausibility constraints both in the mutation design and in candidate selection: transformation parameters are manually tuned to valid domain intervals (defining a constrained sub-space of exploration), the compound transformation limits affine changes (only one affine transform) to avoid unrealistic images, and mutated inputs with low perceptual similarity are rejected using SSIM. The authors also report an empirical plausibility check: they sampled generated images (95% confidence, 5% error) and verified them manually. These points show plausibility is enforced throughout the search and that final generated inputs were assessed quantitatively (SSIM) and via manual inspection.",
                    "evidence": "Section IV.A (Metamorphic Transformation), p.455: \"To infer the valid domain interval of each transformation\u2019s parameters, we manually tune them to set up the appropriate range of values, i.e., high and low boundaries, that preserves the input semantics before and after each transformation... To verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8]... Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.\"; Section IV.C (Swarm-based Metaheuristics), p.456: \"To ensure semantically preserving transformations, we use the valid domain intervals of transformations that we have already tuned manually to create the high and low boundaries vectors, defining the sub-space of exploration.\"; Section VI (Threats to Validity), p.457: \"The manual tuning of metamorphic transformations\u2019 domain and the threshold of SSIM could affect the validity of our results. To mitigate this threat, we selected a sample of our generated images using a confidence level of 95% and an error margin of 5%, and verified them manually. We found them to be correct.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, DeepEvolution, explicitly incorporates mechanisms to enforce input plausibility throughout the test generation process and provides empirical evidence that the resulting inputs are realistic. Plausibility is enforced in two ways: 1) by manually tuning the parameter ranges for metamorphic transformations to ensure they are 'semantically-preserving', and 2) by using a quantitative perceptual quality metric, the Structural Similarity Index (SSIM), to filter and reject any generated inputs that fall below a tuned similarity threshold. This use of SSIM directly addresses the metric's criteria for using task-specific quantitative measures. Furthermore, the paper provides empirical evidence of the final inputs' plausibility by stating that the authors manually verified a statistically significant sample of the generated images and confirmed their correctness.",
                    "evidence": "Page 456, Section IV.A, 'To verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8] which assesses the similarity between two images based on the visual impact of three characteristics: luminance, contrast, and structure... Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.'\n\nPage 458, Section VI. Threats to Validity, 'The manual tuning of metamorphic transformations\u2019 domain and the threshold of SSIM could affect the validity of our results. To mitigate this threat, we selected a sample of our generated images using a confidence level of 95% and an error margin of 5%, and verified them manually. We found them to be correct.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper enforces plausibility constraints both in the mutation design and in candidate selection: transformation parameters are manually tuned to valid domain intervals (defining a constrained sub-space of exploration), the compound transformation limits affine changes (only one affine transform) to avoid unrealistic images, and mutated inputs with low perceptual similarity are rejected using SSIM. The authors also report an empirical plausibility check: they sampled generated images (95% confidence, 5% error) and verified them manually. These points show plausibility is enforced throughout the search and that final generated inputs were assessed quantitatively (SSIM) and via manual inspection.",
                    "evidence": "Section IV.A (Metamorphic Transformation), p.455: \"To infer the valid domain interval of each transformation\u2019s parameters, we manually tune them to set up the appropriate range of values, i.e., high and low boundaries, that preserves the input semantics before and after each transformation... To verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8]... Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.\"; Section IV.C (Swarm-based Metaheuristics), p.456: \"To ensure semantically preserving transformations, we use the valid domain intervals of transformations that we have already tuned manually to create the high and low boundaries vectors, defining the sub-space of exploration.\"; Section VI (Threats to Validity), p.457: \"The manual tuning of metamorphic transformations\u2019 domain and the threshold of SSIM could affect the validity of our results. To mitigate this threat, we selected a sample of our generated images using a confidence level of 95% and an error margin of 5%, and verified them manually. We found them to be correct.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Semantic-level pixel transformations (contrast, blur, rotation, etc.) but no clipping/rounding to ensure IO persistence"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper applies metamorphic transformations to in-memory floating-point image tensors and enforces semantic equivalence via manually tuned parameter domains and an SSIM threshold, but it does not account for standard I/O serialization effects (e.g., 8-bit quantization, rounding, clipping, or image encoding) when generating or validating test inputs. The only quantization discussed in the paper concerns model weight quantization (32-bit to 16-bit) used for a differential oracle, not input serialization. Because the method neither constrains mutations to survive common serialization nor evaluates reproducibility after saving/encoding, fault-inducing perturbations could be lost after standard I/O operations; this matches the metric definition for \"Low.\"",
                    "evidence": "Page 455, IV.A Metamorphic Transformation: \"To infer the valid domain interval of each transformation\u2019s parameters, we manually tune them to set up the appropriate range of values, i.e., high and low boundaries, that preserves the input semantics before and after each transformation, with respect to the data distribution.\" \n\nPage 455, IV.A Metamorphic Transformation: \"To verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8] ... Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.\" \n\nPage 457, V.D RQ3: \"TensorFuzz [5] performs a coverage-guided fuzzing process to generate mutated inputs that are able to expose disagreements between a DNN trained on MNIST (that is 32-bit floating point precision) and its quantized versions where all weights are truncated to 16-bit floating points.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's proposed method, DeepEvolution, relies on applying a set of semantic-preserving metamorphic transformations to generate new test inputs. These transformations include pixel-value changes (contrast, brightness, blur) and affine transformations (rotation, scaling). The metric definition explicitly states that a method that 'applies semantic-preserving metamorphic transformations without explicit enforcement of reproducibility under serialization' should be rated as 'Medium'. While the paper ensures that the transformed inputs are semantically similar to the originals using the Structural Similarity Index (SSIM), it does not describe any mechanism to explicitly account for I/O effects such as rounding or clipping that occur when saving an in-memory floating-point representation of an image to a standard integer-based file format. The transformations are defined over continuous parameter ranges, and there is no discussion of whether the precise numerical perturbations that might trigger a failure would persist after serialization.",
                    "evidence": "Page 455, Section IV-A, 'Metamorphic Transformation': 'First, we gather a list of parametric image-based transformations that can be organised in two groups: 1) Pixel-value transformations: change image contrast, image brightness, image blur, image sharpness and random perturbations within a limited interval. 2) Affine transformations: image translation, image scaling, image shearing, and image rotation.' and 'To verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8] which assesses the similarity between two images based on the visual impact of three characteristics: luminance, contrast, and structure.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "DeepEvolution relies exclusively on in-memory, semantic-preserving metamorphic transformations (pixel-value and affine changes) and filters by SSIM to ensure semantic equivalence, but never addresses or constrains the numerical perturbations with respect to standard I/O serialization (e.g. 8-bit image quantization, rounding, clipping or encoding). According to the metric, methods that apply metamorphic transformations without explicit handling of serialization effects should be rated \u2018Medium\u2019.",
                    "evidence": "Section IV.A (p. 455): \u2018\u2018First, we gather a list of parametric image-based transformations\u2026 pixel-value transformations\u2026 affine transformations\u2026 To verify that generated inputs remain semantically equivalent\u2026 we compute a Structural Similarity Index (SSIM)\u2026\u2019\u2019  Nowhere does the paper mention applying or evaluating inputs after standard image serialization (rounding, quantization, clipping or encoding)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly reuses fault-inducing inputs generated by DeepEvolution to detect divergences between an original model and its quantized version (i.e., it reuses generated inputs to test another model variant), but the fuzzing design is not presented as being explicitly aimed at discovering transferable attacks across independently implemented/trained models. The experiments focus on per-dataset single-model instantiations (LeNet for MNIST, CifarNet for CIFAR-10) and compare original vs. quantized versions; they do not evaluate reuse of generated faults across different model implementations trained for the same task nor claim mechanisms specifically for crafting transferable substitute-model attacks.",
                    "evidence": "Section III (page 455): \"From one generation of candidates to another, DeepEvolution performs follow-up tests with the resulting transformed inputs and stores the failed tests that exhibit erroneous DNNs behavior or induce a divergence between the original DNN and its quantized version.\" \n\nSection V.D (page 456-457) Findings: \"DeepEvolution can effectively detect defects introduced during DNN quantization, outperforming the coverage-guided fuzzing tool TensorFuzz.\" (Table III reports number of \"difference-inducing inputs\" found for quantization defects.)\n\nExperiment setup (page 456): \"For each dataset (i.e., MNIST and CIFAR-10), we took, respectively, the official open-source implementation of Tensorflow models, LeNet [3] and CifarNet [4] to allow the reproducibility of our results and comparisons with our approach.\" \n\nRelated work contrast (page 457, Section VII): Paper cites DeepXplore's use of \"multiple DNNs\u2019 implementations\" for differential testing but does not describe performing similar cross-model reuse in DeepEvolution."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's methodology and evaluation focus on generating test cases to find defects within a single Deep Neural Network (DNN) model or between a model and its own quantized version. The experiments are conducted on two distinct models, LeNet and CifarNet, but they are used for different datasets (MNIST and CIFAR-10, respectively) and are treated as separate, independent evaluation subjects. The paper does not describe any experiment or methodology where fault-inducing inputs generated for one model are subsequently used to test a different model performing the same task. Therefore, the concept of attack transferability is not assessed.",
                    "evidence": "Page 456, Section V, EMPIRICAL EVALUATION: The research questions guiding the evaluation are: \"RQ1: How much can DeepEvolution increase the coverage of generated test cases? RQ2: Can DeepEvolution detect diverse erroneous behaviors in DNN models? RQ3: Can DeepEvolution detect divergences induced by DNN quantization?\". All three questions are focused on the behavior of a single model under test. \n\nPage 456, Section V.A, Experiment Setup: \"DNNs. For each dataset (i.e., MNIST and CIFAR-10), we took, respectively, the official open-source implementation of Tensorflow models, LeNet [3] and CifarNet [4] to allow the reproducibility of our results and comparisons with our approach.\" This shows that the experiments test one specific model per dataset, rather than testing multiple models on a single dataset to check for transferability.\n\nPage 457, Section V.D, RQ3: DNN Quantization Defects: The evaluation for quantization defects involves comparing a model to its own modified version, not to a different model. \"TensorFuzz [5] performs a coverage-guided fuzzing process to generate mutated inputs that are able to expose disagreements between a DNN trained on MNIST (that is 32-bit floating point precision) and its quantized versions...\" This further illustrates that the testing scope is limited to a single model architecture at a time."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DeepEvolution does not demonstrate attack transferability across independently trained model implementations on the same task. The paper only reuses generated fault\u2010inducing inputs to compare a model with its own quantized variant, not to test different models (e.g., using LeNet inputs on another MNIST model or CifarNet inputs on another CIFAR-10 model). There is no experimental evaluation or design mechanism aimed at assessing whether faults transfer across distinct model architectures trained for the same task.",
                    "evidence": "Section III (page 455) describes follow-up tests only between the original DNN and its quantized version. Section V.D (pages 456\u2013457) evaluates quantization defects by comparing each model to its own truncated-precision variant; no experiments reuse inputs across different model implementations performing the same task."
                },
                "manual": {
                    "value": "Low",
                    "why": "Use quantized model for differntial testing based oracle, not for checking fauilure transferability",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic",
                "Differential"
            ]
        }
    },
    "Ma et al. - 2021 - HDTest Differential Fuzz Testing of Brain-Inspired Hyperdimensional Computing": {
        "key": "ma2021hdtest",
        "name": "HDTest",
        "year": "2021",
        "citation_count": "39",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "All failures the paper reports are classic model robustness errors (adversarially-induced mispredictions on MNIST digits). The method generates adversarial inputs that cause incorrect class predictions and evaluates distance/iteration metrics; it does not demonstrate uncovering unsafe, biased, toxic, data\u2011leaking, or security\u2011critical behaviors, nor does it bypass explicit safety, policy, or defense mechanisms in safety\u2011aligned or defended models. The paper frames results as misclassification/adversarial examples and uses retraining to improve robustness, which is consistent with discovering low\u2011severity robustness faults rather than high/medium severity safety or security violations.",
                    "evidence": "Abstract (p.391, Abstract): \"Experimental results on MNIST dataset show that HDTest can generate thousands of adversarial inputs with invisible perturbations that can successfully fool HDC models. On average, HDTest can generate 400 adversarial images within one minute running on commodity computer.\"; Section IV - HDTest Framework (Alg. 1) (p.393, Algorithm 1): comment \"* Indicating successful mis-predcited image generated. */\" (lines 8-11) showing the detection criterion is label disagreement (misprediction); Section V.A - Experimental Setup (p.394, Experimental Setup): \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"; Section V.D - Case Study on Defense Against Adversarial Attacks (p.395, Case Study): \"We run HDTest to generate 1000 adversarial images... after retraining, the rate of successful attack rate drops more than 20%.\" (shows focus on robustness improvement rather than bypassing safety or extracting sensitive information)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, HDTest, is designed and evaluated exclusively for its ability to uncover model robustness errors. Specifically, it focuses on generating adversarial inputs through minor perturbations that cause the Hyperdimensional Computing (HDC) model to produce incorrect classifications (mispredictions). The goal, as stated and demonstrated, is to \"fool HDC models\" and expose these misclassifications. The experiments are conducted on the MNIST handwritten digit dataset, where a failure is defined as predicting the wrong digit. The paper does not explore or uncover any unsafe behaviors (e.g., toxicity, bias) or attempt to bypass any explicit safety, security, or policy mechanisms. The entire scope is confined to prediction inconsistencies under input mutations.",
                    "evidence": "Page 1, Abstract: \"Using handwritten digit classification as an example, we show that HDTest can generate thousands of adversarial inputs with negligible perturbations that can successfully fool HDC models.\"\n\nPage 1, Introduction, Figure 1 Caption: \"An example of adversarial image of HDC by mutating some pixels in the image:(a) the original image as '8'; (b) the pixels mutated; (c) the mutated image wrongly predicted as '3'.\"\n\nPage 4, Section IV, Figure 3: The diagram illustrating the HDTest framework contains a box that explicitly states the \"Objective: Generate mis-predicted images\"."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "All failures the paper reports are classic model robustness errors (adversarially-induced mispredictions on MNIST digits). The method generates adversarial inputs that cause incorrect class predictions and evaluates distance/iteration metrics; it does not demonstrate uncovering unsafe, biased, toxic, data\u2011leaking, or security\u2011critical behaviors, nor does it bypass explicit safety, policy, or defense mechanisms in safety\u2011aligned or defended models. The paper frames results as misclassification/adversarial examples and uses retraining to improve robustness, which is consistent with discovering low\u2011severity robustness faults rather than high/medium severity safety or security violations.",
                    "evidence": "Abstract (p.391, Abstract): \"Experimental results on MNIST dataset show that HDTest can generate thousands of adversarial inputs with invisible perturbations that can successfully fool HDC models. On average, HDTest can generate 400 adversarial images within one minute running on commodity computer.\"; Section IV - HDTest Framework (Alg. 1) (p.393, Algorithm 1): comment \"* Indicating successful mis-predcited image generated. */\" (lines 8-11) showing the detection criterion is label disagreement (misprediction); Section V.A - Experimental Setup (p.394, Experimental Setup): \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"; Section V.D - Case Study on Defense Against Adversarial Attacks (p.395, Case Study): \"We run HDTest to generate 1000 adversarial images... after retraining, the rate of successful attack rate drops more than 20%.\" (shows focus on robustness improvement rather than bypassing safety or extracting sensitive information)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Finds generic misclassifications"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": " It uses hypervector distance purely as a heuristic to guide input mutations - no insight where/how/why the faults occur"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "norm-bounded"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints during fuzzing (they set an explicit distance threshold to keep perturbations \u201cinvisible\u201d and discard candidates beyond it) and reports quantitative distance metrics (normalized L1/L2) and sample images for the final adversarial inputs. However, the paper does not perform stronger perceptual/semantic evaluations (no human study, no task-specific perceptual metrics such as LPIPS/SSIM) nor does it explicitly analyze cumulative drift across multiple mutation iterations beyond the single distance threshold. This matches the \"Medium\" description: plausibility is enforced/validated (bounded constraints and final-distance reporting) but cumulative-effects and perceptual validation are not thoroughly addressed.",
                    "evidence": "Section IV. HDTest Framework (page ~393-394): \"To ensure the added perturbations are within an 'invisible' range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.\" \n\nSection V.A Experimental Setup (page ~394): \"We use different metrics to evaluate HDTest: normalized L1 distance and normalized L2 distance between the mutated image and original image, the average fuzzing iterations and the execution time to successfully generate 1000 adversarial images.\" \n\nSection V.B Mutation Strategies Analysis (page ~394-395): \"We provide sample original images, mutated pixels, and generated images of HDTest in Fig. 4\u20136, under gauss, rand, and shift respectively.\" \n\n(Absence of stronger perceptual/semantic evaluation): Nowhere in the paper do the authors report human evaluation or task-specific perceptual metrics (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity) for the final fault-inducing inputs; their evaluation is limited to L1/L2 distances, iteration counts, runtime, and example figures."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces a form of input plausibility by using a bounded constraint on the perturbations. It explicitly sets an L2 distance threshold and discards any generated images that exceed this limit, with the stated goal of keeping perturbations within an \"invisible\" range. This method constitutes a rule-based constraint on mutations. Furthermore, the paper validates the plausibility of the final generated adversarial inputs by measuring and reporting the normalized L1 and L2 distances from the original inputs, stating that smaller distances are preferred. However, it does not meet the criteria for a \"High\" rating because it does not provide more rigorous empirical evidence of perceptual realism, such as through human studies or by using dedicated perceptual similarity metrics like SSIM or LPIPS. The use of L1/L2 distance serves as a basic validation, fitting the \"Medium\" definition.",
                    "evidence": "Page 4, Section IV, Paragraph 4: 'To ensure the added perturbations are within an \"invisible\" range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.'\n\nPage 4, Section V.A, Paragraph 1: 'We use different metrics to evaluate HDTest: normalized L1 distance and normalized L2 distance between the mutated image and original image, the average fuzzing iterations and the execution time to successfully generate 1000 adversarial images. A smaller distance between the generated image and the original image is preferred by HDTest because it means less perturbation added to the image.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints during fuzzing (they set an explicit distance threshold to keep perturbations \u201cinvisible\u201d and discard candidates beyond it) and reports quantitative distance metrics (normalized L1/L2) and sample images for the final adversarial inputs. However, the paper does not perform stronger perceptual/semantic evaluations (no human study, no task-specific perceptual metrics such as LPIPS/SSIM) nor does it explicitly analyze cumulative drift across multiple mutation iterations beyond the single distance threshold. This matches the \"Medium\" description: plausibility is enforced/validated (bounded constraints and final-distance reporting) but cumulative-effects and perceptual validation are not thoroughly addressed.",
                    "evidence": "Section IV. HDTest Framework (page ~393-394): \"To ensure the added perturbations are within an 'invisible' range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.\" \n\nSection V.A Experimental Setup (page ~394): \"We use different metrics to evaluate HDTest: normalized L1 distance and normalized L2 distance between the mutated image and original image, the average fuzzing iterations and the execution time to successfully generate 1000 adversarial images.\" \n\nSection V.B Mutation Strategies Analysis (page ~394-395): \"We provide sample original images, mutated pixels, and generated images of HDTest in Fig. 4\u20136, under gauss, rand, and shift respectively.\" \n\n(Absence of stronger perceptual/semantic evaluation): Nowhere in the paper do the authors report human evaluation or task-specific perceptual metrics (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity) for the final fault-inducing inputs; their evaluation is limited to L1/L2 distances, iteration counts, runtime, and example figures."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": "In memory floating point mutation, no safeguards for IO preservation"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's fuzzing/mutation design applies fine-grained numerical perturbations (e.g., Gaussian and random noise) and evaluates adversariality entirely by sending in-memory mutated inputs to the HDC classifier, but it does not account for standard I/O serialization effects (quantization, rounding, clipping) nor constrain mutations to guarantee they survive serialization. The authors explicitly describe adding gaussian/random noise and using an L2-distance threshold to keep perturbations 'invisible', and they evaluate success by comparing predictions on the original and mutated inputs in memory. Nowhere do they describe converting/saving mutated inputs to common serialized formats (e.g., 8-bit images) or handling clipping/rounding so that failures remain reproducible after standard I/O operations. Because I/O/serialization effects are ignored, fault-inducing perturbations could be lost or altered when inputs are serialized, so the paper meets the 'Low' level of the metric.",
                    "evidence": "Section III.A Encoding (page 392): \"For the MNIST dataset we use in this paper, since the image size is 28\u00d728 and the pixel range is 0 to 255 in greyscale, we flat a single image into an array with 784 elements with values ranging from 0 to 255.\" \n\nSection IV HDTest Framework / Mutation algorithm (page 393): Algorithm 1 and surrounding text describe in-memory comparison: \"Both the generated input and the original input are then sent to the HDC classifier for prediction. We then check if the two predicted labels are different, and if yes, this indicates a successful generation of an adversarial input.\" (Alg.1 lines and paragraph) \n\nSection IV Mutation strategies / Table I (page 393): lists fine-grained numerical mutations such as \"gauss apply gaussian noise over the entire image\" and \"rand apply random noise over the entire image.\" \n\nSection IV (page 394): \"To ensure the added perturbations are within an 'invisible' range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.\" \n\n(There is no discussion in the paper of serialization, saving mutated inputs to standard image/audio formats, quantization, rounding, or explicit handling of clipping/rounding effects in either test-case generation or oracle evaluation.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's fuzzing method, HDTest, applies numerical mutation strategies like adding Gaussian and random noise to input images. The methodology does not describe any mechanism to account for standard input/output (I/O) serialization effects such as quantization (rounding floating-point pixel values to integers) or clipping (ensuring pixel values remain within the valid [0, 255] range for 8-bit images). The fuzzing loop feeds the mutated inputs, which likely exist as in-memory floating-point arrays after mutation, directly to the model for evaluation. This allows for the possibility that discovered failures are caused by small numerical perturbations that would be lost or altered if the adversarial image were saved to a standard file format (e.g., PNG) and then reloaded. Consequently, the reproducibility of the generated failures is not guaranteed.",
                    "evidence": "Page 4, Table I lists mutation strategies including `gauss` and `rand` which are described as 'apply gaussian noise over the entire image' and 'apply random noise over the entire image'. The paper's algorithm and experimental setup sections do not mention any steps for handling quantization or clipping. Algorithm 1 on page 3 shows that mutated `seeds` are generated (Line 6) and then directly fed to the model `y_seeds = HDC(seeds)` (Line 7) without any intermediate serialization step to ensure the perturbations would persist."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's fuzzing/mutation design applies fine-grained numerical perturbations (e.g., Gaussian and random noise) and evaluates adversariality entirely by sending in-memory mutated inputs to the HDC classifier, but it does not account for standard I/O serialization effects (quantization, rounding, clipping) nor constrain mutations to guarantee they survive serialization. The authors explicitly describe adding gaussian/random noise and using an L2-distance threshold to keep perturbations 'invisible', and they evaluate success by comparing predictions on the original and mutated inputs in memory. Nowhere do they describe converting/saving mutated inputs to common serialized formats (e.g., 8-bit images) or handling clipping/rounding so that failures remain reproducible after standard I/O operations. Because I/O/serialization effects are ignored, fault-inducing perturbations could be lost or altered when inputs are serialized, so the paper meets the 'Low' level of the metric.",
                    "evidence": "Section III.A Encoding (page 392): \"For the MNIST dataset we use in this paper, since the image size is 28\u00d728 and the pixel range is 0 to 255 in greyscale, we flat a single image into an array with 784 elements with values ranging from 0 to 255.\" \n\nSection IV HDTest Framework / Mutation algorithm (page 393): Algorithm 1 and surrounding text describe in-memory comparison: \"Both the generated input and the original input are then sent to the HDC classifier for prediction. We then check if the two predicted labels are different, and if yes, this indicates a successful generation of an adversarial input.\" (Alg.1 lines and paragraph) \n\nSection IV Mutation strategies / Table I (page 393): lists fine-grained numerical mutations such as \"gauss apply gaussian noise over the entire image\" and \"rand apply random noise over the entire image.\" \n\nSection IV (page 394): \"To ensure the added perturbations are within an 'invisible' range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.\" \n\n(There is no discussion in the paper of serialization, saving mutated inputs to standard image/audio formats, quantization, rounding, or explicit handling of clipping/rounding effects in either test-case generation or oracle evaluation.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether adversarial / fault-inducing inputs generated on one HDC model transfer to other models performing the same task. All experiments generate and use adversarial inputs against a single HDC classifier on MNIST (including using generated inputs to retrain that same classifier). The paper does not report reusing seeds/adversarial inputs on other independently-trained HDC models, nor does it include explicit cross-model transferability experiments or mechanisms designed to test transfer across different model implementations.",
                    "evidence": "V.A Experimental Setup (page 394): \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"  V.D Case Study on Defense Against Adversarial Attacks (page 395): \"First, for a specific HDC model, we run HDTest to generate 1000 adversarial images. ... We feed the first subset of adversarial images to the HDC model with correct labels to retrain the HDC model ... Next, we use the second subset of adversarial images to attack the HDC model. Our experimental results show that, after retraining, the rate of successful attack rate drops more than 20%.\"  V.E Discussion (page 395): \"While we only consider one dataset in this paper due to the fact that HDC is still exploring its use in image classification domain, HDTest can be naturally extended to other HDC model structures because it considers a general greybox assumption with only HV distance information.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper introduces HDTest, a differential fuzz testing method specifically designed for Hyperdimensional Computing (HDC) models. The evaluation of HDTest is conducted on a single HDC model architecture trained for the MNIST classification task. The experiments focus on generating adversarial inputs that cause mispredictions in this specific model and then using these generated inputs to retrain and improve the robustness of that same model. The paper does not perform any experiments to test whether the fault-inducing inputs generated for this one model also cause failures in other, independently trained HDC models or in models based on different architectures (e.g., DNNs) performing the same task. Therefore, the transferability of the generated attacks is not demonstrated.",
                    "evidence": "Page 4, Section V-A, Experimental Setup: \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"\n\nPage 6, Section V-D, Case Study on Defense Against Adversarial Attacks: \"First, for a specific HDC model, we run HDTest to generate 1000 adversarial images... We feed the first subset of adversarial images to the HDC model with correct labels to retrain the HDC model, i.e., updating the reference HVs. Next, we use the second subset of adversarial images to attack the HDC model.\" This section describes a process of retraining and then re-attacking the same model, not testing a different one."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether adversarial / fault-inducing inputs generated on one HDC model transfer to other models performing the same task. All experiments generate and use adversarial inputs against a single HDC classifier on MNIST (including using generated inputs to retrain that same classifier). The paper does not report reusing seeds/adversarial inputs on other independently-trained HDC models, nor does it include explicit cross-model transferability experiments or mechanisms designed to test transfer across different model implementations.",
                    "evidence": "V.A Experimental Setup (page 394): \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"  V.D Case Study on Defense Against Adversarial Attacks (page 395): \"First, for a specific HDC model, we run HDTest to generate 1000 adversarial images. ... We feed the first subset of adversarial images to the HDC model with correct labels to retrain the HDC model ... Next, we use the second subset of adversarial images to attack the HDC model. Our experimental results show that, after retraining, the rate of successful attack rate drops more than 20%.\"  V.E Discussion (page 395): \"While we only consider one dataset in this paper due to the fact that HDC is still exploring its use in image classification domain, HDTest can be naturally extended to other HDC model structures because it considers a general greybox assumption with only HV distance information.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Du et al. - 2019 - DeepStellar model-based quantitative analysis of stateful deep learning systems": {
        "key": "du2019deepstellar",
        "name": "DeepStellar",
        "year": "2019",
        "citation_count": "188",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's DeepStellar framework is explicitly used to find and detect adversarial inputs that cause RNNs to produce incorrect or targeted outputs (e.g., targeted adversarial audio transcriptions and misclassifications). Those failures violate user intent and can produce undesired or unsafe outputs, which matches the \"Medium\" severity class. The paper does not claim to bypass explicit safety/policy or security mechanisms nor evaluate defended or safety-aligned models (no results on circumventing defenses, jailbreaks, data-exfiltration, or models with explicit safety mechanisms), so it does not meet the \"High\" category.",
                    "evidence": "Page 477, Abstract: \"we design two algorithms powered by the quantitative measures for adversarial sample detection and coverage-guided test generation.\" \n\nPage 481, Section 5.1 (Adversarial Sample Detection): \"Adversarial sample detection aims to check whether a given input is an adversarial sample at runtime. We propose to use the trace similarity metrics to measure the behavioral differences between two inputs. Based on this idea, we develop a new approach to detect adversarial samples for RNNs.\" \n\nPage 483, Section 6.1 (Data Preparation): \"we generate 1,100 (100 seeds\u00d7 11 targets) adversarial audios, which took about 12 days in total ...\" (shows they generate targeted adversarial ASR examples). \n\nPage 484, Section 6.4 (Results): \"The results confirm that the trace similarity-based method is effective for adversarial sample detection under carefully selected abstraction configurations, with more than 89% prediction accuracy.\" \n\nPage 482, Section 6.1 (Models and Dataset / Table 1): lists standard pre-trained models used (e.g., \"we selected two versions of Mozilla pre-trained DeepSpeech\"), with no mention of testing or bypassing defended / safety-aligned models."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, DeepStellar, focuses on generating and detecting 'adversarial samples' in Recurrent Neural Network (RNN) based systems. These failures are defined as inputs that are slightly perturbed from benign samples to cause incorrect predictions. The evaluation is conducted on image classification (MNIST) and automated speech recognition (DeepSpeech) models. The uncovered failures are misclassifications or incorrect transcriptions, which fall squarely under the definition of 'model robustness errors, such as mispredictions or prediction inconsistencies under semantically preserving input mutations'. The paper does not describe generating or uncovering any unsafe, biased, or toxic behaviors, nor does it test against models with explicit safety or security mechanisms that could be bypassed.",
                    "evidence": "Page 7, Section 5.2 Coverage-Guided Testing of RNNs: 'In this section, we propose a Coverage-Guided Testing (CGT) technique which aims to generate adversarial samples incorrectly recognized by an RNN.' and later in the description of Algorithm 2: 'A mutant is an adversarial sample if it is predicted incorrectly by the network.' This explicitly defines a failure as a misprediction, which is a robustness error."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DeepStellar\u2019s fuzzing uncovers adversarial inputs that cause RNNs to misclassify or mistranscribe (robustness errors) but does not target or bypass any explicit safety, policy, or security mechanisms, nor does it induce hallucinated, biased, toxic, or otherwise unsafe outputs. The failures are classic model\u2010robustness mispredictions under small input perturbations, fitting the \u201cLow\u201d severity category.",
                    "evidence": "Page 479\u2013480, \u00a73.3 and \u00a75.2: \u201ca mutant is an adversarial sample if it is predicted incorrectly by the network.\u201d\nPage 483, \u00a76.1 (Data Preparation): \u201cwe generate \u2026 adversarial audios \u2026\u201d and then measure word\u2010error\u2010rate, showing the focus is on transcription inaccuracies.\nNo sections of the paper describe testing against defended or safety\u2010aligned models or bypassing explicit security/safety policies."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Broad exploration to uncover functional errors"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Shows correlation between coverage criteria and faults"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Design contraints on perturbations but no naturalness measure of final fault-triggering inputs"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility at the level of individual mutation steps: it uses domain-specific, metamorphic transformations with conservative parameters (e.g., small speed/volume/noise changes for audio; small brightness/contrast/translation/rotation for images) and states that these perturbations are imperceptible. However, DeepStellar does not describe mechanisms that prevent or measure cumulative drift across multiple fuzzing/mutation iterations, nor does it provide empirical human evaluation or task-specific perceptual metrics (e.g., SSIM/LPIPS for images, PESQ/STOI for audio, or Perplexity for language) to validate that final fault-inducing inputs remain perceptually/semantically realistic. The coverage-guided testing algorithm (Algorithm 2) accepts mutants based on coverage increase alone, with no stated plausibility filter applied to the retained seeds; the paper therefore enforces plausibility in individual mutation choices but overlooks cumulative-effects and lacks quantitative or user-study evidence for final-input plausibility, matching the rubric for \u201cMedium.\u201d",
                    "evidence": "Section 5.2 Coverage-Guided Testing (p. 481): \"Mutation operators are often specific to the application domains. For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human\u2019s perspective during testing.\" (Algorithm 2 also shows mutants are retained when they increase coverage: \"Randomly pick transformation t with a random parameter p; A = mutate(t, p, a); ... else if CoverageIncrease(cov, Q) then Q <- Q \u222a a' .\")\n\nSection 6.1 Data Preparation (p. 482\u2013483): \"For each ASR model, we generate 10,000 perturbed audios from original benign ones with existing audio data augmentation techniques [5] (i.e., speed and volume adjustment, low/high-frequency filtering, noise blending).... For each MNIST model, we also generate 10,000 benign perturbed samples with existing image transformation techniques [50] (i.e., image contrast, brightness, translation, scaling, shearing, rotation and add white noise). ... For both audio and image case, we set conservative parameters for transformation so that the perturbation on original samples is slight and imperceptible. Note that all the adversarial samples are also with minimal perturbations and not perceptible by human.\"\n\nEvaluation sections (e.g., RQ3 & RQ4, pp. 484\u2013486) report detection AUROC, coverage numbers and counts of adversarial samples, but the paper does not report any human studies or task-specific perceptual/semantic quality metrics (e.g., LPIPS/SSIM for images, PESQ/STOI for audio, or Perplexity for language) to empirically validate final fault-inducing inputs' realism."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's coverage-guided testing (CGT) method is designed with input plausibility in mind. It explicitly states the use of a \"metamorphic mutation strategy... that would keep the prediction meaning from the human's perspective during testing.\" The transformations used for generating perturbed inputs are standard, bounded operations (e.g., image contrast, brightness, rotation), which enforce plausibility within individual mutation steps. The paper also claims that the perturbations are \"slight and imperceptible\" and that adversarial samples are \"not perceptible by human.\" \n\nHowever, the evaluation does not meet the criteria for 'High'. Firstly, the paper does not discuss or provide a mechanism to control for the cumulative effect of mutations over many iterations, which could cause inputs to drift from the plausible distribution. Secondly, the claim of imperceptibility is not substantiated with empirical evidence, such as a human study or quantitative, task-specific metrics (e.g., LPIPS, SSIM, PESQ). This approach aligns with the 'Medium' definition, as it enforces plausibility in individual mutation steps via bounded constraints but does not address cumulative effects or provide rigorous empirical validation of the final outputs' realism.",
                    "evidence": "Page 7, Section 5.2: \"For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human's perspective during testing.\"\n\nPage 8, Section 6.1, Data Preparation: \"For each MNIST model, we also generate 10,000 benign perturbed samples with existing image transformation techniques [50] (i.e., image contrast, brightness, translation, scaling, shearing, rotation and add white noise).\"\n\nPage 8, Section 6.1, Data Preparation: \"For both audio and image case, we set conservative parameters for transformation so that the perturbation on original samples is slight and imperceptible. Note that all the adversarial samples are also with minimal perturbations and not perceptible by human.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility at the level of individual mutation steps: it uses domain-specific, metamorphic transformations with conservative parameters (e.g., small speed/volume/noise changes for audio; small brightness/contrast/translation/rotation for images) and states that these perturbations are imperceptible. However, DeepStellar does not describe mechanisms that prevent or measure cumulative drift across multiple fuzzing/mutation iterations, nor does it provide empirical human evaluation or task-specific perceptual metrics (e.g., SSIM/LPIPS for images, PESQ/STOI for audio, or Perplexity for language) to validate that final fault-inducing inputs remain perceptually/semantically realistic. The coverage-guided testing algorithm (Algorithm 2) accepts mutants based on coverage increase alone, with no stated plausibility filter applied to the retained seeds; the paper therefore enforces plausibility in individual mutation choices but overlooks cumulative-effects and lacks quantitative or user-study evidence for final-input plausibility, matching the rubric for \u201cMedium.\u201d",
                    "evidence": "Section 5.2 Coverage-Guided Testing (p. 481): \"Mutation operators are often specific to the application domains. For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human\u2019s perspective during testing.\" (Algorithm 2 also shows mutants are retained when they increase coverage: \"Randomly pick transformation t with a random parameter p; A = mutate(t, p, a); ... else if CoverageIncrease(cov, Q) then Q <- Q \u222a a' .\")\n\nSection 6.1 Data Preparation (p. 482\u2013483): \"For each ASR model, we generate 10,000 perturbed audios from original benign ones with existing audio data augmentation techniques [5] (i.e., speed and volume adjustment, low/high-frequency filtering, noise blending).... For each MNIST model, we also generate 10,000 benign perturbed samples with existing image transformation techniques [50] (i.e., image contrast, brightness, translation, scaling, shearing, rotation and add white noise). ... For both audio and image case, we set conservative parameters for transformation so that the perturbation on original samples is slight and imperceptible. Note that all the adversarial samples are also with minimal perturbations and not perceptible by human.\"\n\nEvaluation sections (e.g., RQ3 & RQ4, pp. 484\u2013486) report detection AUROC, coverage numbers and counts of adversarial samples, but the paper does not report any human studies or task-specific perceptual/semantic quality metrics (e.g., LPIPS/SSIM for images, PESQ/STOI for audio, or Perplexity for language) to empirically validate final fault-inducing inputs' realism."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Applies deephunter metamorphic mutations that are plausibly I/O-stable but does not apply clipping and rounding to ensure"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper uses semantic-preserving, metamorphic transformations and synthesized inputs (e.g., audio augmentation and TTS-generated reference audios) as its primary mutation and test-generation mechanisms, which provide partial robustness to I/O effects. However, the paper does not explicitly account for serialization effects (quantization, clipping, rounding, or encoding) during mutation or oracle evaluation, nor does it state constraints to ensure that fault-inducing numerical perturbations survive typical serialization. Therefore, it neither enforces end-to-end reproducibility under standard I/O nor completely ignores metamorphic/serialized-input approaches; this matches the metric's \u201cMedium\u201d level (metamorphic transformations without explicit enforcement of reproducibility under serialization).",
                    "evidence": "Section 5.2 (Coverage-Guided Testing of RNNs), p. 482: \"For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human\u2019s perspective during testing.\"; Section 5.1 (Adversarial Sample Detection for RNNs), p. 481: \"For ASR, given an audio input a that is transcribed to texts t by the RNN, we generate the reference audios using off-the-shelf text-to-speech engines (e.g., Google cloud text-to-speech [4]) that generate the audio t with correct and clear pronouncing.\"; Section 6.1 (Data Preparation), p. 483: \"For each ASR model, we generate 10,000 perturbed audios from original benign ones with existing audio data augmentation techniques [5] (i.e., speed and volume adjustment, low/high-frequency filtering, noise blending).\"  (No text in the paper discusses explicit handling of quantization, clipping, rounding, or other serialization effects when producing or evaluating mutated inputs.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's coverage-guided testing (CGT) approach relies on metamorphic transformations to generate new test inputs for both image and audio domains. The paper explicitly lists the transformations used: 'image contrast, brightness, translation, scaling, shearing, rotation and add white noise' for images, and 'speed and volume adjustment, low/high-frequency filtering, noise blending' for audio. The metric definition states that applying 'semantic-preserving metamorphic transformations without explicit enforcement of reproducibility under serialization' corresponds to a 'Medium' value. The paper's methodology for test generation (Algorithm 2) and its evaluation do not describe any process of saving and reloading the mutated inputs to account for I/O effects like quantization, clipping, or rounding. The mutations are applied and evaluated on in-memory data representations, without explicit confirmation that the failures would persist after serialization.",
                    "evidence": "Page 7, Section 5.2: \"For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human's perspective during testing.\"\n\nPage 8, Section 6.1, 'Data Preparation': \"For each MNIST model, we also generate 10,000 benign perturbed samples with existing image transformation techniques [50] (i.e., image contrast, brightness, translation, scaling, shearing, rotation and add white noise).\"\n\nPage 8, Section 6.1, 'Data Preparation': \"For each ASR model, we generate 10,000 perturbed audios from original benign ones with existing audio data augmentation techniques [5] (i.e., speed and volume adjustment, low/high-frequency filtering, noise blending).\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper uses semantic-preserving, metamorphic transformations and synthesized inputs (e.g., audio augmentation and TTS-generated reference audios) as its primary mutation and test-generation mechanisms, which provide partial robustness to I/O effects. However, the paper does not explicitly account for serialization effects (quantization, clipping, rounding, or encoding) during mutation or oracle evaluation, nor does it state constraints to ensure that fault-inducing numerical perturbations survive typical serialization. Therefore, it neither enforces end-to-end reproducibility under standard I/O nor completely ignores metamorphic/serialized-input approaches; this matches the metric's \u201cMedium\u201d level (metamorphic transformations without explicit enforcement of reproducibility under serialization).",
                    "evidence": "Section 5.2 (Coverage-Guided Testing of RNNs), p. 482: \"For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human\u2019s perspective during testing.\"; Section 5.1 (Adversarial Sample Detection for RNNs), p. 481: \"For ASR, given an audio input a that is transcribed to texts t by the RNN, we generate the reference audios using off-the-shelf text-to-speech engines (e.g., Google cloud text-to-speech [4]) that generate the audio t with correct and clear pronouncing.\"; Section 6.1 (Data Preparation), p. 483: \"For each ASR model, we generate 10,000 perturbed audios from original benign ones with existing audio data augmentation techniques [5] (i.e., speed and volume adjustment, low/high-frequency filtering, noise blending).\"  (No text in the paper discusses explicit handling of quantization, clipping, rounding, or other serialization effects when producing or evaluating mutated inputs.)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate whether fault-inducing inputs transfer across different models. All algorithms and experiments are formulated and executed per-target-model (the abstract model M and the RNN D are paired), adversarial examples were generated for particular models only, and the authors explicitly note they could only generate adversarial samples for one DeepSpeech version due to tool compatibility. There are no experiments reusing inputs crafted on one model to test other models performing the same task, nor design elements aimed at discovering transferable attacks.",
                    "evidence": "Section 5.1 (page 481): \"input :D: RNN-based DL system, M: Abstract model of D\" and \"We first collect a set of benign samples B and a set of adversarial samples A.\" (Algorithm 1) \u2014 shows detection is trained per given RNN and its abstract model. Section 5.2 (page 482): \"input :I: Initial seeds, D: RNN-based DL system, M: Abstract model of D\" (Algorithm 2) \u2014 testing is guided against a single RNN instance D with its model M. Section 6.1 Data Preparation (page 483): \"Finally, we only successfully generate adversarial samples for DeepSpeech-0.1.1 because there exists a compatibility issue between DeepSpeech-0.3.0 and the adversarial attack tools [8] we used.\" \u2014 indicates adversarial samples were not produced for multiple models and no cross-model reuse/transfer experiments were performed. Section 6.4 (page 485): \"...we trained a classifier for each model to detect adversarial samples.\" \u2014 confirms evaluation is per-model, not testing transferability across models."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes methods for generating and detecting adversarial samples for Recurrent Neural Networks (RNNs). The core methodology involves creating a model-specific abstraction (a Discrete-Time Markov Chain or DTMC) based on the internal state transitions of a single, trained RNN. The evaluation is conducted on multiple models performing the same tasks (e.g., MNIST-LSTM and MNIST-GRU for image classification). However, the adversarial samples are always generated for a specific model and then evaluated on that same model. The paper does not perform any experiments where fault-inducing inputs generated for one model are reused to test for failures on another model. The methodology is inherently designed to analyze a single model at a time and lacks any mechanism to support the discovery of transferable failures.",
                    "evidence": "Page 7, Algorithm 2, \"Coverage guided testing of RNN\": The algorithm for generating adversarial samples takes a single \"RNN-based DL system, M: Abstract model of D\" as input. All subsequent steps within the testing loop, including prediction, coverage analysis, and failure checking, are performed with respect to this single model `D` and its abstract model `M`. This demonstrates the single-model focus of the generation technique.\n\nPage 10, Section 6.4, \"RQ3. Adversarial Sample Detection\": In the setup for evaluating the detection mechanism, the paper states: \"With the constructed dataset, we trained a classifier for each model to detect adversarial samples.\" This confirms that the detection process is model-specific and does not involve testing the transferability of adversarial examples between different models.\n\nPage 10, Section 6.5, \"RQ4. Coverage-guided Testing\": The evaluation of the coverage-guided testing is presented separately for MNIST-LSTM and MNIST-GRU in Table 6. The accompanying text describes the testing process for each model independently, without any mention of cross-testing the generated adversarial samples between the two models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate whether fault-inducing inputs transfer across different models. All algorithms and experiments are formulated and executed per-target-model (the abstract model M and the RNN D are paired), adversarial examples were generated for particular models only, and the authors explicitly note they could only generate adversarial samples for one DeepSpeech version due to tool compatibility. There are no experiments reusing inputs crafted on one model to test other models performing the same task, nor design elements aimed at discovering transferable attacks.",
                    "evidence": "Section 5.1 (page 481): \"input :D: RNN-based DL system, M: Abstract model of D\" and \"We first collect a set of benign samples B and a set of adversarial samples A.\" (Algorithm 1) \u2014 shows detection is trained per given RNN and its abstract model. Section 5.2 (page 482): \"input :I: Initial seeds, D: RNN-based DL system, M: Abstract model of D\" (Algorithm 2) \u2014 testing is guided against a single RNN instance D with its model M. Section 6.1 Data Preparation (page 483): \"Finally, we only successfully generate adversarial samples for DeepSpeech-0.1.1 because there exists a compatibility issue between DeepSpeech-0.3.0 and the adversarial attack tools [8] we used.\" \u2014 indicates adversarial samples were not produced for multiple models and no cross-model reuse/transfer experiments were performed. Section 6.4 (page 485): \"...we trained a classifier for each model to detect adversarial samples.\" \u2014 confirms evaluation is per-model, not testing transferability across models."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic",
                "Property-based"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ]
        }
    },
    "Huang et al. - 2022 - Coverage-Guided Testing for Recurrent Neural Networks": {
        "key": "huang2021coverage",
        "name": "TESTRNN",
        "year": "2021",
        "citation_count": "66",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": "Even though it can detect backdoor triggers, this is purely diagnostic. The fuzzer exposes low-level functional and robustness failures (misclassifications, metamorphic inconsistencies, and backdoor-induced activation anomalies) without breaching behavioral or safety boundaries."
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's TESTRNN method explicitly uncovers undesired and security-relevant behaviors (adversarial mispredictions and backdoor-triggered misclassifications) \u2014 i.e., failures that violate user intent or safety expectations. However, the paper does not demonstrate bypassing explicit safety/policy/security mechanisms in defended or safety-aligned models (for example, adversarially-trained or otherwise defended models), nor does it claim to induce jailbreaks, data leakage, or unauthorized actions in defended systems. Thus the failures uncovered are higher than mere low-level robustness errors but do not meet the paper-evidence threshold for the highest severity class that requires bypassing explicit defenses.",
                    "evidence": "Page 1191 (Abstract / Introduction): \"In this article, we study the RNN defects, focusing on adversarial samples [1] and backdoor samples [10].\"\n\nPage 1192 (Contributions): \"TESTRNN can not only find adversarial behaviors for the robustness of RNNs (see Section VII-C1) but also identify backdoor inputs for the security of RNNs (see Section VII-C2).\"\n\nPage 1200 (Section VII-A / Experimental Setup): \"We train two handwritten digits recognition models, one of which is benign classifier and the other one is the malicious classifier subject to the backdoor attack in [17].\"\n\nPage 1201 (Section VII-C2, sensitivity / conclusion): \"Answer to RQ4: The TESTRNN metrics can identify the difference between the backdoor input and the normal input (to malicious models).\"\n\nPage 1201 (Section VII-C2, limitation noted): \"We remark that the aforementioned experiment only confirms that test metrics are sensitive to backdoor samples when testing an attacked model.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper focuses on uncovering model robustness errors, specifically adversarial samples and backdoor samples, which lead to mispredictions. The test oracle, which defines failure, is explicitly based on finding prediction inconsistencies under small, semantically preserving input mutations. The experimental evaluation demonstrates the method's ability to find inputs that are slightly perturbed from a seed input but result in a different classification. While the paper also investigates the detection of backdoor inputs, this is framed as another type of defect leading to mispredictions, and the method is not evaluated against models with explicit safety or security mechanisms that could be bypassed. The types of failures uncovered are limited to misclassifications and do not extend to unsafe behaviors like generating toxic or biased content.",
                    "evidence": "Page 8, Section VI-C, \"Test Set Evaluation\": \"A test case x' is said to not pass the oracle if x' is within the norm-ball of some known sample x, i.e., ||x \u2212 x\u2032||2 \u2264 r_oracle, and if x\u2032 has a different classification from x, i.e., \u03c6(x) \u2260 \u03c6(x\u2032). Take the definition in [42], a test case does not pass the oracle is an adversarial sample.\" This quote explicitly defines a failure as a robustness error (a misprediction within a small perturbation radius), which aligns with the 'Low' severity rating."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The failures uncovered by TESTRNN are limited to mispredictions\u2014both adversarial examples (small\u2010perturbation misclassifications) and backdoor\u2010triggered misclassifications\u2014without demonstrating the bypass of any explicit safety, policy, or security mechanisms in defended or safety\u2010aligned models. There is no evidence of inducing unsafe or unauthorized behaviors (such as jailbreaks, data leakage, or toxic/hallucinatory outputs), only prediction inconsistencies under input mutations or backdoor triggers. This aligns with the \u201cLow\u201d severity class, which covers robustness errors without exposing unsafe behavior or bypassing defenses.",
                    "evidence": "Section VI-C (Test Set Evaluation): \u201cA test case x\u2032 is said to not pass the oracle if x\u2032 is within the norm-ball of some known sample x \u2026 and if x\u2032 has a different classification from x \u2026 a test case does not pass the oracle is an adversarial sample.\u201d\n\nSection VII-C1 and VII-C2: All reported failures are misclassifications (adversarial examples and backdoor inputs) on undefended models; no experiments on adversarially trained or safety-aligned models are reported."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "broadly explore the model's internal behavior and flag any unexpected classification (adversarial or backdoor) as a defect."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Linnks faulty behaviors to internal LSTM mechanisms. Correlates coverage with faults. identify where and how the failures arise internally. The analysis is correlational, not causal. "
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Constraint bounded mutation but not evidence if naturalness measure of the generated inputs"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility in individual mutation steps using domain-specific, semantics-preserving mutation policies (e.g., EDA-based text mutations, SMILES enumeration for molecules) and by defining an oracle via norm-balls (r_oracle) intended to capture human-indistinguishability for continuous inputs. These design choices validate that many generated inputs remain realistic. However, the paper does not present any explicit mechanism to prevent cumulative drift across multiple mutation iterations (i.e., it does not enforce a global bound or filtering of plausibility across generations), nor does it report empirical human evaluation or task-specific perceptual/fluency metrics (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) to quantify the perceptual or semantic realism of the final fault-inducing inputs. Therefore the work enforces plausibility at the mutation-step / domain-rule level and validates plausibility conceptually, but overlooks cumulative effects and lacks quantitative or human-subjective evidence for final test cases.",
                    "evidence": "\u2022 Abstract (page 1191): \u201c...owing to its working with finer temporal semantics and the consideration of the naturalness of input perturbation.\u201d\n\n\u2022 Section VI-B, Mutation Policies \u2014 Random Mutation (page 1197): \u201cWhen the LSTM input has continuous values (e.g., image input), Gaussian noises with fixed mean and variance are added to the input. Meanwhile, for discrete value input (e.g., IMDB movie reviews for sentiment analysis), a set of problem-specific mutation functions M are defined.\u201d\n\n\u2022 Section VII-A, Experimental Setup \u2014 Input Mutation (page 1199): \u201cTo avoid this, we take a set M of mutants from the EDA toolkit [48], which was originally designed to augment the training data for improvement on text classification tasks. This ensures the mutated text paragraphs are always valid.\u201d\n\n\u2022 Section VI-C1, Test Oracle (page 1198): \u201cWe define a set of norm-balls, each of which is centered around a data sample with known label. The radius r_oracle of norm-balls intuitively means that a human cannot differentiate between inputs within a norm ball.\u201d\n\n\u2022 Section VII-A4, Oracle Setting (page 1199 / Table II discussion): \u201cNote, we let r_oracle = None for the Lipophilicity model, suggesting that no constraint is imposed on the norm ball.\u201d\n\n\u2022 Algorithm 1 (page 1197): shows orig mapping initialized for seeds and used only in the oracle check (detection): \u201corig[x] <- x for all x \u2208 T0\u201d and later \u201cif ||orig[x] - x'||2 \u2264 r_oracle, and \u03c6(orig[x]) \u2260 \u03c6(x') then Tadv \u2190 Tadv \u222a {x'}\u201d, indicating the paper checks distance to original seeds to label adversarial cases but does not describe rejecting or constraining candidates across successive mutation iterations to prevent cumulative drift.\n\n\u2022 Introduction / Motivation (page 1192): the authors assert their goal that generated cases be realistic: \u201c...our diversified test cases are 'closer' to their seeds (points on the RNN\u2019s data manifold), compared to other state-of-the-art tools, implying higher chance to be seen in the real-life operation, thus preserving the naturalness.\u201d\n\n\u2022 Absence of empirical perceptual/fluency evaluation: The paper does not report any human studies or quantitative perceptual/semantic measures (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) for assessing the realism or naturalness of final fault-inducing inputs (no such metrics or human-evaluation results appear in the Experimental Evaluation sections or tables)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility through bounded perturbations and domain-specific, rule-based mutation strategies designed to preserve semantic validity. For text-based inputs (IMDB dataset), it explicitly uses the EDA toolkit, which performs operations like synonym replacement to ensure the mutated text remains valid and meaningful. For molecular data (Lipophilicity dataset), mutations are constrained to different string representations of the same molecular structure. For continuous inputs like images (MNIST), it relies on bounded Gaussian noise and an L2-norm oracle, asserting that inputs within the norm-ball are indistinguishable to humans. The authors also provide a qualitative validation by comparing a 'natural' and comprehensible text sample generated by their tool against an incomprehensible one from a gradient-based attack (Figure 6). This approach aligns with the 'Medium' definition because it enforces plausibility through bounded/rule-based constraints and provides some validation of the final inputs. However, it does not provide a rigorous empirical evaluation of plausibility (e.g., via human studies or quantitative metrics like SSIM or Perplexity) and does not explicitly address the cumulative effects of mutations across iterations, especially for the continuous image and video domains.",
                    "evidence": "Page 9, Section VII-A, '3) Input Mutation': 'The input to IMDB model is a sequence of words, on which a random change may lead to an unrecognizable (and invalid) text paragraph. To avoid this, we take a set M of mutants from the EDA toolkit [48], which was originally designed to augment the training data for improvement on text classification tasks. This ensures the mutated text paragraphs are always valid.'\n\nPage 12, Section VII-E, 'E. Comparison With Attack-Based Defect Detection (RQ6)': 'It is easy to see that the adversarial review returned by the gradient attack is hard to comprehend, whereas the one from TESTRNN is much easier.' This is supported by the visual comparison in Figure 6.\n\nPage 7, Section VI-C, '1) Test Oracle': 'The radius roracle of norm-balls intuitively means that a human cannot differentiate between inputs within a norm ball. In this article, Euclidean distance, i.e., L2-norm || \u00b7 ||2 is used.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility in individual mutation steps using domain-specific, semantics-preserving mutation policies (e.g., EDA-based text mutations, SMILES enumeration for molecules) and by defining an oracle via norm-balls (r_oracle) intended to capture human-indistinguishability for continuous inputs. These design choices validate that many generated inputs remain realistic. However, the paper does not present any explicit mechanism to prevent cumulative drift across multiple mutation iterations (i.e., it does not enforce a global bound or filtering of plausibility across generations), nor does it report empirical human evaluation or task-specific perceptual/fluency metrics (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) to quantify the perceptual or semantic realism of the final fault-inducing inputs. Therefore the work enforces plausibility at the mutation-step / domain-rule level and validates plausibility conceptually, but overlooks cumulative effects and lacks quantitative or human-subjective evidence for final test cases.",
                    "evidence": "\u2022 Abstract (page 1191): \u201c...owing to its working with finer temporal semantics and the consideration of the naturalness of input perturbation.\u201d\n\n\u2022 Section VI-B, Mutation Policies \u2014 Random Mutation (page 1197): \u201cWhen the LSTM input has continuous values (e.g., image input), Gaussian noises with fixed mean and variance are added to the input. Meanwhile, for discrete value input (e.g., IMDB movie reviews for sentiment analysis), a set of problem-specific mutation functions M are defined.\u201d\n\n\u2022 Section VII-A, Experimental Setup \u2014 Input Mutation (page 1199): \u201cTo avoid this, we take a set M of mutants from the EDA toolkit [48], which was originally designed to augment the training data for improvement on text classification tasks. This ensures the mutated text paragraphs are always valid.\u201d\n\n\u2022 Section VI-C1, Test Oracle (page 1198): \u201cWe define a set of norm-balls, each of which is centered around a data sample with known label. The radius r_oracle of norm-balls intuitively means that a human cannot differentiate between inputs within a norm ball.\u201d\n\n\u2022 Section VII-A4, Oracle Setting (page 1199 / Table II discussion): \u201cNote, we let r_oracle = None for the Lipophilicity model, suggesting that no constraint is imposed on the norm ball.\u201d\n\n\u2022 Algorithm 1 (page 1197): shows orig mapping initialized for seeds and used only in the oracle check (detection): \u201corig[x] <- x for all x \u2208 T0\u201d and later \u201cif ||orig[x] - x'||2 \u2264 r_oracle, and \u03c6(orig[x]) \u2260 \u03c6(x') then Tadv \u2190 Tadv \u222a {x'}\u201d, indicating the paper checks distance to original seeds to label adversarial cases but does not describe rejecting or constraining candidates across successive mutation iterations to prevent cumulative drift.\n\n\u2022 Introduction / Motivation (page 1192): the authors assert their goal that generated cases be realistic: \u201c...our diversified test cases are 'closer' to their seeds (points on the RNN\u2019s data manifold), compared to other state-of-the-art tools, implying higher chance to be seen in the real-life operation, thus preserving the naturalness.\u201d\n\n\u2022 Absence of empirical perceptual/fluency evaluation: The paper does not report any human studies or quantitative perceptual/semantic measures (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) for assessing the realism or naturalness of final fault-inducing inputs (no such metrics or human-evaluation results appear in the Experimental Evaluation sections or tables)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": "Applies low level noise but does not enforce IO persistence with clipping/clamping"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper partially considers serialization/I/O effects but does not fully or explicitly enforce reproducibility under standard serialization (e.g., quantization/encoding). The authors take steps that help make mutations persistently valid for discrete/serialized inputs (text via EDA mutants; SMILES enumeration) and they apply rounding/clipping for image pixel ranges, which mitigate some I/O loss. However, they do not describe explicit handling of standard I/O serialization effects such as 8-bit quantization, encoder rounding/clipping from common formats (PNG/JPEG/WAV/MP3), nor do they state that they generate inputs directly in serialized file formats or verify that failures persist after typical save/load operations. These facts match the definition of Medium: partial consideration (clipping/semantic-preserving metamorphoses) but no explicit enforcement of reproducibility under serialization.",
                    "evidence": "VI.B.1 Random Mutation (page 1197): \"When the LSTM input has continuous values (e.g., image input), Gaussian noises with fixed mean and variance are added to the input.\"  VII.A.3 Input Mutation (page 1199): \"For MNIST model, we add Gaussian noise to input image and round off the decimals around 0 and 1 to make the pixel value stay within the value range.\"  VII.A.3 Input Mutation (page 1199): \"The input to IMDB model is a sequence of words... To avoid this, we take a set M of mutants from the EDA toolkit... This ensures the mutated text paragraphs are always valid.\"  VII.A.4 Oracle Setting (page 1199): \"For continuous inputs, such as images and videos, we calculate the Euclidean distance as the measurement of perturbation.\"  (No section or quote in the paper describes explicit handling or verification of standard I/O serialization effects such as 8-bit quantization or file-format encoding/decoding to ensure failures persist after save/load.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's methodology partially addresses failure reproducibility by ensuring mutations are valid in their respective data domains, but it does not explicitly enforce or verify that failures persist after a full serialization/deserialization cycle. For continuous image data (MNIST), the mutation process includes a step to \"round off the decimals around 0 and 1 to make the pixel value stay within the value range,\" which partially accounts for I/O effects like clipping and quantization. For discrete data like text (IMDB) and chemical representations (Lipophilicity), the paper employs specialized toolkits (EDA and RDKit) that generate valid outputs, which can be considered a form of semantic-preserving transformation. However, the test oracle evaluates the mutated input directly from its in-memory representation without a described serialization step to confirm the failure's persistence. This combination of applying transformations that consider data validity but lacking explicit I/O verification in the oracle aligns with the 'Medium' rating.",
                    "evidence": "Page 9, Section VII-A, \"Input Mutation\": \"For MNIST model, we add Gaussian noise to input image and round off the decimals around 0 and 1 to make the pixel value stay within the value range.\"\nPage 9, Section VII-A, \"Input Mutation\": \"To avoid this, we take a set M of mutants from the EDA toolkit [48]... This ensures the mutated text paragraphs are always valid.\"\nPage 9, Section VII-A, \"Input Mutation\": \"For Lipophilicity model, we take a set M of mutants that change the SMILES string without affecting the molecular structure it represents... The enumerated SMILES strings are the test cases.\"\nPage 8, Section VII-C, \"Test Oracle\": The oracle's definition relies on comparing the in-memory mutated input `x'` to the original input `x` without mentioning a serialization step: \"A test case x' is said to not pass the oracle if x' is within the norm-ball of some known sample x, i.e., ||x - x'||2 \u2264 roracle, and if x' has a different classification from x, i.e., p(x) \u2260 \u03c6(x').\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper partially considers serialization/I/O effects but does not fully or explicitly enforce reproducibility under standard serialization (e.g., quantization/encoding). The authors take steps that help make mutations persistently valid for discrete/serialized inputs (text via EDA mutants; SMILES enumeration) and they apply rounding/clipping for image pixel ranges, which mitigate some I/O loss. However, they do not describe explicit handling of standard I/O serialization effects such as 8-bit quantization, encoder rounding/clipping from common formats (PNG/JPEG/WAV/MP3), nor do they state that they generate inputs directly in serialized file formats or verify that failures persist after typical save/load operations. These facts match the definition of Medium: partial consideration (clipping/semantic-preserving metamorphoses) but no explicit enforcement of reproducibility under serialization.",
                    "evidence": "VI.B.1 Random Mutation (page 1197): \"When the LSTM input has continuous values (e.g., image input), Gaussian noises with fixed mean and variance are added to the input.\"  VII.A.3 Input Mutation (page 1199): \"For MNIST model, we add Gaussian noise to input image and round off the decimals around 0 and 1 to make the pixel value stay within the value range.\"  VII.A.3 Input Mutation (page 1199): \"The input to IMDB model is a sequence of words... To avoid this, we take a set M of mutants from the EDA toolkit... This ensures the mutated text paragraphs are always valid.\"  VII.A.4 Oracle Setting (page 1199): \"For continuous inputs, such as images and videos, we calculate the Euclidean distance as the measurement of perturbation.\"  (No section or quote in the paper describes explicit handling or verification of standard I/O serialization effects such as 8-bit quantization or file-format encoding/decoding to ensure failures persist after save/load.)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Partially handle IO reproducibility by clipping image within 0-1 range (normalized) but did not consider that floating point precision may be lost.",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. All test-case generation and evaluation are performed per-target model (seed inputs are mutated into tests for that same model), and the experiments measure adversarial/backdoor discovery rates per model rather than reusing fault-inducing inputs generated on a surrogate to test other models. The paper also does not describe any fuzzing design or mechanisms aimed at producing inputs intended to transfer across different model implementations performing the same task.",
                    "evidence": "Page 1197, Section VI (Algorithm 1 / Test generation): \"The test suite T is initialized with T0, a corpus of seed inputs (Line 1). New test cases are generated by mutating seed inputs. It keeps the traceability of test cases via a mapping orig that maps each test case generated back to its seed origin.\" \n\nPage 1201, Section VII-C (Detecting RNN Defects): \"As shown in Table V, adversarial samples rate and number of unique adversarial samples in the generated test suite are two important indicators for the robustness evaluation. ... For a set of trained models, we pursue the model, the test suite of which contains less amount of adversarial samples and unique adversarial samples.\" \n\nPage 1202\u20131203, Section VII-F (Comparison With State-of-the-Art Testing Methods): \"We start from 100 seeds drawn from training set and generate 100 000 test cases by DeepStellar and TESTRNN, respectively. The test suites are evaluated for the coverage rate and number of adversarial samples.\" \n\n(These quoted passages show the paper's testing/generation and evaluation are carried out per model/seed and report per-model adversarial/backdoor findings; the paper contains no experimental description or results where fault-inducing inputs crafted on one (surrogate) model are reused to test other models, nor any described mechanisms explicitly aimed at discovering transferable attacks.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a coverage-guided testing method called TESTRNN, which is a white-box approach that generates test cases to maximize structural coverage for a specific RNN model under test. The methodology is designed to operate on a single model at a time. The experimental evaluation, including the comparison of model robustness in Table V, applies the test generation process independently to different models to compare their individual robustness characteristics (e.g., number of adversarial samples found). The paper does not contain any experiments where fault-inducing inputs generated for one model are used to test a different model to assess transferability. Furthermore, the test generation algorithm is guided by internal coverage metrics of the specific model being tested and does not incorporate any mechanisms designed to promote the discovery of transferable attacks.",
                    "evidence": "Page 11, Section VII-C, Table V and accompanying text: The paper evaluates the robustness of two different models per task (e.g., Model 1 and Model 2 for MNIST). However, the methodology described involves running the test generation process separately for each model to compare their individual vulnerability. The text explains the goal as comparing models: \"For a set of trained models, we pursue the model, the test suite of which contains less amount of adversarial samples and unique adversarial samples. For example, we pick up model 2 for Lipophilicity prediction, since the values of two indicators are way smaller than that of model 1.\" This indicates a comparison of the models' robustness via separate testing runs, not a test of attack transferability between them."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. All test-case generation and evaluation are performed per-target model (seed inputs are mutated into tests for that same model), and the experiments measure adversarial/backdoor discovery rates per model rather than reusing fault-inducing inputs generated on a surrogate to test other models. The paper also does not describe any fuzzing design or mechanisms aimed at producing inputs intended to transfer across different model implementations performing the same task.",
                    "evidence": "Page 1197, Section VI (Algorithm 1 / Test generation): \"The test suite T is initialized with T0, a corpus of seed inputs (Line 1). New test cases are generated by mutating seed inputs. It keeps the traceability of test cases via a mapping orig that maps each test case generated back to its seed origin.\" \n\nPage 1201, Section VII-C (Detecting RNN Defects): \"As shown in Table V, adversarial samples rate and number of unique adversarial samples in the generated test suite are two important indicators for the robustness evaluation. ... For a set of trained models, we pursue the model, the test suite of which contains less amount of adversarial samples and unique adversarial samples.\" \n\nPage 1202\u20131203, Section VII-F (Comparison With State-of-the-Art Testing Methods): \"We start from 100 seeds drawn from training set and generate 100 000 test cases by DeepStellar and TESTRNN, respectively. The test suites are evaluated for the coverage rate and number of adversarial samples.\" \n\n(These quoted passages show the paper's testing/generation and evaluation are carried out per model/seed and report per-model adversarial/backdoor findings; the paper contains no experimental description or results where fault-inducing inputs crafted on one (surrogate) model are reused to test other models, nor any described mechanisms explicitly aimed at discovering transferable attacks.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Oracle-guided"
            ]
        }
    },
    "Li et al. - 2021 - Testing DNN-based Autonomous Driving Systems under Critical Environmental Conditions": {
        "key": "li2021testing",
        "name": "TACTIC",
        "year": "2021",
        "citation_count": "51",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's TACTIC method uncovers behaviours that violate safety expectations of autonomous driving systems (large steering-angle divergences under realistic environmental conditions), i.e., unsafe/undesired outputs that could lead to accidents. However, the paper does not claim or demonstrate bypassing explicit safety, policy, or security mechanisms (it targets standard pre-trained driving models and measures steering divergences), nor does it target defended/safety-aligned models or security-critical violations like data leakage or unauthorized actions. Thus the uncovered failures are safety-impactful but not shown to bypass explicit defenses, matching the 'Medium' severity definition.",
                    "evidence": "Page 1, Introduction: \"erroneous or unexpected behaviours that can lead to catastrophic accidents\".\nPage 4, Section 3.2.2 Metamorphic Error Analysis: \"Then the MR is defined as |\u03b8o \u2212 \u03b8t| < \u03f5, where the \u03f5 is a user-defined error bound.\" and \"If the new driving scenes synthesised with this style vector have larger steering angle divergences compared with the original driving scenes, then the style vector is considered to be able to detect more erroneous behaviours.\"\nPage 5, Section 4.2 Results: \"Compared with Rc and DeepRoad, TACTIC detects many more erroneous behaviours in all environmental types for both four error bounds.\" (see Table 1 and Table 2 reporting large numbers of steering-angle errors at 10\u00b0, 20\u00b0, 30\u00b0, 40\u00b0 thresholds).\n(Section names and quotes taken directly from the paper: Introduction; 3.2.2 Metamorphic Error Analysis; 4.2 Comparison with Baselines on Effectiveness; Tables 1\u20132.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, TACTIC, is designed to uncover model robustness errors by identifying inconsistencies in the model's predictions. An 'erroneous behaviour' is explicitly defined using a metamorphic relation where the steering angle predicted for a scene with synthesized environmental conditions is compared to the steering angle for the original scene. A failure is registered if the difference between these two angles exceeds a predefined error bound (\u03b5). This approach directly corresponds to finding 'prediction inconsistencies under semantically preserving input mutations', which is the definition for the 'Low' severity level. The paper does not evaluate whether these steering angle divergences lead to concretely unsafe outcomes (like collisions) nor does it test against models with explicit safety or security mechanisms that could be bypassed.",
                    "evidence": "Page 5, Section 3.2.2. METAMORPHIC ERROR ANALYSIS.: \"Formally, let x_o be an original driving scene and x_t be a newly generated driving scene synthesised based on x_o in a target environmental type. The \u03b8_o and \u03b8_t represent the steering angles for x_o and x_t, respectively. Then the MR is defined as |\u03b8_o - \u03b8_t| < \u03b5, where the \u03b5 is a user-defined error bound.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The failures uncovered by TACTIC are solely model robustness errors\u2014steering\u2010angle prediction inconsistencies under semantically preserving input mutations (realistic environmental variations). The paper measures these errors via a metamorphic relation on steering angles and does not evaluate or demonstrate bypassing any explicit safety, policy, or security mechanisms in defended or safety\u2010aligned models, nor does it induce higher\u2010level unsafe behaviors (e.g., collisions, unauthorized actions, data leakage). Accordingly, under the metric definition, these are low\u2010severity robustness failures.",
                    "evidence": "Section 3.2.2 (Metamorphic Error Analysis): \u2018\u2018Formally, let x_o be an original driving scene and x_t be a newly generated driving scene \u2026 Then the MR is defined as |\u03b8_o - \u03b8_t| < \u03b5 \u2026 If the new driving scenes \u2026 have larger steering\u2010angle divergences \u2026 the style vector is considered to be able to detect more erroneous behaviours.\u2019\u2019\n\nNo sections in the paper describe testing against defended or safety\u2010aligned models, or any bypass of explicit safety/policy/security mechanisms."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Finds inconsistent steering under environmental variations"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Relies on coverage-guided mutation and assumes based on prior work, that higher coverage correlates with fault discovery"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": "Test images are generated through a GAN trained to mimic real driving scenes, so naturalness is preserved throughout fuzzing iterations. Provides human evaluation."
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper enforces input plausibility by performing all mutations/search inside a learned style space produced by a MUNIT image-to-image translation model: style vectors sampled or evolved from that space are decoded by the generator to produce test images, so mutated inputs remain drawn from a distribution of real environmental conditions captured by data. The authors also provide empirical human-evaluation evidence that the final synthesized, fault-inducing driving scenes are perceptually realistic (a user study with 34 participants showing TACTIC images are judged realistic and correctly classified to their environmental types). Together, these show (1) plausibility is enforced throughout the fuzzing/search process by constraining search to the MUNIT style space and (2) final fault-inducing inputs were validated as realistic via a user study.",
                    "evidence": "Page 1, Introduction: \"Because of the richness of styles, the synthesised scenes are realistic enough (as shown by Figure 1(d)) to test whether the DNN-based ADS can perform correctly under the environmental conditions corresponding to the obtained styles.\" \n\nPage 3, Section 3.1 (Environmental Conditions): \"To encode the environmental condition space of an environmental type, we leverage MUNIT ... One insight of MUNIT is that the representation of images can be decomposed into a content space that is domain-invariant and a style space that captures domain-specific characteristics ... we can use the style space of an environmental type in a well-trained MUNIT model to represent the environmental condition space of the type.\" \n\nPage 4, Section 3.3 (Search of Critical Environmental Conditions): \"The (1+1) ES is initialised with an initial individual, which is a style vector randomly sampled from the style space in the MUNIT model. ... a new individual s' is firstly generated based on the current individual s. Then, the fitness values of s and s' is evaluated ... the individual with higher fitness value is kept.\" \n\nPage 6, Section 4.3 (Comparison with Baselines on Image Quality / User Study): \"we conduct a user study to evaluate the realism of the testing driving scenes synthesised by TACTIC ... We received answers from a total of 34 participants ... The results demonstrate that the testing driving scenes synthesised by TACTIC are more realistic than the ones synthesised by affine image transformation (e.g., DeepTest) or high-fidelity simulation (e.g., PreScan) and can better reflect real environments.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's proposed method, TACTIC, is explicitly designed to generate plausible and realistic test inputs. It uses a GAN-based image-to-image translation model (MUNIT) trained on real-world images of various environmental conditions, which inherently constrains the generated outputs to a plausible distribution. The paper goes further and provides direct empirical evidence of this plausibility through a dedicated user study. This study compares the realism of scenes generated by TACTIC against real-world images and other baseline methods. The results show that a significant portion of TACTIC's generated images were considered as realistic as real-world images by human participants. This combination of using a plausibility-aware generation mechanism and empirically validating the realism of the final fault-inducing inputs with a human study directly meets the criteria for a 'High' rating.",
                    "evidence": "Page 8, Section 4.3, 'Comparison with Baselines on Image Quality': 'We conduct a user study to evaluate the realism of the testing driving scenes synthesised by TACTIC... Specifically, we design an online questionnaire consisting of two questions: (1) \u201cWhich driving scene is more realistic?\u201d, and (2) \u201cWhich environmental type does the driving scene belong to?\u201d.'\nPage 8, Section 4.3, Results: 'The results demonstrate that the testing driving scenes synthesised by TACTIC are more realistic than the ones synthesised by affine image transformation (e.g., DeepTest) or high-fidelity simulation (e.g., PreScan) and can better reflect real environments.'\nPage 9, Figure 5 and accompanying text: The results of the user study are presented, showing that for 44% of image pairs (27% 'both realistic' + 16% 'synthesized scene' + 1% 'both unrealistic' is likely a typo in my reading, the text says 44% for TACTIC and DeepRoad where synthesized are not distinguishable from real ones), TACTIC's synthesized scenes were considered as realistic or more realistic than real-world scenes, a stark contrast to DeepTest (6%) and PreScan (14%)."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper enforces input plausibility by performing all mutations/search inside a learned style space produced by a MUNIT image-to-image translation model: style vectors sampled or evolved from that space are decoded by the generator to produce test images, so mutated inputs remain drawn from a distribution of real environmental conditions captured by data. The authors also provide empirical human-evaluation evidence that the final synthesized, fault-inducing driving scenes are perceptually realistic (a user study with 34 participants showing TACTIC images are judged realistic and correctly classified to their environmental types). Together, these show (1) plausibility is enforced throughout the fuzzing/search process by constraining search to the MUNIT style space and (2) final fault-inducing inputs were validated as realistic via a user study.",
                    "evidence": "Page 1, Introduction: \"Because of the richness of styles, the synthesised scenes are realistic enough (as shown by Figure 1(d)) to test whether the DNN-based ADS can perform correctly under the environmental conditions corresponding to the obtained styles.\" \n\nPage 3, Section 3.1 (Environmental Conditions): \"To encode the environmental condition space of an environmental type, we leverage MUNIT ... One insight of MUNIT is that the representation of images can be decomposed into a content space that is domain-invariant and a style space that captures domain-specific characteristics ... we can use the style space of an environmental type in a well-trained MUNIT model to represent the environmental condition space of the type.\" \n\nPage 4, Section 3.3 (Search of Critical Environmental Conditions): \"The (1+1) ES is initialised with an initial individual, which is a style vector randomly sampled from the style space in the MUNIT model. ... a new individual s' is firstly generated based on the current individual s. Then, the fitness values of s and s' is evaluated ... the individual with higher fitness value is kept.\" \n\nPage 6, Section 4.3 (Comparison with Baselines on Image Quality / User Study): \"we conduct a user study to evaluate the realism of the testing driving scenes synthesised by TACTIC ... We received answers from a total of 34 participants ... The results demonstrate that the testing driving scenes synthesised by TACTIC are more realistic than the ones synthesised by affine image transformation (e.g., DeepTest) or high-fidelity simulation (e.g., PreScan) and can better reflect real environments.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Produced synthesized images"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper generates test inputs by applying learned, semantic image-to-image (MUNIT) transformations to produce full synthesized driving images and uses a metamorphic relation (steering-angle consistency) as the oracle. These are coarse, perceptible, semantic-preserving transformations that are less likely to rely on fragile, in-memory floating-point perturbations \u2014 which corresponds to the metric's notion that metamorphic transformations provide only partial enforcement of reproducibility. However, the paper does not explicitly reason about or constrain serialization effects (clipping, quantization, file encoding) nor does it state that it enforces that fault-inducing perturbations survive standard I/O; there is no discussion of handling rounding/quantization/clipping during generation or oracle evaluation. Because TACTIC relies on image-level (GAN) synthesis and metamorphic checking (partial robustness) but does not explicitly account for standard I/O serialization effects, the appropriate rating is Medium.",
                    "evidence": "Section 3.1 (Environmental Conditions), paper: \"To encode the environmental condition space of an environmental type, we leverage MUNIT, a multimodal unsupervised image-to-image translation framework based on Generative Adversarial Networks (GAN).\" and \"Then, to produce x2, M recombines the content c1 with a style vector s2 sampled from the style space S2 of X2 by the decoder G2 of X2, i.e., x2 = G2(c1, s2).\" (Section 3.1) ; Section 3.2.2 (Metamorphic Error Analysis), paper: \"Following the existing work (Tian et al., 2018), we also adopt Metamorphic Relation (MR) (Chen et al., 2020) as the test oracle ... the MR is defined as |\u03b8o \u2212 \u03b8t| < \u03b5, where the \u03b5 is a user-defined error bound.\" (Section 3.2.2) ; Section 4.2 (Setup/Experiments), paper: \"4 critical environmental conditions are generated ... which are separately applied on the Udacity testing dataset to synthesise testing driving scenes (i.e., 4 \u00d7 5614 testing driving scenes are totally generated).\" (Section 4.2) ; \u2014 Nowhere in the method or evaluation (Sections 3 and 4) does the paper discuss handling of standard I/O serialization effects (e.g., clipping, quantization, rounding, image file encoding) or constraining perturbations so they explicitly survive serialization during test generation or oracle evaluation."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's proposed method, TACTIC, uses a GAN-based image-to-image translation model (MUNIT) to apply semantic-preserving metamorphic transformations, specifically altering the environmental conditions (e.g., weather, time of day) of driving scenes. According to the metric definition, using such transformations without explicit enforcement of reproducibility under serialization (e.g., quantization, clipping) is categorized as 'Medium'. The paper does not describe any steps to ensure that the pixel-level changes introduced by the GAN would persist after standard I/O operations, such as saving the image to an 8-bit format like PNG. The focus is on the realism and bug-finding effectiveness of the generated scenes, but there is no mention of constraining the mutations or verifying the oracle's findings post-serialization to account for potential loss of fault-inducing perturbations.",
                    "evidence": "Page 3, Section 3.1, Environmental Conditions: \"To encode the environmental condition space of an environmental type, we leverage MUNIT, a multimodal unsupervised image-to-image translation framework based on Generative Adversarial Networks (GAN).\" This confirms the use of a generative model for transformation. \n\nPage 5, Figure 4: This figure illustrates the semantic nature of the transformations, showing an original scene transformed into 'Rain' and 'Snow in Daytime' versions. \n\nThe paper's methodology sections (3. The TACTIC Approach) and evaluation sections (4. Experimental Evaluation) lack any discussion of handling serialization effects. The metamorphic oracle described in Section 3.2.2 operates on the generated scene `xt` directly, without any mention of an intermediate serialization step to confirm the failure's persistence."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper generates test inputs by applying learned, semantic image-to-image (MUNIT) transformations to produce full synthesized driving images and uses a metamorphic relation (steering-angle consistency) as the oracle. These are coarse, perceptible, semantic-preserving transformations that are less likely to rely on fragile, in-memory floating-point perturbations \u2014 which corresponds to the metric's notion that metamorphic transformations provide only partial enforcement of reproducibility. However, the paper does not explicitly reason about or constrain serialization effects (clipping, quantization, file encoding) nor does it state that it enforces that fault-inducing perturbations survive standard I/O; there is no discussion of handling rounding/quantization/clipping during generation or oracle evaluation. Because TACTIC relies on image-level (GAN) synthesis and metamorphic checking (partial robustness) but does not explicitly account for standard I/O serialization effects, the appropriate rating is Medium.",
                    "evidence": "Section 3.1 (Environmental Conditions), paper: \"To encode the environmental condition space of an environmental type, we leverage MUNIT, a multimodal unsupervised image-to-image translation framework based on Generative Adversarial Networks (GAN).\" and \"Then, to produce x2, M recombines the content c1 with a style vector s2 sampled from the style space S2 of X2 by the decoder G2 of X2, i.e., x2 = G2(c1, s2).\" (Section 3.1) ; Section 3.2.2 (Metamorphic Error Analysis), paper: \"Following the existing work (Tian et al., 2018), we also adopt Metamorphic Relation (MR) (Chen et al., 2020) as the test oracle ... the MR is defined as |\u03b8o \u2212 \u03b8t| < \u03b5, where the \u03b5 is a user-defined error bound.\" (Section 3.2.2) ; Section 4.2 (Setup/Experiments), paper: \"4 critical environmental conditions are generated ... which are separately applied on the Udacity testing dataset to synthesise testing driving scenes (i.e., 4 \u00d7 5614 testing driving scenes are totally generated).\" (Section 4.2) ; \u2014 Nowhere in the method or evaluation (Sections 3 and 4) does the paper discuss handling of standard I/O serialization effects (e.g., clipping, quantization, rounding, image file encoding) or constraining perturbations so they explicitly survive serialization during test generation or oracle evaluation."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Does not mention whether GAN generated image inputs in considers IO effects",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate transferability of fault-inducing inputs across models. TACTIC is defined and executed per subject model (the search takes the subject DNN as input and returns critical conditions for that model), and all experiments generate and evaluate critical styles separately for each ADS. There is no experiment that crafts fault-inducing inputs on one surrogate model and reuses them to test other models, nor does the design include explicit mechanisms aimed at finding transferable attacks across different model implementations.",
                    "evidence": "Page 3, Section 3.3 \"Search of Critical Environmental Conditions\": \"TACTIC receives as inputs the subject DNN-based ADS N, the MUNIT model M trained for the given environmental type, and a set Io of the original driving scenes. The output of TACTIC is a set S of the critical environmental conditions of the given environmental type.\"; Page 4, Section 4.2 \"Comparison with Baselines on Effectiveness - Setup\": \"We execute TACTIC with two coverage-guiding strategies: KMNC (denoted as TACTICKMNC) and NBC (denoted as TACTICNBC), respectively, on each of the three subject DNN-based ADSs, under the five environmental types.\"; Page 4, Section 4.2 \"Setup\" (describing baselines): \"For Rc, when testing the three subject systems, we randomly sample 4 style vectors (equal to the number of critical style vectors generated by TACTIC) for each of the environmental types and separately apply the 4 random style vectors on the Udacity testing dataset to synthesise testing driving scenes...\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a method, TACTIC, to identify critical environmental conditions for a specific, or 'subject', Deep Neural Network (DNN) based Autonomous Driving System (ADS). The search for these conditions is guided by model-specific metrics, such as neuron coverage and steering angle divergence, which are calculated on the subject model. The experimental evaluation involves three different ADS models (Dave-orig, Dave-dropout, Chauffeur). However, the TACTIC framework is executed independently for each model to find its specific critical conditions. The paper does not contain any experiments or discussion on whether the fault-inducing environmental conditions discovered for one model also cause erroneous behavior in the other models. Therefore, it does not demonstrate the transferability of the discovered failures.",
                    "evidence": "Page 6, Section 3.3, \"Search of Critical Environmental Conditions\": \"TACTIC receives as inputs the subject DNN-based ADS N, the MUNIT model M trained for the given environmental type, and a set Io of the original driving scenes.\" This statement clarifies that the method is designed to operate on a single 'subject' model 'N' at a time. \n\nPage 7, Section 4.2, \"Comparison with Baselines on Effectiveness\", paragraph \"Setup\": \"We execute TACTIC with two coverage-guiding strategies: KMNC (denoted as TACTIC KMNC ) and NBC (denoted as TACTIC NBC ), respectively, on each of the three subject DNN-based ADSs, under the five environmental types.\" This shows that the experiments were run separately for each of the three models, rather than generating inputs on one and testing them on others. The results are then presented independently for each model, confirming the lack of a transferability analysis."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate transferability of fault-inducing inputs across models. TACTIC is defined and executed per subject model (the search takes the subject DNN as input and returns critical conditions for that model), and all experiments generate and evaluate critical styles separately for each ADS. There is no experiment that crafts fault-inducing inputs on one surrogate model and reuses them to test other models, nor does the design include explicit mechanisms aimed at finding transferable attacks across different model implementations.",
                    "evidence": "Page 3, Section 3.3 \"Search of Critical Environmental Conditions\": \"TACTIC receives as inputs the subject DNN-based ADS N, the MUNIT model M trained for the given environmental type, and a set Io of the original driving scenes. The output of TACTIC is a set S of the critical environmental conditions of the given environmental type.\"; Page 4, Section 4.2 \"Comparison with Baselines on Effectiveness - Setup\": \"We execute TACTIC with two coverage-guiding strategies: KMNC (denoted as TACTICKMNC) and NBC (denoted as TACTICNBC), respectively, on each of the three subject DNN-based ADSs, under the five environmental types.\"; Page 4, Section 4.2 \"Setup\" (describing baselines): \"For Rc, when testing the three subject systems, we randomly sample 4 style vectors (equal to the number of critical style vectors generated by TACTIC) for each of the environmental types and separately apply the 4 random style vectors on the Udacity testing dataset to synthesise testing driving scenes...\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Oracle-guided"
            ]
        }
    },
    "Wang et al. - 2023 - DistXplore Distribution-Guided Testing for Evaluating and Enhancing Deep Learning Systems": {
        "key": "wang2023distxplore",
        "name": "DistXplore",
        "year": "2023",
        "citation_count": "13",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's method (DistXplore) is explicitly framed as a distribution-guided testing technique for finding misclassified inputs and improving model robustness. All objectives, metrics, and evaluations focus on generating \u2018\u2018erroneous inputs\u2019\u2019 (i.e., mispredictions / misclassifications), making them hard to detect by adversarial-example detectors and using them to enrich training data. The paper evaluates the ability to bypass detection by defenses (Dissector, A2D) and to produce hard-to-detect misclassification errors, but it does not claim to uncover or induce high-impact safety or security failures such as jailbreaks, data leakage, unauthorized actions, or toxic/hallucinatory outputs in safety-aligned models. Therefore according to the metric definitions, DistXplore uncovers only model robustness errors (mispredictions) and should be rated Low.",
                    "evidence": "Page 69, Introduction: \"DL testing aims to generate test cases that evaluate the robustness of DL systems, i.e., discover the data that is valid but cannot be predicted correctly (e.g., the dataset d and e in Fig. 1), and enhance the robustness...\" \n\nPage 71, Section 3.1.1 (Model Evaluation): \"To evaluate the model\u2019s robustness, we aim to generate the erroneous inputs that are hard to be detected by existing defense techniques.\" \n\nPage 72, Section 4 (Setup), RQ1: \"RQ1: How effective is DistXplore in detecting errors that can bypass the defense methods?\" and footnote: \"The error in the paper refers to the erroneous inputs that are missclassi\ufb01ed.\" \n\nPage 75, Section 4.2.1 (Results): \"The overall results show that DistXplore (column DistX) can generate more strong errors that are difficult to be detected by defense techniques compared with baselines.\" \n\n(These quotations show the paper focuses on misclassification/robustness errors and evasion of adversarial detectors, not on bypassing safety/policy mechanisms or producing unsafe/ high-impact behaviors.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, DistXplore, focuses exclusively on generating inputs that are misclassified by the model, which directly corresponds to the definition of 'model robustness errors' in the metric. A footnote explicitly defines an 'error' as an 'erroneous input that is missclassified'. The entire evaluation framework is built around discovering and quantifying these mispredictions. While the paper claims these errors can bypass 'defense methods' (Dissector and A2D), these methods are adversarial example detectors designed to identify perturbed inputs, not explicit safety or security mechanisms that prevent unsafe behavior like toxicity, bias, or jailbreaks. The paper does not present any evidence of generating unsafe or toxic content, nor does it attempt to bypass safety-aligned policies. Therefore, the failures uncovered are limited to robustness errors.",
                    "evidence": "Page 6, Section 4, Footnote 1: 'The error in the paper refers to the erroneous inputs that are missclassified.'\n\nPage 5, Section 3.1.1 Model Evaluation: 'To evaluate the model\u2019s robustness, we aim to generate the erroneous inputs that are hard to be detected by existing defense techniques... Thus, DistXplore aims to generate test cases (in a class) that are statistically similar to the training data in other classes.'\n\nPage 7, Section 4.1.3 Defense Methods: This section describes the defense methods used for evaluation (Dissector and A2D). Both are designed to 'detect adversarial examples' or 'adversarial samples', not to enforce safety policies or prevent unsafe content generation."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's method (DistXplore) is explicitly framed as a distribution-guided testing technique for finding misclassified inputs and improving model robustness. All objectives, metrics, and evaluations focus on generating \u2018\u2018erroneous inputs\u2019\u2019 (i.e., mispredictions / misclassifications), making them hard to detect by adversarial-example detectors and using them to enrich training data. The paper evaluates the ability to bypass detection by defenses (Dissector, A2D) and to produce hard-to-detect misclassification errors, but it does not claim to uncover or induce high-impact safety or security failures such as jailbreaks, data leakage, unauthorized actions, or toxic/hallucinatory outputs in safety-aligned models. Therefore according to the metric definitions, DistXplore uncovers only model robustness errors (mispredictions) and should be rated Low.",
                    "evidence": "Page 69, Introduction: \"DL testing aims to generate test cases that evaluate the robustness of DL systems, i.e., discover the data that is valid but cannot be predicted correctly (e.g., the dataset d and e in Fig. 1), and enhance the robustness...\" \n\nPage 71, Section 3.1.1 (Model Evaluation): \"To evaluate the model\u2019s robustness, we aim to generate the erroneous inputs that are hard to be detected by existing defense techniques.\" \n\nPage 72, Section 4 (Setup), RQ1: \"RQ1: How effective is DistXplore in detecting errors that can bypass the defense methods?\" and footnote: \"The error in the paper refers to the erroneous inputs that are missclassi\ufb01ed.\" \n\nPage 75, Section 4.2.1 (Results): \"The overall results show that DistXplore (column DistX) can generate more strong errors that are difficult to be detected by defense techniques compared with baselines.\" \n\n(These quotations show the paper focuses on misclassification/robustness errors and evasion of adversarial detectors, not on bypassing safety/policy mechanisms or producing unsafe/ high-impact behaviors.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "High",
                    "why": "Guides test generation to minimize distribution differences between source class inputs and target class."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "links the occurrence and detectability of errors to the statistical closeness (measured via MMD) between the distribution of generated test inputs and the distribution of another class at the logits layer."
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "metamorphic transformations can yield decimal-valued pixels internally which may change when re-quantized to integers."
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper primarily applies semantic-preserving metamorphic image transformations (translation, rotation, brightness, etc.) as mutation operators and constrains those transformations for validity, but it does not explicitly account for serialization effects (quantization, rounding, clipping) nor does it state that inputs are generated in serialized formats. According to the metric, metamorphic transformations provide only partial enforcement of failure reproducibility when serialization effects are not explicitly handled; therefore DistXplore matches the 'Medium' level.",
                    "evidence": "Page 71, Section 3.2 \"Distribution-Guided Test Generation\": \"In this paper, we mainly focus on image classification tasks. DistXplore can be easily extended to other domains. We select the diverse image transformations (e.g., translation, rotation, brightness) used in DeepTest [54] and DeepHunter [63]. For each selected gene, the mutation randomly selects a transformation function to mutate it. To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation...\" \n\nPage 71, Algorithm 1 / 3.2: \"A uniform crossover is performed between the selected two chromosomes in the input level, i.e., genes in a chromosome are inputs of the model. Each gene in the chromosome can be selected to mutate with a selection probability...\" \n\nPage 72, Section 4.1.1 \"Implementation\": \"We have implemented DistXplore in Python 3.6 based on DL framework Keras (ver.2.3.1) with TensorFlow (ver.1.15.2).\" \n\n(Throughout Sections 3 and 4 the paper describes mutation as in-memory transformations on image tensors and constraining transformations for validity, but it contains no discussion of serialization, quantization, rounding, clipping, saving to common storage formats (PNG/JPEG), or generating inputs directly in serialized formats.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, DistXplore, uses semantic-preserving metamorphic transformations (e.g., translation, rotation, brightness) as its mutation operators for generating test cases. According to the metric definition, using such transformations provides partial enforcement of failure reproducibility. However, the paper does not describe any explicit mechanism to account for serialization effects like quantization or rounding. The constraints mentioned (L0 and L\u221e norms) are used to guarantee the general validity of the generated inputs by limiting the perturbation magnitude, not to ensure that the fault-inducing perturbations survive standard I/O operations. This lack of explicit handling of serialization effects prevents a 'High' rating, placing the method in the 'Medium' category.",
                    "evidence": "Page 6, Section 3.2, Distribution-Guided Test Generation: \"In this paper, we mainly focus on image classification tasks. DistXplore can be easily extended to other domains. We select the diverse image transformations (e.g., translation, rotation, brightness) used in DeepTest [54] and DeepHunter [63]. ... To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation with both L0 and L\u221e.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper primarily applies semantic-preserving metamorphic image transformations (translation, rotation, brightness, etc.) as mutation operators and constrains those transformations for validity, but it does not explicitly account for serialization effects (quantization, rounding, clipping) nor does it state that inputs are generated in serialized formats. According to the metric, metamorphic transformations provide only partial enforcement of failure reproducibility when serialization effects are not explicitly handled; therefore DistXplore matches the 'Medium' level.",
                    "evidence": "Page 71, Section 3.2 \"Distribution-Guided Test Generation\": \"In this paper, we mainly focus on image classification tasks. DistXplore can be easily extended to other domains. We select the diverse image transformations (e.g., translation, rotation, brightness) used in DeepTest [54] and DeepHunter [63]. For each selected gene, the mutation randomly selects a transformation function to mutate it. To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation...\" \n\nPage 71, Algorithm 1 / 3.2: \"A uniform crossover is performed between the selected two chromosomes in the input level, i.e., genes in a chromosome are inputs of the model. Each gene in the chromosome can be selected to mutate with a selection probability...\" \n\nPage 72, Section 4.1.1 \"Implementation\": \"We have implemented DistXplore in Python 3.6 based on DL framework Keras (ver.2.3.1) with TensorFlow (ver.1.15.2).\" \n\n(Throughout Sections 3 and 4 the paper describes mutation as in-memory transformations on image tensors and constraining transformations for validity, but it contains no discussion of serialization, quantization, rounding, clipping, saving to common storage formats (PNG/JPEG), or generating inputs directly in serialized formats.)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability across models. DistXplore formulates and optimizes test suites using distributions measured on a target model's internal layer outputs (the logits) and the evaluation and robustness enhancement steps retrain or test the same model on which test suites were generated. The experiments run the method separately on multiple models/datasets, but the paper does not reuse fault-inducing inputs crafted on one (surrogate) model to test other models, nor does it describe any mechanism intended to discover or promote attacks that transfer across different model implementations.",
                    "evidence": "\u2022 Section 3.1.1 (page 71, \"Model Evaluation\"): \"The distribution difference is measured on a specific layer of the DNN. In this paper, we select the logits layer, i.e., the layer before the softmax layer...\" (shows method depends on internal layer outputs of the target model).\n\n\u2022 Section 4.1.4 (page 73, \"Configuration of DistXplore\"): \"For each class, we run DistXplore multiple times ... Finally, for each model, we run DistXplore 90 times (i.e., 10 source classes \u00d7 9 target classes).\" (shows runs are per-model; no cross-model reuse is described).\n\n\u2022 Section 4.2.3 (page 75, \"RQ3: Robustness Enhancement\"): \"For each tool, we fine-tune the original model 20 epochs ... by adding the new data generated from each tool, and evaluate the empirical robustness of the new model on the validation dataset.\" (shows generated tests are used to retrain/evaluate the same model rather than testing other models).\n\n\u2022 Section 3.1.2 (page 71, \"Model Enhancement\"): description of target selection and distribution diversity refers to selecting other classes as targets (inter-class guidance), not other models or surrogate-model attacks: \"We select the classes in the same task as targets because the classification is based on their relationships...\".\n\nThere is no text in the paper that (a) states that fault-inducing inputs generated on one model were reused to test other models, (b) measures cross-model transfer of discovered faults, or (c) proposes mechanisms specifically designed to produce inputs intended to transfer across different model implementations."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's methodology and evaluation focus exclusively on generating fault-inducing inputs for a single model under test and then evaluating the properties of those inputs (e.g., their ability to bypass defenses) on that same model. The concept of attack transferability\u2014testing if inputs that cause faults on one model also cause faults on another model performing the same task\u2014is never discussed or evaluated. The experimental setup, described in Section 4, details procedures for generating errors, evaluating their strength against defenses, and using them for retraining. All these steps are performed within the context of the specific model for which the errors were generated. There is no mention of reusing these inputs to test other models.",
                    "evidence": "Page 7, Section 4.1.4, Experiment Setup, RQ Setup: \"To demonstrate the capability of DistXplore in generating strong errors for model evaluation (RQ1), we collect the test suite in the last iteration for every pair (c, c') (i.e., the best chromosome returns from Algo 1). For each model, we collect a total number of 90 chromosomes over 90 pairs, which are used to evaluate the strength of these errors. The strength of errors is measured by the success rate of bypassing defenses.\"\n\nThis quote illustrates that the evaluation is conducted on a per-model basis, where errors are generated for a specific model and their strength is evaluated against defenses on that same model, without assessing transferability to other models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability across models. DistXplore formulates and optimizes test suites using distributions measured on a target model's internal layer outputs (the logits) and the evaluation and robustness enhancement steps retrain or test the same model on which test suites were generated. The experiments run the method separately on multiple models/datasets, but the paper does not reuse fault-inducing inputs crafted on one (surrogate) model to test other models, nor does it describe any mechanism intended to discover or promote attacks that transfer across different model implementations.",
                    "evidence": "\u2022 Section 3.1.1 (page 71, \"Model Evaluation\"): \"The distribution difference is measured on a specific layer of the DNN. In this paper, we select the logits layer, i.e., the layer before the softmax layer...\" (shows method depends on internal layer outputs of the target model).\n\n\u2022 Section 4.1.4 (page 73, \"Configuration of DistXplore\"): \"For each class, we run DistXplore multiple times ... Finally, for each model, we run DistXplore 90 times (i.e., 10 source classes \u00d7 9 target classes).\" (shows runs are per-model; no cross-model reuse is described).\n\n\u2022 Section 4.2.3 (page 75, \"RQ3: Robustness Enhancement\"): \"For each tool, we fine-tune the original model 20 epochs ... by adding the new data generated from each tool, and evaluate the empirical robustness of the new model on the validation dataset.\" (shows generated tests are used to retrain/evaluate the same model rather than testing other models).\n\n\u2022 Section 3.1.2 (page 71, \"Model Enhancement\"): description of target selection and distribution diversity refers to selecting other classes as targets (inter-class guidance), not other models or surrogate-model attacks: \"We select the classes in the same task as targets because the classification is based on their relationships...\".\n\nThere is no text in the paper that (a) states that fault-inducing inputs generated on one model were reused to test other models, (b) measures cross-model transfer of discovered faults, or (c) proposes mechanisms specifically designed to produce inputs intended to transfer across different model implementations."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Does not explicitly enforce naturalness across fuzzing iterations. Apply norm constraints in each mutation step but cumulative effect is ignored. Checks visual validity on the final outputs via human evaluation."
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility via bounded, rule-based mutation constraints (they adopt conservative mutation bounds and use Lp-norm validity constraints) and additionally reports an explicit human validation of final fault-inducing inputs (manual inspection with high \u2018validity ratio\u2019). However, the paper does not report quantitative perceptual metrics (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) nor an explicit analysis of cumulative drift across many fuzzing iterations (it relies on per-mutation constraints and conservative configuration but does not measure or bound accumulated drift over iterations). This matches the 'Medium' level: plausibility of final inputs is validated, and individual mutation steps are constrained, but cumulative-effects across iterations are not explicitly analyzed or controlled.",
                    "evidence": "Page 70, Section 2.1.2 (Data Validity): \"In practice, the Lp norm [40] is usually used to guarantee the validity of the generated data by the existing DL testing and adversarial attack techniques. Specifically, given a valid input x, the new test case x' generated by adding some perturbations on x is considered as valid if ||x' - x|| < \u03b5, where \u03b5 is a safe radius.\"; Page 72, Section 3.2 (Distribution-Guided Test Generation): \"To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation with both 0 and \u221e.\"; Page 76, Section 4.2.2 (RQ2: Efficiency of DistXplore): \"In order to evaluate the validity of the generated inputs, we conducted a manual investigation by randomly selecting 10,000 erroneous inputs from the testing outputs of each model and calculating the average validity ratio. The validity ratios were found to be 98.5%+, 96.5%+, 98.7%+, and 95.3%+ for MNIST, Fashion\u2011MNIST, CIFAR\u201110, and SVHN datasets, respectively.\"; (No mention in the paper of perceptual/semantic quantitative metrics such as LPIPS, SSIM, PESQ, STOI or Perplexity, nor of an explicit assessment of cumulative mutation drift across iterations.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper satisfies the criteria for 'Medium' by both enforcing plausibility in individual mutation steps and validating the plausibility of the final generated inputs. Plausibility is enforced by adopting a 'conservative strategy' that constrains image transformations using both L0 and L\u221e norms, which aligns with the 'bounded or rule-based constraints' mentioned in the metric definition. However, the paper does not discuss or address the potential for cumulative drift over multiple iterations. Crucially, the authors explicitly validate the plausibility of the final fault-inducing inputs through a large-scale manual investigation (a form of human study), reporting high validity ratios across multiple datasets. This direct assessment of the final outputs clearly meets the 'Medium' criteria. The method does not qualify for 'High' because it does not demonstrate a mechanism to ensure inputs remain within a plausible distribution *throughout* the entire fuzzing process to prevent cumulative drift.",
                    "evidence": "Page 6, Section 3.2, \"Distribution-Guided Test Generation\": \"To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation with both L0 and L\u221e.\"\n\nPage 10, Section 4.2.2, \"RQ2: Efficiency of DistXplore\": \"In order to evaluate the validity of the generated inputs, we conducted a manual investigation by randomly selecting 10,000 erroneous inputs from the testing outputs of each model and calculating the average validity ratio. The validity ratios were found to be 98.5%+, 96.5%+, 98.7%+, and 95.3%+ for MNIST, Fashion-MNIST, CIFAR-10, and SVHN datasets, respectively. The results demonstrate that DistXplore is capable of generating valid inputs with high proportions.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility via bounded, rule-based mutation constraints (they adopt conservative mutation bounds and use Lp-norm validity constraints) and additionally reports an explicit human validation of final fault-inducing inputs (manual inspection with high \u2018validity ratio\u2019). However, the paper does not report quantitative perceptual metrics (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) nor an explicit analysis of cumulative drift across many fuzzing iterations (it relies on per-mutation constraints and conservative configuration but does not measure or bound accumulated drift over iterations). This matches the 'Medium' level: plausibility of final inputs is validated, and individual mutation steps are constrained, but cumulative-effects across iterations are not explicitly analyzed or controlled.",
                    "evidence": "Page 70, Section 2.1.2 (Data Validity): \"In practice, the Lp norm [40] is usually used to guarantee the validity of the generated data by the existing DL testing and adversarial attack techniques. Specifically, given a valid input x, the new test case x' generated by adding some perturbations on x is considered as valid if ||x' - x|| < \u03b5, where \u03b5 is a safe radius.\"; Page 72, Section 3.2 (Distribution-Guided Test Generation): \"To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation with both 0 and \u221e.\"; Page 76, Section 4.2.2 (RQ2: Efficiency of DistXplore): \"In order to evaluate the validity of the generated inputs, we conducted a manual investigation by randomly selecting 10,000 erroneous inputs from the testing outputs of each model and calculating the average validity ratio. The validity ratios were found to be 98.5%+, 96.5%+, 98.7%+, and 95.3%+ for MNIST, Fashion\u2011MNIST, CIFAR\u201110, and SVHN datasets, respectively.\"; (No mention in the paper of perceptual/semantic quantitative metrics such as LPIPS, SSIM, PESQ, STOI or Perplexity, nor of an explicit assessment of cumulative mutation drift across iterations.)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ]
        }
    },
    "Liu et al. - 2022 - QATest A Uniform Fuzzing Framework for Question Answering Systems": {
        "key": "liu2022qatest",
        "name": "QATest",
        "year": "2022",
        "citation_count": "21",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "All evidence in the paper shows QATest uncovers robustness errors (mispredictions and inconsistent answers under semantically-preserving transformations) such as wrong spans, irrelevant answers, incorrect <NoAnswer> decisions, and failures to perform reasoning. The authors define a bug as M(q') \u2260 M(q) for transformed (semantically consistent) inputs and illustrate detected bug types in a case study (e.g., \u201c<NoAnswer> questions are answered with irrelevant content\u201d, \u201cAnswers that are unrelated to the question\u201d, etc.). The paper contains no indication that QATest targets or successfully bypasses explicit safety/policy/security mechanisms, tests safety-aligned or adversarially-defended models, or uncovers high-severity violations like jailbreaks, data leakage, or unauthorized actions. Therefore the failures are best categorized as low-severity (model robustness errors) under the given metric.",
                    "evidence": "Page 1, Abstract: \"The experiment results show that the tests generated by QATest detect hundreds of erroneous behaviors of QA systems efficiently.\"; Page 4, Section 3.2 Test Oracle (Definition 3.1): \"A mutant q' \u2208 T(q) is regarded as a bug case of M if M(q') \u2260 M(q).\"; Page 9, Section 5.4 Answer to RQ4: Case Study: \"we show five types of representative erroneous behaviors... (1) <NoAnswer> questions are answered with irrelevant content... (3) Answers that are unrelated to the question... (5) Answers without logical reasoning.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes QATest, a fuzzing framework based on metamorphic testing. The core principle is to apply semantically preserving transformations to input questions and check for prediction inconsistencies. A failure is defined as the model producing a different answer for the transformed question compared to the original, i.e., `M(q') \u2260 M(q)`. This directly aligns with the definition of uncovering model robustness errors. The paper's case study of detected failures (RQ4) categorizes them as various types of incorrect answers, such as answering an answerable question with `<NoAnswer>`, providing answers unrelated to the question, or extracting the wrong keywords. These are all examples of mispredictions or inconsistencies. The paper does not test against models with explicit safety mechanisms, nor does it aim to produce or evaluate unsafe, toxic, or biased outputs.",
                    "evidence": "Page 4, Section 3.2.1, Test Oracle: \"Metamorphic Relations (MRs) refer to the relationship expected to be followed between input and output when the target program is executed multiple times. ... We define a series of semantically consistent transformations T for questions q e Q. The generated question q' can be obtained by applying the transformation t\u2208T to the original question q, and q' has the same intent and meaning as q. Therefore, the QA system should output the same answer for the q and q'.\"\n\nPage 10, Section 5.4, Answer to RQ4: Case Study: \"In this RQ, we investigate and classify the erroneous behaviors of the QA system detected by QATest. In Table 6, we show five types of representative erroneous behaviors. ... (2) Answerable questions are answered with <NoAnswer>. ... the question has a corresponding answer in the context, but the system can't find a matching answer and outputs <NoAnswer>. (3) Answers that are unrelated to the question. Another type of error is that the QA system's answers are not related to the question. In example 3, the question is asking about the year, but the output of the QA system is a person's name.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "All evidence in the paper shows QATest uncovers robustness errors (mispredictions and inconsistent answers under semantically-preserving transformations) such as wrong spans, irrelevant answers, incorrect <NoAnswer> decisions, and failures to perform reasoning. The authors define a bug as M(q') \u2260 M(q) for transformed (semantically consistent) inputs and illustrate detected bug types in a case study (e.g., \u201c<NoAnswer> questions are answered with irrelevant content\u201d, \u201cAnswers that are unrelated to the question\u201d, etc.). The paper contains no indication that QATest targets or successfully bypasses explicit safety/policy/security mechanisms, tests safety-aligned or adversarially-defended models, or uncovers high-severity violations like jailbreaks, data leakage, or unauthorized actions. Therefore the failures are best categorized as low-severity (model robustness errors) under the given metric.",
                    "evidence": "Page 1, Abstract: \"The experiment results show that the tests generated by QATest detect hundreds of erroneous behaviors of QA systems efficiently.\"; Page 4, Section 3.2 Test Oracle (Definition 3.1): \"A mutant q' \u2208 T(q) is regarded as a bug case of M if M(q') \u2260 M(q).\"; Page 9, Section 5.4 Answer to RQ4: Case Study: \"we show five types of representative erroneous behaviors... (1) <NoAnswer> questions are answered with irrelevant content... (3) Answers that are unrelated to the question... (5) Answers without logical reasoning.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility at individual mutation steps and uses automatic, task-relevant measures to prefer authentic candidates (ROUGE-1 quality check and a POS-based ``perplexity priority'' used to select/retain seeds). It also implements realistic transformation operators (back-translation, BERT insertion, WordNet synonyms, entity-alias substitution, OCR/keyboard typo simulation). However, the work does not provide an explicit human evaluation or dedicated empirical assessment of the final fault-inducing inputs\u2019 semantic or perceptual realism, and it does not present a formal mechanism that strictly bounds cumulative drift across many iterative mutations (rather, it relies on selection heuristics such as perplexity priority and ROUGE threshold). These properties match the metric\u2019s \u201cMedium\u201d level: per-step plausibility checks and quantitative plausibility heuristics are present, but cumulative-iteration effects are not strictly enforced nor validated by human study or external fluency/quality evaluations of final failing inputs.",
                    "evidence": "Section 3.1 / Algorithm 1 (p.3): \"quality_score \u2190 Rouge1(s, s'); if quality_score > \u03c4 then // acceptable ... D.append(s')\" \u2014 shows ROUGE-1 is used as a per-generation quality filter.\n\nSection 3.2.5 Quality Assessment (p.3): \"To ensure the quality of generated questions, we employ the ROUGE-1 metric [35] to evaluate the questions generated by metamorphic transformations.\" (ROUGE used to discard low-quality generations).\n\nSection 3.3.2 Perplexity Priority (p.4): \"To ensure that the generated test questions have good authenticity and rationality, we consider applying perplexity priority as guidance for seed data retention... per(q) = p(seq(q))^{-1/n}. In each iteration, QATest selects the generated question with the lowest perplexity score in a generated batch.\" \u2014 shows a quantitative authenticity/prioritization metric used during fuzzing.\n\nSection 4.3 Sentence Transformations (p.6): \"...the back translation operator also employs the pre-trained language model... For the entity alias substitution operator, we apply ... TagMe... For each transformation, if the generated question is exactly the same as the original question, it is considered a failed transformation...\" \u2014 shows use of realistic NLP methods and sensible discard rules for trivial/failed transformations.\n\nSection 6.3 Threats to Validity (p.9): \"...it is impossible to guarantee the reliability of the generated question without manual inspection. To ensure the quality of the generated questions, most of the transformations are implemented with pre-trained language models ... Besides, we apply the existing ROUGE metric to evaluate the quality of the generated data and discard the low-quality generated questions directly.\" \u2014 acknowledges no manual/human evaluation and that quality control is via automatic metrics rather than a human study or external perceptual/fluency evaluation of final failing inputs."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper enforces input plausibility throughout the fuzzing process using two distinct, explicit mechanisms. First, after each mutation, a quality assessment is performed using the ROUGE-1 metric to measure the similarity between the original and transformed question. Generated questions that fall below a certain similarity threshold are discarded, ensuring that individual mutation steps do not drastically alter the input's semantic meaning. Second, to address the cumulative effect of mutations across iterations, the framework uses a 'perplexity priority' criterion to guide the selection of new seeds. This mechanism prioritizes generated questions that are more linguistically fluent and authentic (i.e., have lower perplexity) to be added back into the seed pool for further fuzzing. The combination of ROUGE-based filtering and perplexity-guided seed selection ensures that the generated inputs remain within a plausible distribution throughout the process. These quantitative, task-specific measures (ROUGE for similarity, perplexity for fluency) serve as the empirical evidence that the framework maintains realistic inputs, as required by the metric definition.",
                    "evidence": "1. Page 5, Section 3.2.5, Quality Assessment: \"To ensure the quality of generated questions, we employ the ROUGE-1 metric [35] to evaluate the questions generated by metamorphic transformations.\" \n2. Page 4, Algorithm 1: The algorithm shows that generated questions are only kept if their ROUGE score is above a threshold: \"if quality_score > \u03c4 then // acceptable D.append(s');\"\n3. Page 6, Section 3.3.2, Perplexity Priority: \"To ensure that the generated test questions have good authenticity and rationality, we consider applying perplexity priority as guidance for seed data retention.\"\n4. Page 7, Definition 3.5 (Perplexity priority): The framework selects the question with the lowest perplexity to feed back into the seed set: \"s** = arg min per(q) q\u2208G\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "QATest incorporates per\u2010step plausibility checks\u2014filtering each mutated question by a ROUGE\u20101 similarity threshold and prioritizing seeds with low perplexity to encourage fluent, realistic inputs\u2014but it does not formally bound cumulative drift across multiple iterations nor does it perform an explicit human or task\u2010specific quantitative evaluation of the final fault\u2010inducing inputs. Thus, while individual mutation steps are plausibility\u2010constrained, the framework relies on heuristics rather than a strict mechanism or empirical validation to ensure that deeply mutated inputs remain within the realistic question distribution, which aligns with the \u201cMedium\u201d level.",
                    "evidence": "Section 3.2.5 (Quality Assessment): uses ROUGE\u20101 to discard low\u2010quality transformations on each step\nSection 3.3.2 (Perplexity Priority): guides seed retention by selecting the lowest\u2010perplexity (most fluent) questions\nSection 6.3 (Threats to Validity): acknowledges absence of manual inspection or human study and reliance on automatic metrics rather than external plausibility validation of final failing inputs"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Checks quality drift between successive mutations but overlooks drift across many iterations",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "No cross model failure pattern analysis"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper evaluates QATest by applying its metamorphic transformations and guidance criteria separately to multiple QA systems (UnifiedQA, ALBERT, DrQA, MARL), but it does not present any experiments in which fault-inducing inputs crafted on one model are reused to test other models, nor does it describe design elements intended to discover or prioritize transferable attacks across models. The testing guidance (N-Gram coverage and perplexity priority) and seed selection are aimed at diversity and authenticity of generated questions, not at identifying inputs that transfer across different model implementations. Consequently, the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "1) Abstract (page 1, Abstract): \"To evaluate the performance of QATest, we experiment with it on four QA systems that are designed for different tasks. The experiment results show that the tests generated by QATest detect hundreds of erroneous behaviors of QA systems efficiently.\" \u2014 indicates per-system evaluation but does not claim cross-model reuse. \n\n2) Section 4.1 (page 7, \"Research Questions\"): The four RQs listed are RQ1 (Metamorphic Transformations), RQ2 (Testing Criteria), RQ3 (Test Diversity), and RQ4 (Case Study). None ask about or evaluate transferability across models: \"RQ1 (Metamorphic Transformations): How effective are different transformation methods for generating tests for various types of QA systems?... RQ4 (Case Study): Analysis of the bug types detected by QATest.\" \n\n3) Section 4.2.2 (page 8, \"QA systems\"): \"To evaluate the performance of QATest, we select four state-of-art QA systems as the testing subjects in our experiments. We briefly introduce them as follows: UnifiedQA ..., ALBERT ..., DrQA ..., MARL ...\" \u2014 shows multiple target systems but no description of crafting faults on one and reusing them on others. \n\n4) Section 3.3 (page 4, \"Guidance Criteria Design\") and Section 3.3.1/3.3.2 (N-Gram Coverage / Perplexity Priority): These criteria are defined to improve test diversity and authenticity (e.g., \"N-Gram coverage... select the particular question that can increase the richness of the seed set\" and \"perplexity priority... to ensure that the generated test questions have good authenticity\"), with no mention of mechanisms to identify or prioritize inputs likely to transfer across models."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper does not investigate whether the fault-inducing inputs generated for one model transfer to other models. The evaluation is conducted on four distinct Question Answering (QA) systems, but each is treated as an independent target. The experiments generate test cases from a seed dataset and apply them to a corresponding QA system to find failures. There is no mention of reusing the failure-inducing questions found on one model (e.g., ALBERT) to test another model (e.g., UnifiedQA). The research questions and experimental results focus on the effectiveness, efficiency, and diversity of the fuzzing framework for individual systems. The design of the fuzzer, including its guidance criteria (N-Gram coverage and perplexity priority), is aimed at improving bug detection on a single target rather than discovering shared vulnerabilities or transferable attacks across models.",
                    "evidence": "Page 8, Section 5.1, \"Answer to RQ1: Metamorphic Transformations\": \"To evaluate the effectiveness of each type of MR applied by QATest, we select 500 test cases from each data set and apply the MRs proposed in Section 3.2 to generate test data sets. Then, we record the number of failed tests in the generated test set on the corresponding QA systems, which is shown in Table 3.\" This statement describes an experimental process where each QA system is tested independently. The results presented in Tables 3, 4, and 5 consistently show separate evaluations for each system, with no cross-testing of generated inputs."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper evaluates QATest by applying its metamorphic transformations and guidance criteria separately to multiple QA systems (UnifiedQA, ALBERT, DrQA, MARL), but it does not present any experiments in which fault-inducing inputs crafted on one model are reused to test other models, nor does it describe design elements intended to discover or prioritize transferable attacks across models. The testing guidance (N-Gram coverage and perplexity priority) and seed selection are aimed at diversity and authenticity of generated questions, not at identifying inputs that transfer across different model implementations. Consequently, the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "1) Abstract (page 1, Abstract): \"To evaluate the performance of QATest, we experiment with it on four QA systems that are designed for different tasks. The experiment results show that the tests generated by QATest detect hundreds of erroneous behaviors of QA systems efficiently.\" \u2014 indicates per-system evaluation but does not claim cross-model reuse. \n\n2) Section 4.1 (page 7, \"Research Questions\"): The four RQs listed are RQ1 (Metamorphic Transformations), RQ2 (Testing Criteria), RQ3 (Test Diversity), and RQ4 (Case Study). None ask about or evaluate transferability across models: \"RQ1 (Metamorphic Transformations): How effective are different transformation methods for generating tests for various types of QA systems?... RQ4 (Case Study): Analysis of the bug types detected by QATest.\" \n\n3) Section 4.2.2 (page 8, \"QA systems\"): \"To evaluate the performance of QATest, we select four state-of-art QA systems as the testing subjects in our experiments. We briefly introduce them as follows: UnifiedQA ..., ALBERT ..., DrQA ..., MARL ...\" \u2014 shows multiple target systems but no description of crafting faults on one and reusing them on others. \n\n4) Section 3.3 (page 4, \"Guidance Criteria Design\") and Section 3.3.1/3.3.2 (N-Gram Coverage / Perplexity Priority): These criteria are defined to improve test diversity and authenticity (e.g., \"N-Gram coverage... select the particular question that can increase the richness of the seed set\" and \"perplexity priority... to ensure that the generated test questions have good authenticity\"), with no mention of mechanisms to identify or prioritize inputs likely to transfer across models."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Text level mutation "
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "QATest generates test inputs via semantic, metamorphic text transformations (MR1\u2013MR3) and uses text-based quality checks (ROUGE, perplexity) but does not explicitly account for standard I/O serialization effects (e.g., quantization, clipping, or encoding-induced loss) or enforce that failures persist across serialization. The method therefore relies on semantic-preserving metamorphic transformations (which the metric treats as providing only partial enforcement of reproducibility) without explicit handling of serialization effects, matching the metric's \"Medium\" category.",
                    "evidence": "Section 3.2 Test Generation with MRs (page 3\u20134): \"we implement three textual semantically consistent transformations, i.e., grammatical component-based, sentence structure-based, and adversarial perturbation-based transformations.\" \n\nSection 3.2.1 Test Oracle (page 4): \"Metamorphic Relations (MRs) ... We define a series of semantically consistent transformations T for questions q \u2208 Q. The generated question q' can be obtained by applying the transformation t \u2208 T to the original question q, and q' has the same intent and meaning as q. Therefore, the QA system should output the same answer for the q and q'.\" \n\nSection 3.2.5 Quality Assessment (page 5): \"To ensure the quality of generated questions, we employ the ROUGE-1 metric [35] to evaluate the questions generated by metamorphic transformations.\" \n\nSection 4.3 Sentence Transformations (page 7): \"Most of the transformations are implemented with a tool named nlpaug... We employ the WordNet library... BERT... TagMe...\" \n\n(Throughout the paper there is no mention of explicit handling of I/O serialization effects such as quantization, clipping, or re-encoding to verify that failures persist after standard serialization.)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The method proposed in the paper, QATest, generates test cases by applying metamorphic transformations directly to natural language text to create new questions. These transformations include operations like synonym replacement, reverse translation, introducing typos (keyboard mistakes), and simulating OCR errors. The resulting test inputs are new text strings. The metric definition explicitly states: \"We also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., synthesized speech, rendered images, or generated text) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.\" The perturbations introduced by QATest are at the character or word level (e.g., changing 'number' to 'numbfr'), which are discrete and not susceptible to the numerical fragility issues like rounding, clipping, or quantization that the metric is designed to guard against. By generating inputs directly in their final text format, the framework inherently ensures that any fault-inducing perturbations are stable and reproducible under standard I/O operations.",
                    "evidence": "Page 4, Section 3.2, \"Test Generation with MRs\": \"To simulate the possible question QA systems may receive in real usage scenarios, we implement three textual semantically consistent transformations, i.e., grammatical component-based, sentence structure-based, and adversarial perturbation-based transformations.\"\n\nPage 6, Table 1, \"Example questions with metamorphic transformations.\": This table provides concrete examples of the text-based mutations, such as 'Original question: What is a string over a Greek number when considering a computational problem?' being transformed into 'Keyboard mistake: What is a string over a Greek numbfr when considering a computational problem?'. This demonstrates the generation of discrete, reproducible text inputs."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "QATest generates mutated questions directly as text strings\u2014the final serialized format consumed by QA systems\u2014using semantic and adversarial perturbations at the character and word level. Because these inputs are already in their serialized form (plain text), they are inherently stable under standard I/O operations (no floating\u2010point quantization or clipping applies). This aligns with the metric\u2019s definition that approaches producing inputs directly in serialized formats (e.g., generated text) fully ensure failure reproducibility.",
                    "evidence": "Metric definition: \u201cWe also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., \u2026 generated text) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.\u201d\n\nPaper, Section 3.2 (page 4): \u201cwe implement three textual semantically consistent transformations \u2026 grammatical component\u2010based, sentence structure\u2010based, and adversarial perturbation\u2010based transformations.\u201d\n\nTable 1 (page 6): Examples of text\u2010level mutations (e.g., keyboard mistakes, synonym substitution) producing final question strings without floating\u2010point artifacts."
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Data-driven"
            ]
        }
    },
    "Wang et al. - 2022 - BET black-box efficient testing for convolutional neural networks": {
        "key": "wang2022bet",
        "name": "BET",
        "year": "2022",
        "citation_count": "26",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper describes BET as a black-box testing method that finds \"error-inducing inputs\" which cause mispredictions or inconsistencies (e.g., between a model and its quantized versions). All experiments and metrics (Err-Num, Label-Num, SR, Inact-Rate) focus on misclassification and disagreement detection and on improving model accuracy via fine-tuning. The paper does not describe discovering or exploiting safety/policy/security mechanisms (e.g., jailbreaks, data leakage, unauthorized actions) nor testing defended or safety-aligned models to bypass explicit safeguards. Therefore the failures uncovered are robustness errors (mispredictions/inconsistencies) rather than high- or medium-severity safety/security violations.",
                    "evidence": "Page 164, Abstract: \"It is important to test convolutional neural networks (CNNs) to identify defects (e.g. error-inducing inputs) before deploying them in security-sensitive scenarios.\" \n\nPage 164-165, Introduction: \"They aim at identifying defects of CNN models such as finding adversarial inputs which induce errors. These error-inducing inputs can be used to fine-tune the model to improve its classification accuracy.\" \n\nPage 166, Section 2.2 (Related Work): \"There are two common scenarios for CNN testing, i.e., differential testing (DT) [31, 46] and single model testing (ST) [12, 19]. DT finds error-inducing inputs which trigger behavior inconsistencies among multiple models... ST only focuses on one target model... an error-inducing input x_err is found if C(x_err) \u2260 l.\" \n\nPage 171, Section 4.2 (Evaluation of Black-Box Differential Testing): \"we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\" \n\nPage 172, Tables and Evaluation (4.3 & 4.4): metrics and results (Err-Num, Label-Num, SR, Inact-Rate) all measure misclassifications, label diversity, and success in finding error-inducing inputs; Section 4.4 shows such inputs are used to fine-tune models to \"improve target models\u2019 accuracy by 2 \u223c 3%.\" (no discussion of bypassing defenses or inducing unsafe/hallucinatory/toxic outputs)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, BET, focuses exclusively on identifying 'error-inducing inputs' which are defined as inputs that cause misclassifications or prediction inconsistencies. The entire methodology and evaluation are centered on finding inputs that change a model's prediction label (Single Model Testing) or cause disagreements between two models with similar functionality, such as a model and its quantized version (Differential Testing). The experiments are conducted on standard image classification tasks without any mention of safety, bias, toxicity, or other unsafe behaviors. The goal is to find robustness errors (mispredictions) efficiently, which can then be used to improve the model's classification accuracy. There is no evidence that the method is designed to or capable of uncovering unsafe behaviors or bypassing explicit security mechanisms.",
                    "evidence": "Page 4, Section 2.2, 'Related Work': 'There are two common scenarios for CNN testing, i.e., differential testing (DT) [31, 46] and single model testing (ST) [12, 19]. DT finds error-inducing inputs which trigger behavior inconsistencies among multiple models... For example, given two CNN models C1, C2 with similar functionality, an error-inducing input xerr is found if C1(xerr) \u2260 C2(xerr). ST only focuses on one target model C to find error-inducing inputs and requires labels for test data. For example, given a CNN model under test C, an original input x and the corresponding label l, an error-inducing input xerr is found if C(xerr) \u2260 l.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper describes BET as a black-box testing method that finds \"error-inducing inputs\" which cause mispredictions or inconsistencies (e.g., between a model and its quantized versions). All experiments and metrics (Err-Num, Label-Num, SR, Inact-Rate) focus on misclassification and disagreement detection and on improving model accuracy via fine-tuning. The paper does not describe discovering or exploiting safety/policy/security mechanisms (e.g., jailbreaks, data leakage, unauthorized actions) nor testing defended or safety-aligned models to bypass explicit safeguards. Therefore the failures uncovered are robustness errors (mispredictions/inconsistencies) rather than high- or medium-severity safety/security violations.",
                    "evidence": "Page 164, Abstract: \"It is important to test convolutional neural networks (CNNs) to identify defects (e.g. error-inducing inputs) before deploying them in security-sensitive scenarios.\" \n\nPage 164-165, Introduction: \"They aim at identifying defects of CNN models such as finding adversarial inputs which induce errors. These error-inducing inputs can be used to fine-tune the model to improve its classification accuracy.\" \n\nPage 166, Section 2.2 (Related Work): \"There are two common scenarios for CNN testing, i.e., differential testing (DT) [31, 46] and single model testing (ST) [12, 19]. DT finds error-inducing inputs which trigger behavior inconsistencies among multiple models... ST only focuses on one target model... an error-inducing input x_err is found if C(x_err) \u2260 l.\" \n\nPage 171, Section 4.2 (Evaluation of Black-Box Differential Testing): \"we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\" \n\nPage 172, Tables and Evaluation (4.3 & 4.4): metrics and results (Err-Num, Label-Num, SR, Inact-Rate) all measure misclassifications, label diversity, and success in finding error-inducing inputs; Section 4.4 shows such inputs are used to fine-tune models to \"improve target models\u2019 accuracy by 2 \u223c 3%.\" (no discussion of bypassing defenses or inducing unsafe/hallucinatory/toxic outputs)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Only reports failures without any insight on root-cause"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "norm-bounded"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces bounded, input-centric perturbations throughout its mutator design and explicitly uses an L_infty constraint (L\u221e = 13/255) for all methods, and the mutator logic ensures each pixel is toggled only between original \u00b1 \u03b5 (so cumulative drift beyond the bound is prevented). These design choices constrain mutations to visually small changes and maintain per-pixel plausibility. However, the paper does not provide any empirical assessment of the perceptual or semantic realism of the final fault-inducing inputs (no human study or task-specific perceptual/fluency metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity are reported). According to the metric definition, this corresponds to Medium: plausibility is enforced in mutation design (bounded/rule-based), but the paper omits explicit plausibility validation of final inputs.",
                    "evidence": "Section 4.1 Hyperparameters (page 171): \"We use the widely used L\u221e metric [4] to restrict perturbations for error-inducing inputs on all compared methods... i.e., L\u221e = 13/255 on all datasets (all compared methods use the same setting).\"; Section 3.5 Mutator, \"Perturbation values\" (page 169): \"...we limit the perturbations to each byte to two choices {+\u03b5, \u2212\u03b5}... Given an original input Eo, a current best test case Eb... if Eb[i] = Eo[i], we randomly add perturbation value of +\u03b5 or \u2212\u03b5 to this byte. If Eb[i] = Eo[i] \u2212 \u03b5, we change this byte to Eo[i] + \u03b5. Otherwise, Eb[i] = Eo[i] + \u03b5 must hold and we change this byte to Eo[i] \u2212 \u03b5.\" (shows per-pixel changes remain within \u00b1\u03b5 of original); Section 4.3 Result of Inact-Rate (page 172): \"The Inact-Rate of BET is 0.0%. The perturbations are directly added on pixel values as integers, BET does not suffer the inactivation issue.\" (confirms direct integer pixel edits and bounded changes); Section 4.1 Metrics (page 171): lists evaluation metrics as Err-Num, Label-Num, SR, (and later Inact-Rate) but contains no perceptual or human-evaluation measures (no LPIPS/SSIM/PESQ/STOI/Perplexity or human study reported)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility to a limited extent by using a bounded perturbation constraint. Specifically, it uses the L\u221e norm to restrict the magnitude of modifications applied to the original input images. This is a common technique to ensure the perturbed image does not deviate excessively from the original, thereby acting as a proxy for plausibility. However, the paper does not provide any empirical validation to confirm that the resulting error-inducing inputs are perceptually or semantically realistic. There is no mention of human studies or quantitative, task-specific metrics like SSIM or LPIPS to assess the visual quality of the generated inputs. The evaluation focuses exclusively on the number and diversity of errors found. Therefore, the method meets the criteria for \"Medium\" as it enforces plausibility through a bounded constraint but does not empirically validate the plausibility of the final outputs.",
                    "evidence": "Page 8, Section 4.1 Experimental Setup, subsection Hyperparameters: \"We use the widely used L\u221e metric [4] to restrict perturbations for error-inducing inputs on all compared methods. To set values of L\u221e, we first run all compared methods under different L\u221e for multiple times, and then we choose values that make them perform relatively well, i.e., L\u221e = 13/255 on all datasets (all compared methods use the same setting).\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces bounded, input-centric perturbations throughout its mutator design and explicitly uses an L_infty constraint (L\u221e = 13/255) for all methods, and the mutator logic ensures each pixel is toggled only between original \u00b1 \u03b5 (so cumulative drift beyond the bound is prevented). These design choices constrain mutations to visually small changes and maintain per-pixel plausibility. However, the paper does not provide any empirical assessment of the perceptual or semantic realism of the final fault-inducing inputs (no human study or task-specific perceptual/fluency metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity are reported). According to the metric definition, this corresponds to Medium: plausibility is enforced in mutation design (bounded/rule-based), but the paper omits explicit plausibility validation of final inputs.",
                    "evidence": "Section 4.1 Hyperparameters (page 171): \"We use the widely used L\u221e metric [4] to restrict perturbations for error-inducing inputs on all compared methods... i.e., L\u221e = 13/255 on all datasets (all compared methods use the same setting).\"; Section 3.5 Mutator, \"Perturbation values\" (page 169): \"...we limit the perturbations to each byte to two choices {+\u03b5, \u2212\u03b5}... Given an original input Eo, a current best test case Eb... if Eb[i] = Eo[i], we randomly add perturbation value of +\u03b5 or \u2212\u03b5 to this byte. If Eb[i] = Eo[i] \u2212 \u03b5, we change this byte to Eo[i] + \u03b5. Otherwise, Eb[i] = Eo[i] + \u03b5 must hold and we change this byte to Eo[i] \u2212 \u03b5.\" (shows per-pixel changes remain within \u00b1\u03b5 of original); Section 4.3 Result of Inact-Rate (page 172): \"The Inact-Rate of BET is 0.0%. The perturbations are directly added on pixel values as integers, BET does not suffer the inactivation issue.\" (confirms direct integer pixel edits and bounded changes); Section 4.1 Metrics (page 171): lists evaluation metrics as Err-Num, Label-Num, SR, (and later Inact-Rate) but contains no perceptual or human-evaluation measures (no LPIPS/SSIM/PESQ/STOI/Perplexity or human study reported)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Applies integer perturbations. Shows failure stability empricaclly. Shows ADAPT, DLFUZZ, DeepExplore failures are unstable"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly avoids the continuous\u2192discrete inactivation problem by generating perturbations directly on pixel/byte values (integers) and reports an Inact-Rate of 0.0%, which shows they consider discretization/clipping at the byte/pixel level during mutation and oracle evaluation. However, the paper does not explicitly discuss standard I/O serialization effects (e.g., image encoding/decoding, lossy formats, rounding introduced by file formats like JPEG/PNG or audio encoders) or any explicit handling/verification that failures remain after saving/loading inputs via common storage formats. This is a partial (but not full) treatment of serialization: they enforce reproducibility with respect to integer pixel representation and avoid rounding-inversion issues from gradient-based continuous perturbations, but they do not demonstrate explicit handling of file-format quantization/encoding effects. Under the metric definition, this corresponds to a Medium rating.",
                    "evidence": "Section 3.1 / p.166: \"Note that BET clearly departs from the white-box testing methods that first rely on gradient-based calculation and then transfer calculated decimals back to integers which may lead to ineffective samples (see inactivation issue in Section 4.3).\" Section 3.5 Mutator / p.169: \"Note each byte has 256 kinds of perturbations. Thus, for input with N pixels, the search space of perturbations is 256^N which is too large to explore efficiently. Instead, we limit the perturbations to each byte to two choices {+\u03b5, \u2212\u03b5}, where \u03b5 is a hyperparameter specific to application scenarios.\" Section 4.3 Results (Inact-Rate) / p.172: \"The Inact-Rate of BET is 0.0%. The perturbations are directly added on pixel values as integers, BET does not suffer the inactivation issue.\" Section 4.2 (Differential Testing setup) / p.170: discusses quantized models (8-bit/16-bit) via TensorFlow-Lite (model quantization), but there is no corresponding discussion in the methodology or evaluation about saving inputs to disk or explicitly modelling standard I/O encoding/decoding (PNG/JPEG/etc.) effects that might alter pixel values after serialization."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, BET, is explicitly designed to avoid the loss of fault-inducing perturbations during standard I/O operations by operating directly on integer pixel values. The authors identify this problem as the \"inactivation issue,\" where perturbations calculated in a continuous domain (floating-point) are lost when converted back to discrete integer values for storage or use. BET's mutation strategy involves adding integer values directly to the bytes of the input image, ensuring that the perturbations are inherently robust to rounding and quantization. The paper provides empirical evidence for this by measuring an \"Inact-Rate\" of 0.0% for BET, demonstrating that all discovered failures are reproducible after discretization, which directly addresses the core requirement of this metric.",
                    "evidence": "Page 10, Section 4.3, \"Result of Inact-Rate\": \"The Inact-Rate of BET is 0.0%. The perturbations are directly added on pixel values as integers, BET does not suffer the inactivation issue. However, baseline methods, especially the ones that use gradient-based methods suffer from this problem.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "While BET explicitly avoids the continuous\u2192discrete inactivation issue by generating and applying integer perturbations directly to pixel/byte values\u2014achieving an Inact-Rate of 0.0% and thus ensuring reproducibility with respect to in-memory discretization\u2014it does not address or evaluate any effects introduced by standard I/O serialization (e.g., image encoding/decoding, lossy formats, PNG/JPEG quantization). There is no discussion of writing inputs to disk or validating that failures persist after common file\u2010format operations. Hence, the paper provides only a partial treatment of serialization effects, corresponding to a Medium rating under the Failure Reproducibility metric.",
                    "evidence": "Section 4.3 (Inact-Rate 0.0%) shows integer\u2010level perturbations avoid inactivation when mapping floats to ints, but throughout Sections 3\u20134 there is no consideration of saving/loading via common storage formats (PNG/JPEG) or handling lossy encoding effects."
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "Does not show transferability to other models or common pattern between models. It uses differential testing to determine fault if two model versions gives inconsistent output"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly targets differential testing (DT) and includes a tunable objective function designed to maximize disagreements between a target model and other models (Equation 5). The authors construct surrogate/alternative implementations (quantized 8-bit and 16-bit variants) and generate fault-inducing inputs that cause disagreements between the original model and these other models; they report DT results (Tables 1\u20134) demonstrating many such cross-model disagreement inputs. Thus BET both (1) reuses/queries inputs across multiple models and (2) incorporates an explicit mechanism (the DT objective term summing differences with other models) to discover transferable failures across model implementations.",
                    "evidence": "Page 166, Section 2.2 (Related Work / definition of DT): \"There are two common scenarios for CNN testing, i.e., differential testing (DT) [31, 46] and single model testing (ST) [12, 19]. DT finds error-inducing inputs which trigger behavior inconsistencies among multiple models that have similar functionality ... For example, given two CNN models C1, C2 with similar functionality, an error-inducing input x_err is found if C1(x_err) \u2260 C2(x_err).\" \n\nPage 167, Section 3.3 (Tunable objective function for DT): \"Tunable objective function for DT. For DT, we hope to identify inputs that can maximize the prediction difference between the target model with other models.... The tunable objective function for DT is defined in Equation 5. DOF(x) = C(x)[l_i] + \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] |, ... The second term \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] | aims to find disagreements between the target model C with a batch of other models C\u0302 = {C\u0302_1, C\u0302_2, ...}, as such disagreements indicate error-inducing inputs are found.\" (Equation 5 and surrounding text)\n\nPage 170\u2013171, Section 4.2 (Evaluation: Constructing differential models / Results): \"Constructing differential models. ... We apply these two options to all models and we get corresponding 8-bits and 16-bits quantized models as shown in Table 1. Then, we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\" \n\nPage 171, Section 4.2 (Results): \"Results. Table 2, Table 3 and Table 4 show the results of DT on all datasets and models, respectively. Overall, under the same experimental conditions, BET significantly outperforms DiffChaser on all comparison metrics.\" (The presence of these DT-specific evaluation tables shows BET-generated inputs are evaluated across model variants to demonstrate cross-model disagreements.)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, BET, is explicitly designed and evaluated for a differential testing (DT) scenario, which aligns with the concept of attack transferability. The fuzzing design incorporates an explicit mechanism\u2014a tunable objective function for DT\u2014specifically to discover transferable failures by maximizing the prediction differences between a target model and other related models. The evaluation section is dedicated to demonstrating this capability, where generated inputs are systematically tested against pairs of models (an original and its quantized version) to find disagreements, which are a form of transferable failure.",
                    "evidence": "Page 6, Section 3.3, \"Tunable objective function for DT\": \"For DT, we hope to identify inputs that can maximize the prediction difference between the target model with other models... The tunable objective function for DT is defined in Equation 5... The second term \u03a3|C(x)[lo] \u2013 \u0108i(x)[lo]| aims to find disagreements between the target model C with a batch of other models \u0108 = {C1, C2, ...\u0108n}, as such disagreements indicate error-inducing inputs are found.\"\n\nPage 8, Section 4.2, \"Evaluation of Black-Box Differential Testing\": \"Constructing differential models. To get a convincing DT result, we directly use Tensorflow-Lite... to construct quantized models for DT... Then, we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly targets differential testing (DT) and includes a tunable objective function designed to maximize disagreements between a target model and other models (Equation 5). The authors construct surrogate/alternative implementations (quantized 8-bit and 16-bit variants) and generate fault-inducing inputs that cause disagreements between the original model and these other models; they report DT results (Tables 1\u20134) demonstrating many such cross-model disagreement inputs. Thus BET both (1) reuses/queries inputs across multiple models and (2) incorporates an explicit mechanism (the DT objective term summing differences with other models) to discover transferable failures across model implementations.",
                    "evidence": "Page 166, Section 2.2 (Related Work / definition of DT): \"There are two common scenarios for CNN testing, i.e., differential testing (DT) [31, 46] and single model testing (ST) [12, 19]. DT finds error-inducing inputs which trigger behavior inconsistencies among multiple models that have similar functionality ... For example, given two CNN models C1, C2 with similar functionality, an error-inducing input x_err is found if C1(x_err) \u2260 C2(x_err).\" \n\nPage 167, Section 3.3 (Tunable objective function for DT): \"Tunable objective function for DT. For DT, we hope to identify inputs that can maximize the prediction difference between the target model with other models.... The tunable objective function for DT is defined in Equation 5. DOF(x) = C(x)[l_i] + \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] |, ... The second term \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] | aims to find disagreements between the target model C with a batch of other models C\u0302 = {C\u0302_1, C\u0302_2, ...}, as such disagreements indicate error-inducing inputs are found.\" (Equation 5 and surrounding text)\n\nPage 170\u2013171, Section 4.2 (Evaluation: Constructing differential models / Results): \"Constructing differential models. ... We apply these two options to all models and we get corresponding 8-bits and 16-bits quantized models as shown in Table 1. Then, we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\" \n\nPage 171, Section 4.2 (Results): \"Results. Table 2, Table 3 and Table 4 show the results of DT on all datasets and models, respectively. Overall, under the same experimental conditions, BET significantly outperforms DiffChaser on all comparison metrics.\" (The presence of these DT-specific evaluation tables shows BET-generated inputs are evaluated across model variants to demonstrate cross-model disagreements.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "Uses differential testing based oracle to detect failure, not to check transferability from one to another.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential",
                "Property-based"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Zhang et al. - 2020 - Towards characterizing adversarial defects of deep learning software from the lens of uncertainty": {
        "key": "zhang2020towards",
        "name": "KuK",
        "year": "2020",
        "citation_count": "101",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": "Demostrates its ability to bypass adversarail defense mechanisms"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper's KuK fuzzer intentionally generates \"uncommon\" adversarial inputs that explicitly bypass existing adversarial defense mechanisms. The authors evaluate multiple defenses and report that these uncommon inputs reduce defense effectiveness substantially (e.g., large drops in detection/mitigation success, and concrete examples where defenses fail on the uncommon data). Because the method finds inputs that defeat explicit safety/security safeguards (adversarial defenses), it uncovers high-impact failures under the metric definition.",
                    "evidence": "Page 740, Introduction / Contributions: \"Our further evaluation reveals that the uncommon data generated by our method is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%.\" \n\nPage 740, Introduction / Contributions: \"For example, on the model NIN, the uncommon data achieve 97.5% success rate on bypassing the mutation-based defense, while the common data only make it 5.5%.\"\n\nPage 748, Section 5.2 (RQ4) / Answer to RQ4: \"Answer to RQ4: The uncommon data inputs are not well defended by existing defense techniques while the common data are relatively easier to be defended. In particular, the binary classifier and mutation-based detection approaches are less useful in defending the uncommon data inputs (e.g., with only 5% and 10% on NIN and ResNet-20, respectively).\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper proposes a method, implemented as a tool named KuK, to generate \"uncommon\" input data (adversarial examples) based on uncertainty metrics. A key part of the paper's evaluation is to test whether this generated uncommon data can bypass explicit, state-of-the-art defense mechanisms designed to protect models against adversarial attacks. The results demonstrate that the generated inputs successfully evade a suite of defense techniques\u2014including mutation-based detection, defensive distillation, feature squeezing, and binary classifiers\u2014with significantly higher success rates compared to common adversarial examples. This directly aligns with the 'High' severity rating, which requires uncovering failures by bypassing explicit safety or security mechanisms in defended models.",
                    "evidence": "Page 10, Section 5.2, \"RQ4: Evaluation on Defense Techniques\": \"To demonstrate the usefulness of the generated uncommon data in Table 5, this experiment intends to study whether the data can bypass the existing defense techniques. Setting. Since different defense techniques are proposed on different subject datasets, we selected popular techniques based on the datasets. For MNIST and CIFAR10 dataset, we selected the following defense techniques: binary activation classifier [13], mutation-based adversarial attack detection [53], defensive distillation [40], label smoothing [17], and feature squeezing [57]. For ImageNet, we selected the mutation-based adversarial attack detection [53], input transformations [15] and pixel deflection [43].\" \n\nPage 11, \"Answer to RQ4\": \"The uncommon data inputs are not well defended by existing defense techniques while the common data are relatively easier to be defended. In particular, the binary classifier and mutation-based detection approaches are less useful in defending the uncommon data inputs (e.g., with only 5% and 10% on NIN and ResNet-20, respectively).\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper's KuK fuzzer intentionally generates \"uncommon\" adversarial inputs that explicitly bypass existing adversarial defense mechanisms. The authors evaluate multiple defenses and report that these uncommon inputs reduce defense effectiveness substantially (e.g., large drops in detection/mitigation success, and concrete examples where defenses fail on the uncommon data). Because the method finds inputs that defeat explicit safety/security safeguards (adversarial defenses), it uncovers high-impact failures under the metric definition.",
                    "evidence": "Page 740, Introduction / Contributions: \"Our further evaluation reveals that the uncommon data generated by our method is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%.\" \n\nPage 740, Introduction / Contributions: \"For example, on the model NIN, the uncommon data achieve 97.5% success rate on bypassing the mutation-based defense, while the common data only make it 5.5%.\"\n\nPage 748, Section 5.2 (RQ4) / Answer to RQ4: \"Answer to RQ4: The uncommon data inputs are not well defended by existing defense techniques while the common data are relatively easier to be defended. In particular, the binary classifier and mutation-based detection approaches are less useful in defending the uncommon data inputs (e.g., with only 5% and 10% on NIN and ResNet-20, respectively).\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "aims to maximize model uncertainty and reveal generic misclassifications"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Shows correlatin"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "norm-bounded"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints during generation (bounded pixel perturbations and avoiding certain transformations) but does not provide any empirical evaluation (human study or perceptual/semantic similarity metrics) of the final fault-inducing inputs. Concretely, KuK\u2019s GA uses L_infty bounds and avoids affine/crossover operations that would create invalid images, which constrains individual mutation steps and initialization relative to the original seed. However, the authors do not report task-specific perceptual/semantic measures (e.g., SSIM/LPIPS) or any human evaluation to demonstrate that the final uncommon AEs/BEs remain perceptually or semantically realistic. This matches the metric\u2019s \u201cMedium\u201d level: plausibility is enforced by bounded/rule-based constraints but final-input plausibility is not validated empirically.",
                    "evidence": "Section 4 (Uncommon Data Generation) and Section 5.1 (RQ3 setting):\n\n1) Constraint on mutations / avoiding invalid transforms (Section 4, Population Initialization): \"In order to generate high-quality images (i.e., recognizable by human), we abandon the affine transformation (e.g., translation and rotation [55]) as the crossover may generate invalid images. We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\" (Section 4, p. 746)\n\n2) Explicit L\u221e mutation radius in evaluation settings (Section 5.1): \"For the mutation process, the radius of L\u221e is set as 0.3.\" (Section 5.1, p. 746)\n\n3) Lack of explicit perceptual / human plausibility assessment: the paper\u2019s evaluations (RQ3/RQ4) report counts, PCS/VRO statistics and defense success rates but contain no reported human study or task-specific perceptual/semantic metrics (e.g., SSIM, LPIPS, PESQ, STOI, or Perplexity) for final fault-inducing inputs. For example, the experimental descriptions and results focus on generated counts and PCS/VRO distributions (Tables 4\u20136) and defense bypass rates (Section 5.2), but no perceptual-quality or human-evaluation results are presented (Sections 3\u20135, pp. 743\u2013748)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility using a bounded constraint but does not empirically validate the plausibility of the final generated inputs. The method uses an L\u221e norm to constrain the allowable changes between the original seed image and its generated counterparts, with the explicit goal of generating \"high-quality images (i.e., recognizable by human)\". This use of a bounded perturbation is a mechanism to control plausibility. However, the paper provides no empirical assessment to confirm that this goal was achieved. There are no human studies, quantitative perceptual metrics (e.g., SSIM, LPIPS), or even qualitative examples of the final fault-inducing images to demonstrate their realism or semantic validity. This fulfills the criteria for a \"Medium\" rating, as it includes a plausibility enforcement mechanism but lacks the empirical validation of the final outputs required for a \"High\" rating.",
                    "evidence": "Page 8, Section 4, \"Population Initialization\": \"In order to generate high-quality images (i.e., recognizable by human), we abandon the affine transformation (e.g., translation and rotation [55]) as the crossover may generate invalid images. We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\"\nPage 9, Section 5.1, \"Setting\": \"For the mutation process, the radius of L\u221e is set as 0.3.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints during generation (bounded pixel perturbations and avoiding certain transformations) but does not provide any empirical evaluation (human study or perceptual/semantic similarity metrics) of the final fault-inducing inputs. Concretely, KuK\u2019s GA uses L_infty bounds and avoids affine/crossover operations that would create invalid images, which constrains individual mutation steps and initialization relative to the original seed. However, the authors do not report task-specific perceptual/semantic measures (e.g., SSIM/LPIPS) or any human evaluation to demonstrate that the final uncommon AEs/BEs remain perceptually or semantically realistic. This matches the metric\u2019s \u201cMedium\u201d level: plausibility is enforced by bounded/rule-based constraints but final-input plausibility is not validated empirically.",
                    "evidence": "Section 4 (Uncommon Data Generation) and Section 5.1 (RQ3 setting):\n\n1) Constraint on mutations / avoiding invalid transforms (Section 4, Population Initialization): \"In order to generate high-quality images (i.e., recognizable by human), we abandon the affine transformation (e.g., translation and rotation [55]) as the crossover may generate invalid images. We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\" (Section 4, p. 746)\n\n2) Explicit L\u221e mutation radius in evaluation settings (Section 5.1): \"For the mutation process, the radius of L\u221e is set as 0.3.\" (Section 5.1, p. 746)\n\n3) Lack of explicit perceptual / human plausibility assessment: the paper\u2019s evaluations (RQ3/RQ4) report counts, PCS/VRO statistics and defense success rates but contain no reported human study or task-specific perceptual/semantic metrics (e.g., SSIM, LPIPS, PESQ, STOI, or Perplexity) for final fault-inducing inputs. For example, the experimental descriptions and results focus on generated counts and PCS/VRO distributions (Tables 4\u20136) and defense bypass rates (Section 5.2), but no perceptual-quality or human-evaluation results are presented (Sections 3\u20135, pp. 743\u2013748)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": "Applies small floating point perturbations without clipping/rounding"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's fuzzing / test-generation approach operates by applying floating-point pixel-level mutations (noise, pixel crossover, L\u221e constraints) directly to in-memory image tensors and does not mention or enforce any constraints to preserve perturbations under standard I/O serialization (e.g., 8-bit quantization, clipping, rounding), nor does it generate inputs directly in serialized formats. Because the method neither accounts for serialization effects during test-case generation nor during oracle/evaluation, fault-inducing perturbations introduced in memory can be lost or altered when images are saved or encoded, so the paper ignores I/O effects and therefore rates Low on Failure Reproducibility.",
                    "evidence": "Page 746, Section 4 (Uncommon Data Generation) \u2014 \"Population Initialization. Given an input image, we first generate a set of images by randomly adding noise to it... We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\" \n\nPage 746, Section 4 (Uncommon Data Generation) \u2014 \"For the crossover, we adopt the tournament selection strategy ... The two selected images are used to perform the crossover by randomly exchanging the corresponding pixels. After the crossover, each image is randomly mutated by adding white noise, to increase the diversity of the population.\" \n\nPage 746, Section 5 (Evaluation, Implementation) \u2014 \"We implemented the proposed test generation tool, KuK, in Python based on Keras [5] (2.2.4) with TensorFlow [1] (1.12.0) as backend.\" \n\n(These passages show mutations are applied to image tensors via floating-point noise and pixel operations and evaluated in-memory using standard DL frameworks; the paper contains no discussion of serialization, quantization, clipping, saving to common image formats, or steps to ensure perturbations persist after typical I/O operations.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a genetic algorithm-based method, KuK, to generate test inputs by applying mutations. The primary mutation operators involve adding numerical noise to images. Specifically, the paper mentions \"randomly adding noise to it\" for population initialization and mutating images by \"adding white noise\" during the genetic algorithm's evolution phase. These operations manipulate the in-memory numerical representation of the inputs. The methodology does not describe any steps to account for serialization effects like quantization, rounding, or clipping that would occur when saving these perturbed images to standard file formats (e.g., 8-bit PNG). Consequently, the small, fault-inducing numerical perturbations are likely fragile and may not be reproducible after standard I/O operations, which aligns with the definition for a 'Low' rating.",
                    "evidence": "Page 8, Section 4, \"UNCOMMON DATA GENERATION\": \"Population Initialization. Given an input image, we first generate a set of images by randomly adding noise to it. In order to generate high-quality images (i.e., recognizable by human), we abandon the affine transformation (e.g., translation and rotation [55]) as the crossover may generate invalid images. We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\"\n\nPage 9, Section 4, \"UNCOMMON DATA GENERATION\": \"Crossover and Mutation. ... After the crossover, each image is randomly mutated by adding white noise, to increase the diversity of the population.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's fuzzing / test-generation approach operates by applying floating-point pixel-level mutations (noise, pixel crossover, L\u221e constraints) directly to in-memory image tensors and does not mention or enforce any constraints to preserve perturbations under standard I/O serialization (e.g., 8-bit quantization, clipping, rounding), nor does it generate inputs directly in serialized formats. Because the method neither accounts for serialization effects during test-case generation nor during oracle/evaluation, fault-inducing perturbations introduced in memory can be lost or altered when images are saved or encoded, so the paper ignores I/O effects and therefore rates Low on Failure Reproducibility.",
                    "evidence": "Page 746, Section 4 (Uncommon Data Generation) \u2014 \"Population Initialization. Given an input image, we first generate a set of images by randomly adding noise to it... We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\" \n\nPage 746, Section 4 (Uncommon Data Generation) \u2014 \"For the crossover, we adopt the tournament selection strategy ... The two selected images are used to perform the crossover by randomly exchanging the corresponding pixels. After the crossover, each image is randomly mutated by adding white noise, to increase the diversity of the population.\" \n\nPage 746, Section 5 (Evaluation, Implementation) \u2014 \"We implemented the proposed test generation tool, KuK, in Python based on Keras [5] (2.2.4) with TensorFlow [1] (1.12.0) as backend.\" \n\n(These passages show mutations are applied to image tensors via floating-point noise and pixel operations and evaluated in-memory using standard DL frameworks; the paper contains no discussion of serialization, quantization, clipping, saving to common image formats, or steps to ensure perturbations persist after typical I/O operations.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "evaluates several models independently but does not analyze cross-model consistency in failure behaviors."
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or measure whether fault-inducing inputs crafted on one model transfer to other models performing the same task. All experiments generate and evaluate adversarial / uncommon inputs per target model (attacks and KuK generation are applied to each model separately) and the evaluation of \"transfer\" in the paper is limited to applying inputs crafted on an original model to a defended/retrained version of that same model (a defense transfer scenario), not to distinct other model implementations or architectures. There is no reported experiment that reuses inputs generated on one surrogate model to test other (unseen) models or that explicitly measures cross-model transferability across different model implementations (e.g., LeNet-5 \u2192 NIN or ResNet \u2192 MobileNet).",
                    "evidence": "Page 740, Introduction (research questions): \"RQ4: To what extent are the uncommon samples defended by existing adversarial defense techniques compared with the common ones?\" \u2014 shows focus on defenses, not cross-model transfer.\n\nPage 743, Section 3.1.3 Data Preparation: \"For each dataset, we prepared the following three sets of data: one set of benign examples, one set of AEs generated from the attack methods, and one set of AEs generated by testing tools... For each input in BenignData, we generated four types of AEs with the four attack methods, resulting in a total of 36,000 AEs.\" \u2014 indicates attacks/AEs are generated per model/dataset but does not describe applying them to different model implementations.\n\nPage 748, Section 5.2 Results: \"The reason is that a new model is retrained with these defense techniques, while the attacks are generated regarding to the original one, making it a more challenging transfer attack scenario.\" \u2014 the only 'transfer' discussion refers to transfer to a retrained/defended version of the same model, not to other distinct models or architectures.\n\n(Throughout the Methods and Evaluation sections (e.g., Tables 2, 4, 5 and 6) adversarial/uncommon inputs are reported per model; no experiment or section describes testing inputs crafted on one model against other model implementations to measure cross-model transferability.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper evaluates whether the 'uncommon data' generated for an original model can bypass defense techniques that involve retraining the model (e.g., defensive distillation and label smoothing). The authors explicitly describe this as a 'transfer attack scenario', thus testing if the generated inputs (fault-inducing inputs) transfer to a different but related model. However, the proposed generation method, KuK, is not explicitly designed to discover transferable attacks. Its genetic algorithm is guided by fitness functions based on the uncertainty metrics of a single target model, without incorporating mechanisms to generalize the attack across multiple models.",
                    "evidence": "Page 11, Section 5.2 RQ4: Evaluation on Defense Techniques, 'For other defense techniques, the reduction in success rate appears smaller than that of binary classifier and mutation-based detection. For example, the success rates drop to 78.3% and 76.2% for defensive distillation and label smoothing on NIN. The reason is that a new model is retrained with these defense techniques, while the attacks are generated regarding to the original one, making it a more challenging transfer attack scenario. For example, defensive distillation retrains a more robust model by reducing the gradients. In this case, some of the data, which are uncommon for original model, become common data w.r.t. the retrained model, because of some weight variation. However, it still can be seen from the results that the uncommon data reveal stronger transferability.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not include any experiments or analysis of whether fault-inducing inputs generated on one model transfer to other, independently trained model implementations performing the same task. All adversarial and uncommon inputs are crafted and evaluated per target model (including defense-retrained variants of that same model), but no inputs generated on one architecture are tested on a different architecture or distinct model instance without access. Therefore it does not demonstrate cross-model transferability.",
                    "evidence": "Page 743 (\u00a73.1.3 Data Preparation): adversarial examples are generated \u201cfor each input in BenignData\u2026 on each model,\u201d with no mention of re-using those inputs on different models. \nPage 748 (\u00a75.2 RQ4): discussion of \u201ctransfer attack scenario\u201d refers only to attacks against a retrained/defended version of the same model, not to separate model implementations (e.g., LeNet-5\u2192NIN or ResNet\u2192MobileNet). \nThroughout (e.g. Tables 2, 4\u20136) all results report attacks and defenses per model, with no cross-model evaluation of transferability."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Tests whether the same inputs still cause failures on the defended model (RQ4)",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Pang et al. - 2022 - MDPFuzz testing models solving Markov decision processes": {
        "key": "pang2022mdpfuzz",
        "name": "MDPFuzz",
        "year": "2022",
        "citation_count": "39",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Medium",
                    "why": "Uncovers behaviorally unsafe yet functionally valid faults \u2014 crash-inducing policies within valid operational parameters. These expose specification-level safety violations but do not bypass explicit control or policy mechanisms. expose  safety critical violations that lead to crashes/collisions or unsafe actions in autonomous or multi-agent environments. reveals crash-triggering state sequences in models solving Markov Decision Processes (MDPs) across autonomous driving, aircraft collision avoidance, and robotic control settings. the failures are behaviorally unsafe but functionally valid \u2014 the agent is still \u201cplaying by the rules\u201d of the simulator. Does not bypass any safety or policy mechanisms. It exposes latent unsafe behaviors within unconstrained learned policies, revealing specification-level vulnerabilities rather than policy breaches."
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper demonstrates that MDPFuzz uncovers high-impact, safety-related failures (explicitly defined as crashes such as vehicle or airplane collisions) across multiple real-world MDP scenarios (autonomous driving, aircraft collision avoidance, multi-agent navigation, etc.). These are violations of user safety expectations (unsafe/undesired outputs). However, the paper does not claim to bypass explicit safety- or security-focused defenses, nor does it evaluate or report bypassing defended or safety-aligned models (e.g., adversarially trained or policy-gated systems) or revealing security-critical behaviors like data leakage or jailbreaks. Thus the appropriate classification is Medium: it uncovers unsafe, high-impact outputs but does not demonstrate bypassing explicit safety/security mechanisms.",
                    "evidence": "Page 378, Abstract: \"MDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states.\"; Page 378, Abstract: \"We evaluate five state-of-the-art models for solving MDPs, including supervised DNN, RL, IL, and multi-agent RL... During a 12-hour run, we find over 80 crash-triggering state sequences on each model.\"; Page 385, Section 6 (Testing Oracles (Crash Definition)): \"For DNN-based ACAS Xu, we check collisions of the DNN-controlled airplane with other airplanes.\"; Page 381, Section 4 (Definition 2): \"We regard crashes incurred from a valid and solvable initial state as bugs of a model pi solving MDP.\"; Page 385, Section 6 (MDP Initial State Sampling, Mutation, and Validation): \"We guarantee that there exist optimal actions to avoid the crash and we do not use an initial state that is not solvable.\"; Page 382, Section 5 (Assumptions): \"In blackbox settings, the internal of the agent model (our testing target) is not revealed to MDPFuzz... MDPFuzz has no access to the blackbox model internals.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper introduces MDPFuzz, a testing framework designed to find failures where models solving Markov decision processes (MDPs) enter \"abnormal and dangerous states.\" The methodology explicitly moves beyond finding simple robustness errors like prediction inconsistencies, which would fall under the 'Low' severity category. Instead, it defines its testing oracles based on concrete, unsafe behaviors that violate safety expectations for the given domain. For example, in autonomous driving, a failure is a vehicle collision; for aircraft collision avoidance, it is a collision between planes; for a walking agent, it is falling over. These are clear examples of unsafe behaviors that violate user intent and safety expectations. However, the paper does not describe the tested models as having explicit, separate safety or security mechanisms that are being bypassed (a requirement for 'High' severity). It finds failures in the primary, intended function of state-of-the-art models in safety-critical scenarios, thus fitting the 'Medium' category.",
                    "evidence": "Page 5, Section 4, \"Inconsistencies vs. Crashes\": \"Real-world models for solving MDPs can quickly recover from 'inconsistent' predictions. This demands the creation of oracles over concrete and severe abnormal states, such as collisions in airplane control or autonomous driving.\"\n\nPage 9, Section 6, \"Testing Oracles (Crash Definition)\": \"For RL and IL models in CARLA, we examine whether the model-controlled vehicle collides with other vehicles or buildings. For DNN-based ACAS Xu, we check collisions of the DNN-controlled airplane with other airplanes. For Coop Navi, we check collisions between MARL-controlled agents. For BipedalWalker, we check whether the RL-controlled walking agent falls (i.e., its head touches the ground).\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper demonstrates that MDPFuzz uncovers high-impact, safety-related failures (explicitly defined as crashes such as vehicle or airplane collisions) across multiple real-world MDP scenarios (autonomous driving, aircraft collision avoidance, multi-agent navigation, etc.). These are violations of user safety expectations (unsafe/undesired outputs). However, the paper does not claim to bypass explicit safety- or security-focused defenses, nor does it evaluate or report bypassing defended or safety-aligned models (e.g., adversarially trained or policy-gated systems) or revealing security-critical behaviors like data leakage or jailbreaks. Thus the appropriate classification is Medium: it uncovers unsafe, high-impact outputs but does not demonstrate bypassing explicit safety/security mechanisms.",
                    "evidence": "Page 378, Abstract: \"MDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states.\"; Page 378, Abstract: \"We evaluate five state-of-the-art models for solving MDPs, including supervised DNN, RL, IL, and multi-agent RL... During a 12-hour run, we find over 80 crash-triggering state sequences on each model.\"; Page 385, Section 6 (Testing Oracles (Crash Definition)): \"For DNN-based ACAS Xu, we check collisions of the DNN-controlled airplane with other airplanes.\"; Page 381, Section 4 (Definition 2): \"We regard crashes incurred from a valid and solvable initial state as bugs of a model pi solving MDP.\"; Page 385, Section 6 (MDP Initial State Sampling, Mutation, and Validation): \"We guarantee that there exist optimal actions to avoid the crash and we do not use an initial state that is not solvable.\"; Page 382, Section 5 (Assumptions): \"In blackbox settings, the internal of the agent model (our testing target) is not revealed to MDPFuzz... MDPFuzz has no access to the blackbox model internals.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "Targets a class of faults (collission or dangerous states)"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Provides correlation with internal coverage"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Applies bounded noise"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility during fuzzing by (1) restricting mutations to the MDP initial state rather than arbitrary intermediate states, (2) applying only small/bounded random perturbations, and (3) validating every mutated initial state using environment validation modules and scenario-specific bounds (e.g., speed limits). These design choices ensure mutated inputs remain realistic at each mutation step. However, the paper does not provide an explicit human evaluation or task-specific perceptual/semantic quality metrics (e.g., LPIPS/SSIM, PESQ/STOI, Perplexity) to empirically quantify the perceptual/semantic realism of the final crash-inducing inputs. Nor does it present quantitative perceptual measures for the final fault-inducing inputs; instead it reports internal neuron-activation differences and detection AUCs. Therefore the work fits the \"Medium\" level: plausibility is enforced in mutation/validation steps, but formal empirical perceptual assessments of final fault-inducing inputs are not reported.",
                    "evidence": "Introduction (p. 378): \"Second, instead of mutating arbitrary intermediate states, MDPFuzz only mutates the initial state conservatively to ensure that the sequence is realistic. The mutated initial states are also validated in a deliberate way to ensure realism.\"; Section 6, 'MDP Initial State Sampling, Mutation, and Validation' (p. 385): \"When sampling, the initial states are randomly selected from the state space P. MDPFuzz randomly mutates initial states with small noise. All the mutated states are validated in an automated and deliberate way (see below). That is, we confirm that all mutated initial states are valid and solvable.\"; Section 5 (Assumptions / Alg.1 description) (pp. 382\u2013383): \"We clarify that this assumption is realistic... MDPFuzz only mutates the initial state S0 of an MDP, under the constraint that it is still realistic and solvable after mutation. See Sec. 6 on how mutated initial states are validated in this study.\"; RQ3 Results (p. 387): \"we emphasize that the crashes found by MDPFuzz are not due to unrealistic states. Rather, they share similar visual appearances with normal states and can occur in real-world scenarios.\"; (Counter-evidence / absence) No sections report human studies or quantitative perceptual/linguistic quality metrics (e.g., LPIPS/SSIM, PESQ/STOI, Perplexity) for final fault-inducing inputs\u2014the evaluation focuses on neuron-activation patterns (t-SNE) and detection AUC (Section 7.3\u20137.4)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, MDPFuzz, is designed to maintain the realism of test inputs, which are state sequences in a Markov Decision Process (MDP). It achieves this by exclusively mutating the initial state of a sequence, arguing this preserves the natural temporal coherence of subsequent states generated by the environment's dynamics. Crucially, every mutated initial state undergoes an explicit, automated validation step to ensure it is both plausible and solvable within the task domain. For instance, in the CARLA driving simulator, it relies on the simulator's built-in validation to reject abnormal states, and for the ACAS Xu aircraft collision system, it applies domain-specific constraints like speed limits. This systematic validation of the inputs that trigger failure sequences fulfills a key requirement of the metric. However, the paper does not meet the 'High' criteria because it lacks an independent, empirical evaluation (e.g., a human study or quantitative perceptual metrics) to confirm the realism of the fault-inducing initial states; it only claims they are realistic because they pass the environment's validation checks.",
                    "evidence": "Page 9, Section 6, 'MDP Initial State Sampling, Mutation, and Validation': 'All the mutated states are validated in an automated and deliberate way (see below). That is, we confirm that all mutated states are valid and solvable.'\n\nPage 6, Section 4, 'Design Overview': 'Moreover, we only mutate the initial state rather than arbitrary states to generate more realistic scenarios... For each MDP scenario, we validate mutated initial states in a deliberate way to ensure they are realistic and solvable for an optimal model \u03c0*.'\n\nPage 9, Section 6, 'MDP Initial State Sampling, Mutation, and Validation': 'Particularly, in CARLA, we change the initial positions and angles of all 100 vehicles... Note that CARLA validates and rejects abnormal initial states: all mutated initial states passed its validation... In DNN-based ACAS Xu, we mutate the initial positions and speeds... Moreover, we bound the maximal speed of all airplanes below 1,100 ft/sec, which is within the range of normal speed... We guarantee that there exist optimal actions to avoid the crash and solve the initial states, and we do not use an initial state that is not solvable.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility during fuzzing by (1) restricting mutations to the MDP initial state rather than arbitrary intermediate states, (2) applying only small/bounded random perturbations, and (3) validating every mutated initial state using environment validation modules and scenario-specific bounds (e.g., speed limits). These design choices ensure mutated inputs remain realistic at each mutation step. However, the paper does not provide an explicit human evaluation or task-specific perceptual/semantic quality metrics (e.g., LPIPS/SSIM, PESQ/STOI, Perplexity) to empirically quantify the perceptual/semantic realism of the final crash-inducing inputs. Nor does it present quantitative perceptual measures for the final fault-inducing inputs; instead it reports internal neuron-activation differences and detection AUCs. Therefore the work fits the \"Medium\" level: plausibility is enforced in mutation/validation steps, but formal empirical perceptual assessments of final fault-inducing inputs are not reported.",
                    "evidence": "Introduction (p. 378): \"Second, instead of mutating arbitrary intermediate states, MDPFuzz only mutates the initial state conservatively to ensure that the sequence is realistic. The mutated initial states are also validated in a deliberate way to ensure realism.\"; Section 6, 'MDP Initial State Sampling, Mutation, and Validation' (p. 385): \"When sampling, the initial states are randomly selected from the state space P. MDPFuzz randomly mutates initial states with small noise. All the mutated states are validated in an automated and deliberate way (see below). That is, we confirm that all mutated initial states are valid and solvable.\"; Section 5 (Assumptions / Alg.1 description) (pp. 382\u2013383): \"We clarify that this assumption is realistic... MDPFuzz only mutates the initial state S0 of an MDP, under the constraint that it is still realistic and solvable after mutation. See Sec. 6 on how mutated initial states are validated in this study.\"; RQ3 Results (p. 387): \"we emphasize that the crashes found by MDPFuzz are not due to unrealistic states. Rather, they share similar visual appearances with normal states and can occur in real-world scenarios.\"; (Counter-evidence / absence) No sections report human studies or quantitative perceptual/linguistic quality metrics (e.g., LPIPS/SSIM, PESQ/STOI, Perplexity) for final fault-inducing inputs\u2014the evaluation focuses on neuron-activation patterns (t-SNE) and detection AUC (Section 7.3\u20137.4)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Perturbs continuous-valued initial state vectors within valid physical bounds and validates each mutated state through the simulator"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "MDPFuzz mutates MDP environment/state parameters (semantic, metamorphic transformations) rather than performing fine-grained in-memory tensor perturbations, so the generated inputs are produced by the simulator (rendered frames, environment states) and are inherently more stable than ephemeral floating-point tensor mutations. However, the paper does not describe any explicit handling or checks for standard I/O/serialization effects (quantization, rounding, clipping) nor does it claim to generate inputs directly in serialized formats to ensure persistence of fault-inducing perturbations. Under the metric definition, this corresponds to \u201csemantic-preserving metamorphic transformations without explicit enforcement of reproducibility under serialization,\u201d which maps to Medium.",
                    "evidence": "Section 4 (p. 381) \u2014 \"Mutating Initial States vs. Mutating Intermediate States. ... We thus decide to only mutate the initial state (e.g., re-arrange the environment at timestep 0), allowing to preserve the coherence of the entire MDP state sequence.\" \n\nSection 6 (p. 385) \u2014 \"We add random noise to the initial states and the noise type depends on type of the original data, e.g., adding small Gaussian float numbers from N(0, 1) to vehicles' initial positions, and adding small uniform integers from U(0, 4) to the ground type in BipedalWalker.\" \n\nSection 6 (p. 385) \u2014 \"Particularly, in CARLA, we change the initial positions and angles of all 100 vehicles, including the model-controlled vehicle. Note that CARLA validates and rejects abnormal initial states: all mutated initial states passed its validation.\" \n\n(Throughout the paper, mutation and validation are described as operating on environment/state parameters and simulator-rendered frames; there is no discussion of handling standard I/O serialization effects such as pixel quantization, rounding, clipping, nor any explicit mechanism to constrain perturbations to survive common serialization.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The method, MDPFuzz, mutates the initial state of a Markov Decision Process (MDP) by adding small numerical noise. For several of the evaluated scenarios, such as autonomous driving in CARLA and aircraft collision avoidance in ACAS Xu, the mutation involves perturbing continuous state variables (e.g., positions, speeds) with \"small Gaussian float numbers\". The paper emphasizes that mutated states are validated to ensure they are \"valid and solvable\" within the simulation environment. However, there is no mention of considering the effects of standard I/O operations like quantization, rounding, or clipping. The introduced floating-point perturbations are susceptible to being lost or altered during serialization, meaning the discovered failures might not be reproducible. The metric defines this as a 'Low' level of failure reproducibility because it ignores I/O effects, allowing fault-inducing perturbations to be lost after standard serialization.",
                    "evidence": "Page 9, Section 6, \"MDP Initial State Sampling, Mutation, and Validation\":\n\n- \"MDPFuzz randomly mutates initial states with small noise.\"\n- \"We add random noise to the initial states and the noise type depends on type of the original data, e.g., adding small Gaussian float numbers from N(0, 1) to vehicles\u2019 initial positions, and adding small uniform integers from U(0, 4) to the ground type in BipedalWalker.\"\n- \"All the mutated states are validated in an automated and deliberate way (see below). That is, we confirm that all mutated states are valid and solvable.\" (The subsequent text clarifies that validation ensures the state is solvable within the simulation, not that the perturbation is robust to I/O operations)."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "MDPFuzz\u2019s mutations operate at the level of environment/state parameters (e.g. shifting vehicle positions, angles, ground types) rather than injecting ephemeral floating-point noise directly into the model\u2019s tensor inputs. These are semantic, metamorphic transformations that naturally persist through the simulator\u2019s rendering pipeline (and any associated rounding or quantization) but the paper does not describe any explicit handling or analysis of standard I/O effects (such as pixel\u2010level quantization, rounding or clipping). Under the Failure Reproducibility metric, this precisely matches the \u201csemantic\u2010preserving metamorphic transformations without explicit enforcement of reproducibility under serialization\u201d category, which corresponds to a Medium rating.",
                    "evidence": "Section 4 (p. 381): \u201cMutating Initial States vs. Mutating Intermediate States. \u2026 We thus decide to only mutate the initial state (e.g., re-arrange the environment at timestep 0), allowing to preserve the coherence of the entire MDP state sequence.\u201d\n\nSection 6 (p. 385): \u201cMDPFuzz randomly mutates initial states with small noise. \u2026 adding small Gaussian float numbers from N(0, 1) to vehicles\u2019 initial positions, and adding small uniform integers from U(0, 4) to the ground type in BipedalWalker.\u201d\n\nMetric Definition: \u201cMedium: Considers serialization effects partially (e.g., clipping but not rounding), or applies semantic\u2010preserving metamorphic transformations without explicit enforcement of reproducibility under serialization.\u201d"
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "No cross model failure pattern analysis"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate attack transferability. MDPFuzz evaluates and finds crash-triggering initial states per target model (it reports crashes separately for five different models/scenarios), uses those findings to train per-model detectors and to repair individual models, and explicitly operates in blackbox settings without building surrogate models. The paper contains no experiment where fault-inducing inputs found on one model are reused to test another model, nor does it describe mechanisms (e.g., surrogate-model attacks, cross-model validation, or explicit cross-model transfer tests) intended to discover transferable attacks across different model implementations.",
                    "evidence": "Abstract (p.378): \"We evaluate five state-of-the-art models for solving MDPs, including supervised DNN, RL, IL, and multi-agent RL.\" (shows multiple models were evaluated but not that inputs were reused across them). \n\nSection 7.1 RQ1 / Results (p.386): \"Table 3 summarizes the findings of fuzzing each model... Overall, MDPFuzz detects a significant number of crashes from all test cases.\" (reports per-model crash counts; no cross-model reuse reported). \n\nSection 5 Assumptions (Alg.1 / p.382): \"In blackbox settings, the internal of the agent model (our testing target) is not revealed to MDPFuzz... MDPFuzz can collect the state sequence {S_t} and obtain the cumulative reward r from the environment.\" (describes blackbox testing and lack of building surrogate/internal models). \n\nSection 7.4 (p.387): \"we repair the models used by DNN-based ACAS Xu with the findings of MDPFuzz...\" (describes per-model repair using findings, not transfer of fault-inducing inputs to other models). \n\nSection 8 Discussion (p.388): \"Another threat is that bugs may be potentially overlooked since MDPFuzz only mutates the entry states (Sec. 5). Nevertheless ...\" (limits of their approach are discussed; no claim of enabling transferable/substitute-model attacks)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper evaluates the proposed fuzzer, MDPFuzz, on five distinct models, including a Reinforcement Learning (RL) and an Imitation Learning (IL) model for the same autonomous driving task (CARLA). However, the evaluation is conducted independently for each model. The paper reports the number of crash-triggering inputs found for each model in isolation and does not perform any experiments to test if the inputs that cause a failure in one model also cause a failure in another model, even those performing the same task. The design of MDPFuzz focuses on finding failures for a single model under test and lacks any mechanism explicitly designed to discover transferable attacks or shared vulnerabilities across different models.",
                    "evidence": "Page 10, Section 7.1 RQ1: Performance on Finding Crashes, Setup: \"We use the evaluation setup described in Sec. 6. That is, we launch MDPFuzz to fuzz each MDP model (listed in Table 3) and detect crashes. We collect all error-triggering inputs for analysis.\" This statement indicates that each model was fuzzed as a separate, independent target. Table 3 on the same page presents the results for each model as a distinct line item, with no cross-model evaluation or mention of transferring discovered failure-inducing inputs."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate attack transferability. MDPFuzz evaluates and finds crash-triggering initial states per target model (it reports crashes separately for five different models/scenarios), uses those findings to train per-model detectors and to repair individual models, and explicitly operates in blackbox settings without building surrogate models. The paper contains no experiment where fault-inducing inputs found on one model are reused to test another model, nor does it describe mechanisms (e.g., surrogate-model attacks, cross-model validation, or explicit cross-model transfer tests) intended to discover transferable attacks across different model implementations.",
                    "evidence": "Abstract (p.378): \"We evaluate five state-of-the-art models for solving MDPs, including supervised DNN, RL, IL, and multi-agent RL.\" (shows multiple models were evaluated but not that inputs were reused across them). \n\nSection 7.1 RQ1 / Results (p.386): \"Table 3 summarizes the findings of fuzzing each model... Overall, MDPFuzz detects a significant number of crashes from all test cases.\" (reports per-model crash counts; no cross-model reuse reported). \n\nSection 5 Assumptions (Alg.1 / p.382): \"In blackbox settings, the internal of the agent model (our testing target) is not revealed to MDPFuzz... MDPFuzz can collect the state sequence {S_t} and obtain the cumulative reward r from the environment.\" (describes blackbox testing and lack of building surrogate/internal models). \n\nSection 7.4 (p.387): \"we repair the models used by DNN-based ACAS Xu with the findings of MDPFuzz...\" (describes per-model repair using findings, not transfer of fault-inducing inputs to other models). \n\nSection 8 Discussion (p.388): \"Another threat is that bugs may be potentially overlooked since MDPFuzz only mutates the entry states (Sec. 5). Nevertheless ...\" (limits of their approach are discussed; no claim of enabling transferable/substitute-model attacks)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided",
                "Data-driven"
            ]
        }
    }
}