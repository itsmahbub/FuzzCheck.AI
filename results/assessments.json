{
    "Liu et al. - 2024 - AUTODAN GENERATING STEALTHY JAILBREAK PROMPTS ON ALIGNED LARGE LANGUAGE MODELS": {
        "key": "liuautodan",
        "name": "AutoDAN",
        "year": "2024",
        "citation_count": "834",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "High",
                    "why": "Guides towared output that start with a predified content"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Reports what fails without analyzing why. It provides tranferability rationale, not failure patterns or root cause"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "High",
                    "why": "Mutation does not rely on guidance from the model and shows transferability"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper both (1) reuses fault-inducing prompts crafted on one model to test other models (explicit cross-model transfer experiments and reported ASR numbers), and (2) designs the fuzzing/optimization to produce semantically meaningful, lexical-level prompts (hierarchical GA, LLM-based diversification, momentum word scoring) which the authors argue and empirically show improves transfer to black-box models. Thus it demonstrates reuse across models and includes explicit mechanisms intended to discover transferable failures.",
                    "evidence": "Page 1, Abstract: \"Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability...\"; Page 8, Section 4.2 Results - Transferability: \"We conduct the evaluations by taking the jailbreak prompts with their corresponding requests and targeting another LLM. The results are shown in Tab. 2. AutoDAN exhibits a much better transferability in attacking the black-box LLMs compared with the baseline.\"; Page 8, Table 2 (Cross-model transferability): entries showing transfer ASR (examples) \u2014 \"Vicuna-7B -> Guanaco-7b: AutoDAN-HGA ASR 0.7058 Recheck 0.6712\" and \"Llama2-7b-chat -> Vicuna-7B: AutoDAN-HGA ASR 0.7288 Recheck 0.7019\"; Page 8, Table 2 caption / discussion: \"We hypothesize that this is because the AutoDAN generates prompts at a semantic level without relying on direct guidance from gradient information on the tokens, thereby avoiding overfitting on white-box models.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper explicitly evaluates the transferability of its generated jailbreak prompts. It conducts cross-model experiments where prompts generated on a source model are tested against other target models, and the results are presented in a dedicated table (Table 2). However, the design of the attack method, AutoDAN, does not incorporate explicit mechanisms aimed at discovering transferable attacks. The method uses a hierarchical genetic algorithm to generate semantically meaningful prompts. The authors speculate that the observed high transferability is a beneficial consequence of their semantic-level optimization approach, which avoids the overfitting common in gradient-based methods, rather than being an explicit design goal with specific supporting mechanisms.",
                    "evidence": "Page 8, Section 'Transferability': 'We further investigate the transferability of our method... We conduct the evaluations by taking the jailbreak prompts with their corresponding requests and targeting another LLM. The results are shown in Tab. 2. AutoDAN exhibits a much better transferability in attacking the black-box LLMs compared with the baseline. We speculate that the potential reason is the semantically meaningful jailbreak prompts may be inherently more transferable than the methods based on tokens\u2019 gradients.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "While the paper clearly evaluates whether fault-inducing prompts generated on one model transfer to others (via cross-model experiments in Section 4.2 and Table 2), the core AutoDAN design does not include mechanisms explicitly tailored for transferability (e.g. multi-model surrogate optimization or fitness functions that aggregate performance across targets). The authors observe that semantic-level optimization \u2018inherently\u2019 improves transfer, but they do not incorporate transferability as an explicit objective or design feature of their hierarchical genetic algorithm.",
                    "evidence": "Page 8, Section 4.2 (\u201cTransferability\u201d): \u201cWe conduct the evaluations by taking the jailbreak prompts with their corresponding requests and targeting another LLM. The results are shown in Tab. 2. AutoDAN exhibits a much better transferability \u2026 We speculate that the potential reason is the semantically meaningful jailbreak prompts may be inherently more transferable\u2026\u201d; Table 2 \u2013 cross-model ASR results; no mention of multi-model or transfer-focused optimization in Method (Sections 3.2\u20133.4)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Shows transferabilty but the fuzzing design is not explicit about trasferable attack discovery. It is a byproduct and the authors gives a hypothesis why they transfer",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Rule-based",
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "He et al. - 2024 - Curiosity-Driven Testing for Sequential Decision-Making Process": {
        "key": "he2024curiosity",
        "name": "CureFuzz",
        "year": "2024",
        "citation_count": "12",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Medium",
                    "why": "Uncovers crash-triggering scenarios (e.g., collisions, falls, or loss of control) across autonomous driving, aviation, and robotics systems. exposes behaviorally unsafe yet functionally valid failures \u2014 revealing unintended consequences of learned policies rather than breaches of safety enforcement mechanisms."
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "Steer toward safetly faults (crash-triggering scenarios) in in autonomous driving and other sequential decision-making systems"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Mutates continuous feature vectors representing simulator states (positions, angles, velocities) and enforces validity checks to keep them within legal bounds."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper contains no experiments or claims that fault-inducing inputs found on one model were reused to test other models for the same task, nor does CureFuzz include design elements aimed at producing transferable attacks across different model implementations. The evaluation applies CureFuzz separately to multiple SDMs and uses found scenarios to repair the same model (fine-tuning + re-test), but it does not measure whether seeds discovered on a surrogate transfer to unseen targets or test multiple implementations with the same seeds.",
                    "evidence": "Introduction (quote): \"We evaluate CureFuzz by applying it to well-known SDMs that use various learning algorithms. ... The experimental results demonstrate that CureFuzz effectively and efficiently identifies a significant number of catastrophic failures across all considered SDMs.\" (Introduction) \u2014 shows per-model evaluation but no claim of cross-model reuse.\n\nSection 5, RQ3 (quote): \"we utilize CureFuzz again to test the newly fine-tuned SDM. For the repaired ACAS Xu, 55 faults and 4 distinct types of crash-triggering scenarios are detected. We find that the crashes found by CureFuzz are reduced by 73% compared with the original DNN model.\" (Section 5, RQ3) \u2014 demonstrates using found scenarios to repair and retest the same model, not transfer to other models.\n\nSection 4.1 Research Questions (quote): \"To evaluate the performance of CureFuzz and comprehensively understand the impact, we formulate the three research questions: RQ1: How effective is CureFuzz in finding crash-triggering scenarios? RQ2: Can CureFuzz be effectively guided with the curiosity mechanism? RQ3: Can we use the crashes found by CureFuzz to improve SDMs?\" (Section 4.1) \u2014 the listed RQs do not include any question about transferability of fault-inducing inputs across different model implementations."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper introduces CUREFUZZ, a fuzzing method to find crash-triggering scenarios for sequential decision-makers (SDMs). The evaluation is conducted across multiple SDMs in various environments (e.g., two different models for the CARLA autonomous driving task, one based on DRL and one on IL). However, each SDM is treated as an independent and isolated target for the fuzzer. The experiments aim to demonstrate the effectiveness of the CUREFUZZ method on each target individually compared to baseline methods. The paper does not perform any experiments to test whether the crash-triggering scenarios discovered for one SDM would also cause failures (i.e., transfer) to another SDM performing the same task. The design of the fuzzer is focused on finding novel and diverse scenarios for the specific model under test, with no explicit mechanisms for discovering transferable failures.",
                    "evidence": "Page 8, Section 5 EXPERIMENT RESULTS, Tables 1, 2, and 3: The results for different models, even within the same environment, are presented in separate rows. For instance, 'Carla (RL)' and 'Carla (IL)' are evaluated independently. The paper discusses the performance of CUREFUZZ on each of these models but does not mention reusing the crash scenarios found for 'Carla (RL)' to test 'Carla (IL)' or vice-versa. The research questions also do not address transferability. For example, RQ1 asks: \"How effective is CUREFUZZ in finding crash-triggering scenarios?\" which is evaluated per model, not across models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper contains no experiments or claims that fault-inducing inputs found on one model were reused to test other models for the same task, nor does CureFuzz include design elements aimed at producing transferable attacks across different model implementations. The evaluation applies CureFuzz separately to multiple SDMs and uses found scenarios to repair the same model (fine-tuning + re-test), but it does not measure whether seeds discovered on a surrogate transfer to unseen targets or test multiple implementations with the same seeds.",
                    "evidence": "Introduction (quote): \"We evaluate CureFuzz by applying it to well-known SDMs that use various learning algorithms. ... The experimental results demonstrate that CureFuzz effectively and efficiently identifies a significant number of catastrophic failures across all considered SDMs.\" (Introduction) \u2014 shows per-model evaluation but no claim of cross-model reuse.\n\nSection 5, RQ3 (quote): \"we utilize CureFuzz again to test the newly fine-tuned SDM. For the repaired ACAS Xu, 55 faults and 4 distinct types of crash-triggering scenarios are detected. We find that the crashes found by CureFuzz are reduced by 73% compared with the original DNN model.\" (Section 5, RQ3) \u2014 demonstrates using found scenarios to repair and retest the same model, not transfer to other models.\n\nSection 4.1 Research Questions (quote): \"To evaluate the performance of CureFuzz and comprehensively understand the impact, we formulate the three research questions: RQ1: How effective is CureFuzz in finding crash-triggering scenarios? RQ2: Can CureFuzz be effectively guided with the curiosity mechanism? RQ3: Can we use the crashes found by CureFuzz to improve SDMs?\" (Section 4.1) \u2014 the listed RQs do not include any question about transferability of fault-inducing inputs across different model implementations."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ],
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "Zhou et al. - 2025 - Understanding the Effectiveness of Coverage Criteria for Large Language Models A Special Angle from": {
        "key": "zhou2025understanding",
        "name": "Zhou et al.",
        "year": "2025",
        "citation_count": "7",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "GPT-4 generates  new jailbreak queries through prompt rewriting. The generated contents are supposed to be natural but there was no measure reported how natural are those. The attack suffix is related to jailbreak detection, not creation"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "No analysis of fault transferability in input level of mutation level"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether fault-inducing inputs (jailbreak prompts) crafted on one model transfer to other models. The generation experiments and the coverage-guided jailbreak-case generation are conducted targeting a single model (Llama-2-7b-chat). Detection and prioritization experiments are performed per-model (training/validating classifiers or thresholds for each model), but the paper does not report experiments where inputs crafted on a surrogate model are reused to test different target models, nor does it describe mechanisms explicitly designed to discover transferable attacks across models.",
                    "evidence": "Section V.C (Jailbreak Case Generation), p. 739: \"Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round ... Each method ultimately generates 250 new jailbreak queries to evaluate effectiveness.\"; Section V.A (Jailbreak Detection), p. 738: \"... we train an MLP model that uses the number of activated neurons as input ... We validate its effectiveness on OPT-125M, Llama-2-7B-Chat, and Pythia-12B.\"; Section III.A (Motivation: A Cluster Analysis Experiment), p. 732: \"Experimental Setup: We first introduce the setup for our cluster analysis experiments using the Llama-2-7b-chat [38] model as our target.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes and evaluates methods for jailbreak detection, prioritization, and generation using coverage criteria. The experiments are conducted across four different LLMs. However, the evaluation of fault-inducing inputs (jailbreak attacks) is performed on a per-model basis. Specifically, in the 'Jailbreak Case Generation' application, new jailbreak queries are generated using coverage guidance from a single target model (Llama-2-7b-chat). The paper evaluates the success of this generation method for that specific model but does not subsequently test whether these generated queries are effective (i.e., transferable) in causing failures in the other models studied in the paper (OPT-125M, Pythia-12B, Gemma-2-27B-it). The research does not demonstrate or test whether fault-inducing inputs discovered on one model trigger failures in others.",
                    "evidence": "Page 11, Section V. APPLICATION, C. Jailbreak Case Generation: \"Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting. The query with the highest coverage increase is selected as the next seed in the coverage-guided approach. For comparison, a random strategy selects seeds randomly from rewritten candidates. Each method ultimately generates 250 new jailbreak queries to evaluate effectiveness.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether fault-inducing inputs (jailbreak prompts) crafted on one model transfer to other models. The generation experiments and the coverage-guided jailbreak-case generation are conducted targeting a single model (Llama-2-7b-chat). Detection and prioritization experiments are performed per-model (training/validating classifiers or thresholds for each model), but the paper does not report experiments where inputs crafted on a surrogate model are reused to test different target models, nor does it describe mechanisms explicitly designed to discover transferable attacks across models.",
                    "evidence": "Section V.C (Jailbreak Case Generation), p. 739: \"Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round ... Each method ultimately generates 250 new jailbreak queries to evaluate effectiveness.\"; Section V.A (Jailbreak Detection), p. 738: \"... we train an MLP model that uses the number of activated neurons as input ... We validate its effectiveness on OPT-125M, Llama-2-7B-Chat, and Pythia-12B.\"; Section III.A (Motivation: A Cluster Analysis Experiment), p. 732: \"Experimental Setup: We first introduce the setup for our cluster analysis experiments using the Llama-2-7b-chat [38] model as our target.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Provides correlation with internal coverage"
                }
            }
        },
        "taxonomy": {
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "Gong et al. - PaPillon Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs": {
        "key": "gong2025papillon",
        "name": "PAPILLON",
        "year": "2025",
        "citation_count": "0",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "Optimizes toward semantic policy-violation behavior (jailbreak) but not a specific output."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Reports only observable failures but no investigation of root-cause"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Mutate in text level"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Medium",
                    "why": "Performs experiments on transferability"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly evaluates and demonstrates that fault-inducing prompts generated by PAPILLON on one model can be reused to successfully attack other models (cross-model and multimodal transfer experiments are reported). However, the PAPILLON design and mutation operators are described with goals of semantic coherence, short prompt length, and seed-pool/fuzzing effectiveness rather than with explicit mechanisms specifically intended to discover or maximize transferability across different model implementations (no surrogate-model training, ensemble surrogate testing, or transfer-focused mutators are described). Therefore the work shows reuse/testing of fault-inducing inputs for transferability but does not present an explicit design aimed at producing transferable attacks.",
                    "evidence": "Page 2401, Abstract: \"Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses.\"; Page 2412, Section 5.4 (Attack Transferability): \"We further investigate the transferability of PAPILLON. Transferability refers to the extent to which prompts designed to jailbreak the target model can successfully jailbreak another model.\" and \"Overall, PAPILLON demonstrates notable attack transferability across different models.\"; Page 2420, Table 17 (Cross-model transferability): shows the cross-model success-rate matrix (e.g., \"Baichuan2-7B -> Vicuna-7B-v1.3: 70%\", many non-zero cross-model ASR entries); Page 2407, Section 4.2 (Template Mutation and Optimization): \"we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.\" (describes focus on semantic coherence and prompt length rather than explicit transferability mechanisms)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper extensively evaluates the transferability of its generated jailbreak prompts. It includes a specific section (5.4 Attack Transferability) and tables (Table 5 and Table 17) demonstrating that prompts created for a source model can successfully trigger failures in other target models, including different LLMs and even multimodal models (MLLMs). This confirms that fault-inducing inputs are reused to test transferability. However, the paper's methodological description of the PAPILLON framework does not mention any explicit design choices or mechanisms specifically aimed at discovering or enhancing transferable attacks. The design goals of the mutation operators focus on maintaining semantic coherence and shortening prompt length to evade defenses and reduce costs, not on cross-model generalization. Therefore, while transferability is a demonstrated outcome, it is not an explicit goal of the fuzzing design itself.",
                    "evidence": "Page 13, Section 5.4 Attack Transferability: \"We further investigate the transferability of PAPILLON. Transferability refers to the extent to which prompts designed to jailbreak the target model can successfully jailbreak another model. ... LLM transferability. We first evaluate transferability by using jailbreak prompts generated by the source model to target another LLM. The results are shown in Table 17 (appendix). ... MLLM transferability. We then assess the transferability of the PAPILLON-generated jailbreak prompts to multimodal large language models (MLLMs). ... The results are presented in Table 5.\"\n\nPage 9, Section 4.2 Template Mutation and Optimization: \"Optimization schemes. When optimizing the jailbreaking templates, we control both semantic coherence and prompt length through system prompt instructions and the hyperparameter of the output length respectively.\" This section details the design goals of the attack generation, which do not include transferability."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper explicitly evaluates and demonstrates that fault-inducing prompts generated by PAPILLON on one model can be reused to successfully attack other models (cross-model and multimodal transfer experiments are reported). However, the PAPILLON design and mutation operators are described with goals of semantic coherence, short prompt length, and seed-pool/fuzzing effectiveness rather than with explicit mechanisms specifically intended to discover or maximize transferability across different model implementations (no surrogate-model training, ensemble surrogate testing, or transfer-focused mutators are described). Therefore the work shows reuse/testing of fault-inducing inputs for transferability but does not present an explicit design aimed at producing transferable attacks.",
                    "evidence": "Page 2401, Abstract: \"Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses.\"; Page 2412, Section 5.4 (Attack Transferability): \"We further investigate the transferability of PAPILLON. Transferability refers to the extent to which prompts designed to jailbreak the target model can successfully jailbreak another model.\" and \"Overall, PAPILLON demonstrates notable attack transferability across different models.\"; Page 2420, Table 17 (Cross-model transferability): shows the cross-model success-rate matrix (e.g., \"Baichuan2-7B -> Vicuna-7B-v1.3: 70%\", many non-zero cross-model ASR entries); Page 2407, Section 4.2 (Template Mutation and Optimization): \"we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.\" (describes focus on semantic coherence and prompt length rather than explicit transferability mechanisms)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ]
        }
    },
    "Du et al. - 2018 - DeepCruiser Automated Guided Testing for Stateful Deep Learning Systems": {
        "key": "du2018deepcruiser",
        "name": "DeepCruiser",
        "year": "2018",
        "citation_count": "45",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Provides heuristic explanations by showing correlation with state variation"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Ensures each audio is mutated at most once per transformation category (volume/speed/clearness) to prevent unnatural accumulation but does not measure naturalness on the final fuzzed inputs. The human validation is on single step transformation"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Uses stable, perceptual transformations (volume, speed, clearness) likely to survive saving, though persistence isn\u2019t explicitly verified."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "mutation and coverage strategies are tightly coupled to a specific RNN-based ASR model and training setup"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper evaluates DeepCruiser only on a single RNN-based ASR implementation (Mozilla DeepSpeech-0.3.0) and does not report reusing fault-inducing inputs to test other models nor describe design mechanisms explicitly intended to discover transferable attacks across different models. Therefore it does not demonstrate whether faults found on one model transfer to other models performing the same task.",
                    "evidence": "Page 16, Section 7.1 Datasets and Experiment Setup: \"We selected Mozilla\u2019s implementation of DeepSpeech-0.3.0 [42]... All our experiments were conducted on a pre-trained English model released along with DeepSpeech-0.3.0, which was trained with Fisher [43], LibriSpeech [44], Switchboard [45], and a pre-release snapshot of the English Common Voice training corpus [46].\"; Page 3, Introduction / contribution summary: \"We implement our testing framework, DeepCruiser, and perform in-depth evaluation on a state-of-the-art practical RNN based ASR system to demonstrate the usefulness.\" (demonstrates evaluation on a single ASR system and no cross-model transfer tests are reported)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's entire evaluation is conducted on a single, specific deep learning model for Automated Speech Recognition (ASR). The authors explicitly state they use Mozilla's DeepSpeech-0.3.0 model for all their experiments. The goal of the proposed framework, DeepCruiser, is to generate tests that increase coverage and find defects within this specific model. The paper does not mention testing the fault-inducing audio inputs generated for DeepSpeech on any other ASR models to check for transferability. Consequently, it does not demonstrate whether the discovered failures are specific to this one model or represent shared vulnerabilities across different models.",
                    "evidence": "Page 15, Section 7.1 Datasets and Experiment Setup: \"We selected Mozilla's implementation of DeepSpeech-0.3.0 [42] which produces one of the state-of-the-art open source ASR models. All our experiments were conducted on a pre-trained English model released along with DeepSpeech-0.3.0, which was trained with Fisher [43], LibriSpeech [44], Switchboard [45], and a pre-release snapshot of the English Common Voice training corpus [46].\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper evaluates DeepCruiser only on a single RNN-based ASR implementation (Mozilla DeepSpeech-0.3.0) and does not report reusing fault-inducing inputs to test other models nor describe design mechanisms explicitly intended to discover transferable attacks across different models. Therefore it does not demonstrate whether faults found on one model transfer to other models performing the same task.",
                    "evidence": "Page 16, Section 7.1 Datasets and Experiment Setup: \"We selected Mozilla\u2019s implementation of DeepSpeech-0.3.0 [42]... All our experiments were conducted on a pre-trained English model released along with DeepSpeech-0.3.0, which was trained with Fisher [43], LibriSpeech [44], Switchboard [45], and a pre-release snapshot of the English Common Voice training corpus [46].\"; Page 3, Introduction / contribution summary: \"We implement our testing framework, DeepCruiser, and perform in-depth evaluation on a state-of-the-art practical RNN based ASR system to demonstrate the usefulness.\" (demonstrates evaluation on a single ASR system and no cross-model transfer tests are reported)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Yuan et al. - 2023 - Revisiting Neuron Coverage for DNN Testing A Layer-Wise and Distribution-Aware Criterion": {
        "key": "yuan2023revisiting",
        "name": "NLC",
        "year": "2023",
        "citation_count": "36",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "shows heuristic link between coverage and errors."
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Uses inception score and FID score to measure natualness quality of the final inputs but overlook cumulave effects of mutations across iteration"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Plausibly stable due to large semantic changes, but applied in normalized float domain, so some discovered failures could vanish after re-encoding."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models performing the same task, nor does it describe mechanisms intended to discover or encourage transferable attacks. All fuzzing and adversarial-example experiments are conducted per tested model (i.e., inputs are generated/assessed against the same target DNN), and the evaluation reports coverage/faults per model without reusing generated fault-inducing inputs across different model implementations. Thus the work does not address attack transferability.",
                    "evidence": "VI.B (Guiding Input Mutation in DNN Testing), Alg. 1 (page 1209): \"Tested DNN: D; Criterion: C;\" \u2014 the fuzzing algorithm is defined in terms of a single tested DNN. \n\nVI.A.2 (Fault-Revealing Capability of Test Suites) (page 1208): \"AEs are generated using the training data (see 'Clarification' below for results of AEs generated using test data), and all algorithms attacking the three models reach over 98% success rates.\" \u2014 adversarial examples are generated and evaluated against the same model(s) they attack. \n\nVI (Evaluation) and Tables (e.g., Table VI, Table VII) (pages 1206\u20131208): All reported results present coverage and faults per model (ResNet, VGG, MobileNet, BigGAN, etc.), with no experiment or section reporting reuse of fault-inducing inputs across different models or measurements of transferability. (No section title or experiment in the paper addresses transferability or substitute-model attacks.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper focuses on designing a new coverage criterion (NLC) and evaluating its effectiveness for assessing test suites and guiding fuzzing for a single, specific DNN model at a time. The experiments involve generating fault-inducing inputs (either through adversarial attacks or fuzzing) on a given model and then evaluating the outcome on that same model. The paper does not contain any experiments or discussion on whether the fault-inducing inputs generated for one model (e.g., ResNet50) can also trigger failures in other models performing the same task (e.g., VGG16 or MobileNetV2). The fuzzing design and the NLC metric itself are aimed at exploring the internal state of a single model, not at finding shared vulnerabilities across different models.",
                    "evidence": "Page 10, Section VI-B, \"Guiding Input Mutation in DNN Testing\", Algorithm 1: The fuzzing algorithm is defined with a single \"Tested DNN: D\" as input. The core feedback loop in line 7, `if is_valid($,s) and coverage_inc(C,D) then`, clearly shows that the decision to keep a mutated input is based on its ability to increase the coverage metric `C` on that specific DNN `D`. This demonstrates that the fuzzing process is tailored to a single model, not designed for finding transferable inputs. The subsequent results in Table XI also report fault-triggering rates for individual models that were fuzzed, with no cross-evaluation."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models performing the same task, nor does it describe mechanisms intended to discover or encourage transferable attacks. All fuzzing and adversarial-example experiments are conducted per tested model (i.e., inputs are generated/assessed against the same target DNN), and the evaluation reports coverage/faults per model without reusing generated fault-inducing inputs across different model implementations. Thus the work does not address attack transferability.",
                    "evidence": "VI.B (Guiding Input Mutation in DNN Testing), Alg. 1 (page 1209): \"Tested DNN: D; Criterion: C;\" \u2014 the fuzzing algorithm is defined in terms of a single tested DNN. \n\nVI.A.2 (Fault-Revealing Capability of Test Suites) (page 1208): \"AEs are generated using the training data (see 'Clarification' below for results of AEs generated using test data), and all algorithms attacking the three models reach over 98% success rates.\" \u2014 adversarial examples are generated and evaluated against the same model(s) they attack. \n\nVI (Evaluation) and Tables (e.g., Table VI, Table VII) (pages 1206\u20131208): All reported results present coverage and faults per model (ResNet, VGG, MobileNet, BigGAN, etc.), with no experiment or section reporting reuse of fault-inducing inputs across different models or measurements of transferability. (No section title or experiment in the paper addresses transferability or substitute-model attacks.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Xie et al. - 2019 - DeepHunter a coverage-guided fuzz testing framework for deep neural networks": {
        "key": "xie2019deephunter",
        "name": "DeepHunter",
        "year": "2019",
        "citation_count": "518",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "It uses model coverage to guide input generation but does not provide any expeirment showing the correlation with coverage"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "User study was conducted on the images generated by single-step metamorphic mutations, not on the final fault-triggering fuzzed inputs after multiple chained transformations. Allows at most one affine transformation to reduce that risk."
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "The metamorphic transformations likely to persist but not validated. Works on floating point input without applying rounding."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly defines and implements a mechanism to detect faults that transfer from an original model to other models (specifically quantized versions), and the evaluation reuses tests generated on the original (unquantized) model to detect errors in the quantized models. Definition 3 formalizes a \u201cquantization erroneous behavior\u201d (i.e., a fault that manifests in one model but not the other), DeepHunter\u2019s workflow and experiments use that relation, and RQ4 reports results where tests generated on the original model are applied to quantized variants to measure such transfer. This demonstrates both reuse of fault-inducing inputs across models performing the same task (original vs. quantized) and an explicit design for discovering transferable failures.",
                    "evidence": "Page 149, Section 3.2.1 (Transformation and Mutation) \u2014 Definition 3: \"Given a DNN F and its quantized version F', a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x) \u2227 F'(x) = O(x), a mutant x' \u2208 M(x) is a quantization erroneous behavior between F and F' if (F(x) = F(x') \u2228 F'(x) = F'(x')) \u2227 F(x') \u2260 F(x).\" \n\nPage 154, Section 5.4 (RQ4. Error Detection for Quantization) \u2014 experimental procedure and results: \"In this experiment, all initial seeds of each dataset are unable to detect the errors before and after quantization for all cases... At runtime, we allocate 10 hours for test generation on each original unquantized model... Table 6 summarizes the results in detecting errors introduced during quantization by DeepHunter+Prob and TensorFuzz.\" \n\nPage 154, Section 5.4 (Answer to RQ4): \"With corner-region based criteria, DeepHunter tends to be more useful for detecting errors introduced during the DNN quantization. A larger QR often introduces larger decision logic inconsistencies before and after quantization.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed framework, DeepHunter, is designed and evaluated for testing a single Deep Neural Network (DNN) model. The methodology, including the test generation algorithm and oracles, is focused on finding defects within one specific target model or between that model and its quantized version. The experimental evaluation is conducted on several different DNNs, but each is tested in isolation. The paper does not include any experiments where fault-inducing inputs generated for one model (e.g., LeNet-5) are used to test another model performing the same task (e.g., a different MNIST classifier). The authors explicitly state this single-model focus in the related work section, distinguishing their approach from differential testing techniques that compare multiple DNNs.",
                    "evidence": "Page 11, Section 6 RELATED WORK, 'Different from DeepXplore and DiffChaser which focus on differential testing, DeepHunter is a grey-box testing technique focusing on a single DNN model.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "DeepHunter explicitly evaluates the transferability of fault\u2010inducing inputs by first generating mutants on an original (32\u2010bit) DNN model and then reusing those same tests to detect errors in its quantized counterparts. Definition 3 formalizes the notion of a \u201cquantization erroneous behavior\u201d (i.e., faults that appear in one model version but not the other), and RQ4\u2019s experimental setup and results demonstrate this cross\u2010model reuse. This matches the \u201cHigh\u201d level of the Attack Transferability metric, since the framework both reuses inputs across models performing the same task (original versus quantized) and incorporates mechanisms to discover these transferable failures.",
                    "evidence": "Page 149, Section 3.2.1 \u2013 Definition 3: Formalizes \u201cquantization erroneous behavior\u201d between F and its quantized version F\u2032, describing tests generated on one model and applied to the other.\nPage 154, Section 5.4 \u2013 RQ4: Describes running DeepHunter on the original model and applying the generated tests to quantized versions, with Table 6 summarizing the errors detected across models."
                },
                "manual": {
                    "value": "Low",
                    "why": "The oracle definitions is that if same input produce different output to a model and its quanitized version, then it is fault-triggering input. But it is not transferability.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic",
                "Differential"
            ]
        }
    },
    "Guo et al. - 2018 - DLFuzz differential fuzzing testing of deep learning systems": {
        "key": "guo2018dlfuzz",
        "name": "DLFuzz",
        "year": "2018",
        "citation_count": "360",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Finds generic misclassifications"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Does not show any correlation between errors and coverage"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "norm-bounded perturbation"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": "relies on imperceptible, float-level perturbations that can vanish after quantization or saving, with no persistence verification."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across different models. DLFuzz is explicitly designed to avoid cross-referencing multiple models (unlike DeepXplore) and treats an input as a fault when the mutated input and the original input produce different labels on the same CNN under test. The evaluation compares DLFuzz to DeepXplore on per-model metrics (neuron coverage, number of adversarial inputs, perturbation size, time) but does not reuse adversarial inputs generated on one model to test other models or report whether such inputs trigger failures across models. The only related experiment (augmenting training sets) retrains the same CNNs with adversarial images and is not a transferability test. Therefore the paper does not demonstrate transferable attacks nor include mechanisms explicitly aimed at discovering transferable failures across models.",
                    "evidence": "Abstract (p.739, Abstract): \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\" \n\nSection 3.1 Architecture (p.740, 3.1 Architecture): \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.\" \n\nIntroduction / Evaluation summary (p.740, end of Intro / start of 3): \"Compared with DeepXplore, DLFuzz does not need extra efforts to collect similar functional DL systems for cross-referencing label check...\" \n\nResults (p.742, 4.2 Result): \"we incorporated 114 adversarial images into the training set of three CNNs on MNIST and retrained them trying to increase their accuracy.\" (This describes retraining with generated adversarial inputs, not testing those inputs on other, unseen models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper presents DLFuzz as a method for testing a single Deep Learning (DL) system in isolation. A core contribution highlighted is its ability to find incorrect behaviors without needing \"cross-referencing oracles from other DL systems with the same functionality.\" The methodology focuses on generating adversarial inputs for one specific model under test by maximizing prediction differences between an original and a mutated input for that same model. The experimental evaluation, while conducted on multiple models (e.g., LeNet-1, LeNet-4, VGG-16), treats each model as an independent target. The results are reported on a per-model basis, and there is no mention of any experiment where fault-inducing inputs generated for one model are used to test another model to assess transferability.",
                    "evidence": "Page 2, Abstract: \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\"\n\nPage 5, Section 5, Conclusion: \"DLFuzz also overcomes the trouble of relying on multiple DL systems of the similar functionality in DeepXplore.\"\n\nPage 5, Table 1: \"Effectiveness of DLFuzz compared with DeepXplore.\" This table presents the results for each model (LeNet-1, LeNet-4, LeNet-5, VGG16, VGG19, ResNet50) individually, comparing DLFuzz to DeepXplore on that specific model. There is no data or discussion regarding the transferability of adversarial inputs generated for one model to any of the others."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across different models. DLFuzz is explicitly designed to avoid cross-referencing multiple models (unlike DeepXplore) and treats an input as a fault when the mutated input and the original input produce different labels on the same CNN under test. The evaluation compares DLFuzz to DeepXplore on per-model metrics (neuron coverage, number of adversarial inputs, perturbation size, time) but does not reuse adversarial inputs generated on one model to test other models or report whether such inputs trigger failures across models. The only related experiment (augmenting training sets) retrains the same CNNs with adversarial images and is not a transferability test. Therefore the paper does not demonstrate transferable attacks nor include mechanisms explicitly aimed at discovering transferable failures across models.",
                    "evidence": "Abstract (p.739, Abstract): \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\" \n\nSection 3.1 Architecture (p.740, 3.1 Architecture): \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.\" \n\nIntroduction / Evaluation summary (p.740, end of Intro / start of 3): \"Compared with DeepXplore, DLFuzz does not need extra efforts to collect similar functional DL systems for cross-referencing label check...\" \n\nResults (p.742, 4.2 Result): \"we incorporated 114 adversarial images into the training set of three CNNs on MNIST and retrained them trying to increase their accuracy.\" (This describes retraining with generated adversarial inputs, not testing those inputs on other, unseen models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Whitebox"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Prediction-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Yu et al. - LLM-Fuzzer Scaling Assessment of Large Language Model Jailbreaks": {
        "key": "yu2024llm",
        "name": "LLM-Fuzzer",
        "year": "2024",
        "citation_count": "40",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "aims to generate many jailbreak templates that trigger harmful content without specifying exact outcomes"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Does not provide any explanation why those errors occur."
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Mutations are purely at the text level"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly evaluates transferability: templates (fault-inducing inputs) generated by LLM-FUZZER on a set of surrogate models are reused to test many other models, and the fuzzer's design includes an explicit mechanism (oracle reward modification) to encourage generation of templates that succeed across multiple models. The authors both describe this cross-model generation/evaluation procedure (Experiment III) and report strong transfer results (EASR > 80% on many target models) and comparisons to other methods.",
                    "evidence": "Section 4.2 Experiment III (p.4664\u20134665): \"To address the question of transferability, we chose to create templates using gpt-3.5-turbo-0125, Llama-2-7B-Chat, and Vicuna-7B.... in the template reward assignment by the oracle, a zero reward was imposed if the template\u2019s all successful jailbreak attempts were attributed solely to one or two LLMs.\" \n\nSection 4.2 Experiment III (p.4665): \"Following the exhaustion of the 150k query budget, our experiment involved calculating the average ASR for each template generated across the three target models. The template with the highest average ASR, denoted as 'top-1', and the top five templates with the highest average ASR, known as 'top-5', were selected for further experimentation.\" \n\nSection 4.3 Results for Experiment III (p.4666\u20134668): \"These templates displayed a notable degree of transferability... As depicted in Figure 4, when evaluated against Vicuna-13B, Baichuan-13B, ChatGLM-6B, Llama-2-13B/70B, Claude1, and PaLM2, the EASR for LLM-FUZZER consistently exceeded 80%, having a huge advantage over EASRs of other methods.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper satisfies both conditions for a 'High' rating. First, it explicitly tests the transferability of the jailbreak templates generated by LLM-FUZZER. Templates are created using a set of three models and then evaluated against a larger, diverse set of unseen open-source and commercial LLMs. Second, the fuzzing design incorporates an explicit mechanism to support the discovery of transferable failures. Specifically for the transferability experiment, the reward mechanism of the fuzzer is modified to query multiple models simultaneously and penalize templates that are not universally effective, thereby actively guiding the fuzzer towards generating transferable jailbreaks.",
                    "evidence": "Page 10-11, Section 4.2, Experiment Design: \"Experiment III. To address the question of transferability, we chose to create templates using gpt-3.5-turbo-0125, Llama-2-7B-Chat, and Vicuna-7B. This approach was taken to ensure that the generated templates would be applicable across various LLMs. We followed a similar procedure with the previous experiment but made two slight modifications. First, in each fuzzing iteration, we queried 100 questions for each template across all three models, resulting in a total of 300 responses. Second, in the template reward assignment by the oracle, a zero reward was imposed if the template's all successful jailbreak attempts were attributed solely to one or two LLMs. This modification aimed to bolster universality by discouraging the generation of templates that could exploit only one or two specific LLMs.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly evaluates transferability: templates (fault-inducing inputs) generated by LLM-FUZZER on a set of surrogate models are reused to test many other models, and the fuzzer's design includes an explicit mechanism (oracle reward modification) to encourage generation of templates that succeed across multiple models. The authors both describe this cross-model generation/evaluation procedure (Experiment III) and report strong transfer results (EASR > 80% on many target models) and comparisons to other methods.",
                    "evidence": "Section 4.2 Experiment III (p.4664\u20134665): \"To address the question of transferability, we chose to create templates using gpt-3.5-turbo-0125, Llama-2-7B-Chat, and Vicuna-7B.... in the template reward assignment by the oracle, a zero reward was imposed if the template\u2019s all successful jailbreak attempts were attributed solely to one or two LLMs.\" \n\nSection 4.2 Experiment III (p.4665): \"Following the exhaustion of the 150k query budget, our experiment involved calculating the average ASR for each template generated across the three target models. The template with the highest average ASR, denoted as 'top-1', and the top five templates with the highest average ASR, known as 'top-5', were selected for further experimentation.\" \n\nSection 4.3 Results for Experiment III (p.4666\u20134668): \"These templates displayed a notable degree of transferability... As depicted in Figure 4, when evaluated against Vicuna-13B, Baichuan-13B, ChatGLM-6B, Llama-2-13B/70B, Claude1, and PaLM2, the EASR for LLM-FUZZER consistently exceeded 80%, having a huge advantage over EASRs of other methods.\""
                },
                "manual": {
                    "value": "High",
                    "why": "Transferability is encouraged in mutation by rewarding less to mutations that transfer less.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "Tian et al. - 2018 - DeepTest automated testing of deep-neural-network-driven autonomous cars": {
        "key": "tian2018deeptest",
        "name": "DeepTest",
        "year": "2018",
        "citation_count": "1791",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Aims to find errors in steering angle predictions under realistic environmental variations."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Shows correlation between coverage and errors"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Does not verify the naturalness of the fault-revealing mutated images through any human study or algorithmic naturalness metric. Individual transformations are naturalistic but chained mutations might not be"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Performs semantic, real-world image transformations (brightness, contrast, blur, fog, rain, etc.) but does not ensure IO stability using clipping and rounding"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "No cross model failure analysis"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across models. DeepTest generates synthetic tests and reports erroneous behaviors for three different DNNs, but it treats each model's testing separately and does not (a) reuse inputs crafted on one model to test other models, (b) measure overlap of discovered faults across models, or (c) include design elements explicitly aimed at finding transferable attacks. The authors explicitly contrast their approach with prior differential-testing work (DeepXplore) that used multiple DNNs; DeepTest is presented as a single-model, metamorphic-relation-driven approach rather than a substitute-model transfer-attack methodology.",
                    "evidence": "\u2022 Paper objective / scope (multiple models tested but separately): \u201cWe use DeepTest to systematically test three top performing DNN models from the Udacity driving challenge.\u201d (Abstract / Contributions, p.304). \n\n\u2022 Explicit distinction from multi-model differential testing (shows DeepTest is single-model focused): \u201cPei et al. [70] proposed DeepXplore, a whitebox differential testing algorithm for systematically finding inputs that can trigger inconsistencies between multiple DNNs. \u2026 By contrast, our graybox methods use neuron coverage for guided test generation in a single DNN and leverage metamorphic relations to identify erroneous behaviors without requiring multiple DNNs.\u201d (Related Work \u2014 Testing and verification of machine learning / DeepXplore comparison, p.312). \n\n\u2022 Results present per-model erroneous behaviors but no cross-model reuse/transfer experiments or overlap analysis: e.g., Tables and text report errors per model (Table 8: \u201cNumber of unique erroneous behaviors reported by DeepTest for different models...\u201d and surrounding discussion) and aggregate counts, but contain no experiment where inputs found on one model are applied to other models to test transferability (Results / RQ4 and Table 8 discussion, pp.310\u2013311)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper introduces DeepTest, a method for finding erroneous behaviors in a single Deep Neural Network (DNN) by generating synthetic inputs that maximize neuron coverage. The evaluation is performed on three separate DNN models (Chauffeur, Rambo, and Epoch) that perform the same task of autonomous driving. However, the paper's methodology and experiments focus on testing each model in isolation. The results are presented separately for each model, highlighting which types of transformations induce errors in which models. There is no experiment or discussion where a fault-inducing input generated for one model is then used to test another model to check for transferability. The paper does not demonstrate whether the discovered failures are model-specific or shared across different architectures.",
                    "evidence": "Page 10, Section 5. RESULTS, Table 8: 'Number of unique erroneous behaviors reported by DeepTest for different models with \u03bb = 5'. The accompanying text states: 'Interestingly, some models are more prone to erroneous behaviors for some transformations than others. For example, Rambo produces 23 erroneous cases for shear, while the other two models do not show any such cases. Similarly, DeepTest finds 650 instances of erroneous behavior in Chauffeur for rain but only 64 and 27 for Epoch and Rambo respectively.' This evidence shows that bugs were identified and counted for each model independently, rather than testing if a bug found in one model transfers to others."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across models. DeepTest generates synthetic tests and reports erroneous behaviors for three different DNNs, but it treats each model's testing separately and does not (a) reuse inputs crafted on one model to test other models, (b) measure overlap of discovered faults across models, or (c) include design elements explicitly aimed at finding transferable attacks. The authors explicitly contrast their approach with prior differential-testing work (DeepXplore) that used multiple DNNs; DeepTest is presented as a single-model, metamorphic-relation-driven approach rather than a substitute-model transfer-attack methodology.",
                    "evidence": "\u2022 Paper objective / scope (multiple models tested but separately): \u201cWe use DeepTest to systematically test three top performing DNN models from the Udacity driving challenge.\u201d (Abstract / Contributions, p.304). \n\n\u2022 Explicit distinction from multi-model differential testing (shows DeepTest is single-model focused): \u201cPei et al. [70] proposed DeepXplore, a whitebox differential testing algorithm for systematically finding inputs that can trigger inconsistencies between multiple DNNs. \u2026 By contrast, our graybox methods use neuron coverage for guided test generation in a single DNN and leverage metamorphic relations to identify erroneous behaviors without requiring multiple DNNs.\u201d (Related Work \u2014 Testing and verification of machine learning / DeepXplore comparison, p.312). \n\n\u2022 Results present per-model erroneous behaviors but no cross-model reuse/transfer experiments or overlap analysis: e.g., Tables and text report errors per model (Table 8: \u201cNumber of unique erroneous behaviors reported by DeepTest for different models...\u201d and surrounding discussion) and aggregate counts, but contain no experiment where inputs found on one model are applied to other models to test transferability (Results / RQ4 and Table 8 discussion, pp.310\u2013311)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Xian Yuen et al. - 2023 - ASDF A Differential Testing Framework for Automatic Speech Recognition Systems": {
        "key": "yuen2023asdf",
        "name": "ASDF",
        "year": "2023",
        "citation_count": "8",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Performs phonetic analysis to correlated phonemes with errors."
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Synthesized inputs might not be natural and no measure of naturnaless provided"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "The speech input is generated using TTS library which are fed to the ASR model. There is not in memory mutation on the audio."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper describes a differential testing framework that runs multiple ASR systems in parallel and uses cross-referencing to identify failed cases, and it leverages failures to generate transformed test texts. However, it does not demonstrate or evaluate whether fault-inducing inputs discovered on one model are reused to test other (unseen) target models, nor does it describe explicit mechanisms or experiments aimed at measuring cross-model transferability of faults. All descriptions pertain to comparing outputs across selected ASR systems within the same test run and generating transformations from observed failures, but no reuse-on-surrogate \u2192 test-on-other-model workflow or transferability analysis is presented.",
                    "evidence": "Page 461, Section II (Tool Design): \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\" \n\nPage 461-462, Section III (Core Functionalities): \"The ASR systems currently available in ASDF are DeepSpeech [19], wav2letter [20], and wav2vec2 [21].\" and \"Cross-referencing is performed amongst the selected ASR systems to ensure that the test cases are valid and can be determined by at least one ASR service.\" \n\nPage 462, Section IV (Usage): \"A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\" \n\n(There are no sections or quotes in the paper that describe generating fault-inducing inputs on a surrogate model and then reusing them to test other unseen target models, nor experiments measuring transferability across models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper describes a differential testing framework, ASDF, where a failure is defined by a disagreement among multiple Automated Speech Recognition (ASR) models. A test case is considered a failure for a specific ASR system only if at least one other system under test transcribes the audio correctly. This design principle means the framework is explicitly built to find model-specific failures, not failures that transfer across different models. The methodology does not involve generating a fault-inducing input on one model and then reusing it to test for transferability on others; rather, it tests a single input on all models simultaneously to find discrepancies. Furthermore, the framework discards inputs that cannot be correctly transcribed by at least one ASR service, which would include cases where a failure might be transferable across all tested models. Therefore, the paper does not demonstrate whether fault-inducing inputs discovered for one model also trigger failures in others.",
                    "evidence": "Page 1, Section II, TOOL DESIGN: \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\" \n\nPage 2, Section IV, USAGE: \"A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper describes a differential testing framework that runs multiple ASR systems in parallel and uses cross-referencing to identify failed cases, and it leverages failures to generate transformed test texts. However, it does not demonstrate or evaluate whether fault-inducing inputs discovered on one model are reused to test other (unseen) target models, nor does it describe explicit mechanisms or experiments aimed at measuring cross-model transferability of faults. All descriptions pertain to comparing outputs across selected ASR systems within the same test run and generating transformations from observed failures, but no reuse-on-surrogate \u2192 test-on-other-model workflow or transferability analysis is presented.",
                    "evidence": "Page 461, Section II (Tool Design): \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\" \n\nPage 461-462, Section III (Core Functionalities): \"The ASR systems currently available in ASDF are DeepSpeech [19], wav2letter [20], and wav2vec2 [21].\" and \"Cross-referencing is performed amongst the selected ASR systems to ensure that the test cases are valid and can be determined by at least one ASR service.\" \n\nPage 462, Section IV (Usage): \"A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\" \n\n(There are no sections or quotes in the paper that describe generating fault-inducing inputs on a surrogate model and then reusing them to test other unseen target models, nor experiments measuring transferability across models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ]
        }
    },
    "Odena et al. - 2019 - TensorFuzz Debugging Neural Networks with Coverage-Guided Fuzzing": {
        "key": "odena2019tensorfuzz",
        "name": "TensorFuzz",
        "year": "2019",
        "citation_count": "426",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "does not deliberately target particular outputs or behaviors. Objective function mechanism allows arbitrary user-defined checks to enable diverse failure detection but the exploration is not targeted toward any specific class of outputs "
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "norm-bounded"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": "Mutations occur in-memory on float tensors and are never verified for persistence after I/O."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly reuses fault-inducing inputs generated by fuzzing one model (the 32-bit model) to test another implementation (the 16-bit quantized/truncated model) and reports that many such inputs cause disagreements. However, the fuzzing framework is presented as a general-purpose CGF + PBT tool rather than being explicitly designed primarily to discover transferable attacks across different model implementations; the cross-model experiment is an application/example rather than a described core design goal. Thus the work demonstrates transferability reuse but the fuzzing design is not explicitly aimed at discovering transferable attacks across models.",
                    "evidence": "Section 6, 'TensorFuzz surfaces disagreements between models and their quantized versions': \"We then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. ... With these settings, the fuzzer was able to generate disagreements for 70% of the examples we tried.\" Also in same subsection: \"As a baseline experiment, we trained an MNIST classifier (this time without intentionally introducing numerical issues) using 32-bit floating point numbers. We then truncated all weights and activations to 16-bits. We then compared the predictions of the 32-bit and the 16-bit model on the MNIST test set and found 0 disagreements.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper does not evaluate whether the fault-inducing inputs discovered through fuzzing on one model can trigger failures in other, distinct models performing the same task. The experiments focus on finding bugs within a single model instance (e.g., numerical errors), comparing a model to its own quantized version, or testing the correctness of specific code implementations. There is no investigation into the transferability of these discovered faults to different model architectures or independently trained models, which is the core of the Attack Transferability metric.",
                    "evidence": "Page 7, Section \"TensorFuzz surfaces disagreements between models and their quantized versions\": The paper describes an experiment comparing a 32-bit MNIST classifier to its 16-bit quantized version. It states, \"We then compared the predictions of the 32-bit and the 16-bit model on the MNIST test set and found 0 disagreements. We then ran the fuzzer... the fuzzer was able to generate disagreements for 70% of the examples we tried.\" While this involves two model variants, it is a test of a model against its direct, transformed derivative, not a test of transferability to a separate, independently trained model. The other experiments in the paper similarly focus on single models or codebases without testing for transferability."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper includes an explicit experiment in which fault-inducing inputs generated by fuzzing a 32-bit MNIST classifier are reused to test a 16-bit quantized version, demonstrating that many of these inputs transfer and cause disagreements. This satisfies the criterion of reusing fuzzed inputs across two models performing the same task. However, the overall fuzzing framework is presented as a general CGF+PBT tool rather than being specifically designed to discover transferable failures across arbitrary surrogate and target models, so it lacks an explicit architectural mechanism for transferability beyond this illustrative application.",
                    "evidence": "Section 6, \u201cTensorFuzz surfaces disagreements between models and their quantized versions\u2019: \u201cWe then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. ... With these settings, the fuzzer was able to generate disagreements for 70% of the examples we tried.\u201d"
                },
                "manual": {
                    "value": "Low",
                    "why": "It is not transferability. It is differential testing based oracle.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Property-based",
                "Differential"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ]
        }
    },
    "Chao et al. - 2025 - Jailbreaking Black Box Large Language Models in Twenty Queries": {
        "key": "chao2025jailbreaking",
        "name": "PAIR",
        "year": "2025",
        "citation_count": "806",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "Targets specific malicious intents (e.g. bomb-making, phishing) but does not enforce a fixed phrase or label."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis of discovered faults. It provides transferability rationale"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Fuzzed inputs are text prompts generated by another LLM, not numerical perturbations"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "High",
                    "why": "Generates new test cases by semantic mutations. The authors attributed transferability to this mutation "
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly evaluates transferability by reusing jailbreak prompts found on one model to test other models (Section IV-B, Table VI) and reports transfer percentages. Furthermore, the PAIR design intentionally generates semantic, prompt-level (interpretable) attacks\u2014via attacker system-prompt templates, chain-of-thought improvement assessments, and parallel streams\u2014which the authors argue and evidence as supporting cross-model transferability (they compare PAIR vs GCG transfer and attribute better transfer to PAIR's semantic prompts). Thus the paper both demonstrates reuse of fault-inducing inputs across models and incorporates design choices aimed at discovering transferable failures.",
                    "evidence": "Abstract (p. 23): \"PAIR also achieves competitive jailbreaking success rates and transferability on open and closed\u2011source LLMs, including GPT-3.5/4, Vicuna, and Gemini.\"; Introduction / Contributions (p. 23): \"We also find that PAIR jailbreaks are often more trans\u2011ferrable to other LLMs than jailbreaks generated by GCG.\"; Section IV-B 'Jailbreak transfer experiments' (p. 28): \"For PAIR, we use the successful jailbreaks found for GPT-4 and Vicuna... Our results in Table VI indicate that PAIR\u2019s Vicuna prompts transfer more readily than those generated by GCG on all models except GPT-3.5, and PAIR\u2019s GPT-4 prompts transfer well on Vicuna, GPT-3.5, and Gemini. We believe that this is largely attributable to the fact that PAIR\u2019s prompts are semantic, and they therefore target similar vulnerabilities across LLMs...\"; Table VI caption (p. 28): \"Jailbreak transferability. We report the jailbreaking percentage of prompts that successfully jailbreak a source LLM when transferred to downstream LLM.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper extensively evaluates the transferability of the jailbreak prompts generated by its proposed method, PAIR. Fault-inducing inputs (prompts) generated on source models (GPT-4 and Vicuna) are systematically reused to test their effectiveness on a suite of other target models, with results clearly presented. This fulfills the primary condition for a 'Medium' rating. However, the paper does not meet the criteria for a 'High' rating because the PAIR method's design is not explicitly aimed at discovering transferable attacks. The core algorithm is an iterative refinement process focused on jailbreaking a single, specific target model. The authors attribute the observed high transferability to the semantic nature of the generated prompts, which is a beneficial byproduct of the method rather than an explicit mechanism integrated into the design to optimize for transferability across models.",
                    "evidence": "Page 6, Section IV-B, Jailbreak transfer experiments: \"We next evaluate the transferability of the attacks generated in \u00a7 IV-A. For PAIR, we use the successful jailbreaks found for GPT-4 and Vicuna... Our results in Table VI indicate that PAIR\u2019s Vicuna prompts transfer more readily than those generated by GCG on all models except GPT-3.5, and PAIR\u2019s GPT-4 prompts transfer well on Vicuna, GPT-3.5, and Gemini. We believe that this is largely attributable to the fact that PAIR\u2019s prompts are semantic, and they therefore target similar vulnerabilities across LLMs, which are generally trained on similar datasets.\" This section, along with Table VI, shows the reuse of prompts to test transferability, but the explanation attributes this property to the semantic nature of the prompts, not to an explicit design choice for promoting transferability."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper clearly demonstrates reuse of fault-inducing inputs (jailbreak prompts) generated on one model to test their effectiveness on other models (see Section IV-B, Table VI). However, while the authors attribute high transferability to the semantic, interpretable nature of PAIR\u2019s prompt-level attacks, the core design of PAIR is focused on efficiently generating jailbreaks on a single target model rather than explicitly optimizing for cross-model transfer. Thus it meets the criteria for Medium: it reuses inputs for transfer tests but does not incorporate mechanisms explicitly aimed at discovering transferable failures.",
                    "evidence": "Section IV-B (p. 28): \u201cFor PAIR, we use the successful jailbreaks found for GPT-4 and Vicuna\u2026 Our results in Table VI indicate that PAIR\u2019s Vicuna prompts transfer more readily than those generated by GCG\u2026 We believe that this is largely attributable to the fact that PAIR\u2019s prompts are semantic, and they therefore target similar vulnerabilities across LLMs.\u201d; Table VI caption: \u201cJailbreak transferability. We report the jailbreaking percentage of prompts that successfully jailbreak a source LLM when transferred to downstream LLM.\u201d"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Transferability is a byproduct in this paper, not a design goal",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ]
        }
    },
    "Asyrofi et al. - 2020 - CrossASR Efficient Differential Testing of Automatic Speech Recognition via Text-To-Speech": {
        "key": "asyrofi2020crossasr",
        "name": "CrossASR",
        "year": "2020",
        "citation_count": "25",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": "transcription error"
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": "Performs human study"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Uses TTS to generate the input audio - non in memory mutation"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "Utilize multiple ASRs to find faults using differential oracle but does not analyze if same error is reproducible to other model"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models, nor does it present experiments or analyses measuring transferability. CrossASR runs each synthesized audio against multiple ASRs simultaneously (differential testing) and reports per-ASR failure counts, but it does not (a) create inputs targeted at one surrogate ASR and then reuse them to test other ASRs, (b) measure overlap/transfer of failures across models, or (c) claim design choices explicitly intended to discover transferable attacks. The classifier (failure probability predictor) is trained with labels per ASR to predict likely failures, but the approach and experiments focus on efficiently finding failures per ASR rather than on demonstrating or quantifying transferability across ASRs.",
                    "evidence": "Page 642, I. Introduction: \"CrossASR performs a black-box differential testing to uncover erroneous behaviors on ASR systems by cross-referencing different ASRs to detect different transcriptions from ASRs.\" \n\nPage 643, II-B Test Selection Engine: \"For this training, a text has m labels, each corresponds to an ASR.\" \n\nPage 646, III-D RQ3 / Table V: \"Table V shows the total number of failed test cases that is generated from the entire 20,000 texts for all combinations of experimented TTSes and ASRs.\" (Table V reports per-ASR failed-case counts but no cross-model transfer/overlap statistics.) \n\nPage 649, VI Conclusion / Future Work: \"In the future, we plan to evaluate CrossASR using additional ASRs and TTSes.\" (Indicates no current evaluation of transferability across broader sets of models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, CrossASR, is based on differential testing, not attack transferability. It generates a single audio input and feeds it to multiple Automatic Speech Recognition (ASR) systems simultaneously. A failure is defined based on a disagreement among the outputs of these systems. Specifically, a test case is considered a failure for a particular ASR if its transcription does not match the input text, while at least one other ASR's transcription does. The methodology does not involve generating a fault-inducing input on a single source/surrogate model and then reusing that specific input to test for failures on other target models. The evaluation counts the total number of failures detected for each ASR system individually but does not analyze or report on whether the same audio inputs caused failures across multiple ASRs. Therefore, it does not demonstrate the transferability of fault-inducing inputs.",
                    "evidence": "Page 3, Section II-A, Architecture, Case 2: \"At least one of the ASR transcribed texts that matches with the input Text while at least one of the other ASR Transcribed Texts do not match with the input Text. In this case, CrossASR considers cases where the transcribed text does not match input text as failed test cases...\" This describes the core differential testing mechanism, where a failure in one model is identified by its disagreement with a successful transcription from another model for the same input, rather than testing for transferability of a known failing input. \n\nPage 6, Table V, \"NUMBER OF FAILED TEST CASES GENERATED FOR EACH COMBINATION OF TTSES AND ASRS\": This table presents the total number of failures for each ASR model as separate, aggregated counts. For example, using the Google TTS, it found 4,036 failures for Deepspeech and 2,539 for Deepspeech2. The paper does not analyze the intersection of these sets to determine how many specific audio files caused failures in both models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models, nor does it present experiments or analyses measuring transferability. CrossASR runs each synthesized audio against multiple ASRs simultaneously (differential testing) and reports per-ASR failure counts, but it does not (a) create inputs targeted at one surrogate ASR and then reuse them to test other ASRs, (b) measure overlap/transfer of failures across models, or (c) claim design choices explicitly intended to discover transferable attacks. The classifier (failure probability predictor) is trained with labels per ASR to predict likely failures, but the approach and experiments focus on efficiently finding failures per ASR rather than on demonstrating or quantifying transferability across ASRs.",
                    "evidence": "Page 642, I. Introduction: \"CrossASR performs a black-box differential testing to uncover erroneous behaviors on ASR systems by cross-referencing different ASRs to detect different transcriptions from ASRs.\" \n\nPage 643, II-B Test Selection Engine: \"For this training, a text has m labels, each corresponds to an ASR.\" \n\nPage 646, III-D RQ3 / Table V: \"Table V shows the total number of failed test cases that is generated from the entire 20,000 texts for all combinations of experimented TTSes and ASRs.\" (Table V reports per-ASR failed-case counts but no cross-model transfer/overlap statistics.) \n\nPage 649, VI Conclusion / Future Work: \"In the future, we plan to evaluate CrossASR using additional ASRs and TTSes.\" (Indicates no current evaluation of transferability across broader sets of models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Asyrofi et al. - 2021 - CrossASR++ a modular differential testing framework for automatic speech recognition": {
        "key": "asyrofi2021crossasr++",
        "name": "CrossASR++",
        "year": "2021",
        "citation_count": "25",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "TTS engines are inherently designed to produce natural-sounding speech from text, thereby providing a theoretical justification and design constraint for input naturalness."
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "Uses multiple ASRs in differential oracle, does not show transferability of failures across errors"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper describes a differential testing tool that generates the same TTS-produced inputs and feeds them to multiple ASRs for cross-referencing (so inputs are evaluated across ASRs concurrently), but it does not evaluate or claim that fault-inducing inputs discovered on one model are reused to test transferability to other (unseen) models, nor does it include experiments that measure cross-model transfer of faults or support surrogate-model attacks. The evaluation focuses on counting failed test cases per SUT and on how adding more ASRs or changing estimator/visibility affects the number of failures found, without demonstrating or measuring whether inputs that fail one model also cause failures in other models beyond the concurrent cross-referencing setup or in unseen targets.",
                    "evidence": "Page 1576, Section 2.1 Workflow: \"we use all ASR systems (including the System Under Test (SUT)) to convert the audio files back to texts (Line 11), and then we perform cross-reference (Line 12). For a piece of audio and its corresponding transcription, there are three possible situations: ... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\"  Page 1577, Section 3 RQ1 and Table 1: RQ1 is \"How many failed test cases can CrossASR++ find?\" and results report counts of failed test cases per SUT under different configurations (no analysis of reuse/transfer to unseen models).  Page 1577, Section 3 RQ2 / Table 2: \"By simply adding one more ASR for cross-referencing, we can increase the number of failed test cases uncovered for 4 SUTs by 25.07%, 39.63%, 20.95% and 8.17% respectively.\" (shows cross-referencing effect but not a transferability experiment).  Page 1578, Conclusion: \"CrossASR++ , an extensible tool that performs black-box differential testing on ASR systems... The extensibility allows CrossASR++ to flexibly utilizes more tools to boost its performance on uncovering failed test cases.\" (no mention of evaluating or supporting transferable/substitute-model attacks)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, CrossASR++, is a differential testing framework. A test case is defined as a failure for a specific System Under Test (SUT) if that SUT fails to transcribe an audio input correctly, while at least one other ASR system succeeds. This design inherently focuses on finding disagreements between models, rather than shared vulnerabilities or transferable failures. The case where all ASR models fail on the same input is categorized as an \"indeterminable test case\" and is not treated as a successful discovery of a fault. The paper does not reuse the inputs that are found to cause failures in one model to evaluate if they also induce failures in other models. Therefore, it does not demonstrate the transferability of fault-inducing inputs.",
                    "evidence": "Page 3, Section 2.1 Workflow, \"For a piece of audio and its corresponding transcription, there are three possible situations: ... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT. 3) If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case because a TTS may generate an invalid audio.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper describes a differential testing tool that generates the same TTS-produced inputs and feeds them to multiple ASRs for cross-referencing (so inputs are evaluated across ASRs concurrently), but it does not evaluate or claim that fault-inducing inputs discovered on one model are reused to test transferability to other (unseen) models, nor does it include experiments that measure cross-model transfer of faults or support surrogate-model attacks. The evaluation focuses on counting failed test cases per SUT and on how adding more ASRs or changing estimator/visibility affects the number of failures found, without demonstrating or measuring whether inputs that fail one model also cause failures in other models beyond the concurrent cross-referencing setup or in unseen targets.",
                    "evidence": "Page 1576, Section 2.1 Workflow: \"we use all ASR systems (including the System Under Test (SUT)) to convert the audio files back to texts (Line 11), and then we perform cross-reference (Line 12). For a piece of audio and its corresponding transcription, there are three possible situations: ... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\"  Page 1577, Section 3 RQ1 and Table 1: RQ1 is \"How many failed test cases can CrossASR++ find?\" and results report counts of failed test cases per SUT under different configurations (no analysis of reuse/transfer to unseen models).  Page 1577, Section 3 RQ2 / Table 2: \"By simply adding one more ASR for cross-referencing, we can increase the number of failed test cases uncovered for 4 SUTs by 25.07%, 39.63%, 20.95% and 8.17% respectively.\" (shows cross-referencing effect but not a transferability experiment).  Page 1578, Conclusion: \"CrossASR++ , an extensible tool that performs black-box differential testing on ASR systems... The extensibility allows CrossASR++ to flexibly utilizes more tools to boost its performance on uncovering failed test cases.\" (no mention of evaluating or supporting transferable/substitute-model attacks)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Data-driven"
            ]
        }
    },
    "Asyrofi et al. - 2021 - Can Differential Testing Improve Automatic Speech Recognition Systems": {
        "key": "asyrofi2021can",
        "name": "Asyrofi et al.",
        "year": "2021",
        "citation_count": "16",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "Does not demonstrate if fuzzed input on one model aslo causes error in another model"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. While CrossASR++ feeds each synthesized audio to multiple ASR systems simultaneously (so generated inputs are executed on several models), the authors do not (a) craft fault-inducing inputs on a surrogate model and then reuse them to test other models, nor (b) analyze whether fault-inducing inputs discovered for one model also trigger failures in other models. The experiments focus on (i) fine-tuning the SUT with generated test cases to reduce its failures, and (ii) fine-tuning one cross-referenced ASR to improve CrossASR++\u2019s ability to find failures. These evaluations measure improvement in detection and model quality, not the transferability of faults between independently-trained target models.",
                    "evidence": "Page 675, Section II.A: \"The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1). The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio.\" \n\nPage 677, Section III.C (RQ2 - Experiment Design & Result): \"In this research question, we want to investigate whether fine-tuning one of the cross-referenced ASR systems using generated test cases can boost the performance of CrossASR++ to uncover failed test cases... For the evolved CrossASR++, we continuously evolve DeepSpeech and always utilize DeepSpeech as one of the ASR systems used for cross-referencing... By comparing the number of failed test cases found by the static and evolved CrossASR++ head-to-head, it is clear that the evolved CrossASR++ uncovers more failed test cases than the static one for each ASR system.\" \n\n(These excerpts show that inputs are evaluated across multiple ASRs and that the paper measures improvements from fine-tuning, but there is no experiment or analysis that shows fault-inducing inputs crafted on one (surrogate) model transfer to other models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, evolutionary differential testing, uses multiple Automatic Speech Recognition (ASR) models for cross-referencing. However, this is done to create a reliable test oracle\u2014a test case is deemed valid if at least one model can correctly transcribe the audio. A failure is then identified specifically for the System Under Test (SUT) if it fails on this valid test case. The paper does not investigate or demonstrate whether these fault-inducing inputs for one SUT also trigger failures in other ASR models. The generated failed test cases are used to fine-tune and improve the SUT itself, rather than being reused to test for attack transferability across different models. The core focus is on improving a single model or the testing tool, not on discovering shared vulnerabilities.",
                    "evidence": "Page 2, Section II-A, \"CrossASR++\": \"A valid test case can be either a failed test case or a successful test case. If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT. Otherwise, a successful test case is found.\" This quote illustrates that a failure is defined and recorded for a single, specific SUT, not in the context of transferability to other models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. While CrossASR++ feeds each synthesized audio to multiple ASR systems simultaneously (so generated inputs are executed on several models), the authors do not (a) craft fault-inducing inputs on a surrogate model and then reuse them to test other models, nor (b) analyze whether fault-inducing inputs discovered for one model also trigger failures in other models. The experiments focus on (i) fine-tuning the SUT with generated test cases to reduce its failures, and (ii) fine-tuning one cross-referenced ASR to improve CrossASR++\u2019s ability to find failures. These evaluations measure improvement in detection and model quality, not the transferability of faults between independently-trained target models.",
                    "evidence": "Page 675, Section II.A: \"The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1). The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio.\" \n\nPage 677, Section III.C (RQ2 - Experiment Design & Result): \"In this research question, we want to investigate whether fine-tuning one of the cross-referenced ASR systems using generated test cases can boost the performance of CrossASR++ to uncover failed test cases... For the evolved CrossASR++, we continuously evolve DeepSpeech and always utilize DeepSpeech as one of the ASR systems used for cross-referencing... By comparing the number of failed test cases found by the static and evolved CrossASR++ head-to-head, it is clear that the evolved CrossASR++ uncovers more failed test cases than the static one for each ASR system.\" \n\n(These excerpts show that inputs are evaluated across multiple ASRs and that the paper measures improvements from fine-tuning, but there is no experiment or analysis that shows fault-inducing inputs crafted on one (surrogate) model transfer to other models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Data-driven"
            ]
        }
    },
    "Pei et al. - 2017 - DeepXplore Automated Whitebox Testing of Deep Learning Systems": {
        "key": "pei2017deepxplore",
        "name": "DeepXplore",
        "year": "2017",
        "citation_count": "1814",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": "reveals incorrect corner-case behaviors but does not bypass explicity safety mechinasims"
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "performs untargeted exploration to reveal generic model faults (differential behaviour, inaccuracies). No targeted misclassification."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Enforces domain specific constraints like keeping pixel values between 0 and 255, but it can produce decimal point pixel value within that range and does not perform rounding"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's testing approach is explicitly differential: it requires whitebox access to multiple DNNs and constructs inputs by optimizing across those models so that they disagree. DeepXplore does not present or evaluate the scenario where fault-inducing inputs are crafted on a single surrogate model and then reused (without access) to trigger failures in other, unseen target models. In other words, the paper demonstrates generating inputs that cause differences among available models, but it does not demonstrate or measure transferability of fault-inducing inputs from one model to other independent/unknown models (the kind of surrogate-model transfer evaluated by the metric).",
                    "evidence": "Page 1, Abstract: \"Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles to avoid manual checking.\"; Page 5, Section 3 (Overview): \"DeepXplore takes unlabeled test inputs as seeds and generates new tests that cover a large number of neurons ... while causing the tested DNNs to behave differently.\"; Page 6, Section 4.2 / Algorithm 1: algorithm inputs show \"dnns \u2190 multiple DNNs under test\" and the algorithm selects \"d = randomly select one dnn from dnns\" (showing the method relies on multiple available models during generation); Page 14, Section 8 (Limitations): \"First, differential testing requires at least two different DNNs with the same functionality. Further, if two DNNs only differ slightly ... DeepXplore will take longer to find difference-inducing inputs...\" (stating the necessity of multiple models and acknowledging that DeepXplore cannot operate in the single-surrogate transfer scenario)."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's core methodology, differential testing, is explicitly designed to discover transferable failures by finding inputs that cause different behaviors across multiple deep learning models with similar functionality. The method does not simply generate a fault-inducing input for one model and then test its transferability on others as a secondary step. Instead, it uses multiple models as 'cross-referencing oracles' from the outset. The fuzzing design incorporates an explicit mechanism\u2014a joint optimization function\u2014that directly aims to maximize the behavioral differences between the models under test. This process inherently generates inputs that demonstrate failures across different models.",
                    "evidence": "Page 7, Section 4.2, 'Maximizing differential behaviors': 'The first objective of the optimization problem is to generate test inputs that can induce different behaviors in the tested DNNs, i.e., different DNNs will classify the same input into different classes.' and Page 8, Equation (2) and its description: 'obj1(x) = \u2211k\u2260j Fk(x)[c] - \u03bb1 \u00b7 Fj(x)[c] where \u03bb1 is a parameter to balance the objective terms between the DNNs Fk\u2260j that maintain the same class outputs as before and the DNN Fj that produce different class outputs.' This objective function is the explicit mechanism to find transferable/differential failures."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DeepXplore\u2019s approach requires whitebox access to multiple DNNs during test generation and optimizes inputs to provoke disagreements among them. It does not generate fault-inducing inputs on a surrogate model and then reuse those inputs (without internal access) to trigger failures in independently trained or unseen target models. Thus, it does not evaluate or support the traditional notion of attack transferability from one model to another.",
                    "evidence": "Page 1 (Abstract): \u201cNext, we leverage multiple DL systems with similar functionality as cross-referencing oracles\u2026\u201d; Page 5 (Section 3 Overview): \u201cDeepXplore takes unlabeled test inputs as seeds \u2026 while causing the tested DNNs to behave differently.\u201d; Page 6 (Algorithm 1 inputs): \u201cdnns \u2190 multiple DNNs under test\u201d and selection of one DNN from dnns for optimization; Page 14 (Section 8 Limitations): \u201cdifferential testing requires at least two different DNNs with the same functionality\u2026 DeepXplore cannot operate in the single-surrogate transfer scenario.\u201d"
                },
                "manual": {
                    "value": "Low",
                    "why": "It is differential testing based oracle. Not transferability.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Whitebox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Prediction-guided"
            ]
        }
    },
    "Gao et al. - 2020 - Fuzz testing based data augmentation to improve robustness of deep neural networks": {
        "key": "gao2020fuzz",
        "name": "Sensei",
        "year": "2020",
        "citation_count": "169",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Aims to find generic misclassifications"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Apply large semantic level mutations and perform floating point operations (zoom, contrast, shear), which may produce non-discrete pixel values. They don't perform clipping/rounding to ensure IO persistence."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "No analysis of fault transferability in input level or mutation level"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or claim that fault-inducing inputs found on one (surrogate) model transfer to other models. Sensei is designed as an in-training data-augmentation method that generates variants per model/seed during that model's training; the experiments measure per-model robust accuracy after applying Sensei to each model independently. The paper explicitly contrasts its approach with multi-model differential testing (DeepXplore) and states that Sensei does not require multiple models, indicating the design and evaluation do not target discovering or measuring transferable faults across models.",
                    "evidence": "p.1148, Abstract: \"We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets.\"; p.1150, Section 3.2 (Overview): \"Note that, Sensei is in-training data augmentation approach, i.e., data generation and augmentation happen on-the-fly during training.\"; p.1156, Section 6 (Related Work): \"DeepXplore [24] generates test inputs that lead to exhibit different behaviors by different models for the same task. Our approach does not require multiple models.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a data augmentation technique, SENSEI, to improve the robustness of a single Deep Neural Network (DNN) model during its training phase. The method uses a genetic algorithm to generate high-loss variants of training inputs, which are then immediately used to train that same model. The entire process of generating and utilizing these 'fault-inducing' inputs is self-contained within the training loop of one specific model. The experiments evaluate the robustness of multiple individual models after they have been trained with SENSEI, but they do not test whether the variants generated for one model can cause failures in other models performing the same task. The paper does not mention or evaluate the concept of attack transferability between different models.",
                    "evidence": "Page 5, Algorithm 2, 'Overall algorithm': The algorithm describes a training process for a single model 'M'. In each epoch, for each training input 'X[i]', variants are generated and evaluated using the current model 'M' (line 14: `f := fitness(M, children);`). The model 'M' is then retrained on the augmented data (line 19: `M := train(X, Y);`). This shows the generated inputs are specific to and consumed by the model being trained, not transferred to other models.\n\nPage 7, Section 4.3, 'Evaluation Metric': The paper's own evaluation metric is defined for a single model's performance: 'A DNN is robust around x if and only if M(x') = c for all x' \u2208 X'. This confirms that the evaluation is focused on the robustness of an individual model, not on cross-model failure patterns.\n\nPage 9, Table 2, 'The robust accuracy for Random, W-10 and SENSEI': This table presents the results for 15 different models. Each model is trained and evaluated independently. The methodology does not involve generating inputs on one model (e.g., GTSRB-1) and testing them on another (e.g., GTSRB-2)."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or claim that fault-inducing inputs found on one (surrogate) model transfer to other models. Sensei is designed as an in-training data-augmentation method that generates variants per model/seed during that model's training; the experiments measure per-model robust accuracy after applying Sensei to each model independently. The paper explicitly contrasts its approach with multi-model differential testing (DeepXplore) and states that Sensei does not require multiple models, indicating the design and evaluation do not target discovering or measuring transferable faults across models.",
                    "evidence": "p.1148, Abstract: \"We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets.\"; p.1150, Section 3.2 (Overview): \"Note that, Sensei is in-training data augmentation approach, i.e., data generation and augmentation happen on-the-fly during training.\"; p.1156, Section 6 (Related Work): \"DeepXplore [24] generates test inputs that lead to exhibit different behaviors by different models for the same task. Our approach does not require multiple models.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "coonstrained transformation but no naturalness measure of the fuzzed inputs"
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ],
            "oracle": [
                "Metamorphic",
                "Property-based"
            ]
        }
    },
    "You et al. - 2023 - Regression Fuzzing for Deep Learning Systems": {
        "key": "you2023regression",
        "name": "DRFuzz",
        "year": "2023",
        "citation_count": "27",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Intargeted exploration for generic regression misclassifications - any inputs that cause the new model to misclassify examples previously classified correctly"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "does not analyze the root cause of regression faults, acknowledging that identifying their causes remains an open challenge. It provides behavior-level explanations (e.g., which classes or behaviors regress, per-class fault heatmaps, and faulty-behavior tuples). Interprets regression faults as evidence of bias or overfitting (e.g., specific classes suffering more after retraining)."
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": "Every batch of mutated inputs is passed through the GAN discriminator and only natural mutated inputs are kept for execution and further mutation."
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Generates and evaluates mutated inputs entirely in memory using both pixel level and image level semantic mutations. They use GAN-based fidelity filtering to ensure naturalness but does not ensure if the mutation will survive I/O."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper defines and evaluates regression-fault-triggering inputs strictly with respect to a prior model M1 and its regression model M2 (i.e., it detects inputs where M1 is correct and M2 is incorrect). All described generation, fidelity filtering, and evaluation steps execute mutated inputs on that M1\u2013M2 pair. The experiments use multiple dataset/model pairs but do not evaluate whether inputs found on one model transfer to other models performing the same task, nor does the design include explicit mechanisms to discover or measure transferable attacks across different model implementations. Therefore the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "Section II (Definition), Definition 2 \u2014 \"Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\" (Section II, Definition 2)\n\nSection III (Approach) \u2014 \"the mutated inputs are passed into the GAN-based Fidelity Assurance model ... The inputs that do not trigger regression faults will be maintained ... the mutated inputs ... are then executed by both the original model and the regression model. If the two models provide different prediction results, then a regression-fault-triggering input is identified.\" (Section III, overview / Approach)\n\nSection IV-A1 (Subjects / Experimental setup) \u2014 \"we used four pairs of datasets and DNN models as subjects, i.e., LeNet-5 on MNIST, VGG16 on CIFAR10, AlexNet on Fashion-MNIST (FM), and ResNet18 on SVHN\" (Section IV-A1 Subjects). \n\nThroughout the paper (Approach and Evaluation sections) all testing and analysis focus on M1 vs. M2 pairs; there is no experiment or statement reusing inputs crafted on one model to test different, unseen target models nor any mechanism intended to discover transferable attacks across model implementations."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's entire methodology and evaluation are focused on 'regression fuzzing', which it defines as finding faults between a prior version of a model (M1) and its subsequent version (M2). The goal is to find inputs that were correctly classified by M1 but are misclassified by M2 after an evolution event (e.g., fine-tuning, pruning). The experiments are designed around creating these M1/M2 pairs and evaluating the proposed fuzzer, DRFuzz, in its ability to find such regression-specific faults. The paper does not perform any experiments to check if the fault-inducing inputs found for a specific model (e.g., LeNet-5) also trigger failures in other, different model architectures (e.g., VGG16) performing the same task. Therefore, it does not demonstrate attack transferability across models.",
                    "evidence": "Page 2, Section II. DEFINITION, Definition 2. Regression fault: \"Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1 [x] = y \u2227 cM2 [x] != y.\" This definition explicitly frames the problem as a comparison between two versions of the same model, not between different models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper defines and evaluates regression-fault-triggering inputs strictly with respect to a prior model M1 and its regression model M2 (i.e., it detects inputs where M1 is correct and M2 is incorrect). All described generation, fidelity filtering, and evaluation steps execute mutated inputs on that M1\u2013M2 pair. The experiments use multiple dataset/model pairs but do not evaluate whether inputs found on one model transfer to other models performing the same task, nor does the design include explicit mechanisms to discover or measure transferable attacks across different model implementations. Therefore the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "Section II (Definition), Definition 2 \u2014 \"Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\" (Section II, Definition 2)\n\nSection III (Approach) \u2014 \"the mutated inputs are passed into the GAN-based Fidelity Assurance model ... The inputs that do not trigger regression faults will be maintained ... the mutated inputs ... are then executed by both the original model and the regression model. If the two models provide different prediction results, then a regression-fault-triggering input is identified.\" (Section III, overview / Approach)\n\nSection IV-A1 (Subjects / Experimental setup) \u2014 \"we used four pairs of datasets and DNN models as subjects, i.e., LeNet-5 on MNIST, VGG16 on CIFAR10, AlexNet on Fashion-MNIST (FM), and ResNet18 on SVHN\" (Section IV-A1 Subjects). \n\nThroughout the paper (Approach and Evaluation sections) all testing and analysis focus on M1 vs. M2 pairs; there is no experiment or statement reusing inputs crafted on one model to test different, unseen target models nor any mechanism intended to discover transferable attacks across model implementations."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Zhang et al. - 2019 - Life after Speech Recognition Fuzzing Semantic Misinterpretation for Voice Assistant Applications": {
        "key": "zhang2019life",
        "name": "LipFuzzer",
        "year": "2019",
        "citation_count": "79",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Medium",
                    "why": "It does not bypass permissions or authentication; instead, it tricks the VA into misinterpreting intent. The resulting misbehavior (e.g., invoking the wrong vApp or disclosing data to a malicious app). Expose semantically incorrect but functionally valid behaviors that breach user-intent and specification expectations\u2014posing integrity and trust risks without directly bypassing safety or security mechanisms."
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "untargeted exploration to expose semantic or intent-classification inconsistencies, sometimes aligning with attacker intent (e.g., LAPSUS examples), though its discovery of attacker intent is incidental rather than goal-optimized."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Empirically isolates the NLU Intent Classifier as the primary source of error and correlates misinterpretation patterns with linguistic mutation types"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "They perform human study to train the linguistic model that help selection potential fault triggering input. However, the synthesized voice commands are not measured for naturalness"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "evaluates two voice assistant systems independently but does not analyze whether failures generalize"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or report experiments where fault-inducing inputs (LAPSUS) discovered on one model/target are reused to test other models to show cross-model transferability. Although LipFuzzer is applied to both Amazon Alexa and Google Assistant stores (i.e., the authors run the tool on multiple platforms), the stated evaluation goals and the described experiments focus on finding and verifying LAPSUS per platform. The paper does not present (a) an explicit experiment that generates faults on a surrogate model and then reuses those same inputs on different/unknown target models, nor (b) design elements explicitly aimed at discovering or measuring the transferability of discovered fault-inducing inputs across independently implemented models performing the same task.",
                    "evidence": "Section/Pages and quotes from the paper:\n\n1) Abstract (p.1): \"We also use LipFuzzer to evaluate both Amazon Alexa and Google Assistant vApp platforms.\" \u2014 supports that both platforms were evaluated but does not indicate reuse/transfer experiments.\n\n2) Evaluation goals (Section VI, p.11): \"More specifically, our evaluation has three goals: (i) We empirically verify that the problematic Intent Classifier can lead to speech misinterpretation related to LAPSUS. (ii) We show LipFuzzer's performance in terms of the LAPSUS Models' accuracy and effectiveness. (iii) We use LipFuzzer to reveal that problematic templates widely exist in both Amazon Alexa and Google Assistant platforms.\" \u2014 none of these goals mentions generating fault-inducing inputs on one model and testing them on other models to assess transferability.\n\n3) vApp store experiment setup (Section VI-C, p.12): \"we apply the same LAPSUS Models used in LipFuzzer evaluation.\" and \"we evaluate LipFuzzer by using templates crawled from the Amazon Alexa Store and Google Assistant Store.\" \u2014 indicates application across stores but does not describe experiments that generate LAPSUS on one platform and then reuse them to test the other platform as a transferability study.\n\n(These quotes are from the paper's Abstract, Section VI (Evaluation), and Section VI-C (vApp Store Evaluation).)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper applies its fuzzing methodology to two different major Voice Assistant platforms (Amazon Alexa and Google Assistant), which represent distinct models performing the same task of semantic interpretation. The design of the fuzzer, LipFuzzer, explicitly incorporates mechanisms to discover transferable failures. It achieves this by building platform-agnostic 'LAPSUS Models' based on general linguistic knowledge of common speech errors (e.g., pronunciation, vocabulary, grammar) rather than on the specific implementation of any single platform's Natural Language Understanding (NLU) component. By using these general models to generate fault-inducing inputs for applications on both platforms, the paper demonstrates that the same classes of vulnerabilities exist across different models, fulfilling both criteria for a 'High' rating.",
                    "evidence": "Page 12, Section VI-C, \"vApp Store Evaluation\": \"1) Experiment Setup: We evaluate LipFuzzer by using templates crawled from the Amazon Alexa Store and Google Assistant Store. For Amazon Alexa Store, we acquired a seed template dataset of 98,261 voice commands from 32,892 vApps. For Google Assistant Store, we gathered 2,328 vApps with 9,044 voice commands.\" This is further supported by Table IV on Page 13, which shows the fuzzing results for both platforms. The design for transferability is evidenced on Page 6, Section IV-A, \"Fuzzing Input & Output\": \"The input of linguistic modeling is linguistic knowledge data shown in 1 of Figure 4. We choose LAPSUS-related linguistic knowledge from multiple sources [12] [26] [1] [20] [24].\" This confirms the models are built from general, platform-agnostic linguistic sources."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "Although the authors apply LipFuzzer separately to both Amazon Alexa and Google Assistant vApp platforms, they do not generate fault-inducing inputs on one platform and then reuse those exact inputs to test the other. The evaluation treats each platform in isolation, and the paper does not include any explicit experiment or analysis showing that LAPSUS examples found on a surrogate (e.g., Alexa) transfer to an unseen target (e.g., Google Assistant). Thus, it does not demonstrate cross-model transferability of specific fault-inducing inputs.",
                    "evidence": "Section VI\u2013C (vApp Store Evaluation, pp. 12\u201313): the authors describe running LipFuzzer \u201cby using templates crawled from the Amazon Alexa Store\u201d and \u201ctemplates crawled from the Google Assistant Store\u201d but do not report reusing the same LAPSUS across both platforms.  Section VI (Evaluation Goals, p. 11): none of the three stated goals mention testing the transfer of inputs from one model to another."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Data-driven"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Woodlief et al. - 2022 - Semantic image fuzzing of AI perception systems": {
        "key": "woodlief2022semantic",
        "name": "semSensFuzz",
        "year": "2022",
        "citation_count": "18",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "generates safety-critical scenarios (cars, pedestrians, obstacles) to expose perception failures."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Offers descriptive correlations between input changes and output errors, but no analysis on root cause"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Applies design constratraints but no measure of naturalness of the fault-triggering fuzzed inputs"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Perturbations occur at a semantic level that is plausibly I/O-stable, but the paper neither enforces nor verifies persistence after I/O transformations, and minor artifacts from the mutation implementation may cause non-persistent faults."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper reuses fault-inducing inputs generated by semImFuzz to test multiple different perception models: the same set of 150,000 semantically mutated tests was executed against each of the five SUTs, and the authors report aggregated counts of inconsistencies across SUTs (e.g., \"1210 SUT inconsistencies resulting from 884 mutations\"), which demonstrates reuse of generated failures to evaluate other models. However, the paper does not present design elements or explicit mechanisms aimed at producing or measuring transferable adversarial attacks (e.g., surrogate-model training, cross-model transfer rate analyses, or targeted transferability objectives). The evaluation reports per-SUT inconsistency counts and notes which mutations trigger errors, but it does not characterize transfer rates or describe methods explicitly designed to discover transferable security failures across models. These facts match the metric definition for Medium: inputs are reused to test transferability but the fuzzing design is not explicitly aimed at discovering transferable attacks.",
                    "evidence": "Section 6.2 (Tests Generated), p. 1966: \"For evaluation, we ran each SUT on 150,000 tests generated by semImFuzz comprised of 50,000 'Add Car' mutations, 50,000 'Add Person' mutations, and 50,000 'Change Car Color' mutations.\" \n\nSection 6.4.1 (RQ1 Results: Finding Inconsistencies), p. 1966: \"We first remark that semImFuzz found 1210 SUT inconsistencies resulting from 884 mutations and that each SUT had over 100 inconsistencies.\" \n\n(Absence of transfer-focused design) The paper's evaluation and methodology sections (Sections 3, 4, 6) describe mutation design, conformity checks, and per-SUT inconsistency reporting but do not describe surrogate-model-based strategies, explicit cross-model transferability metrics, or procedures to maximize/measure transfer of failures between models. For example, Section 6 (Evaluation) frames results as per-SUT inconsistency counts and severity drops rather than transfer rates or cross-model vulnerability analyses."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's evaluation reuses the generated fault-inducing inputs across multiple models but the fuzzing design is not explicitly aimed at discovering transferable attacks. The methodology involves creating a single set of 150,000 mutated inputs and then executing this same set against five different perception models (SUTs) performing the same task. This experimental design inherently tests for transferability by observing how different models react to the same set of inputs. However, the design of the fuzzer, `semImFuzz`, focuses on generating semantically valid and conforming mutations based on real-world constraints (e.g., adding cars to roads, changing colors) rather than incorporating explicit mechanisms to discover vulnerabilities that are common across different model architectures. Therefore, it satisfies the condition of reusing inputs but lacks the explicit design goal for transferability required for a 'High' rating.",
                    "evidence": "Page 9, Section 6.2 Tests Generated, \"For evaluation, we ran each SUT on 150,000 tests generated by semImFuzz comprised of 50,000 \"Add Car\" mutations, 50,000 \"Add Person\" mutations, and 50,000 \u201cChange Car Color\u201d mutations.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper reuses fault-inducing inputs generated by semImFuzz to test multiple different perception models: the same set of 150,000 semantically mutated tests was executed against each of the five SUTs, and the authors report aggregated counts of inconsistencies across SUTs (e.g., \"1210 SUT inconsistencies resulting from 884 mutations\"), which demonstrates reuse of generated failures to evaluate other models. However, the paper does not present design elements or explicit mechanisms aimed at producing or measuring transferable adversarial attacks (e.g., surrogate-model training, cross-model transfer rate analyses, or targeted transferability objectives). The evaluation reports per-SUT inconsistency counts and notes which mutations trigger errors, but it does not characterize transfer rates or describe methods explicitly designed to discover transferable security failures across models. These facts match the metric definition for Medium: inputs are reused to test transferability but the fuzzing design is not explicitly aimed at discovering transferable attacks.",
                    "evidence": "Section 6.2 (Tests Generated), p. 1966: \"For evaluation, we ran each SUT on 150,000 tests generated by semImFuzz comprised of 50,000 'Add Car' mutations, 50,000 'Add Person' mutations, and 50,000 'Change Car Color' mutations.\" \n\nSection 6.4.1 (RQ1 Results: Finding Inconsistencies), p. 1966: \"We first remark that semImFuzz found 1210 SUT inconsistencies resulting from 884 mutations and that each SUT had over 100 inconsistencies.\" \n\n(Absence of transfer-focused design) The paper's evaluation and methodology sections (Sections 3, 4, 6) describe mutation design, conformity checks, and per-SUT inconsistency reporting but do not describe surrogate-model-based strategies, explicit cross-model transferability metrics, or procedures to maximize/measure transfer of failures between models. For example, Section 6 (Evaluation) frames results as per-SUT inconsistency counts and severity drops rather than transfer rates or cross-model vulnerability analyses."
                },
                "manual": {
                    "value": "Low",
                    "why": "Mutated inputs are evaluated independently across multiple inputs but no measure whether same failures discovered by the fauzzer transfer to other models",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Data-driven"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Lee et al. - 2020 - Effective white-box testing of deep neural networks with adaptive neuron-selection strategy": {
        "key": "lee2020effective",
        "name": "ADAPT",
        "year": "2020",
        "citation_count": "81",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Correlates neuron features with faults"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Norm-bounded"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper presents Adapt as a white-box testing technique that operates on a single target DNN (Algorithm 2 takes a single DNN as input) and evaluates it per-model. The experiments report per-model coverage and adversarial inputs but do not test or report whether fault-inducing inputs produced on one model transfer to other models. The authors explicitly note that DeepXplore's differential (multi-DNN) testing was not used in its multi-model mode: \u201cwe used it with a single DNN and labeled data in our experiments.\u201d There is no experiment, analysis, or mechanism described to generate or evaluate transferable (cross-model) fault-inducing inputs or surrogate-model attacks. Therefore the paper does not demonstrate attack transferability across models.",
                    "evidence": "Section 3.2 (Algorithm 2): \"1: procedure Adapt(DNN, I,Cov)\" (Algorithm 2 signature shows Adapt operates on a single DNN) \u2014 (page 168, Section 3.2, Algorithm 2).\n\nSection 4.1 Experimental Setup: \"DeepXplore originally performs differential testing requiring multiple DNNs without labeled data but we used it with a single DNN and labeled data in our experiments.\" (shows the authors evaluated tools in single-model mode and did not perform multi-model/differential transfer experiments) \u2014 (page 170, Section 4.1 Experimental Setup).\n\nOverall evaluation description and results (Sections 4.2\u20134.3) report per-model coverage and adversarial inputs but contain no experiments or discussion on reusing inputs across models or on surrogate-model transferability \u2014 (pages 171\u2013173, Sections 4.2\u20134.3)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's methodology and experimental evaluation focus on generating fault-inducing inputs for a single, white-box model at a time. The proposed method, ADAPT, is designed to optimize its neuron-selection strategy based on the internal structure and feedback from the specific model under test. The experiments are conducted independently for each of the four models (LeNet-4, LeNet-5, VGG-19, ResNet-50). The results report the number of adversarial inputs found for each model individually. There is no procedure or result mentioned where fault-inducing inputs generated for one model (e.g., LeNet-4) are tested against another model performing the same task (e.g., LeNet-5) to assess transferability. The paper even notes that it adapted a baseline technique, DeepXplore, to work with a single DNN, moving away from its original multi-DNN differential testing approach.",
                    "evidence": "Page 7, Section 4.1, \"Experimental Setup\": \"DeepXplore originally performs differential testing requiring multiple DNNs without labeled data but we used it with a single DNN and labeled data in our experiments.\" This quote shows a deliberate choice to focus on single-model testing, not cross-model transferability. Additionally, Table 3 on page 9, \"Effectiveness for finding adversarial inputs,\" presents results for each model in separate, independent sections, with no indication that inputs generated for one model were ever tested on another."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper presents Adapt as a white-box testing technique that operates on a single target DNN (Algorithm 2 takes a single DNN as input) and evaluates it per-model. The experiments report per-model coverage and adversarial inputs but do not test or report whether fault-inducing inputs produced on one model transfer to other models. The authors explicitly note that DeepXplore's differential (multi-DNN) testing was not used in its multi-model mode: \u201cwe used it with a single DNN and labeled data in our experiments.\u201d There is no experiment, analysis, or mechanism described to generate or evaluate transferable (cross-model) fault-inducing inputs or surrogate-model attacks. Therefore the paper does not demonstrate attack transferability across models.",
                    "evidence": "Section 3.2 (Algorithm 2): \"1: procedure Adapt(DNN, I,Cov)\" (Algorithm 2 signature shows Adapt operates on a single DNN) \u2014 (page 168, Section 3.2, Algorithm 2).\n\nSection 4.1 Experimental Setup: \"DeepXplore originally performs differential testing requiring multiple DNNs without labeled data but we used it with a single DNN and labeled data in our experiments.\" (shows the authors evaluated tools in single-model mode and did not perform multi-model/differential transfer experiments) \u2014 (page 170, Section 4.1 Experimental Setup).\n\nOverall evaluation description and results (Sections 4.2\u20134.3) report per-model coverage and adversarial inputs but contain no experiments or discussion on reusing inputs across models or on surrogate-model transferability \u2014 (pages 171\u2013173, Sections 4.2\u20134.3)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Whitebox"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Ben Braiek and Khomh - 2019 - DeepEvolution A Search-Based Testing Approach for Deep Neural Networks": {
        "key": "braiek2019deepevolution",
        "name": "DeepEvolution",
        "year": "2019",
        "citation_count": "53",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Uses neuron coveage as heuristic guide, does not show correlation between coverage and neurons"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": "Enforces naturalness across iterations by computing SSIM after each mutation and rejecting any visually distorted image."
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Semantic-level pixel transformations (contrast, blur, rotation, etc.) but no clipping/rounding to ensure IO persistence"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly reuses fault-inducing inputs generated by DeepEvolution to detect divergences between an original model and its quantized version (i.e., it reuses generated inputs to test another model variant), but the fuzzing design is not presented as being explicitly aimed at discovering transferable attacks across independently implemented/trained models. The experiments focus on per-dataset single-model instantiations (LeNet for MNIST, CifarNet for CIFAR-10) and compare original vs. quantized versions; they do not evaluate reuse of generated faults across different model implementations trained for the same task nor claim mechanisms specifically for crafting transferable substitute-model attacks.",
                    "evidence": "Section III (page 455): \"From one generation of candidates to another, DeepEvolution performs follow-up tests with the resulting transformed inputs and stores the failed tests that exhibit erroneous DNNs behavior or induce a divergence between the original DNN and its quantized version.\" \n\nSection V.D (page 456-457) Findings: \"DeepEvolution can effectively detect defects introduced during DNN quantization, outperforming the coverage-guided fuzzing tool TensorFuzz.\" (Table III reports number of \"difference-inducing inputs\" found for quantization defects.)\n\nExperiment setup (page 456): \"For each dataset (i.e., MNIST and CIFAR-10), we took, respectively, the official open-source implementation of Tensorflow models, LeNet [3] and CifarNet [4] to allow the reproducibility of our results and comparisons with our approach.\" \n\nRelated work contrast (page 457, Section VII): Paper cites DeepXplore's use of \"multiple DNNs\u2019 implementations\" for differential testing but does not describe performing similar cross-model reuse in DeepEvolution."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's methodology and evaluation focus on generating test cases to find defects within a single Deep Neural Network (DNN) model or between a model and its own quantized version. The experiments are conducted on two distinct models, LeNet and CifarNet, but they are used for different datasets (MNIST and CIFAR-10, respectively) and are treated as separate, independent evaluation subjects. The paper does not describe any experiment or methodology where fault-inducing inputs generated for one model are subsequently used to test a different model performing the same task. Therefore, the concept of attack transferability is not assessed.",
                    "evidence": "Page 456, Section V, EMPIRICAL EVALUATION: The research questions guiding the evaluation are: \"RQ1: How much can DeepEvolution increase the coverage of generated test cases? RQ2: Can DeepEvolution detect diverse erroneous behaviors in DNN models? RQ3: Can DeepEvolution detect divergences induced by DNN quantization?\". All three questions are focused on the behavior of a single model under test. \n\nPage 456, Section V.A, Experiment Setup: \"DNNs. For each dataset (i.e., MNIST and CIFAR-10), we took, respectively, the official open-source implementation of Tensorflow models, LeNet [3] and CifarNet [4] to allow the reproducibility of our results and comparisons with our approach.\" This shows that the experiments test one specific model per dataset, rather than testing multiple models on a single dataset to check for transferability.\n\nPage 457, Section V.D, RQ3: DNN Quantization Defects: The evaluation for quantization defects involves comparing a model to its own modified version, not to a different model. \"TensorFuzz [5] performs a coverage-guided fuzzing process to generate mutated inputs that are able to expose disagreements between a DNN trained on MNIST (that is 32-bit floating point precision) and its quantized versions...\" This further illustrates that the testing scope is limited to a single model architecture at a time."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DeepEvolution does not demonstrate attack transferability across independently trained model implementations on the same task. The paper only reuses generated fault\u2010inducing inputs to compare a model with its own quantized variant, not to test different models (e.g., using LeNet inputs on another MNIST model or CifarNet inputs on another CIFAR-10 model). There is no experimental evaluation or design mechanism aimed at assessing whether faults transfer across distinct model architectures trained for the same task.",
                    "evidence": "Section III (page 455) describes follow-up tests only between the original DNN and its quantized version. Section V.D (pages 456\u2013457) evaluates quantization defects by comparing each model to its own truncated-precision variant; no experiments reuse inputs across different model implementations performing the same task."
                },
                "manual": {
                    "value": "Low",
                    "why": "Use quantized model for differntial testing based oracle, not for checking fauilure transferability",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic",
                "Differential"
            ]
        }
    },
    "Ma et al. - 2021 - HDTest Differential Fuzz Testing of Brain-Inspired Hyperdimensional Computing": {
        "key": "ma2021hdtest",
        "name": "HDTest",
        "year": "2021",
        "citation_count": "39",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Finds generic misclassifications"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": " It uses hypervector distance purely as a heuristic to guide input mutations - no insight where/how/why the faults occur"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "norm-bounded"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": "In memory floating point mutation, no safeguards for IO preservation"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether adversarial / fault-inducing inputs generated on one HDC model transfer to other models performing the same task. All experiments generate and use adversarial inputs against a single HDC classifier on MNIST (including using generated inputs to retrain that same classifier). The paper does not report reusing seeds/adversarial inputs on other independently-trained HDC models, nor does it include explicit cross-model transferability experiments or mechanisms designed to test transfer across different model implementations.",
                    "evidence": "V.A Experimental Setup (page 394): \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"  V.D Case Study on Defense Against Adversarial Attacks (page 395): \"First, for a specific HDC model, we run HDTest to generate 1000 adversarial images. ... We feed the first subset of adversarial images to the HDC model with correct labels to retrain the HDC model ... Next, we use the second subset of adversarial images to attack the HDC model. Our experimental results show that, after retraining, the rate of successful attack rate drops more than 20%.\"  V.E Discussion (page 395): \"While we only consider one dataset in this paper due to the fact that HDC is still exploring its use in image classification domain, HDTest can be naturally extended to other HDC model structures because it considers a general greybox assumption with only HV distance information.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper introduces HDTest, a differential fuzz testing method specifically designed for Hyperdimensional Computing (HDC) models. The evaluation of HDTest is conducted on a single HDC model architecture trained for the MNIST classification task. The experiments focus on generating adversarial inputs that cause mispredictions in this specific model and then using these generated inputs to retrain and improve the robustness of that same model. The paper does not perform any experiments to test whether the fault-inducing inputs generated for this one model also cause failures in other, independently trained HDC models or in models based on different architectures (e.g., DNNs) performing the same task. Therefore, the transferability of the generated attacks is not demonstrated.",
                    "evidence": "Page 4, Section V-A, Experimental Setup: \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"\n\nPage 6, Section V-D, Case Study on Defense Against Adversarial Attacks: \"First, for a specific HDC model, we run HDTest to generate 1000 adversarial images... We feed the first subset of adversarial images to the HDC model with correct labels to retrain the HDC model, i.e., updating the reference HVs. Next, we use the second subset of adversarial images to attack the HDC model.\" This section describes a process of retraining and then re-attacking the same model, not testing a different one."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether adversarial / fault-inducing inputs generated on one HDC model transfer to other models performing the same task. All experiments generate and use adversarial inputs against a single HDC classifier on MNIST (including using generated inputs to retrain that same classifier). The paper does not report reusing seeds/adversarial inputs on other independently-trained HDC models, nor does it include explicit cross-model transferability experiments or mechanisms designed to test transfer across different model implementations.",
                    "evidence": "V.A Experimental Setup (page 394): \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"  V.D Case Study on Defense Against Adversarial Attacks (page 395): \"First, for a specific HDC model, we run HDTest to generate 1000 adversarial images. ... We feed the first subset of adversarial images to the HDC model with correct labels to retrain the HDC model ... Next, we use the second subset of adversarial images to attack the HDC model. Our experimental results show that, after retraining, the rate of successful attack rate drops more than 20%.\"  V.E Discussion (page 395): \"While we only consider one dataset in this paper due to the fact that HDC is still exploring its use in image classification domain, HDTest can be naturally extended to other HDC model structures because it considers a general greybox assumption with only HV distance information.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Du et al. - 2019 - DeepStellar model-based quantitative analysis of stateful deep learning systems": {
        "key": "du2019deepstellar",
        "name": "DeepStellar",
        "year": "2019",
        "citation_count": "188",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Broad exploration to uncover functional errors"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Shows correlation between coverage criteria and faults"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Design contraints on perturbations but no naturalness measure of final fault-triggering inputs"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Applies deephunter metamorphic mutations that are plausibly I/O-stable but does not apply clipping and rounding to ensure"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate whether fault-inducing inputs transfer across different models. All algorithms and experiments are formulated and executed per-target-model (the abstract model M and the RNN D are paired), adversarial examples were generated for particular models only, and the authors explicitly note they could only generate adversarial samples for one DeepSpeech version due to tool compatibility. There are no experiments reusing inputs crafted on one model to test other models performing the same task, nor design elements aimed at discovering transferable attacks.",
                    "evidence": "Section 5.1 (page 481): \"input :D: RNN-based DL system, M: Abstract model of D\" and \"We first collect a set of benign samples B and a set of adversarial samples A.\" (Algorithm 1) \u2014 shows detection is trained per given RNN and its abstract model. Section 5.2 (page 482): \"input :I: Initial seeds, D: RNN-based DL system, M: Abstract model of D\" (Algorithm 2) \u2014 testing is guided against a single RNN instance D with its model M. Section 6.1 Data Preparation (page 483): \"Finally, we only successfully generate adversarial samples for DeepSpeech-0.1.1 because there exists a compatibility issue between DeepSpeech-0.3.0 and the adversarial attack tools [8] we used.\" \u2014 indicates adversarial samples were not produced for multiple models and no cross-model reuse/transfer experiments were performed. Section 6.4 (page 485): \"...we trained a classifier for each model to detect adversarial samples.\" \u2014 confirms evaluation is per-model, not testing transferability across models."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes methods for generating and detecting adversarial samples for Recurrent Neural Networks (RNNs). The core methodology involves creating a model-specific abstraction (a Discrete-Time Markov Chain or DTMC) based on the internal state transitions of a single, trained RNN. The evaluation is conducted on multiple models performing the same tasks (e.g., MNIST-LSTM and MNIST-GRU for image classification). However, the adversarial samples are always generated for a specific model and then evaluated on that same model. The paper does not perform any experiments where fault-inducing inputs generated for one model are reused to test for failures on another model. The methodology is inherently designed to analyze a single model at a time and lacks any mechanism to support the discovery of transferable failures.",
                    "evidence": "Page 7, Algorithm 2, \"Coverage guided testing of RNN\": The algorithm for generating adversarial samples takes a single \"RNN-based DL system, M: Abstract model of D\" as input. All subsequent steps within the testing loop, including prediction, coverage analysis, and failure checking, are performed with respect to this single model `D` and its abstract model `M`. This demonstrates the single-model focus of the generation technique.\n\nPage 10, Section 6.4, \"RQ3. Adversarial Sample Detection\": In the setup for evaluating the detection mechanism, the paper states: \"With the constructed dataset, we trained a classifier for each model to detect adversarial samples.\" This confirms that the detection process is model-specific and does not involve testing the transferability of adversarial examples between different models.\n\nPage 10, Section 6.5, \"RQ4. Coverage-guided Testing\": The evaluation of the coverage-guided testing is presented separately for MNIST-LSTM and MNIST-GRU in Table 6. The accompanying text describes the testing process for each model independently, without any mention of cross-testing the generated adversarial samples between the two models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate whether fault-inducing inputs transfer across different models. All algorithms and experiments are formulated and executed per-target-model (the abstract model M and the RNN D are paired), adversarial examples were generated for particular models only, and the authors explicitly note they could only generate adversarial samples for one DeepSpeech version due to tool compatibility. There are no experiments reusing inputs crafted on one model to test other models performing the same task, nor design elements aimed at discovering transferable attacks.",
                    "evidence": "Section 5.1 (page 481): \"input :D: RNN-based DL system, M: Abstract model of D\" and \"We first collect a set of benign samples B and a set of adversarial samples A.\" (Algorithm 1) \u2014 shows detection is trained per given RNN and its abstract model. Section 5.2 (page 482): \"input :I: Initial seeds, D: RNN-based DL system, M: Abstract model of D\" (Algorithm 2) \u2014 testing is guided against a single RNN instance D with its model M. Section 6.1 Data Preparation (page 483): \"Finally, we only successfully generate adversarial samples for DeepSpeech-0.1.1 because there exists a compatibility issue between DeepSpeech-0.3.0 and the adversarial attack tools [8] we used.\" \u2014 indicates adversarial samples were not produced for multiple models and no cross-model reuse/transfer experiments were performed. Section 6.4 (page 485): \"...we trained a classifier for each model to detect adversarial samples.\" \u2014 confirms evaluation is per-model, not testing transferability across models."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic",
                "Property-based"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ]
        }
    },
    "Huang et al. - 2022 - Coverage-Guided Testing for Recurrent Neural Networks": {
        "key": "huang2021coverage",
        "name": "TESTRNN",
        "year": "2021",
        "citation_count": "66",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": "Even though it can detect backdoor triggers, this is purely diagnostic. The fuzzer exposes low-level functional and robustness failures (misclassifications, metamorphic inconsistencies, and backdoor-induced activation anomalies) without breaching behavioral or safety boundaries."
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "broadly explore the model's internal behavior and flag any unexpected classification (adversarial or backdoor) as a defect."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Linnks faulty behaviors to internal LSTM mechanisms. Correlates coverage with faults. identify where and how the failures arise internally. The analysis is correlational, not causal. "
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Constraint bounded mutation but not evidence if naturalness measure of the generated inputs"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": "Applies low level noise but does not enforce IO persistence with clipping/clamping"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. All test-case generation and evaluation are performed per-target model (seed inputs are mutated into tests for that same model), and the experiments measure adversarial/backdoor discovery rates per model rather than reusing fault-inducing inputs generated on a surrogate to test other models. The paper also does not describe any fuzzing design or mechanisms aimed at producing inputs intended to transfer across different model implementations performing the same task.",
                    "evidence": "Page 1197, Section VI (Algorithm 1 / Test generation): \"The test suite T is initialized with T0, a corpus of seed inputs (Line 1). New test cases are generated by mutating seed inputs. It keeps the traceability of test cases via a mapping orig that maps each test case generated back to its seed origin.\" \n\nPage 1201, Section VII-C (Detecting RNN Defects): \"As shown in Table V, adversarial samples rate and number of unique adversarial samples in the generated test suite are two important indicators for the robustness evaluation. ... For a set of trained models, we pursue the model, the test suite of which contains less amount of adversarial samples and unique adversarial samples.\" \n\nPage 1202\u20131203, Section VII-F (Comparison With State-of-the-Art Testing Methods): \"We start from 100 seeds drawn from training set and generate 100 000 test cases by DeepStellar and TESTRNN, respectively. The test suites are evaluated for the coverage rate and number of adversarial samples.\" \n\n(These quoted passages show the paper's testing/generation and evaluation are carried out per model/seed and report per-model adversarial/backdoor findings; the paper contains no experimental description or results where fault-inducing inputs crafted on one (surrogate) model are reused to test other models, nor any described mechanisms explicitly aimed at discovering transferable attacks.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a coverage-guided testing method called TESTRNN, which is a white-box approach that generates test cases to maximize structural coverage for a specific RNN model under test. The methodology is designed to operate on a single model at a time. The experimental evaluation, including the comparison of model robustness in Table V, applies the test generation process independently to different models to compare their individual robustness characteristics (e.g., number of adversarial samples found). The paper does not contain any experiments where fault-inducing inputs generated for one model are used to test a different model to assess transferability. Furthermore, the test generation algorithm is guided by internal coverage metrics of the specific model being tested and does not incorporate any mechanisms designed to promote the discovery of transferable attacks.",
                    "evidence": "Page 11, Section VII-C, Table V and accompanying text: The paper evaluates the robustness of two different models per task (e.g., Model 1 and Model 2 for MNIST). However, the methodology described involves running the test generation process separately for each model to compare their individual vulnerability. The text explains the goal as comparing models: \"For a set of trained models, we pursue the model, the test suite of which contains less amount of adversarial samples and unique adversarial samples. For example, we pick up model 2 for Lipophilicity prediction, since the values of two indicators are way smaller than that of model 1.\" This indicates a comparison of the models' robustness via separate testing runs, not a test of attack transferability between them."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. All test-case generation and evaluation are performed per-target model (seed inputs are mutated into tests for that same model), and the experiments measure adversarial/backdoor discovery rates per model rather than reusing fault-inducing inputs generated on a surrogate to test other models. The paper also does not describe any fuzzing design or mechanisms aimed at producing inputs intended to transfer across different model implementations performing the same task.",
                    "evidence": "Page 1197, Section VI (Algorithm 1 / Test generation): \"The test suite T is initialized with T0, a corpus of seed inputs (Line 1). New test cases are generated by mutating seed inputs. It keeps the traceability of test cases via a mapping orig that maps each test case generated back to its seed origin.\" \n\nPage 1201, Section VII-C (Detecting RNN Defects): \"As shown in Table V, adversarial samples rate and number of unique adversarial samples in the generated test suite are two important indicators for the robustness evaluation. ... For a set of trained models, we pursue the model, the test suite of which contains less amount of adversarial samples and unique adversarial samples.\" \n\nPage 1202\u20131203, Section VII-F (Comparison With State-of-the-Art Testing Methods): \"We start from 100 seeds drawn from training set and generate 100 000 test cases by DeepStellar and TESTRNN, respectively. The test suites are evaluated for the coverage rate and number of adversarial samples.\" \n\n(These quoted passages show the paper's testing/generation and evaluation are carried out per model/seed and report per-model adversarial/backdoor findings; the paper contains no experimental description or results where fault-inducing inputs crafted on one (surrogate) model are reused to test other models, nor any described mechanisms explicitly aimed at discovering transferable attacks.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Oracle-guided"
            ]
        }
    },
    "Li et al. - 2021 - Testing DNN-based Autonomous Driving Systems under Critical Environmental Conditions": {
        "key": "li2021testing",
        "name": "TACTIC",
        "year": "2021",
        "citation_count": "51",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Finds inconsistent steering under environmental variations"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Relies on coverage-guided mutation and assumes based on prior work, that higher coverage correlates with fault discovery"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": "Test images are generated through a GAN trained to mimic real driving scenes, so naturalness is preserved throughout fuzzing iterations. Provides human evaluation."
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Produced synthesized images"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate transferability of fault-inducing inputs across models. TACTIC is defined and executed per subject model (the search takes the subject DNN as input and returns critical conditions for that model), and all experiments generate and evaluate critical styles separately for each ADS. There is no experiment that crafts fault-inducing inputs on one surrogate model and reuses them to test other models, nor does the design include explicit mechanisms aimed at finding transferable attacks across different model implementations.",
                    "evidence": "Page 3, Section 3.3 \"Search of Critical Environmental Conditions\": \"TACTIC receives as inputs the subject DNN-based ADS N, the MUNIT model M trained for the given environmental type, and a set Io of the original driving scenes. The output of TACTIC is a set S of the critical environmental conditions of the given environmental type.\"; Page 4, Section 4.2 \"Comparison with Baselines on Effectiveness - Setup\": \"We execute TACTIC with two coverage-guiding strategies: KMNC (denoted as TACTICKMNC) and NBC (denoted as TACTICNBC), respectively, on each of the three subject DNN-based ADSs, under the five environmental types.\"; Page 4, Section 4.2 \"Setup\" (describing baselines): \"For Rc, when testing the three subject systems, we randomly sample 4 style vectors (equal to the number of critical style vectors generated by TACTIC) for each of the environmental types and separately apply the 4 random style vectors on the Udacity testing dataset to synthesise testing driving scenes...\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a method, TACTIC, to identify critical environmental conditions for a specific, or 'subject', Deep Neural Network (DNN) based Autonomous Driving System (ADS). The search for these conditions is guided by model-specific metrics, such as neuron coverage and steering angle divergence, which are calculated on the subject model. The experimental evaluation involves three different ADS models (Dave-orig, Dave-dropout, Chauffeur). However, the TACTIC framework is executed independently for each model to find its specific critical conditions. The paper does not contain any experiments or discussion on whether the fault-inducing environmental conditions discovered for one model also cause erroneous behavior in the other models. Therefore, it does not demonstrate the transferability of the discovered failures.",
                    "evidence": "Page 6, Section 3.3, \"Search of Critical Environmental Conditions\": \"TACTIC receives as inputs the subject DNN-based ADS N, the MUNIT model M trained for the given environmental type, and a set Io of the original driving scenes.\" This statement clarifies that the method is designed to operate on a single 'subject' model 'N' at a time. \n\nPage 7, Section 4.2, \"Comparison with Baselines on Effectiveness\", paragraph \"Setup\": \"We execute TACTIC with two coverage-guiding strategies: KMNC (denoted as TACTIC KMNC ) and NBC (denoted as TACTIC NBC ), respectively, on each of the three subject DNN-based ADSs, under the five environmental types.\" This shows that the experiments were run separately for each of the three models, rather than generating inputs on one and testing them on others. The results are then presented independently for each model, confirming the lack of a transferability analysis."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate transferability of fault-inducing inputs across models. TACTIC is defined and executed per subject model (the search takes the subject DNN as input and returns critical conditions for that model), and all experiments generate and evaluate critical styles separately for each ADS. There is no experiment that crafts fault-inducing inputs on one surrogate model and reuses them to test other models, nor does the design include explicit mechanisms aimed at finding transferable attacks across different model implementations.",
                    "evidence": "Page 3, Section 3.3 \"Search of Critical Environmental Conditions\": \"TACTIC receives as inputs the subject DNN-based ADS N, the MUNIT model M trained for the given environmental type, and a set Io of the original driving scenes. The output of TACTIC is a set S of the critical environmental conditions of the given environmental type.\"; Page 4, Section 4.2 \"Comparison with Baselines on Effectiveness - Setup\": \"We execute TACTIC with two coverage-guiding strategies: KMNC (denoted as TACTICKMNC) and NBC (denoted as TACTICNBC), respectively, on each of the three subject DNN-based ADSs, under the five environmental types.\"; Page 4, Section 4.2 \"Setup\" (describing baselines): \"For Rc, when testing the three subject systems, we randomly sample 4 style vectors (equal to the number of critical style vectors generated by TACTIC) for each of the environmental types and separately apply the 4 random style vectors on the Udacity testing dataset to synthesise testing driving scenes...\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Oracle-guided"
            ]
        }
    },
    "Wang et al. - 2023 - DistXplore Distribution-Guided Testing for Evaluating and Enhancing Deep Learning Systems": {
        "key": "wang2023distxplore",
        "name": "DistXplore",
        "year": "2023",
        "citation_count": "13",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "High",
                    "why": "Guides test generation to minimize distribution differences between source class inputs and target class."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "links the occurrence and detectability of errors to the statistical closeness (measured via MMD) between the distribution of generated test inputs and the distribution of another class at the logits layer."
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "metamorphic transformations can yield decimal-valued pixels internally which may change when re-quantized to integers."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability across models. DistXplore formulates and optimizes test suites using distributions measured on a target model's internal layer outputs (the logits) and the evaluation and robustness enhancement steps retrain or test the same model on which test suites were generated. The experiments run the method separately on multiple models/datasets, but the paper does not reuse fault-inducing inputs crafted on one (surrogate) model to test other models, nor does it describe any mechanism intended to discover or promote attacks that transfer across different model implementations.",
                    "evidence": "\u2022 Section 3.1.1 (page 71, \"Model Evaluation\"): \"The distribution difference is measured on a specific layer of the DNN. In this paper, we select the logits layer, i.e., the layer before the softmax layer...\" (shows method depends on internal layer outputs of the target model).\n\n\u2022 Section 4.1.4 (page 73, \"Configuration of DistXplore\"): \"For each class, we run DistXplore multiple times ... Finally, for each model, we run DistXplore 90 times (i.e., 10 source classes \u00d7 9 target classes).\" (shows runs are per-model; no cross-model reuse is described).\n\n\u2022 Section 4.2.3 (page 75, \"RQ3: Robustness Enhancement\"): \"For each tool, we fine-tune the original model 20 epochs ... by adding the new data generated from each tool, and evaluate the empirical robustness of the new model on the validation dataset.\" (shows generated tests are used to retrain/evaluate the same model rather than testing other models).\n\n\u2022 Section 3.1.2 (page 71, \"Model Enhancement\"): description of target selection and distribution diversity refers to selecting other classes as targets (inter-class guidance), not other models or surrogate-model attacks: \"We select the classes in the same task as targets because the classification is based on their relationships...\".\n\nThere is no text in the paper that (a) states that fault-inducing inputs generated on one model were reused to test other models, (b) measures cross-model transfer of discovered faults, or (c) proposes mechanisms specifically designed to produce inputs intended to transfer across different model implementations."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's methodology and evaluation focus exclusively on generating fault-inducing inputs for a single model under test and then evaluating the properties of those inputs (e.g., their ability to bypass defenses) on that same model. The concept of attack transferability\u2014testing if inputs that cause faults on one model also cause faults on another model performing the same task\u2014is never discussed or evaluated. The experimental setup, described in Section 4, details procedures for generating errors, evaluating their strength against defenses, and using them for retraining. All these steps are performed within the context of the specific model for which the errors were generated. There is no mention of reusing these inputs to test other models.",
                    "evidence": "Page 7, Section 4.1.4, Experiment Setup, RQ Setup: \"To demonstrate the capability of DistXplore in generating strong errors for model evaluation (RQ1), we collect the test suite in the last iteration for every pair (c, c') (i.e., the best chromosome returns from Algo 1). For each model, we collect a total number of 90 chromosomes over 90 pairs, which are used to evaluate the strength of these errors. The strength of errors is measured by the success rate of bypassing defenses.\"\n\nThis quote illustrates that the evaluation is conducted on a per-model basis, where errors are generated for a specific model and their strength is evaluated against defenses on that same model, without assessing transferability to other models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability across models. DistXplore formulates and optimizes test suites using distributions measured on a target model's internal layer outputs (the logits) and the evaluation and robustness enhancement steps retrain or test the same model on which test suites were generated. The experiments run the method separately on multiple models/datasets, but the paper does not reuse fault-inducing inputs crafted on one (surrogate) model to test other models, nor does it describe any mechanism intended to discover or promote attacks that transfer across different model implementations.",
                    "evidence": "\u2022 Section 3.1.1 (page 71, \"Model Evaluation\"): \"The distribution difference is measured on a specific layer of the DNN. In this paper, we select the logits layer, i.e., the layer before the softmax layer...\" (shows method depends on internal layer outputs of the target model).\n\n\u2022 Section 4.1.4 (page 73, \"Configuration of DistXplore\"): \"For each class, we run DistXplore multiple times ... Finally, for each model, we run DistXplore 90 times (i.e., 10 source classes \u00d7 9 target classes).\" (shows runs are per-model; no cross-model reuse is described).\n\n\u2022 Section 4.2.3 (page 75, \"RQ3: Robustness Enhancement\"): \"For each tool, we fine-tune the original model 20 epochs ... by adding the new data generated from each tool, and evaluate the empirical robustness of the new model on the validation dataset.\" (shows generated tests are used to retrain/evaluate the same model rather than testing other models).\n\n\u2022 Section 3.1.2 (page 71, \"Model Enhancement\"): description of target selection and distribution diversity refers to selecting other classes as targets (inter-class guidance), not other models or surrogate-model attacks: \"We select the classes in the same task as targets because the classification is based on their relationships...\".\n\nThere is no text in the paper that (a) states that fault-inducing inputs generated on one model were reused to test other models, (b) measures cross-model transfer of discovered faults, or (c) proposes mechanisms specifically designed to produce inputs intended to transfer across different model implementations."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Does not explicitly enforce naturalness across fuzzing iterations. Apply norm constraints in each mutation step but cumulative effect is ignored. Checks visual validity on the final outputs via human evaluation."
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ]
        }
    },
    "Liu et al. - 2022 - QATest A Uniform Fuzzing Framework for Question Answering Systems": {
        "key": "liu2022qatest",
        "name": "QATest",
        "year": "2022",
        "citation_count": "21",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "No cross model failure pattern analysis"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper evaluates QATest by applying its metamorphic transformations and guidance criteria separately to multiple QA systems (UnifiedQA, ALBERT, DrQA, MARL), but it does not present any experiments in which fault-inducing inputs crafted on one model are reused to test other models, nor does it describe design elements intended to discover or prioritize transferable attacks across models. The testing guidance (N-Gram coverage and perplexity priority) and seed selection are aimed at diversity and authenticity of generated questions, not at identifying inputs that transfer across different model implementations. Consequently, the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "1) Abstract (page 1, Abstract): \"To evaluate the performance of QATest, we experiment with it on four QA systems that are designed for different tasks. The experiment results show that the tests generated by QATest detect hundreds of erroneous behaviors of QA systems efficiently.\" \u2014 indicates per-system evaluation but does not claim cross-model reuse. \n\n2) Section 4.1 (page 7, \"Research Questions\"): The four RQs listed are RQ1 (Metamorphic Transformations), RQ2 (Testing Criteria), RQ3 (Test Diversity), and RQ4 (Case Study). None ask about or evaluate transferability across models: \"RQ1 (Metamorphic Transformations): How effective are different transformation methods for generating tests for various types of QA systems?... RQ4 (Case Study): Analysis of the bug types detected by QATest.\" \n\n3) Section 4.2.2 (page 8, \"QA systems\"): \"To evaluate the performance of QATest, we select four state-of-art QA systems as the testing subjects in our experiments. We briefly introduce them as follows: UnifiedQA ..., ALBERT ..., DrQA ..., MARL ...\" \u2014 shows multiple target systems but no description of crafting faults on one and reusing them on others. \n\n4) Section 3.3 (page 4, \"Guidance Criteria Design\") and Section 3.3.1/3.3.2 (N-Gram Coverage / Perplexity Priority): These criteria are defined to improve test diversity and authenticity (e.g., \"N-Gram coverage... select the particular question that can increase the richness of the seed set\" and \"perplexity priority... to ensure that the generated test questions have good authenticity\"), with no mention of mechanisms to identify or prioritize inputs likely to transfer across models."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper does not investigate whether the fault-inducing inputs generated for one model transfer to other models. The evaluation is conducted on four distinct Question Answering (QA) systems, but each is treated as an independent target. The experiments generate test cases from a seed dataset and apply them to a corresponding QA system to find failures. There is no mention of reusing the failure-inducing questions found on one model (e.g., ALBERT) to test another model (e.g., UnifiedQA). The research questions and experimental results focus on the effectiveness, efficiency, and diversity of the fuzzing framework for individual systems. The design of the fuzzer, including its guidance criteria (N-Gram coverage and perplexity priority), is aimed at improving bug detection on a single target rather than discovering shared vulnerabilities or transferable attacks across models.",
                    "evidence": "Page 8, Section 5.1, \"Answer to RQ1: Metamorphic Transformations\": \"To evaluate the effectiveness of each type of MR applied by QATest, we select 500 test cases from each data set and apply the MRs proposed in Section 3.2 to generate test data sets. Then, we record the number of failed tests in the generated test set on the corresponding QA systems, which is shown in Table 3.\" This statement describes an experimental process where each QA system is tested independently. The results presented in Tables 3, 4, and 5 consistently show separate evaluations for each system, with no cross-testing of generated inputs."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper evaluates QATest by applying its metamorphic transformations and guidance criteria separately to multiple QA systems (UnifiedQA, ALBERT, DrQA, MARL), but it does not present any experiments in which fault-inducing inputs crafted on one model are reused to test other models, nor does it describe design elements intended to discover or prioritize transferable attacks across models. The testing guidance (N-Gram coverage and perplexity priority) and seed selection are aimed at diversity and authenticity of generated questions, not at identifying inputs that transfer across different model implementations. Consequently, the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "1) Abstract (page 1, Abstract): \"To evaluate the performance of QATest, we experiment with it on four QA systems that are designed for different tasks. The experiment results show that the tests generated by QATest detect hundreds of erroneous behaviors of QA systems efficiently.\" \u2014 indicates per-system evaluation but does not claim cross-model reuse. \n\n2) Section 4.1 (page 7, \"Research Questions\"): The four RQs listed are RQ1 (Metamorphic Transformations), RQ2 (Testing Criteria), RQ3 (Test Diversity), and RQ4 (Case Study). None ask about or evaluate transferability across models: \"RQ1 (Metamorphic Transformations): How effective are different transformation methods for generating tests for various types of QA systems?... RQ4 (Case Study): Analysis of the bug types detected by QATest.\" \n\n3) Section 4.2.2 (page 8, \"QA systems\"): \"To evaluate the performance of QATest, we select four state-of-art QA systems as the testing subjects in our experiments. We briefly introduce them as follows: UnifiedQA ..., ALBERT ..., DrQA ..., MARL ...\" \u2014 shows multiple target systems but no description of crafting faults on one and reusing them on others. \n\n4) Section 3.3 (page 4, \"Guidance Criteria Design\") and Section 3.3.1/3.3.2 (N-Gram Coverage / Perplexity Priority): These criteria are defined to improve test diversity and authenticity (e.g., \"N-Gram coverage... select the particular question that can increase the richness of the seed set\" and \"perplexity priority... to ensure that the generated test questions have good authenticity\"), with no mention of mechanisms to identify or prioritize inputs likely to transfer across models."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Text level mutation "
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Data-driven"
            ]
        }
    },
    "Wang et al. - 2022 - BET black-box efficient testing for convolutional neural networks": {
        "key": "wang2022bet",
        "name": "BET",
        "year": "2022",
        "citation_count": "26",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Only reports failures without any insight on root-cause"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "norm-bounded"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Applies integer perturbations. Shows failure stability empricaclly. Shows ADAPT, DLFUZZ, DeepExplore failures are unstable"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "Does not show transferability to other models or common pattern between models. It uses differential testing to determine fault if two model versions gives inconsistent output"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly targets differential testing (DT) and includes a tunable objective function designed to maximize disagreements between a target model and other models (Equation 5). The authors construct surrogate/alternative implementations (quantized 8-bit and 16-bit variants) and generate fault-inducing inputs that cause disagreements between the original model and these other models; they report DT results (Tables 1\u20134) demonstrating many such cross-model disagreement inputs. Thus BET both (1) reuses/queries inputs across multiple models and (2) incorporates an explicit mechanism (the DT objective term summing differences with other models) to discover transferable failures across model implementations.",
                    "evidence": "Page 166, Section 2.2 (Related Work / definition of DT): \"There are two common scenarios for CNN testing, i.e., differential testing (DT) [31, 46] and single model testing (ST) [12, 19]. DT finds error-inducing inputs which trigger behavior inconsistencies among multiple models that have similar functionality ... For example, given two CNN models C1, C2 with similar functionality, an error-inducing input x_err is found if C1(x_err) \u2260 C2(x_err).\" \n\nPage 167, Section 3.3 (Tunable objective function for DT): \"Tunable objective function for DT. For DT, we hope to identify inputs that can maximize the prediction difference between the target model with other models.... The tunable objective function for DT is defined in Equation 5. DOF(x) = C(x)[l_i] + \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] |, ... The second term \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] | aims to find disagreements between the target model C with a batch of other models C\u0302 = {C\u0302_1, C\u0302_2, ...}, as such disagreements indicate error-inducing inputs are found.\" (Equation 5 and surrounding text)\n\nPage 170\u2013171, Section 4.2 (Evaluation: Constructing differential models / Results): \"Constructing differential models. ... We apply these two options to all models and we get corresponding 8-bits and 16-bits quantized models as shown in Table 1. Then, we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\" \n\nPage 171, Section 4.2 (Results): \"Results. Table 2, Table 3 and Table 4 show the results of DT on all datasets and models, respectively. Overall, under the same experimental conditions, BET significantly outperforms DiffChaser on all comparison metrics.\" (The presence of these DT-specific evaluation tables shows BET-generated inputs are evaluated across model variants to demonstrate cross-model disagreements.)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, BET, is explicitly designed and evaluated for a differential testing (DT) scenario, which aligns with the concept of attack transferability. The fuzzing design incorporates an explicit mechanism\u2014a tunable objective function for DT\u2014specifically to discover transferable failures by maximizing the prediction differences between a target model and other related models. The evaluation section is dedicated to demonstrating this capability, where generated inputs are systematically tested against pairs of models (an original and its quantized version) to find disagreements, which are a form of transferable failure.",
                    "evidence": "Page 6, Section 3.3, \"Tunable objective function for DT\": \"For DT, we hope to identify inputs that can maximize the prediction difference between the target model with other models... The tunable objective function for DT is defined in Equation 5... The second term \u03a3|C(x)[lo] \u2013 \u0108i(x)[lo]| aims to find disagreements between the target model C with a batch of other models \u0108 = {C1, C2, ...\u0108n}, as such disagreements indicate error-inducing inputs are found.\"\n\nPage 8, Section 4.2, \"Evaluation of Black-Box Differential Testing\": \"Constructing differential models. To get a convincing DT result, we directly use Tensorflow-Lite... to construct quantized models for DT... Then, we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly targets differential testing (DT) and includes a tunable objective function designed to maximize disagreements between a target model and other models (Equation 5). The authors construct surrogate/alternative implementations (quantized 8-bit and 16-bit variants) and generate fault-inducing inputs that cause disagreements between the original model and these other models; they report DT results (Tables 1\u20134) demonstrating many such cross-model disagreement inputs. Thus BET both (1) reuses/queries inputs across multiple models and (2) incorporates an explicit mechanism (the DT objective term summing differences with other models) to discover transferable failures across model implementations.",
                    "evidence": "Page 166, Section 2.2 (Related Work / definition of DT): \"There are two common scenarios for CNN testing, i.e., differential testing (DT) [31, 46] and single model testing (ST) [12, 19]. DT finds error-inducing inputs which trigger behavior inconsistencies among multiple models that have similar functionality ... For example, given two CNN models C1, C2 with similar functionality, an error-inducing input x_err is found if C1(x_err) \u2260 C2(x_err).\" \n\nPage 167, Section 3.3 (Tunable objective function for DT): \"Tunable objective function for DT. For DT, we hope to identify inputs that can maximize the prediction difference between the target model with other models.... The tunable objective function for DT is defined in Equation 5. DOF(x) = C(x)[l_i] + \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] |, ... The second term \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] | aims to find disagreements between the target model C with a batch of other models C\u0302 = {C\u0302_1, C\u0302_2, ...}, as such disagreements indicate error-inducing inputs are found.\" (Equation 5 and surrounding text)\n\nPage 170\u2013171, Section 4.2 (Evaluation: Constructing differential models / Results): \"Constructing differential models. ... We apply these two options to all models and we get corresponding 8-bits and 16-bits quantized models as shown in Table 1. Then, we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\" \n\nPage 171, Section 4.2 (Results): \"Results. Table 2, Table 3 and Table 4 show the results of DT on all datasets and models, respectively. Overall, under the same experimental conditions, BET significantly outperforms DiffChaser on all comparison metrics.\" (The presence of these DT-specific evaluation tables shows BET-generated inputs are evaluated across model variants to demonstrate cross-model disagreements.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "Uses differential testing based oracle to detect failure, not to check transferability from one to another.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential",
                "Property-based"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Zhang et al. - 2020 - Towards characterizing adversarial defects of deep learning software from the lens of uncertainty": {
        "key": "zhang2020towards",
        "name": "KuK",
        "year": "2020",
        "citation_count": "101",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": "Demostrates its ability to bypass adversarail defense mechanisms"
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "aims to maximize model uncertainty and reveal generic misclassifications"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Shows correlatin"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "norm-bounded"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": "Applies small floating point perturbations without clipping/rounding"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "evaluates several models independently but does not analyze cross-model consistency in failure behaviors."
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or measure whether fault-inducing inputs crafted on one model transfer to other models performing the same task. All experiments generate and evaluate adversarial / uncommon inputs per target model (attacks and KuK generation are applied to each model separately) and the evaluation of \"transfer\" in the paper is limited to applying inputs crafted on an original model to a defended/retrained version of that same model (a defense transfer scenario), not to distinct other model implementations or architectures. There is no reported experiment that reuses inputs generated on one surrogate model to test other (unseen) models or that explicitly measures cross-model transferability across different model implementations (e.g., LeNet-5 \u2192 NIN or ResNet \u2192 MobileNet).",
                    "evidence": "Page 740, Introduction (research questions): \"RQ4: To what extent are the uncommon samples defended by existing adversarial defense techniques compared with the common ones?\" \u2014 shows focus on defenses, not cross-model transfer.\n\nPage 743, Section 3.1.3 Data Preparation: \"For each dataset, we prepared the following three sets of data: one set of benign examples, one set of AEs generated from the attack methods, and one set of AEs generated by testing tools... For each input in BenignData, we generated four types of AEs with the four attack methods, resulting in a total of 36,000 AEs.\" \u2014 indicates attacks/AEs are generated per model/dataset but does not describe applying them to different model implementations.\n\nPage 748, Section 5.2 Results: \"The reason is that a new model is retrained with these defense techniques, while the attacks are generated regarding to the original one, making it a more challenging transfer attack scenario.\" \u2014 the only 'transfer' discussion refers to transfer to a retrained/defended version of the same model, not to other distinct models or architectures.\n\n(Throughout the Methods and Evaluation sections (e.g., Tables 2, 4, 5 and 6) adversarial/uncommon inputs are reported per model; no experiment or section describes testing inputs crafted on one model against other model implementations to measure cross-model transferability.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper evaluates whether the 'uncommon data' generated for an original model can bypass defense techniques that involve retraining the model (e.g., defensive distillation and label smoothing). The authors explicitly describe this as a 'transfer attack scenario', thus testing if the generated inputs (fault-inducing inputs) transfer to a different but related model. However, the proposed generation method, KuK, is not explicitly designed to discover transferable attacks. Its genetic algorithm is guided by fitness functions based on the uncertainty metrics of a single target model, without incorporating mechanisms to generalize the attack across multiple models.",
                    "evidence": "Page 11, Section 5.2 RQ4: Evaluation on Defense Techniques, 'For other defense techniques, the reduction in success rate appears smaller than that of binary classifier and mutation-based detection. For example, the success rates drop to 78.3% and 76.2% for defensive distillation and label smoothing on NIN. The reason is that a new model is retrained with these defense techniques, while the attacks are generated regarding to the original one, making it a more challenging transfer attack scenario. For example, defensive distillation retrains a more robust model by reducing the gradients. In this case, some of the data, which are uncommon for original model, become common data w.r.t. the retrained model, because of some weight variation. However, it still can be seen from the results that the uncommon data reveal stronger transferability.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not include any experiments or analysis of whether fault-inducing inputs generated on one model transfer to other, independently trained model implementations performing the same task. All adversarial and uncommon inputs are crafted and evaluated per target model (including defense-retrained variants of that same model), but no inputs generated on one architecture are tested on a different architecture or distinct model instance without access. Therefore it does not demonstrate cross-model transferability.",
                    "evidence": "Page 743 (\u00a73.1.3 Data Preparation): adversarial examples are generated \u201cfor each input in BenignData\u2026 on each model,\u201d with no mention of re-using those inputs on different models. \nPage 748 (\u00a75.2 RQ4): discussion of \u201ctransfer attack scenario\u201d refers only to attacks against a retrained/defended version of the same model, not to separate model implementations (e.g., LeNet-5\u2192NIN or ResNet\u2192MobileNet). \nThroughout (e.g. Tables 2, 4\u20136) all results report attacks and defenses per model, with no cross-model evaluation of transferability."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Tests whether the same inputs still cause failures on the defended model (RQ4)",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Pang et al. - 2022 - MDPFuzz testing models solving Markov decision processes": {
        "key": "pang2022mdpfuzz",
        "name": "MDPFuzz",
        "year": "2022",
        "citation_count": "39",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Medium",
                    "why": "Uncovers behaviorally unsafe yet functionally valid faults \u2014 crash-inducing policies within valid operational parameters. These expose specification-level safety violations but do not bypass explicit control or policy mechanisms. expose  safety critical violations that lead to crashes/collisions or unsafe actions in autonomous or multi-agent environments. reveals crash-triggering state sequences in models solving Markov Decision Processes (MDPs) across autonomous driving, aircraft collision avoidance, and robotic control settings. the failures are behaviorally unsafe but functionally valid \u2014 the agent is still \u201cplaying by the rules\u201d of the simulator. Does not bypass any safety or policy mechanisms. It exposes latent unsafe behaviors within unconstrained learned policies, revealing specification-level vulnerabilities rather than policy breaches."
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "Targets a class of faults (collission or dangerous states)"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Provides correlation with internal coverage"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Applies bounded noise"
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Perturbs continuous-valued initial state vectors within valid physical bounds and validates each mutated state through the simulator"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "No cross model failure pattern analysis"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate attack transferability. MDPFuzz evaluates and finds crash-triggering initial states per target model (it reports crashes separately for five different models/scenarios), uses those findings to train per-model detectors and to repair individual models, and explicitly operates in blackbox settings without building surrogate models. The paper contains no experiment where fault-inducing inputs found on one model are reused to test another model, nor does it describe mechanisms (e.g., surrogate-model attacks, cross-model validation, or explicit cross-model transfer tests) intended to discover transferable attacks across different model implementations.",
                    "evidence": "Abstract (p.378): \"We evaluate five state-of-the-art models for solving MDPs, including supervised DNN, RL, IL, and multi-agent RL.\" (shows multiple models were evaluated but not that inputs were reused across them). \n\nSection 7.1 RQ1 / Results (p.386): \"Table 3 summarizes the findings of fuzzing each model... Overall, MDPFuzz detects a significant number of crashes from all test cases.\" (reports per-model crash counts; no cross-model reuse reported). \n\nSection 5 Assumptions (Alg.1 / p.382): \"In blackbox settings, the internal of the agent model (our testing target) is not revealed to MDPFuzz... MDPFuzz can collect the state sequence {S_t} and obtain the cumulative reward r from the environment.\" (describes blackbox testing and lack of building surrogate/internal models). \n\nSection 7.4 (p.387): \"we repair the models used by DNN-based ACAS Xu with the findings of MDPFuzz...\" (describes per-model repair using findings, not transfer of fault-inducing inputs to other models). \n\nSection 8 Discussion (p.388): \"Another threat is that bugs may be potentially overlooked since MDPFuzz only mutates the entry states (Sec. 5). Nevertheless ...\" (limits of their approach are discussed; no claim of enabling transferable/substitute-model attacks)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper evaluates the proposed fuzzer, MDPFuzz, on five distinct models, including a Reinforcement Learning (RL) and an Imitation Learning (IL) model for the same autonomous driving task (CARLA). However, the evaluation is conducted independently for each model. The paper reports the number of crash-triggering inputs found for each model in isolation and does not perform any experiments to test if the inputs that cause a failure in one model also cause a failure in another model, even those performing the same task. The design of MDPFuzz focuses on finding failures for a single model under test and lacks any mechanism explicitly designed to discover transferable attacks or shared vulnerabilities across different models.",
                    "evidence": "Page 10, Section 7.1 RQ1: Performance on Finding Crashes, Setup: \"We use the evaluation setup described in Sec. 6. That is, we launch MDPFuzz to fuzz each MDP model (listed in Table 3) and detect crashes. We collect all error-triggering inputs for analysis.\" This statement indicates that each model was fuzzed as a separate, independent target. Table 3 on the same page presents the results for each model as a distinct line item, with no cross-model evaluation or mention of transferring discovered failure-inducing inputs."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate attack transferability. MDPFuzz evaluates and finds crash-triggering initial states per target model (it reports crashes separately for five different models/scenarios), uses those findings to train per-model detectors and to repair individual models, and explicitly operates in blackbox settings without building surrogate models. The paper contains no experiment where fault-inducing inputs found on one model are reused to test another model, nor does it describe mechanisms (e.g., surrogate-model attacks, cross-model validation, or explicit cross-model transfer tests) intended to discover transferable attacks across different model implementations.",
                    "evidence": "Abstract (p.378): \"We evaluate five state-of-the-art models for solving MDPs, including supervised DNN, RL, IL, and multi-agent RL.\" (shows multiple models were evaluated but not that inputs were reused across them). \n\nSection 7.1 RQ1 / Results (p.386): \"Table 3 summarizes the findings of fuzzing each model... Overall, MDPFuzz detects a significant number of crashes from all test cases.\" (reports per-model crash counts; no cross-model reuse reported). \n\nSection 5 Assumptions (Alg.1 / p.382): \"In blackbox settings, the internal of the agent model (our testing target) is not revealed to MDPFuzz... MDPFuzz can collect the state sequence {S_t} and obtain the cumulative reward r from the environment.\" (describes blackbox testing and lack of building surrogate/internal models). \n\nSection 7.4 (p.387): \"we repair the models used by DNN-based ACAS Xu with the findings of MDPFuzz...\" (describes per-model repair using findings, not transfer of fault-inducing inputs to other models). \n\nSection 8 Discussion (p.388): \"Another threat is that bugs may be potentially overlooked since MDPFuzz only mutates the entry states (Sec. 5). Nevertheless ...\" (limits of their approach are discussed; no claim of enabling transferable/substitute-model attacks)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided",
                "Data-driven"
            ]
        }
    }
}