```json
{
  "value": "Medium",
  "why": "The paper enforces plausibility at individual mutation steps and uses automatic, task-relevant measures to prefer authentic candidates (ROUGE-1 quality check and a POS-based ``perplexity priority'' used to select/retain seeds). It also implements realistic transformation operators (back-translation, BERT insertion, WordNet synonyms, entity-alias substitution, OCR/keyboard typo simulation). However, the work does not provide an explicit human evaluation or dedicated empirical assessment of the final fault-inducing inputs’ semantic or perceptual realism, and it does not present a formal mechanism that strictly bounds cumulative drift across many iterative mutations (rather, it relies on selection heuristics such as perplexity priority and ROUGE threshold). These properties match the metric’s “Medium” level: per-step plausibility checks and quantitative plausibility heuristics are present, but cumulative-iteration effects are not strictly enforced nor validated by human study or external fluency/quality evaluations of final failing inputs.",
  "evidence": "Section 3.1 / Algorithm 1 (p.3): \"quality_score ← Rouge1(s, s'); if quality_score > τ then // acceptable ... D.append(s')\" — shows ROUGE-1 is used as a per-generation quality filter.\n\nSection 3.2.5 Quality Assessment (p.3): \"To ensure the quality of generated questions, we employ the ROUGE-1 metric [35] to evaluate the questions generated by metamorphic transformations.\" (ROUGE used to discard low-quality generations).\n\nSection 3.3.2 Perplexity Priority (p.4): \"To ensure that the generated test questions have good authenticity and rationality, we consider applying perplexity priority as guidance for seed data retention... per(q) = p(seq(q))^{-1/n}. In each iteration, QATest selects the generated question with the lowest perplexity score in a generated batch.\" — shows a quantitative authenticity/prioritization metric used during fuzzing.\n\nSection 4.3 Sentence Transformations (p.6): \"...the back translation operator also employs the pre-trained language model... For the entity alias substitution operator, we apply ... TagMe... For each transformation, if the generated question is exactly the same as the original question, it is considered a failed transformation...\" — shows use of realistic NLP methods and sensible discard rules for trivial/failed transformations.\n\nSection 6.3 Threats to Validity (p.9): \"...it is impossible to guarantee the reliability of the generated question without manual inspection. To ensure the quality of the generated questions, most of the transformations are implemented with pre-trained language models ... Besides, we apply the existing ROUGE metric to evaluate the quality of the generated data and discard the low-quality generated questions directly.\" — acknowledges no manual/human evaluation and that quality control is via automatic metrics rather than a human study or external perceptual/fluency evaluation of final failing inputs."
}
```