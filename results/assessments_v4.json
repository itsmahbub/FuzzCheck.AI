{
    "Liu et al. - 2024 - AUTODAN GENERATING STEALTHY JAILBREAK PROMPTS ON ALIGNED LARGE LANGUAGE MODELS": {
        "key": "liuautodan",
        "name": "AutoDAN",
        "year": "2024",
        "citation_count": "834",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly targets aligned/safety-trained LLMs and demonstrates that AutoDAN generates jailbreak prompts that cause those models to produce undesired/harmful outputs and to bypass explicit defenses. The authors state the attack goal as disrupting human-aligned refusals, evaluate on aligned models (e.g., Llama2-7b-chat) and show high attack success rates even when a perplexity-based defense is applied. They also report successful transfers to commercial models (e.g., GPT-3.5) in black-box settings. These points indicate AutoDAN uncovers high-impact failures that bypass alignment and safety mechanisms, matching the metric's definition of 'High.'",
                    "evidence": "Page 1, Abstract: \"AutoDAN: ... generating stealthy jailbreak prompts on aligned large language models\" and \"we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively.\" \n\nPage 3, Section 3.1 PRELIMINARIES (Threat model): \"The main goal of this type of attack is to disrupt the human-aligned values of LLMs or other constraints imposed by the model developer, compelling them to respond to malicious questions posed by adversaries with correct answers, rather than refusing to answer.\" \n\nPage 7, Section 4.2 RESULTS (Effectiveness against defense): \"As demonstrated in Tab. 3, the perplexity defense significantly undermines the effectiveness of the token-level jailbreak attack, i.e., GCG attack. However, the semantically meaningful jailbreak prompts AutoDAN (and the original handcrafted DAN) is not influenced.\" (Table 3 shows AutoDAN-HGA achieving high ASR on Llama2-7b-chat even under perplexity defense.)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper introduces AutoDAN, a method specifically designed to generate 'stealthy jailbreak prompts' for 'aligned Large Language Models (LLMs)'. Aligned LLMs are models equipped with explicit safety mechanisms to prevent them from producing harmful or malicious content. A jailbreak, by definition, is an attack that bypasses these safety features. The paper explicitly evaluates AutoDAN's ability to bypass a defense mechanism, demonstrating its effectiveness against perplexity-based defenses where a baseline attack's effectiveness is significantly reduced. This directly aligns with the 'High' severity metric, as the method's primary goal and demonstrated capability is to uncover high-impact failures (producing forbidden content) by bypassing explicit safety and policy mechanisms in safety-aligned models.",
                    "evidence": "Page 1, Abstract: '...jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. ... Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively.'\n\nPage 1, Introduction: '...jailbreak attacks have been discovered: combining the jailbreak prompt with malicious questions (e.g., how to steal someone\u2019s identity) can mislead the aligned LLMs to bypass the safety feature and consequently generate responses that compose harmful, discriminatory, violent, or sensitive content...'\n\nPage 8, Table 3 Caption: 'Effectiveness against perplexity defense. The results indicate that our method adeptly bypasses this type of defense, whereas GCG attack exhibits a substantial reduction in its attack strength. The evaluation highlights the importance of the preserving semantic meaningfulness of jailbreak prompts when confronting with defenses.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly targets aligned/safety-trained LLMs and demonstrates that AutoDAN generates jailbreak prompts that cause those models to produce undesired/harmful outputs and to bypass explicit defenses. The authors state the attack goal as disrupting human-aligned refusals, evaluate on aligned models (e.g., Llama2-7b-chat) and show high attack success rates even when a perplexity-based defense is applied. They also report successful transfers to commercial models (e.g., GPT-3.5) in black-box settings. These points indicate AutoDAN uncovers high-impact failures that bypass alignment and safety mechanisms, matching the metric's definition of 'High.'",
                    "evidence": "Page 1, Abstract: \"AutoDAN: ... generating stealthy jailbreak prompts on aligned large language models\" and \"we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively.\" \n\nPage 3, Section 3.1 PRELIMINARIES (Threat model): \"The main goal of this type of attack is to disrupt the human-aligned values of LLMs or other constraints imposed by the model developer, compelling them to respond to malicious questions posed by adversaries with correct answers, rather than refusing to answer.\" \n\nPage 7, Section 4.2 RESULTS (Effectiveness against defense): \"As demonstrated in Tab. 3, the perplexity defense significantly undermines the effectiveness of the token-level jailbreak attack, i.e., GCG attack. However, the semantically meaningful jailbreak prompts AutoDAN (and the original handcrafted DAN) is not influenced.\" (Table 3 shows AutoDAN-HGA achieving high ASR on Llama2-7b-chat even under perplexity defense.)"
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly defines and optimizes for attacker-chosen target outputs (affirmative answers beginning with a fixed prefix) and uses a fitness function that maximizes the conditional probability of those target tokens. AutoDAN's genetic search (including termination based on refusal-keyword checks) is driven to produce prompts that cause the model to emit a specific target-style response rather than merely finding generic misbehavior, so the fuzzer deliberately steers exploration toward predefined target outcomes.",
                    "evidence": "Page 3, Sec. 3.1 Preliminaries (Formulation): \"a common solution (Zou et al., 2023; Lapid et al., 2023) is to designate the target response as affirmative, such as answers beginning with 'Sure, here is how to [Qi].' By anchoring the target response to text with consistent beginnings, it becomes feasible to express the attack loss function used for optimization in terms of conditional probability.\"; Page 3, Sec. 3.1 (Threat model): \"The objective of jailbreak attacks is to ensure that the responses in R are predominantly answers closely associated with the malicious questions in Q, rather than refusal messages aligned with human values.\"; Page 5, Sec. 3.3 Fitness Evaluation: \"we adopt the log-likelihood that was introduced by Zou et al. (2023) as the loss function, namely, given a specific jailbreak prompt Ji, the loss can be calculated by: L_Ji = -log(P(r_{m+1}, r_{m+2}, ..., r_{m+k} | t_1, t_2, ..., t_m)) ... we define the fitness score of Ji as S_Ji = -L_Ji.\"; Page 5, Alg. 2 AutoDAN-HGA / Sec. 3.5 Termination Criteria: algorithm/control flow uses refusal-keyword list L_refuse and loops \"while model responses have word in Lrefuse and iteration not exhausted do\", showing the search is explicitly checking for and steering against refusal outputs."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, AutoDAN, is designed to automatically generate 'jailbreak prompts' that cause a Large Language Model (LLM) to bypass its safety features and respond to malicious queries. This goal falls into the category of discovering a 'broader class of security-violating behavior' (i.e., jailbreaks), rather than steering the model towards a specific, predefined output string. The optimization process does not target a complete, attacker-chosen response. Instead, its fitness function is designed to maximize the probability of the LLM producing an affirmative prefix (e.g., 'Sure, here is how to...'), which serves as a proxy for a successful jailbreak. This indicates the exploration is guided toward a class of unsafe behavior, aligning with the 'Medium' definition.",
                    "evidence": "Page 4, Section 3.1, Formulation: 'Intuitively, it is impractical to set a specific target for the response to a single malicious question, as pinpointing an appropriate answer for a given malicious query is challenging and might compromise generalizability to other questions. Consequently, a common solution (Zou et al., 2023; Lapid et al., 2023) is to designate the target response as affirmative, such as answers beginning with \u201cSure, here is how to [Qi].\" By anchoring the target response to text with consistent beginnings, it becomes feasible to express the attack loss function used for optimization in terms of conditional probability.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "AutoDAN explicitly defines a precise attacker\u2010chosen target output (the affirmative prefix \u201cSure, here is how to [Qi]\u2026\u201d) and uses a fitness function that maximizes the likelihood of that exact token sequence. The genetic search is driven to produce prompts that induce the model to emit this specific target response rather than merely discovering generic misbehavior, satisfying the \u2018High\u2019 level of targeted attack discovery.",
                    "evidence": "Sec. 3.1 (Formulation): \u201ca common solution \u2026 is to designate the target response as affirmative, such as answers beginning with \u2018Sure, here is how to [Qi]\u2026\u2019 \u2026 express the attack loss function \u2026 in terms of conditional probability.\u201d; Sec. 3.3 (Fitness Evaluation): \u201cL_Ji = \u2013log P(r_{m+1},\u2026,r_{m+k} | t_1,\u2026,t_m)\u2026 define the fitness score of Ji as S_Ji = \u2013L_Ji.\u201d; Alg. 2 & Sec. 3.5 (Termination Criteria): loop \u201cwhile model responses have [refusal] word \u2026\u201d, showing search is steered against refusals toward the specific affirmative output."
                },
                "manual": {
                    "value": "High",
                    "why": "Guides towared output that start with a predified content",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight via statistical analyses of failures (attack success rates across models, defenses, cross-sample universality, ablations) and interprets these patterns to form hypotheses about causes (e.g., overfitting of token-gradient methods). However, it does not perform analysis of observable model internals (neuron activations, coverage, layer-wise behavior) or present empirical diagnostics that directly link failures to underlying model vulnerabilities (e.g., concrete evidence of reliance on non-robust/spurious features or dataset bias). Thus it meets the criterion for Medium: statistical/failure-pattern analysis is present, but internal-model diagnostic analysis is absent.",
                    "evidence": "Page 6, Section 4.1 (Metrics): \"The first metric is the keyword-based attack success rate (ASR) ... The second metric is the GPT recheck attack success rate (Recheck). ... In both metrics, we report the final success rate calculated by Isuccess/Itotal. For stealthiness, we use standard Sentence Perplexity (PPL) evaluated by GPT-2 as the metric.\"  \n\nPage 8, Section 4.2 (Transferability): \"We speculate that the potential reason is the semantically meaningful jailbreak prompts may be inherently more transferable than the methods based on tokens\u2019 gradients. As GCG-like method directly optimizes the jailbreak prompt by the gradient information, it is likely for the algorithm to get relatively overfitting in the white-box model.\"  \n\nPage 7, Section 4.2 (Effectiveness against defense): \"As demonstrated in Tab. 3, the perplexity defense significantly undermines the effectiveness of the token-level jailbreak attack, i.e., GCG attack. However, the semantically meaningful jailbreak prompts AutoDAN (and the original handcrafted DAN) is not influenced. These findings underscore the capability of our method to generate semantically meaningful content similar to benign text, verifying the stealthiness of our method.\"  \n\nPage 5, Section 3.3 (Fitness Evaluation): \"Since the goal of jailbreak attacks can be formulated as Eq. 2, we can directly use a function that calculates this likelihood for evaluating the fitness of the individuals in genetic algorithms. Here, we adopt the log-likelihood that was introduced by Zou et al. (2023) as the loss function...\"  \n\nNote on absence of internal-model diagnostics: The paper's methodological and evaluation sections (e.g., Sections 3 and 4, Tables 1\u20135, Ablation studies) report and analyze output-level statistics (ASR, Recheck, PPL), cross-model and cross-sample patterns, and module ablations, but contain no sections or quoted analyses of model internals such as neuron activation frequency, layer-wise behavior, coverage metrics, or empirical linking of failures to specific spurious/non-robust features (beyond the speculative statement about overfitting above)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by analyzing statistical failure patterns, specifically the transferability of its generated jailbreak prompts across different models and the universality across different malicious requests. The authors observe that the semantically meaningful prompts generated by their method (AutoDAN) are more effective at causing failures (successful jailbreaks) on models they were not optimized for (transferability) compared to the baseline. They provide a diagnostic hypothesis for this observed pattern, speculating that by optimizing at the semantic/lexical level rather than the token-gradient level, their method generates more general attacks that exploit 'common flaws for language models' and avoid 'overfitting on white-box models'. This analysis moves beyond simply reporting failure rates and provides insight into the nature of the vulnerabilities being exploited, qualifying it for a 'Medium' rating.",
                    "evidence": "Page 8, Section 4.2, Transferability. 'AutoDAN exhibits a much better transferability in attacking the black-box LLMs compared with the baseline. We speculate that the potential reason is the semantically meaningful jailbreak prompts may be inherently more transferable than the methods based on tokens' gradients. As GCG-like method directly optimizes the jailbreak prompt by the gradient information, it is likely for the algorithm to get relatively overfitting in the white-box model. On the contrary, since lexical-level data such as words usually cannot be updated according to specific gradient information, optimizing at the lexical-level may make it easier to generate the more general jailbreak prompts, which may be common flaws for language models.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight via statistical analyses of failures (attack success rates across models, defenses, cross-sample universality, ablations) and interprets these patterns to form hypotheses about causes (e.g., overfitting of token-gradient methods). However, it does not perform analysis of observable model internals (neuron activations, coverage, layer-wise behavior) or present empirical diagnostics that directly link failures to underlying model vulnerabilities (e.g., concrete evidence of reliance on non-robust/spurious features or dataset bias). Thus it meets the criterion for Medium: statistical/failure-pattern analysis is present, but internal-model diagnostic analysis is absent.",
                    "evidence": "Page 6, Section 4.1 (Metrics): \"The first metric is the keyword-based attack success rate (ASR) ... The second metric is the GPT recheck attack success rate (Recheck). ... In both metrics, we report the final success rate calculated by Isuccess/Itotal. For stealthiness, we use standard Sentence Perplexity (PPL) evaluated by GPT-2 as the metric.\"  \n\nPage 8, Section 4.2 (Transferability): \"We speculate that the potential reason is the semantically meaningful jailbreak prompts may be inherently more transferable than the methods based on tokens\u2019 gradients. As GCG-like method directly optimizes the jailbreak prompt by the gradient information, it is likely for the algorithm to get relatively overfitting in the white-box model.\"  \n\nPage 7, Section 4.2 (Effectiveness against defense): \"As demonstrated in Tab. 3, the perplexity defense significantly undermines the effectiveness of the token-level jailbreak attack, i.e., GCG attack. However, the semantically meaningful jailbreak prompts AutoDAN (and the original handcrafted DAN) is not influenced. These findings underscore the capability of our method to generate semantically meaningful content similar to benign text, verifying the stealthiness of our method.\"  \n\nPage 5, Section 3.3 (Fitness Evaluation): \"Since the goal of jailbreak attacks can be formulated as Eq. 2, we can directly use a function that calculates this likelihood for evaluating the fitness of the individuals in genetic algorithms. Here, we adopt the log-likelihood that was introduced by Zou et al. (2023) as the loss function...\"  \n\nNote on absence of internal-model diagnostics: The paper's methodological and evaluation sections (e.g., Sections 3 and 4, Tables 1\u20135, Ablation studies) report and analyze output-level statistics (ASR, Recheck, PPL), cross-model and cross-sample patterns, and module ablations, but contain no sections or quoted analyses of model internals such as neuron activation frequency, layer-wise behavior, coverage metrics, or empirical linking of failures to specific spurious/non-robust features (beyond the speculative statement about overfitting above)."
                },
                "manual": {
                    "value": "Low",
                    "why": "It provides tranferability rationale, not failure patterns or root cause",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly designs per-mutation constraints and mechanisms to preserve semantic plausibility (LLM-based diversification for initialization and mutation, synonym-based/momentum word replacement, and a low mutation probability), and it validates final fault-inducing prompts with a task-specific quantitative measure (sentence perplexity / PPL) and defense experiments (perplexity-based defense). However, the paper does not describe an explicit mechanism that enforces or measures plausibility across the entire evolutionary trajectory (i.e., cumulative drift across iterations) nor report iterative/plausibility traces; instead it relies on per-step design choices and final PPL measurements. Thus it validates final-input plausibility and constrains individual mutation steps but does not address cumulative-effects across many iterations, matching the metric's Medium level.",
                    "evidence": "3.2 Population Initialization (page 4): \"we employ LLMs as the agents responsible for revising the prototype prompt ... the modifications proposed by LLM can preserve the inherent logical flow and meaning of the original sentences, while simultaneously introducing diversity in word selection and sentence structure.\"; 3.4 Paragraph-level (page 5): \"We let the LLM-based diversification introduced in Alg. 5 to also serve as the mutation function due to its ability to preserve the global meaning and introduce diversity.\"; D.2 Implementation details (Appendix D, page 16): \"...a mutation rate of 0.01... We employ GPT-4-0613 as the base model for the Recheck metric and also as the base model for LLM-based Diversification.\"; 4.1 Metrics (page 6): \"For stealthiness, we use standard Sentence Perplexity (PPL) evaluated by GPT-2 as the metric.\"; 4.2 Results / Table 1 (page 6\u20137): \"Compared with the baseline, our method exhibits superior performance in terms of PPL, indicating more semantically meaningful and stealthy attacks being generated.\"; 3.5 Termination Criteria (page 5): \"If the algorithm has exhausted the maximum number of iterations or no keyword in a set list Lrefuse is detected in the LLM\u2019s top K words of its response, AutoDAN will terminate...\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's primary objective is to generate 'stealthy' and 'semantically meaningful' jailbreak prompts, which directly aligns with the concept of input plausibility. The proposed method, AutoDAN, enforces this plausibility throughout its generation process. Unlike methods that perform simple token-level mutations, AutoDAN employs a hierarchical genetic algorithm that operates on sentences and words. Crucially, its mutation operator uses an LLM to revise sentences, a mechanism designed to preserve logical flow and meaning, thus preventing the cumulative semantic drift common in iterative fuzzing. Furthermore, the paper provides explicit empirical evidence of this plausibility. It uses Sentence Perplexity (PPL), a standard quantitative measure for linguistic fluency, as a core evaluation metric for 'stealthiness'. The results demonstrate that AutoDAN's generated prompts have significantly lower (i.e., better) perplexity scores than token-based attack baselines (GCG) and are comparable to human-written prompts, confirming that the final fault-inducing inputs are realistic and semantically valid.",
                    "evidence": "Page 7, Section 4.1, Metrics: \"For stealthiness, we use standard Sentence Perplexity (PPL) evaluated by GPT-2 as the metric.\"\n\nPage 7, Section 4.2, Table 1 & Attack Effectiveness and Stealthiness: The results in Table 1 show AutoDAN-HGA achieving a PPL of 54.3820 on Llama2-7b-chat, vastly lower than the GCG baseline's 1027.5585. The text states: \"Compared with the baseline, our method exhibits superior performance in terms of PPL, indicating more semantically meaningful and stealthy attacks being generated.\"\n\nPage 6, Section 3.4.2, Paragraph-level: selection, crossover and mutation: \"We let the LLM-based diversification introduced in Alg. 5 to also serve as the mutation function due to its ability to preserve the global meaning and introduce diversity.\" Algorithm 5 on page 14 details this function, which instructs an LLM to revise a sentence, thus enforcing semantic plausibility during the mutation step."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "AutoDAN incorporates per-step mechanisms to preserve semantic plausibility\u2014initializing with human-crafted prompts, using an LLM to rewrite rather than randomly perturb text, employing a low mutation rate, and validating final prompts with sentence-level perplexity and a perplexity-based defense. However, the paper does not enforce or measure plausibility cumulatively across iterations (e.g., via intermediate PPL traces or explicit drift bounds), nor does it report any plausibility assessments for inputs generated mid-fuzzing. As a result, it constrains individual mutation steps and validates only the final outputs, matching the \"Medium\" level of the Input Plausibility metric.",
                    "evidence": "Section 3.2 (page 4): LLM-based diversification ensures each mutation preserves meaning; Section 3.4.2 (page 5): low mutation probability and LLM-based mutation function to maintain semantic coherence; Section 4.1 (page 6): final stealthiness evaluated via Sentence Perplexity (PPL); Section 4.2 Table 1 (pages 6\u20137): reports final PPL scores and perplexity-defense results\u2014no intermediate plausibility measures or cumulative drift analysis are provided."
                },
                "manual": {
                    "value": "High",
                    "why": "Use LLM to rewrite mutated prompt to ensure plausibility in each mutation step and measure plausibility of the final prompt with sentence-level perplexity ",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper's method operates directly on textual prompts (discrete, serialized text) rather than applying fine-grained numeric mutations to in-memory floating-point tensors. AutoDAN generates final jailbreak prompts as text strings (via sentence- and word-level crossover/mutation and LLM-based diversification) and evaluates success using textual oracles (keyword checks and an LLM re-check). Because inputs are produced as serialized text (generated prompts) and the oracle operates on model text outputs, the failures do not rely on fragile in-memory numerical perturbations that could be lost by standard I/O serialization (quantization/rounding). Thus, under the metric definition\u2014where approaches that generate inputs directly in serialized formats (e.g., generated text) are considered reproducible\u2014AutoDAN ensures reproducibility under standard I/O operations.",
                    "evidence": "Abstract (p.1): \"AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm.\"; Section 3.4.2 (p.5), paragraph describing hierarchical view: \"Our method views the jailbreak prompt as a combination of paragraph-level population, i.e., different combination of sentences, while these sentences, in turn, are formed by sentence-level population, for example, different words.\"; Section 4.2 (p.8): \"We conduct these evaluations by generating a jailbreak prompt for each malicious request in the dataset and testing the final responses from the victim LLM.\"; Appendix B, Alg.5 (p.13): \"Please revise the following sentence with no change to its length and only output the revised version, the sentence is: prompt\" (shows the mutation/generation operates on serialized text strings)."
                },
                "gemini": {
                    "value": "High",
                    "why": "The method, AutoDAN, generates jailbreak prompts, which are a form of text. Text is inherently a discrete and serialized format. Unlike continuous data such as images or audio, text does not suffer from information loss or alteration due to standard I/O operations like rounding, quantization, or clipping. The fault-inducing properties of the generated prompts are based on their lexical, semantic, and structural content, all of which are perfectly preserved when saved to and loaded from a file. The assessment metric explicitly states that approaches generating inputs 'directly in serialized formats (e.g., ... generated text)' are considered reproducible and rated as 'High' because they are 'inherently stable with respect to standard I/O operations'. The paper's method falls directly into this category by operating on and producing text-based prompts.",
                    "evidence": "Page 2, Section 2, Introduction: \"Based on the aforementioned insights, we propose AutoDAN, a hierarchical genetic algorithm tailored specifically for structured discrete data like prompt text.\"\n\nAnd from the metric definition itself: \"We also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., synthesized speech, rendered images, or generated text) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper's method operates directly on textual prompts (discrete, serialized text) rather than applying fine-grained numeric mutations to in-memory floating-point tensors. AutoDAN generates final jailbreak prompts as text strings (via sentence- and word-level crossover/mutation and LLM-based diversification) and evaluates success using textual oracles (keyword checks and an LLM re-check). Because inputs are produced as serialized text (generated prompts) and the oracle operates on model text outputs, the failures do not rely on fragile in-memory numerical perturbations that could be lost by standard I/O serialization (quantization/rounding). Thus, under the metric definition\u2014where approaches that generate inputs directly in serialized formats (e.g., generated text) are considered reproducible\u2014AutoDAN ensures reproducibility under standard I/O operations.",
                    "evidence": "Abstract (p.1): \"AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm.\"; Section 3.4.2 (p.5), paragraph describing hierarchical view: \"Our method views the jailbreak prompt as a combination of paragraph-level population, i.e., different combination of sentences, while these sentences, in turn, are formed by sentence-level population, for example, different words.\"; Section 4.2 (p.8): \"We conduct these evaluations by generating a jailbreak prompt for each malicious request in the dataset and testing the final responses from the victim LLM.\"; Appendix B, Alg.5 (p.13): \"Please revise the following sentence with no change to its length and only output the revised version, the sentence is: prompt\" (shows the mutation/generation operates on serialized text strings)."
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper both (1) reuses fault-inducing prompts crafted on one model to test other models (explicit cross-model transfer experiments and reported ASR numbers), and (2) designs the fuzzing/optimization to produce semantically meaningful, lexical-level prompts (hierarchical GA, LLM-based diversification, momentum word scoring) which the authors argue and empirically show improves transfer to black-box models. Thus it demonstrates reuse across models and includes explicit mechanisms intended to discover transferable failures.",
                    "evidence": "Page 1, Abstract: \"Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability...\"; Page 8, Section 4.2 Results - Transferability: \"We conduct the evaluations by taking the jailbreak prompts with their corresponding requests and targeting another LLM. The results are shown in Tab. 2. AutoDAN exhibits a much better transferability in attacking the black-box LLMs compared with the baseline.\"; Page 8, Table 2 (Cross-model transferability): entries showing transfer ASR (examples) \u2014 \"Vicuna-7B -> Guanaco-7b: AutoDAN-HGA ASR 0.7058 Recheck 0.6712\" and \"Llama2-7b-chat -> Vicuna-7B: AutoDAN-HGA ASR 0.7288 Recheck 0.7019\"; Page 8, Table 2 caption / discussion: \"We hypothesize that this is because the AutoDAN generates prompts at a semantic level without relying on direct guidance from gradient information on the tokens, thereby avoiding overfitting on white-box models.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper explicitly evaluates the transferability of its generated jailbreak prompts. It conducts cross-model experiments where prompts generated on a source model are tested against other target models, and the results are presented in a dedicated table (Table 2). However, the design of the attack method, AutoDAN, does not incorporate explicit mechanisms aimed at discovering transferable attacks. The method uses a hierarchical genetic algorithm to generate semantically meaningful prompts. The authors speculate that the observed high transferability is a beneficial consequence of their semantic-level optimization approach, which avoids the overfitting common in gradient-based methods, rather than being an explicit design goal with specific supporting mechanisms.",
                    "evidence": "Page 8, Section 'Transferability': 'We further investigate the transferability of our method... We conduct the evaluations by taking the jailbreak prompts with their corresponding requests and targeting another LLM. The results are shown in Tab. 2. AutoDAN exhibits a much better transferability in attacking the black-box LLMs compared with the baseline. We speculate that the potential reason is the semantically meaningful jailbreak prompts may be inherently more transferable than the methods based on tokens\u2019 gradients.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "While the paper clearly evaluates whether fault-inducing prompts generated on one model transfer to others (via cross-model experiments in Section 4.2 and Table 2), the core AutoDAN design does not include mechanisms explicitly tailored for transferability (e.g. multi-model surrogate optimization or fitness functions that aggregate performance across targets). The authors observe that semantic-level optimization \u2018inherently\u2019 improves transfer, but they do not incorporate transferability as an explicit objective or design feature of their hierarchical genetic algorithm.",
                    "evidence": "Page 8, Section 4.2 (\u201cTransferability\u201d): \u201cWe conduct the evaluations by taking the jailbreak prompts with their corresponding requests and targeting another LLM. The results are shown in Tab. 2. AutoDAN exhibits a much better transferability \u2026 We speculate that the potential reason is the semantically meaningful jailbreak prompts may be inherently more transferable\u2026\u201d; Table 2 \u2013 cross-model ASR results; no mention of multi-model or transfer-focused optimization in Method (Sections 3.2\u20133.4)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Shows transferabilty but the fuzzing design is not explicit about trasferable attack discovery. It is a byproduct and the authors gives a hypothesis why they transfer",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Rule-based",
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "He et al. - 2024 - Curiosity-Driven Testing for Sequential Decision-Making Process": {
        "key": "he2024curiosity",
        "name": "CureFuzz",
        "year": "2024",
        "citation_count": "12",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper shows CureFuzz uncovers safety-critical, undesired behaviors (crash-triggering scenarios such as vehicle/aircraft collisions and robot falls) that violate user safety expectations, but it does not present any results or claims that it bypasses explicit safety, policy, or security mechanisms (e.g., adversarially-trained or safety-aligned models), nor does it report security-critical violations like data leakage, jailbreaks, or unauthorized actions. Therefore the failures discovered are high-impact safety violations in the sense of causing crashes, but they are not described as bypassing explicit defenses or security mechanisms, which matches the \"Medium\" category.",
                    "evidence": "Page 3, Section 3.1 Assumption: \"CureFuzz is designed to uncover crash-triggering scenarios that eventually lead to the crash of SDMs.\"; Page 6, Section 4.2.1 Autonomous Driving: \"In the CARLA simulator ... CureFuzz checks for the situations when the SDM-controlled vehicle experiences a collision with other vehicles or buildings.\"; Page 6, Section 4.2.2 Aircraft Collision Avoidance: \"In ACAS Xu, CureFuzz simply aims to find the scenarios when there are collisions between the SDM-controlled airplane with other airplanes.\"; Page 6-7, Section 4.2.3 Video Game / BipedalWalker: \"We aim to find the scenarios when the robot falls.\"; Page 10, RQ3 Results: \"For the repaired ACAS Xu, 55 faults and 4 distinct types of crash-triggering scenarios are detected. We find that the crashes found by CureFuzz are reduced by 73% compared with the original DNN model.\" (shows focus on crash detection and repair rather than bypassing defenses)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, CUREFUZZ, is designed to uncover 'crash-triggering scenarios' in sequential decision-makers (SDMs). These failures are explicitly defined as catastrophic and unsafe behaviors, such as an autonomous vehicle colliding with pedestrians or a robot falling. This directly aligns with the 'Medium' severity definition, which includes uncovering failures that violate safety expectations and produce unsafe behavior. The paper does not mention testing against models with explicit, separate safety or security mechanisms, nor does it claim to bypass such defenses. Instead, it finds unsafe behaviors inherent in the standard models' learned policies. Therefore, it does not meet the criteria for 'High' but is significantly more severe than 'Low,' which is limited to robustness errors like mispredictions.",
                    "evidence": "Page 5, Section 3.2, Approach Overview: \"CUREFUZZ is designed to uncover crash-triggering scenarios that eventually lead to the crash of SDMs. It is crucial to note that the definition of a 'crash' can vary across different environments. For instance, within the context of autonomous driving, a crash could be defined as an incident where an autonomous vehicle collides with pedestrians. In the context of robotics control, a crash can refer to the falling of a walking robot. We employ the term 'crash' to represent its broader conceptual meaning. Furthermore, our methodology focuses on catastrophic failure rather than minor deviations from optimal performance.\"\n\nPage 7, Section 4.2, Experiment Subject and Environment: \"In CARLA, CUREFUZZ checks for the situations when the SDM-controlled vehicle experiences a collision with other vehicles or buildings.\" and \"In ACAS XU, CUREFUZZ simply aims to find the scenarios when there are collisions between the SDM-controlled airplane with other airplanes.\" and for BipedalWalker, \"We aim to find the scenarios when the robot falls.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper shows CureFuzz uncovers safety-critical, undesired behaviors (crash-triggering scenarios such as vehicle/aircraft collisions and robot falls) that violate user safety expectations, but it does not present any results or claims that it bypasses explicit safety, policy, or security mechanisms (e.g., adversarially-trained or safety-aligned models), nor does it report security-critical violations like data leakage, jailbreaks, or unauthorized actions. Therefore the failures discovered are high-impact safety violations in the sense of causing crashes, but they are not described as bypassing explicit defenses or security mechanisms, which matches the \"Medium\" category.",
                    "evidence": "Page 3, Section 3.1 Assumption: \"CureFuzz is designed to uncover crash-triggering scenarios that eventually lead to the crash of SDMs.\"; Page 6, Section 4.2.1 Autonomous Driving: \"In the CARLA simulator ... CureFuzz checks for the situations when the SDM-controlled vehicle experiences a collision with other vehicles or buildings.\"; Page 6, Section 4.2.2 Aircraft Collision Avoidance: \"In ACAS Xu, CureFuzz simply aims to find the scenarios when there are collisions between the SDM-controlled airplane with other airplanes.\"; Page 6-7, Section 4.2.3 Video Game / BipedalWalker: \"We aim to find the scenarios when the robot falls.\"; Page 10, RQ3 Results: \"For the repaired ACAS Xu, 55 faults and 4 distinct types of crash-triggering scenarios are detected. We find that the crashes found by CureFuzz are reduced by 73% compared with the original DNN model.\" (shows focus on crash detection and repair rather than bypassing defenses)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Uncovers crash-triggering scenarios (e.g., collisions, falls, or loss of control) across autonomous driving, aviation, and robotics systems.",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's fuzzer (CureFuzz) is explicitly designed to discover crash-triggering scenarios (broad classes of catastrophic failures such as collisions or falls) by steering exploration via novelty (curiosity), cumulative reward (low reward \u2192 higher crash probability), and robustness. It does not define or steer the search toward attacker-chosen, specific target outputs (e.g., a particular label, phrase, or command). Therefore it supports discovery of broader classes of security-violating behaviors rather than targeted, attacker-specified outputs.",
                    "evidence": "Page 1, Abstract: \"CureFuzz ... aims to reveal a diverse set of crash-triggering scenarios\".\nPage 4, Section 3.1 Assumption: \"CureFuzz is designed to uncover crash-triggering scenarios that eventually lead to the crash of SDMs. It is crucial to note that the definition of a \u2018crash\u2019 can vary across different environments... we employ the term \u2018crash\u2019 to represent its broader conceptual meaning. Furthermore, our methodology focuses on catastrophic failure rather than minor deviations from optimal performance.\"\nPage 5, Section 3.3 A Curiosity-driven Search Strategy: \"The difference between the outputs of these two networks, referred to as the prediction error, serves as a proxy for the novelty of a given state... This prediction error serves as an intrinsic reward signal for each state. In CureFuzz ... the novelty score of a scenario is the mean of all intrinsic rewards. The intrinsic reward for the \u2018novel\u2019 inputs should be higher ... thereby driving the fuzzer to explore those novel states.\"\nPage 6, Section 3.4 CureFuzz Architecture (Seed energy estimation): \"To balance these objectives, the estimation process is based on multiple factors. These factors are intrinsic reward (novelty measure), cumulative reward (probability of triggering crashes), and robustness (probability of triggering unseen states). ... CureFuzz prioritizes the seeds with low cumulative reward. Thus a seed with a lower cumulative reward is accordingly given a higher energy estimation.\"\nPage 8, Section 5 RQ1 (Evaluation): The evaluation and reported metrics focus on \"the total number of detected crash-triggering scenarios, and the distinct types of crash-triggering scenarios,\" demonstrating the method aims to find classes of catastrophic failures rather than produce specific targeted outputs."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, CUREFUZZ, is designed to discover 'crash-triggering scenarios'. A 'crash' is defined as a broad class of catastrophic, security-violating behavior, such as a vehicle collision or a robot falling, rather than a specific, predefined model output. The fuzzing process is explicitly steered towards discovering any scenario that leads to these general unsafe behaviors by prioritizing seeds based on their novelty and probability of triggering a crash. This aligns with the 'Medium' definition, as it supports the discovery of broader classes of unsafe behaviors without targeting a specific, attacker-chosen output.",
                    "evidence": "Page 5, Section 3.2, Approach Overview: \"CUREFUZZ is designed to uncover crash-triggering scenarios that eventually lead to the crash of SDMs. It is crucial to note that the definition of a 'crash' can vary across different environments. For instance, within the context of autonomous driving, a crash could be defined as an incident where an autonomous vehicle collides with pedestrians. In the context of robotics control, a crash can refer to the falling of a walking robot. We employ the term 'crash' to represent its broader conceptual meaning. Furthermore, our methodology focuses on catastrophic failure rather than minor deviations from optimal performance.\"\n\nPage 7, Section 4.2, Experiment Subject and Environment: The paper defines the target failures for each experiment as a class of unsafe behavior: \"In CARLA, CUREFUZZ checks for the situations when the SDM-controlled vehicle experiences a collision with other vehicles or buildings.\" (4.2.1); \"In ACAS Xu, CUREFUZZ simply aims to find the scenarios when there are collisions between the SDM-controlled airplane with other airplanes.\" (4.2.2); \"We aim to find the scenarios when the robot falls.\" (4.2.3)."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper's fuzzer (CureFuzz) is explicitly designed to discover crash-triggering scenarios (broad classes of catastrophic failures such as collisions or falls) by steering exploration via novelty (curiosity), cumulative reward (low reward \u2192 higher crash probability), and robustness. It does not define or steer the search toward attacker-chosen, specific target outputs (e.g., a particular label, phrase, or command). Therefore it supports discovery of broader classes of security-violating behaviors rather than targeted, attacker-specified outputs.",
                    "evidence": "Page 1, Abstract: \"CureFuzz ... aims to reveal a diverse set of crash-triggering scenarios\".\nPage 4, Section 3.1 Assumption: \"CureFuzz is designed to uncover crash-triggering scenarios that eventually lead to the crash of SDMs. It is crucial to note that the definition of a \u2018crash\u2019 can vary across different environments... we employ the term \u2018crash\u2019 to represent its broader conceptual meaning. Furthermore, our methodology focuses on catastrophic failure rather than minor deviations from optimal performance.\"\nPage 5, Section 3.3 A Curiosity-driven Search Strategy: \"The difference between the outputs of these two networks, referred to as the prediction error, serves as a proxy for the novelty of a given state... This prediction error serves as an intrinsic reward signal for each state. In CureFuzz ... the novelty score of a scenario is the mean of all intrinsic rewards. The intrinsic reward for the \u2018novel\u2019 inputs should be higher ... thereby driving the fuzzer to explore those novel states.\"\nPage 6, Section 3.4 CureFuzz Architecture (Seed energy estimation): \"To balance these objectives, the estimation process is based on multiple factors. These factors are intrinsic reward (novelty measure), cumulative reward (probability of triggering crashes), and robustness (probability of triggering unseen states). ... CureFuzz prioritizes the seeds with low cumulative reward. Thus a seed with a lower cumulative reward is accordingly given a higher energy estimation.\"\nPage 8, Section 5 RQ1 (Evaluation): The evaluation and reported metrics focus on \"the total number of detected crash-triggering scenarios, and the distinct types of crash-triggering scenarios,\" demonstrating the method aims to find classes of catastrophic failures rather than produce specific targeted outputs."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Steer toward safety faults (crash-triggering scenarios) in in autonomous driving and other sequential decision-making systems",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight beyond merely reporting failures: it analyzes observable model behavior (state coverage, a robustness/sensitivity measure, and distributional patterns of crashes) and performs ablation studies showing the role of the curiosity signal. These constitute statistical and behavioral diagnostics that help explain where and when the SDMs fail. However, the paper does not perform deeper internal model analysis (e.g., neuron activations, layer-wise behavior) nor does it directly link discovered failures to specific underlying model vulnerabilities such as reliance on spurious/non-robust features, dataset bias, or overfitting. Therefore the appropriate rating is Medium.",
                    "evidence": "p.6, Section 3.4 (Algorithm 3 - Robustness Estimation): \"Robustness is then measured as the Euclidean distance between the final states of S and S_\u03b4, which essentially is |S_{-1} - S_\u03b4_{-1}|. Robustness is a measure of how sensitive an SDM\u2019s behavior is to slight perturbations in the original state.\" \n\np.8, Section 5 (Coverage Analysis): \"Coverage analysis plays a crucial role in evaluating the thoroughness and completeness of a testing technique. ... we measure the environmental state coverage ... we discretize the state space ...\" (Table 1 reports state coverage comparisons across methods.)\n\np.9, Section 5 (Total Number of Crashes / visualization): \"In Figure 2, ... G-Model tends to find the crashes that the agents are positioned around the boundary of the valid input. In contrast, CureFuzz covers more spread positions.\" (Figure 2 visualizes distributional differences in crash-triggering positions.)\n\np.10, Section 5 (RQ2 ablation / role of curiosity): \"For this research question, we ... remove the curiosity component from CureFuzz and conduct the fuzzing again ... We can observe that the addition of the curiosity mechanism plays a vital role in boosting the effectiveness of CureFuzz ...\" (Figure 3 and Table 5 show ablation results.)\n\np.11, Section 6 (Discussion - epistemic uncertainty): \"The curiosity mechanism\u2019s optimization process ... The prediction error of the target network and the predictor network then serves as a proxy for measuring uncertainty, specifically epistemic uncertainty. Epistemic uncertainty would be particularly high in regions of the input space where few similar examples were seen ... By focusing on areas of high epistemic uncertainty ... the testing process inherently explores a wider variety of scenarios.\" \n\np.10, Section 5 (RQ3 repair result): \"For the repaired ACAS Xu, 55 faults and 4 distinct types of crash-triggering scenarios are detected. We find that the crashes found by CureFuzz are reduced by 73% compared with the original DNN model.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight through a statistical analysis of failure patterns. Specifically, it analyzes the spatial distribution of crash-triggering scenarios for one of the experimental subjects (Coop Navi). It visualizes the agent positions in the state space that lead to crashes and observes that a baseline method finds crashes concentrated near the boundary of the valid input space, whereas the proposed method, CUREFUZZ, finds crashes in more widely distributed positions. This constitutes an analysis of failure patterns. However, the paper does not extend this analysis to explain *why* these failures occur by linking them to underlying model vulnerabilities such as overfitting, dataset bias, or reliance on spurious features, which would be necessary for a 'High' rating.",
                    "evidence": "Page 9, Section 5 EXPERIMENT RESULTS, Figure 2 and the related discussion on page 8: 'Recall that the goal of the Coop Navi game is to find the scenarios when the agents collide with each other, in Figure 2, we select the median performance from the five repeated experiments for CUREFUZZ and G-Model, and plot the positions of the agents in the crash-triggering scenarios found by different methods. While the valid range of the agent\u2019s position is from -1 to 1, from Figure 2, we can see that G-Model tends to find the crashes that the agents are positioned around the boundary of the valid input. In contrast, CUREFUZZ covers more spread positions.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight beyond merely reporting failures: it analyzes observable model behavior (state coverage, a robustness/sensitivity measure, and distributional patterns of crashes) and performs ablation studies showing the role of the curiosity signal. These constitute statistical and behavioral diagnostics that help explain where and when the SDMs fail. However, the paper does not perform deeper internal model analysis (e.g., neuron activations, layer-wise behavior) nor does it directly link discovered failures to specific underlying model vulnerabilities such as reliance on spurious/non-robust features, dataset bias, or overfitting. Therefore the appropriate rating is Medium.",
                    "evidence": "p.6, Section 3.4 (Algorithm 3 - Robustness Estimation): \"Robustness is then measured as the Euclidean distance between the final states of S and S_\u03b4, which essentially is |S_{-1} - S_\u03b4_{-1}|. Robustness is a measure of how sensitive an SDM\u2019s behavior is to slight perturbations in the original state.\" \n\np.8, Section 5 (Coverage Analysis): \"Coverage analysis plays a crucial role in evaluating the thoroughness and completeness of a testing technique. ... we measure the environmental state coverage ... we discretize the state space ...\" (Table 1 reports state coverage comparisons across methods.)\n\np.9, Section 5 (Total Number of Crashes / visualization): \"In Figure 2, ... G-Model tends to find the crashes that the agents are positioned around the boundary of the valid input. In contrast, CureFuzz covers more spread positions.\" (Figure 2 visualizes distributional differences in crash-triggering positions.)\n\np.10, Section 5 (RQ2 ablation / role of curiosity): \"For this research question, we ... remove the curiosity component from CureFuzz and conduct the fuzzing again ... We can observe that the addition of the curiosity mechanism plays a vital role in boosting the effectiveness of CureFuzz ...\" (Figure 3 and Table 5 show ablation results.)\n\np.11, Section 6 (Discussion - epistemic uncertainty): \"The curiosity mechanism\u2019s optimization process ... The prediction error of the target network and the predictor network then serves as a proxy for measuring uncertainty, specifically epistemic uncertainty. Epistemic uncertainty would be particularly high in regions of the input space where few similar examples were seen ... By focusing on areas of high epistemic uncertainty ... the testing process inherently explores a wider variety of scenarios.\" \n\np.10, Section 5 (RQ3 repair result): \"For the repaired ACAS Xu, 55 faults and 4 distinct types of crash-triggering scenarios are detected. We find that the crashes found by CureFuzz are reduced by 73% compared with the original DNN model.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Analyzes the distribution of crash-triggering scenarios ",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "CureFuzz enforces plausibility constraints at each mutation step by restricting mutated seeds to the environment's legitimate state space and discarding illegal or initially crashing states (per-environment validity checks and simulator-based filtering). It also uses only \u2018small random perturbations\u2019 when mutating seeds. However, the paper does not describe mechanisms that explicitly prevent or measure cumulative drift across many fuzzing iterations nor report any human or quantitative perceptual/semantic plausibility assessments (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) for final fault-inducing inputs. The evaluation metrics focus on coverage and crash counts rather than input realism. This matches the metric\u2019s Medium definition: plausibility is enforced at individual mutation steps or by rule-based validity checks, but cumulative-effects and explicit plausibility validation of final fault-inducing inputs are not addressed.",
                    "evidence": "3.4 CureFuzz Architecture (Seed Mutation): \"The Seed-Mutation function generates a new mutated state by applying a small random perturbation to a selected state. ... We need to ensure that the mutated seed lies in the legitimate state spaces , and the mutated seed would not trigger an initial crash. In our experiments, we have carefully addressed this concern and verified the validity of each mutated seed, details are given in Section 4.\" (Section 3.4)\n\n4.2.1 Autonomous Driving (CARLA): \"We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.\" (Section 4.2.1)\n\n4.2.2 Aircraft Collision Avoidance (ACAS Xu): \"Given the predefined range of acceptable states, states that fall outside legal space are automatically discarded.\" (Section 4.2.2)\n\n4.2.3 Coop Navi (MARL) / BipedalWalker: \"With a clear definition of permissible positions and velocities, CureFuzz ignores any illegal states.\" and \"Since the valid ground types are known, illegal states can be easily detected and discarded.\" (Section 4.2.3 and 4.2.4)\n\n4.3 Implementation (mutation magnitude): \"When mutating a seed, the magnitude of the random perturbation is a critical factor that can impact the performance of the fuzzer. We re-use the code implementation from Pang et al. for generating mutations.\" (Section 4.3)\n\n5 Evaluation metrics (lack of perceptual plausibility measures): \"Our evaluation focuses on three key metrics: environmental state coverage, the total number of detected crash-triggering scenarios, and the distinct types of crash-triggering scenarios.\" (Section 5, RQ1 description)  -- no human studies or perceptual/semantic quality metrics (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) are reported."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility through rule-based constraints applied at each mutation step, which aligns with the 'Medium' rating. For each of the four experimental environments, the authors describe a specific mechanism to ensure that the mutated input state remains within a valid or 'legal' space, and they explicitly discard any states that violate these rules. For example, in the autonomous driving environment, the CARLA simulator is used to validate the mutated state, and in the aircraft collision avoidance system, states outside a predefined 'legal space' are discarded. However, the paper does not address the potential for cumulative drift, where a series of individually plausible mutations could lead to an overall unrealistic scenario. Furthermore, it lacks any empirical validation (such as human studies or quantitative metrics like LPIPS or SSIM) to confirm that the final crash-triggering scenarios are perceptually or semantically realistic.",
                    "evidence": "Page 7, Section 4.2.1 Autonomous Driving: 'We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.'\nPage 7, Section 4.2.2 Aircraft Collision Avoidance: 'Given the predefined range of acceptable states, states that fall outside legal space are automatically discarded.'\nPage 7, Section 4.2.3 Video Game: 'With a clear definition of permissible positions and velocities, CUREFUZZ ignores any illegal states.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "CureFuzz enforces plausibility constraints at each mutation step by restricting mutated seeds to the environment's legitimate state space and discarding illegal or initially crashing states (per-environment validity checks and simulator-based filtering). It also uses only \u2018small random perturbations\u2019 when mutating seeds. However, the paper does not describe mechanisms that explicitly prevent or measure cumulative drift across many fuzzing iterations nor report any human or quantitative perceptual/semantic plausibility assessments (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) for final fault-inducing inputs. The evaluation metrics focus on coverage and crash counts rather than input realism. This matches the metric\u2019s Medium definition: plausibility is enforced at individual mutation steps or by rule-based validity checks, but cumulative-effects and explicit plausibility validation of final fault-inducing inputs are not addressed.",
                    "evidence": "3.4 CureFuzz Architecture (Seed Mutation): \"The Seed-Mutation function generates a new mutated state by applying a small random perturbation to a selected state. ... We need to ensure that the mutated seed lies in the legitimate state spaces , and the mutated seed would not trigger an initial crash. In our experiments, we have carefully addressed this concern and verified the validity of each mutated seed, details are given in Section 4.\" (Section 3.4)\n\n4.2.1 Autonomous Driving (CARLA): \"We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.\" (Section 4.2.1)\n\n4.2.2 Aircraft Collision Avoidance (ACAS Xu): \"Given the predefined range of acceptable states, states that fall outside legal space are automatically discarded.\" (Section 4.2.2)\n\n4.2.3 Coop Navi (MARL) / BipedalWalker: \"With a clear definition of permissible positions and velocities, CureFuzz ignores any illegal states.\" and \"Since the valid ground types are known, illegal states can be easily detected and discarded.\" (Section 4.2.3 and 4.2.4)\n\n4.3 Implementation (mutation magnitude): \"When mutating a seed, the magnitude of the random perturbation is a critical factor that can impact the performance of the fuzzer. We re-use the code implementation from Pang et al. for generating mutations.\" (Section 4.3)\n\n5 Evaluation metrics (lack of perceptual plausibility measures): \"Our evaluation focuses on three key metrics: environmental state coverage, the total number of detected crash-triggering scenarios, and the distinct types of crash-triggering scenarios.\" (Section 5, RQ1 description)  -- no human studies or perceptual/semantic quality metrics (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) are reported."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper performs mutations on in-memory numeric environment states (positions, angles, speeds, ground types) and evaluates crashes by executing the mutated states in the simulator, but it does not describe any handling of standard I/O/serialization effects (rounding, quantization, clipping) nor constraining perturbations to survive serialization. For CARLA the SDM consumes RGB images, yet CureFuzz mutates underlying world-state floats (positions/angles) and uses the simulator to render and evaluate states without any explicit discussion of ensuring that the fault-inducing perturbations persist under serialization (e.g., 8-bit image quantization) or of generating inputs directly in serialized formats. Therefore failures discovered could be fragile numeric artifacts that would not persist after standard I/O operations, and the paper does not explicitly account for serialization effects during mutation or oracle evaluation.",
                    "evidence": "Page 6, Section 4.2.1 (Autonomous Driving): \"In the CARLA simulator, at each timestep, the SDM receives an RGB image and its current velocity as inputs.\" \n\nPage 6, Section 4.2.1 (Autonomous Driving): \"When CureFuzz mutates a given state, small perturbations are randomly generated and added to the positions and angles of these vehicles. We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.\" \n\nPage 5, Section 3.4 (Seed Mutation): \"Once a seed is selected from the corpus, the SeedMutation function generates a new mutated state by applying a small random perturbation to a selected state. The mutated state is fed into the EnvMonitor function to generate its corresponding state sequence and collect the cumulative reward. We need to ensure that the mutated seed lies in the legitimate state spaces, and the mutated seed would not trigger an initial crash. In our experiments, we have carefully addressed this concern and verified the validity of each mutated seed, details are given in Section 4.\" \n\nPage 7, Section 4.3 (Implementation): \"When mutating a seed, the magnitude of the random perturbation is a critical factor that can impact the performance of the fuzzer. We re-use the code implementation from Pang et al. for generating mutations.\" \n\n(Note: the paper contains no statements about handling serialization, quantization, rounding, clipping, or about generating inputs directly in serialized formats such as saved images/audio/text to ensure reproducibility under standard I/O.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper describes its mutation strategy as applying 'a small random perturbation to a selected state'. The validation of these mutated states focuses on ensuring they are legal within the specific domain of the simulation environment, such as preventing initial collisions in CARLA or staying within speed limits in ACAS Xu. The methodology does not mention or account for standard I/O operations like rounding, quantization, or clipping that would occur during serialization to a storage format. The checks performed are for domain-validity, not for I/O-reproducibility. Consequently, the small numerical perturbations introduced during fuzzing could be fragile and lost upon serialization, which means failures may not be reproducible. This approach aligns with the 'Low' rating, as it ignores I/O effects during both mutation and oracle evaluation.",
                    "evidence": "Page 6, Section 6, Seed Mutation: 'Once a seed is selected from the corpus, the Seed-Mutation function generates a new mutated state by applying a small random perturbation to a selected state. ... We need to ensure that the mutated seed lies in the legitimate state spaces, and the mutated seed would not trigger an initial crash.'\nPage 7, Section 4.2.1, Autonomous Driving: 'When CUREFUZZ mutates a given state, small perturbations are randomly generated and added to the positions and angles of these vehicles. We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.'\nPage 7, Section 4.2.2, Aircraft Collision Avoidance: 'Given the predefined range of acceptable states, states that fall outside legal space are automatically discarded.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper performs mutations on in-memory numeric environment states (positions, angles, speeds, ground types) and evaluates crashes by executing the mutated states in the simulator, but it does not describe any handling of standard I/O/serialization effects (rounding, quantization, clipping) nor constraining perturbations to survive serialization. For CARLA the SDM consumes RGB images, yet CureFuzz mutates underlying world-state floats (positions/angles) and uses the simulator to render and evaluate states without any explicit discussion of ensuring that the fault-inducing perturbations persist under serialization (e.g., 8-bit image quantization) or of generating inputs directly in serialized formats. Therefore failures discovered could be fragile numeric artifacts that would not persist after standard I/O operations, and the paper does not explicitly account for serialization effects during mutation or oracle evaluation.",
                    "evidence": "Page 6, Section 4.2.1 (Autonomous Driving): \"In the CARLA simulator, at each timestep, the SDM receives an RGB image and its current velocity as inputs.\" \n\nPage 6, Section 4.2.1 (Autonomous Driving): \"When CureFuzz mutates a given state, small perturbations are randomly generated and added to the positions and angles of these vehicles. We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.\" \n\nPage 5, Section 3.4 (Seed Mutation): \"Once a seed is selected from the corpus, the SeedMutation function generates a new mutated state by applying a small random perturbation to a selected state. The mutated state is fed into the EnvMonitor function to generate its corresponding state sequence and collect the cumulative reward. We need to ensure that the mutated seed lies in the legitimate state spaces, and the mutated seed would not trigger an initial crash. In our experiments, we have carefully addressed this concern and verified the validity of each mutated seed, details are given in Section 4.\" \n\nPage 7, Section 4.3 (Implementation): \"When mutating a seed, the magnitude of the random perturbation is a critical factor that can impact the performance of the fuzzer. We re-use the code implementation from Pang et al. for generating mutations.\" \n\n(Note: the paper contains no statements about handling serialization, quantization, rounding, clipping, or about generating inputs directly in serialized formats such as saved images/audio/text to ensure reproducibility under standard I/O.)"
                },
                "manual": {
                    "value": "High",
                    "why": "Mutates continuous feature vectors representing simulator states (positions, angles, velocities) and enforces validity checks to keep them within legal bounds.",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper contains no experiments or claims that fault-inducing inputs found on one model were reused to test other models for the same task, nor does CureFuzz include design elements aimed at producing transferable attacks across different model implementations. The evaluation applies CureFuzz separately to multiple SDMs and uses found scenarios to repair the same model (fine-tuning + re-test), but it does not measure whether seeds discovered on a surrogate transfer to unseen targets or test multiple implementations with the same seeds.",
                    "evidence": "Introduction (quote): \"We evaluate CureFuzz by applying it to well-known SDMs that use various learning algorithms. ... The experimental results demonstrate that CureFuzz effectively and efficiently identifies a significant number of catastrophic failures across all considered SDMs.\" (Introduction) \u2014 shows per-model evaluation but no claim of cross-model reuse.\n\nSection 5, RQ3 (quote): \"we utilize CureFuzz again to test the newly fine-tuned SDM. For the repaired ACAS Xu, 55 faults and 4 distinct types of crash-triggering scenarios are detected. We find that the crashes found by CureFuzz are reduced by 73% compared with the original DNN model.\" (Section 5, RQ3) \u2014 demonstrates using found scenarios to repair and retest the same model, not transfer to other models.\n\nSection 4.1 Research Questions (quote): \"To evaluate the performance of CureFuzz and comprehensively understand the impact, we formulate the three research questions: RQ1: How effective is CureFuzz in finding crash-triggering scenarios? RQ2: Can CureFuzz be effectively guided with the curiosity mechanism? RQ3: Can we use the crashes found by CureFuzz to improve SDMs?\" (Section 4.1) \u2014 the listed RQs do not include any question about transferability of fault-inducing inputs across different model implementations."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper introduces CUREFUZZ, a fuzzing method to find crash-triggering scenarios for sequential decision-makers (SDMs). The evaluation is conducted across multiple SDMs in various environments (e.g., two different models for the CARLA autonomous driving task, one based on DRL and one on IL). However, each SDM is treated as an independent and isolated target for the fuzzer. The experiments aim to demonstrate the effectiveness of the CUREFUZZ method on each target individually compared to baseline methods. The paper does not perform any experiments to test whether the crash-triggering scenarios discovered for one SDM would also cause failures (i.e., transfer) to another SDM performing the same task. The design of the fuzzer is focused on finding novel and diverse scenarios for the specific model under test, with no explicit mechanisms for discovering transferable failures.",
                    "evidence": "Page 8, Section 5 EXPERIMENT RESULTS, Tables 1, 2, and 3: The results for different models, even within the same environment, are presented in separate rows. For instance, 'Carla (RL)' and 'Carla (IL)' are evaluated independently. The paper discusses the performance of CUREFUZZ on each of these models but does not mention reusing the crash scenarios found for 'Carla (RL)' to test 'Carla (IL)' or vice-versa. The research questions also do not address transferability. For example, RQ1 asks: \"How effective is CUREFUZZ in finding crash-triggering scenarios?\" which is evaluated per model, not across models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper contains no experiments or claims that fault-inducing inputs found on one model were reused to test other models for the same task, nor does CureFuzz include design elements aimed at producing transferable attacks across different model implementations. The evaluation applies CureFuzz separately to multiple SDMs and uses found scenarios to repair the same model (fine-tuning + re-test), but it does not measure whether seeds discovered on a surrogate transfer to unseen targets or test multiple implementations with the same seeds.",
                    "evidence": "Introduction (quote): \"We evaluate CureFuzz by applying it to well-known SDMs that use various learning algorithms. ... The experimental results demonstrate that CureFuzz effectively and efficiently identifies a significant number of catastrophic failures across all considered SDMs.\" (Introduction) \u2014 shows per-model evaluation but no claim of cross-model reuse.\n\nSection 5, RQ3 (quote): \"we utilize CureFuzz again to test the newly fine-tuned SDM. For the repaired ACAS Xu, 55 faults and 4 distinct types of crash-triggering scenarios are detected. We find that the crashes found by CureFuzz are reduced by 73% compared with the original DNN model.\" (Section 5, RQ3) \u2014 demonstrates using found scenarios to repair and retest the same model, not transfer to other models.\n\nSection 4.1 Research Questions (quote): \"To evaluate the performance of CureFuzz and comprehensively understand the impact, we formulate the three research questions: RQ1: How effective is CureFuzz in finding crash-triggering scenarios? RQ2: Can CureFuzz be effectively guided with the curiosity mechanism? RQ3: Can we use the crashes found by CureFuzz to improve SDMs?\" (Section 4.1) \u2014 the listed RQs do not include any question about transferability of fault-inducing inputs across different model implementations."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ],
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "Zhou et al. - 2025 - Understanding the Effectiveness of Coverage Criteria for Large Language Models A Special Angle from": {
        "key": "zhou2025understanding",
        "name": "Zhou et al.",
        "year": "2025",
        "citation_count": "7",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly targets jailbreak attacks (high-impact security failures) and demonstrates that coverage-guided techniques both (1) generate successful jailbreak examples that bypass model alignment/safety behavior and (2) detect jailbreak queries in safety-aligned models. The authors report that attack queries can \"bypass\" safety neurons in well-aligned models and show coverage-guided generation produces substantially more successful jailbreaks than random; they also evaluate on an aligned model (Llama-2-7B-Chat) and report high success/detection results. These statements and empirical results indicate the method uncovers high-severity failures (jailbreaks that violate safety/policy mechanisms) rather than only low-level robustness errors.",
                    "evidence": "Page 730, Abstract (Application summary): \"we develop a real-time jailbreak detection mechanism that achieves high accuracy (93.61% on average) ... Lastly, we introduce a coverage-guided approach for generating jailbreak attack examples, enabling systematic refinement of prompts to uncover vulnerabilities.\"; Page 736, IV.B / RQ1 (attack bypass observation): \"...we observe significant coverage differences between normal (SN) and abnormal outputs (SNJ) in well-aligned models. Well-aligned models have safety neurons to reject malicious queries [46], but attack queries often bypass these and activate neurons linked to abnormal behaviors.\"; Page 739, V.C (generation results): \"As shown in Figure 6, the coverage-guided method outperforms the random strategy in generating successful jailbreak queries... These results confirm that coverage gains effectively guide query generation toward unexplored areas, increasing the number of successful jailbreak cases and enhancing robustness testing for LLMs.\"; Page 738\u2013739, V.A / Table V (detection on aligned model): \"As shown in Table V, our method achieves high average accuracies: 95.23% on OPT-125M, 93.92% on Llama-2-7B-Chat, and 91.69% on Pythia-12B, demonstrating its effectiveness across models.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's primary focus is on jailbreak attacks, which are explicitly defined as attempts to bypass the safety and alignment mechanisms of Large Language Models (LLMs) to generate harmful or disallowed content. This directly corresponds to the 'High' severity metric, as it involves uncovering high-impact failures by bypassing explicit safety mechanisms in safety-aligned models. The entire study, including the experimental design, evaluation standards, and proposed applications (Real-time Jailbreak Detection, Test Case Prioritization, and Jailbreak Case Generation), is centered on understanding, detecting, and generating these security-critical violations in models that are designed to prevent them.",
                    "evidence": "Page 1, Abstract: 'In this paper, we conduct a comprehensive empirical study to evaluate the effectiveness of traditional coverage criteria in identifying such inadequacies, exemplified by the significant security concern of jailbreak attacks.'\n\nPage 3, Section II-D, Jailbreak Attack and Defenses: 'Despite these efforts, jailbreak attacks have emerged that bypass alignment measures, causing LLMs to generate harmful outputs and compromising their security [8]\u2013[12].'\n\nPage 5, Section IV-A, Setup: The paper explicitly uses safety-aligned models for evaluation. 'These models include OPT-125M [42], Llama-2-7B-Chat [38], Pythia-12B [43], and Gemma-2-27B-it [44].'\n\nPage 9, Section V, Application: The proposed applications aim to address these high-severity failures directly. 'First, we propose Real-time Jailbreak Detection, using activation features from coverage criteria to classify queries as normal or jailbreak... Third, we investigate Jailbreak Case Generation, where coverage-guided methods refine prompts to generate adversarial examples.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly targets jailbreak attacks (high-impact security failures) and demonstrates that coverage-guided techniques both (1) generate successful jailbreak examples that bypass model alignment/safety behavior and (2) detect jailbreak queries in safety-aligned models. The authors report that attack queries can \"bypass\" safety neurons in well-aligned models and show coverage-guided generation produces substantially more successful jailbreaks than random; they also evaluate on an aligned model (Llama-2-7B-Chat) and report high success/detection results. These statements and empirical results indicate the method uncovers high-severity failures (jailbreaks that violate safety/policy mechanisms) rather than only low-level robustness errors.",
                    "evidence": "Page 730, Abstract (Application summary): \"we develop a real-time jailbreak detection mechanism that achieves high accuracy (93.61% on average) ... Lastly, we introduce a coverage-guided approach for generating jailbreak attack examples, enabling systematic refinement of prompts to uncover vulnerabilities.\"; Page 736, IV.B / RQ1 (attack bypass observation): \"...we observe significant coverage differences between normal (SN) and abnormal outputs (SNJ) in well-aligned models. Well-aligned models have safety neurons to reject malicious queries [46], but attack queries often bypass these and activate neurons linked to abnormal behaviors.\"; Page 739, V.C (generation results): \"As shown in Figure 6, the coverage-guided method outperforms the random strategy in generating successful jailbreak queries... These results confirm that coverage gains effectively guide query generation toward unexplored areas, increasing the number of successful jailbreak cases and enhancing robustness testing for LLMs.\"; Page 738\u2013739, V.A / Table V (detection on aligned model): \"As shown in Table V, our method achieves high average accuracies: 95.23% on OPT-125M, 93.92% on Llama-2-7B-Chat, and 91.69% on Pythia-12B, demonstrating its effectiveness across models.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's methods explicitly steer testing toward discovering jailbreaks \u2014 a broad class of security-violating behaviors \u2014 using coverage-guided generation and a binary jailbreak detector. The exploration and mutation components (coverage-guided selection of rewritten prompts) prioritize inputs that increase coverage and produce jailbreak successes, while the oracle is a binary jailbreak classifier. However, the work does not aim to induce attacker-chosen specific model outputs (specific labels, phrases, or commands); it seeks to find inputs that cause the model to exhibit the broader 'jailbreak' behavior. Therefore it supports discovery of broader classes of security-violating behaviors rather than targeted outputs.",
                    "evidence": "Page 730, Abstract: \"we introduce a coverage-guided approach for generating jailbreak attack examples, enabling systematic refinement of prompts to uncover vulnerabilities.\"; Page 733, Section III.C (Standards for Coverage Evaluation): \"In the context of LLMs, we regard abnormal behaviors caused by jailbreak attacks as faults.\"; Page 738, Section V.A (Jailbreak Detection): \"Specifically, we train an MLP model that uses the number of activated neurons as input to derive a binary result indicating whether a query triggers a jailbreak response.\"; Page 739, Section V.C (Jailbreak Case Generation): \"the query with the highest coverage increase is selected as the next seed in the coverage-guided approach.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method supports the discovery of a broad class of security-violating behaviors, specifically 'jailbreak attacks,' which cause the model to generate harmful or malicious content. The exploration is guided by coverage criteria to find inputs that trigger this general category of failure (i.e., bypassing safety measures). It does not, however, steer the exploration toward generating a specific, predefined target output (e.g., a particular harmful sentence), which would be required for a 'High' rating. The goal is to induce any successful jailbreak, which is a class of unsafe behavior, rather than performing untargeted exploration for generic faults.",
                    "evidence": "Page 9, Section V, Application: The paper describes three applications of its findings, all centered around jailbreaks. One application is 'Jailbreak Case Generation,' where the stated goal is to use coverage to 'guide the creation of attack examples' and 'refine prompts to generate adversarial examples.' The evaluation of this method counts the 'Number of Successful Jailbreak Queries' (Figure 6), confirming the target is the class of 'successful jailbreak' behavior, not a specific output string. The text states: 'By utilizing coverage to guide the creation of attack examples, Jailbreak Case Generation method identifies areas of the model that remain unexplored. Iterative refinement of prompts based on coverage gains ensures that the generated cases are effective in exposing vulnerabilities...'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper's methods explicitly steer testing toward discovering jailbreaks \u2014 a broad class of security-violating behaviors \u2014 using coverage-guided generation and a binary jailbreak detector. The exploration and mutation components (coverage-guided selection of rewritten prompts) prioritize inputs that increase coverage and produce jailbreak successes, while the oracle is a binary jailbreak classifier. However, the work does not aim to induce attacker-chosen specific model outputs (specific labels, phrases, or commands); it seeks to find inputs that cause the model to exhibit the broader 'jailbreak' behavior. Therefore it supports discovery of broader classes of security-violating behaviors rather than targeted outputs.",
                    "evidence": "Page 730, Abstract: \"we introduce a coverage-guided approach for generating jailbreak attack examples, enabling systematic refinement of prompts to uncover vulnerabilities.\"; Page 733, Section III.C (Standards for Coverage Evaluation): \"In the context of LLMs, we regard abnormal behaviors caused by jailbreak attacks as faults.\"; Page 738, Section V.A (Jailbreak Detection): \"Specifically, we train an MLP model that uses the number of activated neurons as input to derive a binary result indicating whether a query triggers a jailbreak response.\"; Page 739, Section V.C (Jailbreak Case Generation): \"the query with the highest coverage increase is selected as the next seed in the coverage-guided approach.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper\u2019s coverage-guided generation and mutation processes do not enforce or measure input plausibility across iterations, nor do the authors report any human evaluation or task-specific perceptual/linguistic metrics (e.g., Perplexity used as a plausibility check) for the final fault-inducing inputs. Generated attack queries are produced via LLM rewriting (GPT-4) or by appending adversarial suffixes (GCG), and selection is based solely on coverage increase, with no mechanism described to constrain cumulative drift or to filter for semantic/naturalness. While the authors use perplexity-based methods as baselines for jailbreak detection, they do not use such measures to ensure that fuzzed/generated inputs remain realistic; likewise, no human study or quantitative plausibility metrics (e.g., Perplexity reported for final examples, LPIPS/SSIM/PESQ/STOI) are reported. Therefore the work does not enforce or assess input plausibility as required by the metric.",
                    "evidence": "III-A Motivation (page 732): \"Attack queries are generated by appending adversarial suffixes to rejected queries using GCG [7]. These queries aim to trigger the model to output malicious content (i.e., abnormal behaviors).\" \n\nV.C Jailbreak Case Generation (page 739): \"Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting. The query with the highest coverage increase is selected as the next seed in the coverage-guided approach.\" \n\nV.A Baseline (page 738-739): \"Baseline: We select the most widely used perplexity filter [33], the state-of-the-art method PARDEN [35], and self-reminder [49] as baselines for comparison.\" (shows perplexity is used as a baseline detector, not as an enforced plausibility constraint for generated inputs)\n\nEvaluation / General (pages 736-740, various): No sections or reported results describe human evaluation or quantitative perceptual/linguistic measures (e.g., Perplexity reported for generated fault-inducing prompts or other task-specific plausibility scores); coverage-guided selection is driven by coverage increase alone."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a coverage-guided method for generating jailbreak attack prompts. The generation process uses GPT-4 for 'prompt rewriting,' and the selection of subsequent prompts is guided entirely by maximizing 'coverage increase.' The evaluation of the generated prompts is based on their success rate in triggering a jailbreak, not on their plausibility or realism as user queries. The paper does not describe any mechanism to enforce input plausibility during the generation process, nor does it report any assessment of the plausibility of the final, successful jailbreak prompts, either through human evaluation or quantitative metrics like perplexity for linguistic fluency. The focus is entirely on the efficacy of the generated attacks, without consideration for whether the inputs are realistic within a normal threat model.",
                    "evidence": "Page 11, Section V-C, Jailbreak Case Generation: 'Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting. The query with the highest coverage increase is selected as the next seed in the coverage-guided approach.' and 'Evaluation results: As shown in Figure 6, the coverage-guided method outperforms the random strategy in generating successful jailbreak queries.' This demonstrates that the generation and selection process is driven by coverage and success rate, with no mention of plausibility constraints or validation."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper\u2019s coverage-guided generation and mutation processes do not enforce or measure input plausibility across iterations, nor do the authors report any human evaluation or task-specific perceptual/linguistic metrics (e.g., Perplexity used as a plausibility check) for the final fault-inducing inputs. Generated attack queries are produced via LLM rewriting (GPT-4) or by appending adversarial suffixes (GCG), and selection is based solely on coverage increase, with no mechanism described to constrain cumulative drift or to filter for semantic/naturalness. While the authors use perplexity-based methods as baselines for jailbreak detection, they do not use such measures to ensure that fuzzed/generated inputs remain realistic; likewise, no human study or quantitative plausibility metrics (e.g., Perplexity reported for final examples, LPIPS/SSIM/PESQ/STOI) are reported. Therefore the work does not enforce or assess input plausibility as required by the metric.",
                    "evidence": "III-A Motivation (page 732): \"Attack queries are generated by appending adversarial suffixes to rejected queries using GCG [7]. These queries aim to trigger the model to output malicious content (i.e., abnormal behaviors).\" \n\nV.C Jailbreak Case Generation (page 739): \"Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting. The query with the highest coverage increase is selected as the next seed in the coverage-guided approach.\" \n\nV.A Baseline (page 738-739): \"Baseline: We select the most widely used perplexity filter [33], the state-of-the-art method PARDEN [35], and self-reminder [49] as baselines for comparison.\" (shows perplexity is used as a baseline detector, not as an enforced plausibility constraint for generated inputs)\n\nEvaluation / General (pages 736-740, various): No sections or reported results describe human evaluation or quantitative perceptual/linguistic measures (e.g., Perplexity reported for generated fault-inducing prompts or other task-specific plausibility scores); coverage-guided selection is driven by coverage increase alone."
                },
                "manual": {
                    "value": "Medium",
                    "why": "GPT-4 generates  new jailbreak queries through prompt rewriting. The generated contents are supposed to be natural but there was no measure reported how natural are those. The attack suffix is related to jailbreak detection, not creation",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper studies coverage criteria and jailbreak detection for LLMs using textual prompts, tokenization, hidden-state inspection, and prompt-generation (paraphrasing/attack-suffix) techniques. It does not discuss applying fine-grained numeric mutations to floating-point input tensors nor does it account for standard I/O serialization effects (clipping, quantization, rounding) during test generation or oracle evaluation. Because the design and evaluation focus on discrete text prompts and internal activations without any explicit handling or constraints to ensure persistence of fault-inducing perturbations under serialization, the work ignores I/O effects relevant to the Failure Reproducibility metric.",
                    "evidence": "Page 731, Section II-A (Model Inference Process): \"The text is tokenized into discrete elements, each mapped to a dense embedding vector representing token semantics for further processing by the model.\" \n\nPage 732, Section III-A (Motivation: A Cluster Analysis Experiment): \"We extract the hidden states h from the 4th, 9th, 16th, and 31st transformer blocks for these types of queries and conduct k-means clustering [40].\" \n\nPage 734, Section IV-A(2) (Test Suite Construction / Synonymous Query Dataset): \"To construct synonymous queries, we use GPT-4 to generate corresponding synonymous paraphrases for the first 500 queries from the Alpaca-gpt4 dataset...\" \n\nPage 732, Section III-A (Attack query generation): \"Attack queries are generated by appending adversarial suffixes to rejected queries using GCG [7].\" \n\n(Throughout the paper there is no mention of handling serialization, quantization, clipping, or other I/O effects when generating inputs or when evaluating whether an input induces a failure.)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's methodology for generating and selecting test cases for Large Language Models (LLMs) operates entirely within the text domain. The inputs are prompts sourced from datasets, created by paraphrasing, or generated by appending suffixes or rewriting existing prompts. Text is an inherently serialized format, and the perturbations (the specific words and structures that constitute a 'jailbreak attack') are symbolic, not numerical. These inputs are not susceptible to the kind of information loss (e.g., from quantization, rounding, or clipping) that the metric describes for image or audio data when saved to disk. The metric definition explicitly states: 'We also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., ... generated text) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.' The paper's approach of generating and using text prompts perfectly aligns with this criterion, ensuring that the fault-inducing inputs are fully reproducible under any standard I/O operation.",
                    "evidence": "Page 4, Section III-A, 'Findings': 'Attack queries are generated by appending adversarial suffixes to rejected queries using GCG [7]. These queries aim to trigger the model to output malicious content (i.e., abnormal behaviors).' This describes a text-based generation method. \nPage 6, Section IV-A, 'Test Suite Construction': 'To construct synonymous queries, we use GPT-4 to generate corresponding synonymous paraphrases for the first 500 queries from the Alpaca-gpt4 dataset...'\nPage 11, Section V-C, 'Jailbreak Case Generation': 'Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting.' All these methods generate test cases directly in their final, serialized text format."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "All of the paper\u2019s test generation and evaluation is performed in the textual domain: inputs are discrete text prompts that are paraphrased, attacked via suffixes, or rewritten by GPT-4. Text is inherently a serialized format not subject to quantization, clipping or rounding losses. According to the Failure Reproducibility metric, fuzzing approaches that generate inputs directly in serialized formats (including text) are deemed fully reproducible under standard I/O operations. Therefore, the faults induced by these adversarial text prompts will persist unchanged through any standard serialization/deserialization pipeline.",
                    "evidence": "Page 731, Section III-A: \u201cAttack queries are generated by appending adversarial suffixes to rejected queries using GCG [7].\u201d\nPage 734, Section IV-A(2): \u201cTo construct synonymous queries, we use GPT-4 to generate corresponding synonymous paraphrases for the first 500 queries from the Alpaca-gpt4 dataset\u2026\u201d\nPage 738, Section V-C: \u201cOver five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting.\u201d\n(At no point does the paper apply fine-grained floating-point mutations or consider clipping, rounding, or quantization\u2014inputs remain plain text.)"
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether fault-inducing inputs (jailbreak prompts) crafted on one model transfer to other models. The generation experiments and the coverage-guided jailbreak-case generation are conducted targeting a single model (Llama-2-7b-chat). Detection and prioritization experiments are performed per-model (training/validating classifiers or thresholds for each model), but the paper does not report experiments where inputs crafted on a surrogate model are reused to test different target models, nor does it describe mechanisms explicitly designed to discover transferable attacks across models.",
                    "evidence": "Section V.C (Jailbreak Case Generation), p. 739: \"Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round ... Each method ultimately generates 250 new jailbreak queries to evaluate effectiveness.\"; Section V.A (Jailbreak Detection), p. 738: \"... we train an MLP model that uses the number of activated neurons as input ... We validate its effectiveness on OPT-125M, Llama-2-7B-Chat, and Pythia-12B.\"; Section III.A (Motivation: A Cluster Analysis Experiment), p. 732: \"Experimental Setup: We first introduce the setup for our cluster analysis experiments using the Llama-2-7b-chat [38] model as our target.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes and evaluates methods for jailbreak detection, prioritization, and generation using coverage criteria. The experiments are conducted across four different LLMs. However, the evaluation of fault-inducing inputs (jailbreak attacks) is performed on a per-model basis. Specifically, in the 'Jailbreak Case Generation' application, new jailbreak queries are generated using coverage guidance from a single target model (Llama-2-7b-chat). The paper evaluates the success of this generation method for that specific model but does not subsequently test whether these generated queries are effective (i.e., transferable) in causing failures in the other models studied in the paper (OPT-125M, Pythia-12B, Gemma-2-27B-it). The research does not demonstrate or test whether fault-inducing inputs discovered on one model trigger failures in others.",
                    "evidence": "Page 11, Section V. APPLICATION, C. Jailbreak Case Generation: \"Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting. The query with the highest coverage increase is selected as the next seed in the coverage-guided approach. For comparison, a random strategy selects seeds randomly from rewritten candidates. Each method ultimately generates 250 new jailbreak queries to evaluate effectiveness.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether fault-inducing inputs (jailbreak prompts) crafted on one model transfer to other models. The generation experiments and the coverage-guided jailbreak-case generation are conducted targeting a single model (Llama-2-7b-chat). Detection and prioritization experiments are performed per-model (training/validating classifiers or thresholds for each model), but the paper does not report experiments where inputs crafted on a surrogate model are reused to test different target models, nor does it describe mechanisms explicitly designed to discover transferable attacks across models.",
                    "evidence": "Section V.C (Jailbreak Case Generation), p. 739: \"Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round ... Each method ultimately generates 250 new jailbreak queries to evaluate effectiveness.\"; Section V.A (Jailbreak Detection), p. 738: \"... we train an MLP model that uses the number of activated neurons as input ... We validate its effectiveness on OPT-125M, Llama-2-7B-Chat, and Pythia-12B.\"; Section III.A (Motivation: A Cluster Analysis Experiment), p. 732: \"Experimental Setup: We first introduce the setup for our cluster analysis experiments using the Llama-2-7b-chat [38] model as our target.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "No analysis of fault transferability",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides substantive diagnostic insight into why jailbreak failures occur by analyzing observable model internals and statistical failure patterns: it performs a clustering analysis of hidden states, measures neuron-level coverage differences across criteria, layers, blocks, and tokens, introduces a Relative Coverage Growth (RCG) metric, and uses activated-neuron features to build a jailbreak detector. These analyses connect discovered failures to internal activation patterns and layer/block/token contributions (i.e., model internals and statistical patterns). However, the paper does not demonstrate a causal linkage from failures to broader underlying model vulnerabilities such as reliance on non-robust or spurious features, dataset bias, or overfitting \u2014 it offers plausible hypotheses (e.g., neurons associated with knowledge domains) but no direct causal evidence. Therefore the correct rating is Medium.",
                    "evidence": "III.A Motivation: A Cluster Analysis Experiment (p. 732): \"In summary, our clustering analysis demonstrates that the internal states of the model include features capable of representing and distinguishing the \u2018logic\u2019 of different behaviors. This confirms the feasibility of using internal states to design coverage criteria for LLMs.\"; IV.B RQ1 results / statistical coverage patterns (pp. 735\u2013736, Table IV): \"The average coverage growth calculated on the Attention layer for NC, TKNC, TKNP, TFC, and NLC registers at 3.16%, 7.50%, 33.02%, 24.09%, and 2.77%, respectively...\" (demonstrates statistical analysis of failure/attack vs. normal inputs); IV.C RQ2 (p. 736): definition of RCG and layer/block analysis: \"RCG = max(C_SNJ - C_SNS / C_SN, 0)\" and discussion that \"Attention layers are more effective than MLP layers for optimizing coverage analysis in LLMs\" and that certain blocks show high RCG (identifying block-level diagnostic signals); V.A Jailbreak Detection (pp. 738\u2013739 & Table V): \"Due to the significant differences in the parts of the model covered by normal queries and jailbreak attacks, we design a detection method based on the number of activated neurons (the feature used by NC). Specifically, we train an MLP model that uses the number of activated neurons as input to derive a binary result...\" (shows use of internal activation features for diagnostics/applications); IV.B (p. 736) speculative interpretation without causal proof: \"This may be because LLMs associate specific neurons with different knowledge domains... Abnormal behaviors... activate distinct knowledge areas, triggering neurons linked to negative or harmful knowledge domains,\" (suggests a hypothesis but does not establish causal linkage to spurious features, dataset bias, or overfitting)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides significant diagnostic insight into why jailbreak attacks (failures) succeed by analyzing observable model internals. It establishes a clear correlation between successful attacks and distinct neuron activation patterns. The authors analyze various coverage criteria (NC, TKNC, etc.) across different model layers and blocks to characterize these failures. They explain that jailbreak queries bypass the model's 'safety neurons' and activate different sets of neurons associated with distinct 'knowledge domains' compared to normal or rejected queries. This constitutes a detailed statistical analysis of failure patterns through the lens of model internals (neuron coverage and characteristics), which directly aligns with the definition of the 'Medium' value. The paper does not, however, extend this analysis to link these observable phenomena to underlying root causes of vulnerability like dataset bias or overfitting, which would be required for a 'High' rating.",
                    "evidence": "Page 7, Section IV.B, \"Evaluation against Requirement 2\": \"Furthermore, we observe significant coverage differences between normal (SN) and abnormal outputs (SNJ) in well-aligned models. Well-aligned models have safety neurons to reject malicious queries [46], but attack queries often bypass these and activate neurons linked to abnormal behaviors. As a result, normal and abnormal behaviors activate different neurons, causing coverage differences. ... This may be because LLMs associate specific neurons with different knowledge domains [20], [47], leading to varying activation patterns depending on the query type. Abnormal behaviors, such as bias or harmful content, activate distinct knowledge areas, triggering neurons linked to negative or harmful knowledge domains, which results in noticeable activation differences.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides substantive diagnostic insight into why jailbreak failures occur by analyzing observable model internals and statistical failure patterns: it performs a clustering analysis of hidden states, measures neuron-level coverage differences across criteria, layers, blocks, and tokens, introduces a Relative Coverage Growth (RCG) metric, and uses activated-neuron features to build a jailbreak detector. These analyses connect discovered failures to internal activation patterns and layer/block/token contributions (i.e., model internals and statistical patterns). However, the paper does not demonstrate a causal linkage from failures to broader underlying model vulnerabilities such as reliance on non-robust or spurious features, dataset bias, or overfitting \u2014 it offers plausible hypotheses (e.g., neurons associated with knowledge domains) but no direct causal evidence. Therefore the correct rating is Medium.",
                    "evidence": "III.A Motivation: A Cluster Analysis Experiment (p. 732): \"In summary, our clustering analysis demonstrates that the internal states of the model include features capable of representing and distinguishing the \u2018logic\u2019 of different behaviors. This confirms the feasibility of using internal states to design coverage criteria for LLMs.\"; IV.B RQ1 results / statistical coverage patterns (pp. 735\u2013736, Table IV): \"The average coverage growth calculated on the Attention layer for NC, TKNC, TKNP, TFC, and NLC registers at 3.16%, 7.50%, 33.02%, 24.09%, and 2.77%, respectively...\" (demonstrates statistical analysis of failure/attack vs. normal inputs); IV.C RQ2 (p. 736): definition of RCG and layer/block analysis: \"RCG = max(C_SNJ - C_SNS / C_SN, 0)\" and discussion that \"Attention layers are more effective than MLP layers for optimizing coverage analysis in LLMs\" and that certain blocks show high RCG (identifying block-level diagnostic signals); V.A Jailbreak Detection (pp. 738\u2013739 & Table V): \"Due to the significant differences in the parts of the model covered by normal queries and jailbreak attacks, we design a detection method based on the number of activated neurons (the feature used by NC). Specifically, we train an MLP model that uses the number of activated neurons as input to derive a binary result...\" (shows use of internal activation features for diagnostics/applications); IV.B (p. 736) speculative interpretation without causal proof: \"This may be because LLMs associate specific neurons with different knowledge domains... Abnormal behaviors... activate distinct knowledge areas, triggering neurons linked to negative or harmful knowledge domains,\" (suggests a hypothesis but does not establish causal linkage to spurious features, dataset bias, or overfitting)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Provides correlation with internal coverage",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "Gong et al. - PaPillon Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs": {
        "key": "gong2025papillon",
        "name": "PAPILLON",
        "year": "2025",
        "citation_count": "0",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper proposes an automated black-box jailbreaking framework whose explicit goal is to bypass aligned models' safety mechanisms and elicit harmful outputs. The authors evaluate PAPILLON against safety-aligned/proprietary models (e.g., GPT-3.5, GPT-4, Gemini-Pro) and test its resiliency against specific defenses (perplexity filters, SmoothLLM, Llama Guard, and hybrid defenses), reporting high attack success rates even when defenses are enabled. This directly matches the metric's definition of high-severity failures: bypassing explicit safety/policy mechanisms to induce jailbreaks and unsafe outputs.",
                    "evidence": "Page 2401, Abstract: \"we introduce a novel jailbreaking attack framework called PAPILLON, which is an automated, black-box jailbreaking attack framework...\"; Page 2406, Section 3 Threat Model: \"The target LLM is assumed to be an aligned model, fine-tuned with instruction tuning or reinforcement learning from human feedback (RLHF), which typically refuses to respond to unethical or harmful questions.\"; Page 2406, Section 3 Threat Model: \"The objective of the attack is to craft jailbreak prompts ... 1) To circumvent the safety mechanisms of the target LLM, ensuring that it responds to a harmful query instead of opting not to answer.\"; Page 2412, Section 6 Resiliency against Defenses and Table 6 (results): show PAPILLON retains high ASR against defenses (e.g., \"GPT-4: PAPILLON 80% \u2192 After Llama Guard 64%\" in Table 6); Page 2412, Table 7 (hybrid defenses): show PAPILLON maintains high ASR under combined defenses (e.g., \"GPT-4: PAPILLON 80% \u2192 Perplexity + SmoothLLM 80%\")."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper introduces PAPILLON, a fuzz testing framework explicitly designed to generate jailbreak prompts that bypass the safety and policy mechanisms of large language models (LLMs). The goal is to induce the models to produce harmful or offensive content, which constitutes a high-impact security failure. The evaluation is specifically conducted on safety-aligned models (e.g., GPT-4, Llama-2-chat) and explicitly tests the method's resilience against several state-of-the-art defense mechanisms, including Perplexity Filters, SmoothLLM, and Llama Guard. The paper demonstrates that PAPILLON achieves a high attack success rate even when these explicit safety safeguards are in place, directly meeting the criteria for the 'High' severity rating.",
                    "evidence": "Page 14, Section 6.3 Llama Guard: \"Llama Guard [17] is a safeguard model based on the Llama-2-7B architecture, designed to classify prompts and outputs in AI-human interactions as \u201csafe\u201d or \u201cunsafe\u201d. Unsafe prompts are filtered to defend against jailbreak attempts. In our study, we targeted LLMs equipped with Llama Guard, with results presented in Table 6. The experimental findings reveal that we can achieve a high attack success rate even under Llama Guard\u2019s defense.\"\n\nPage 14, Section 6.4 Hybrid Defense: \"In addition to single defenses, we also consider two combinations of multiple defenses, referred to as hybrid defenses... Hybrid defenses generally provide stronger protection than single defenses. However, PAPILLON still achieves a high attack success rate in such cases.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper proposes an automated black-box jailbreaking framework whose explicit goal is to bypass aligned models' safety mechanisms and elicit harmful outputs. The authors evaluate PAPILLON against safety-aligned/proprietary models (e.g., GPT-3.5, GPT-4, Gemini-Pro) and test its resiliency against specific defenses (perplexity filters, SmoothLLM, Llama Guard, and hybrid defenses), reporting high attack success rates even when defenses are enabled. This directly matches the metric's definition of high-severity failures: bypassing explicit safety/policy mechanisms to induce jailbreaks and unsafe outputs.",
                    "evidence": "Page 2401, Abstract: \"we introduce a novel jailbreaking attack framework called PAPILLON, which is an automated, black-box jailbreaking attack framework...\"; Page 2406, Section 3 Threat Model: \"The target LLM is assumed to be an aligned model, fine-tuned with instruction tuning or reinforcement learning from human feedback (RLHF), which typically refuses to respond to unethical or harmful questions.\"; Page 2406, Section 3 Threat Model: \"The objective of the attack is to craft jailbreak prompts ... 1) To circumvent the safety mechanisms of the target LLM, ensuring that it responds to a harmful query instead of opting not to answer.\"; Page 2412, Section 6 Resiliency against Defenses and Table 6 (results): show PAPILLON retains high ASR against defenses (e.g., \"GPT-4: PAPILLON 80% \u2192 After Llama Guard 64%\" in Table 6); Page 2412, Table 7 (hybrid defenses): show PAPILLON maintains high ASR under combined defenses (e.g., \"GPT-4: PAPILLON 80% \u2192 Perplexity + SmoothLLM 80%\")."
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "PAPILLON is explicitly designed to steer fuzzing toward producing jailbreaks\u2014i.e., causing models to generate harmful or policy-violating responses for given harmful questions\u2014by using question-dependent mutation operators and a two-level judge that checks both harmfulness and relevance. However, the paper frames the attacker goal as eliciting harmful/unsafe behavior (jailbreaks) and relevant harmful answers to the posed questions, not as inducing specific, attacker-chosen target outputs or exact phrases. Thus it addresses broader classes of security-violating behavior (jailbreaks) rather than precise targeted outputs.",
                    "evidence": "Abstract (p.2401): \"attackers create jailbreak prompts to mislead the model to produce harmful or offensive content.\"; Section 3 Threat Model (p.2406): \"The objective of the attack is to craft jailbreak prompts for a collection of harmful or unethical questions that compel the target LLM to provide actual responses. Specifically, our goals are: 1) To circumvent the safety mechanisms of the target LLM... 2) To ensure that the responses to these questions are not only correct and relevant but also contain the harmful content intended by the query.\"; Section 4.2 (Template Mutation and Optimization, p.2408): \"we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.\"; Section 4.3 Two-level Judge Module (p.2409): \"To evaluate whether a response is jailbroken, the judge model should be based on its harmfulness and its relevance to the harmful question... The first judge model is a fine-tuned RoBERTa model... We further used GPT-3.5 turbo for an additional round of judgment... This additional check improved the credibility of the jailbreak assessment and ensured the response content aligned with the query.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper presents PAPILLON, a fuzz testing framework explicitly designed to discover 'jailbreaks' in Large Language Models (LLMs). The goal is to automatically generate prompts that cause an LLM to produce harmful, offensive, or otherwise policy-violating content in response to a harmful question. This aligns with the 'Medium' value, as it supports the discovery of a broader class of security-violating behaviors (i.e., jailbreaks) rather than steering exploration toward a specific, predefined output string (High) or performing untargeted exploration for generic failures (Low). The framework's mutation operators and its two-level judge oracle are specifically designed to guide the search process toward finding successful jailbreak responses, which are defined by their harmfulness and relevance to the input query.",
                    "evidence": "Page 8, Section 3, Threat Model: 'The objective of the attack is to craft jailbreak prompts for a collection of harmful or unethical questions that compel the target LLM to provide actual responses. Specifically, our goals are: 1) To circumvent the safety mechanisms of the target LLM, ensuring that it responds to a harmful query instead of opting not to answer. 2) To ensure that the responses to these questions are not only correct and relevant but also contain the harmful content intended by the query.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "PAPILLON is explicitly designed to steer fuzzing toward producing jailbreaks\u2014i.e., causing models to generate harmful or policy-violating responses for given harmful questions\u2014by using question-dependent mutation operators and a two-level judge that checks both harmfulness and relevance. However, the paper frames the attacker goal as eliciting harmful/unsafe behavior (jailbreaks) and relevant harmful answers to the posed questions, not as inducing specific, attacker-chosen target outputs or exact phrases. Thus it addresses broader classes of security-violating behavior (jailbreaks) rather than precise targeted outputs.",
                    "evidence": "Abstract (p.2401): \"attackers create jailbreak prompts to mislead the model to produce harmful or offensive content.\"; Section 3 Threat Model (p.2406): \"The objective of the attack is to craft jailbreak prompts for a collection of harmful or unethical questions that compel the target LLM to provide actual responses. Specifically, our goals are: 1) To circumvent the safety mechanisms of the target LLM... 2) To ensure that the responses to these questions are not only correct and relevant but also contain the harmful content intended by the query.\"; Section 4.2 (Template Mutation and Optimization, p.2408): \"we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.\"; Section 4.3 Two-level Judge Module (p.2409): \"To evaluate whether a response is jailbroken, the judge model should be based on its harmfulness and its relevance to the harmful question... The first judge model is a fine-tuned RoBERTa model... We further used GPT-3.5 turbo for an additional round of judgment... This additional check improved the credibility of the jailbreak assessment and ensured the response content aligned with the query.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Search toward semantic policy-violation behavior (jailbreak) but not a specific output.",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides substantial statistical analysis of failure patterns (e.g., attack success rate changes across prompt length, mutator variants, pre-jailbreak budgets, judges, defenses, and cross-model transferability) and links some failure modes to observable input properties (notably prompt perplexity and semantic coherence) that enable evasion of detection. However, it does not analyze internal model signals (neuron activations, coverage, logits, layer behavior) nor does it explicitly link failures to deeper underlying model vulnerabilities such as dataset bias or overfitting. Therefore the paper meets the 'Medium' level: it offers statistical/failure-pattern diagnostics but not analysis of model internals or causes like spurious features or training data issues.",
                    "evidence": "Section 4.2 (Maintain semantic coherence), p.2408\u20132409: \"Experiments show that the average perplexity of our generated jailbreak prompts for GPT-3.5 Turbo is as low as 34.61, significantly below the average perplexity threshold of 58.83.\"; Section 6.1 (Perplexity Filter), p.2411\u20132412: \"As shown in Table 16 (appendix), when applying the perplexity filter [1] to PAPILLON, we observe that PAPILLON maintains a high attack success rate. The ASR drop of PAPILLON is less than 10% in nearly all cases. The potential reason is that the jailbreak prompts generated by PAPILLON maintain semantic coherence with low perplexity. This allows these jailbreak prompts to bypass the perplexity-based detection mechanism by closely mimicking regular, benign prompts, thus evading detection.\"; Section 5.3 (Ablation Studies) and Tables (e.g., Table 11, Table 12, Table 15, Table 16, Table 17), pp.2410\u20132420: the paper reports systematic experiments showing how ASR/AQ vary with mutator combinations, pre-jailbreak budget, prompt length, defenses, and cross-model transferability (statistical failure patterns) but contains no analysis of model internals (neurons, coverage) or explicit attribution of failures to dataset bias/overfitting."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight into why its jailbreak attacks succeed by analyzing the patterns and characteristics of the generated prompts. The authors correlate successful attacks with specific input properties, such as high semantic coherence (low perplexity) and the use of complex, contextual structures (e.g., role-playing scenarios). This analysis of failure patterns explains how the method bypasses specific defenses. For example, success against perplexity filters is attributed to maintaining semantic coherence, while success against Llama Guard is linked to the ability to craft complex prompts that the defense misclassifies. The paper also provides a high-level diagnosis of the model's vulnerability, suggesting in its conclusion a \"critical limitation in aligned LLMs\" where safety training fails against prompts that do not contain explicitly malicious cues. However, it does not analyze observable model internals, as it is a black-box approach, nor does it deeply investigate underlying vulnerabilities like dataset bias or overfitting, which prevents it from meeting the 'High' criteria.",
                    "evidence": "Page 15, Section 8, \"Conclusion and Future Work\": \"Finally, our findings reveal a critical limitation in aligned LLMs: they are effective in generating jailbreaking templates that do not necessarily exhibit explicit malicious intent. This raises significant safety concerns, indicating the need for further research on alignment strategies to mitigate such behaviors.\"\n\nPage 13, Section 6.1, \"Perplexity Filter\": \"The potential reason is that the jailbreak prompts generated by PAPILLON maintain semantic coherence with low perplexity. This allows these jailbreak prompts to bypass the perplexity-based detection mechanism by closely mimicking regular, benign prompts, thus evading detection.\"\n\nPage 14, Section 6.3, \"Llama Guard\": \"This is attributed to our ability to craft more complex and mutated prompts, which Llama Guard misclassifies as \u201csafe,\u201d enabling us to bypass the safeguard.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides substantial statistical analysis of failure patterns (e.g., attack success rate changes across prompt length, mutator variants, pre-jailbreak budgets, judges, defenses, and cross-model transferability) and links some failure modes to observable input properties (notably prompt perplexity and semantic coherence) that enable evasion of detection. However, it does not analyze internal model signals (neuron activations, coverage, logits, layer behavior) nor does it explicitly link failures to deeper underlying model vulnerabilities such as dataset bias or overfitting. Therefore the paper meets the 'Medium' level: it offers statistical/failure-pattern diagnostics but not analysis of model internals or causes like spurious features or training data issues.",
                    "evidence": "Section 4.2 (Maintain semantic coherence), p.2408\u20132409: \"Experiments show that the average perplexity of our generated jailbreak prompts for GPT-3.5 Turbo is as low as 34.61, significantly below the average perplexity threshold of 58.83.\"; Section 6.1 (Perplexity Filter), p.2411\u20132412: \"As shown in Table 16 (appendix), when applying the perplexity filter [1] to PAPILLON, we observe that PAPILLON maintains a high attack success rate. The ASR drop of PAPILLON is less than 10% in nearly all cases. The potential reason is that the jailbreak prompts generated by PAPILLON maintain semantic coherence with low perplexity. This allows these jailbreak prompts to bypass the perplexity-based detection mechanism by closely mimicking regular, benign prompts, thus evading detection.\"; Section 5.3 (Ablation Studies) and Tables (e.g., Table 11, Table 12, Table 15, Table 16, Table 17), pp.2410\u20132420: the paper reports systematic experiments showing how ASR/AQ vary with mutator combinations, pre-jailbreak budget, prompt length, defenses, and cross-model transferability (statistical failure patterns) but contains no analysis of model internals (neurons, coverage) or explicit attribution of failures to dataset bias/overfitting."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Provides rationale that semantic coherence of the input causes the failure to bypass perplexity based defense but not how the failures are related to model weakness.",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly designs mutation operators and optimization constraints to preserve semantic coherence and limit token-length during every mutation step (using an LLM helper and length hyperparameters), and it provides empirical evidence that final fault-inducing prompts remain realistic: quantitative perplexity measurements (low average perplexity) and a human evaluation (interpretability/naturalness scores and alignment checks) demonstrating that generated prompts are natural and align with human judgments. These elements show both (1) mechanisms that enforce plausibility during fuzzing and (2) empirical validation that the final inputs are perceptually/semantically realistic.",
                    "evidence": "Section 4.2 (Template Mutation and Optimization), p.2408: \"In this paper, we propose using GPT-driven mutation operators to create jailbreaking templates, allowing us to preserve the semantic integrity of the generated templates throughout the mutation process.\"; Section 4.2 (Optimization schemes), p.2408\u20132409: \"Ensuring that the generated adversarial prompts are readable (low perplexity) is crucial... Experiments show that the average perplexity of our generated jailbreak prompts for GPT-3.5 Turbo is as low as 34.61, significantly below the average perplexity threshold of 58.83.\"; Section 4.2 (Optimization schemes), p.2409: \"By customizing the length of these templates (with a default setting of 200 tokens) during the mutation process, we can maintain a high success rate even with shorter prompts when setting system instructions.\"; Section 7 (Human Evaluation), p.2412\u20132413: \"A total of 30 volunteers participated... For interpretability... Naturalness... The results are summarized in Table 9. We observed that our generated jailbreak prompts match or surpass manually crafted ones in interpretability and naturalness.\"; Section 7 (Human Evaluation), p.2412: \"We observed that for a score of 8 or higher, all outputs were consistently aligned with human evaluation, achieving a 100% alignment rate. This indicates that a score of 8 serves as an appropriate threshold...\"."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, PAPILLON, is explicitly designed to maintain the plausibility and semantic coherence of its generated inputs (jailbreak prompts) throughout the fuzzing process. The mutation strategies leverage an LLM helper specifically to generate natural and fluent prompts, directly addressing the limitations of prior work that produced 'gibberish' or 'unintelligible' outputs. The paper provides strong empirical evidence for the plausibility of the final, fault-inducing prompts through two distinct mechanisms: 1) a quantitative assessment using perplexity, a standard measure for linguistic fluency, demonstrating that their prompts have low perplexity scores, and 2) a dedicated human evaluation study that measures the 'interpretability' and 'naturalness' of the generated prompts, finding them to be superior to manually crafted ones.",
                    "evidence": "Page 10, Section 4.2, 'Maintain semantic coherence': 'Ensuring that the generated adversarial prompts are readable (low perplexity) is crucial for evading existing jailbreak defenses based on perplexity filters. In this paper, we propose using GPT-driven mutation operators to create jailbreaking templates, allowing us to preserve the semantic integrity of the generated templates throughout the mutation process. Experiments show that the average perplexity of our generated jailbreak prompts for GPT-3.5 Turbo is as low as 34.61, significantly below the average perplexity threshold of 58.83.'\n\nPage 14, Section 7, 'Human Evaluation': 'Prompts quality. To evaluate the interpretability and naturalness of the generated prompts, we developed two criteria for quantifiable human evaluation. For interpretability, we measure how easily humans can understand the mechanism behind the jailbreak prompt... Naturalness is assessed based on the readability of the prompt... We observed that our generated jailbreak prompts match or surpass manually crafted ones in interpretability and naturalness.' Table 9 on page 15 provides the quantitative results of this human study, showing PAPILLON's prompts achieved higher scores for both interpretability (4.64 vs 3.98) and naturalness (4.60 vs 3.66) compared to a baseline of manually crafted prompts."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly designs mutation operators and optimization constraints to preserve semantic coherence and limit token-length during every mutation step (using an LLM helper and length hyperparameters), and it provides empirical evidence that final fault-inducing prompts remain realistic: quantitative perplexity measurements (low average perplexity) and a human evaluation (interpretability/naturalness scores and alignment checks) demonstrating that generated prompts are natural and align with human judgments. These elements show both (1) mechanisms that enforce plausibility during fuzzing and (2) empirical validation that the final inputs are perceptually/semantically realistic.",
                    "evidence": "Section 4.2 (Template Mutation and Optimization), p.2408: \"In this paper, we propose using GPT-driven mutation operators to create jailbreaking templates, allowing us to preserve the semantic integrity of the generated templates throughout the mutation process.\"; Section 4.2 (Optimization schemes), p.2408\u20132409: \"Ensuring that the generated adversarial prompts are readable (low perplexity) is crucial... Experiments show that the average perplexity of our generated jailbreak prompts for GPT-3.5 Turbo is as low as 34.61, significantly below the average perplexity threshold of 58.83.\"; Section 4.2 (Optimization schemes), p.2409: \"By customizing the length of these templates (with a default setting of 200 tokens) during the mutation process, we can maintain a high success rate even with shorter prompts when setting system instructions.\"; Section 7 (Human Evaluation), p.2412\u20132413: \"A total of 30 volunteers participated... For interpretability... Naturalness... The results are summarized in Table 9. We observed that our generated jailbreak prompts match or surpass manually crafted ones in interpretability and naturalness.\"; Section 7 (Human Evaluation), p.2412: \"We observed that for a score of 8 or higher, all outputs were consistently aligned with human evaluation, achieving a 100% alignment rate. This indicates that a score of 8 serves as an appropriate threshold...\"."
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "PAPILLON generates final test inputs as textual jailbreak prompts (serialized text) produced by its mutators and sent directly to target LLMs; textual prompts are inherently stable to typical numeric I/O effects (quantization/rounding) described in the metric. The paper explicitly defines the 'jailbreak prompt' as the final input string, describes using an LLM mutator to produce these templates (controlling token length via max_new_tokens), and treats those generated texts as the inputs used for both execution and judging. Because PAPILLON produces inputs directly in serialized text form (and controls prompt/token length), the fault-inducing inputs it discovers are reproducible under standard I/O operations per the metric\u2019s allowance for serialized-format inputs.",
                    "evidence": "Page 2407, Section 4.1: \"10The jailbreak prompt is the final input to the LLM, created by replacing the placeholders (denotes as [INSERT PROMPT HERE]) in the jailbreaking template with the harmful question.\"; Page 2408, Section 4.2: \"In PAPILLON, we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.\"; Page 2410, Section 5.1 (Experimental Setup): \"The length of the generated prompts is controlled using the max_new_tokens hyperparameter. Specifically, for the \u2018role-play' and \u2018contextualization' mutations, this value is set to 200 tokens, while an additional 100 tokens are allocated for the \u2018expand' mutation.\"; Page 2411, Section 5.3 (Attack Transferability - MLLM transferability): \"successful jailbreaking templates are printed on white-background images and used as image inputs...\" (shows the authors generate serialized text prompts as the primary inputs; when converted to images for MLLMs they do not claim special handling of image quantization, indicating the core method relies on serialized text)."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper proposes PAPILLON, a fuzzing framework where the inputs are text prompts for Large Language Models (LLMs). The mutation process uses a helper LLM to generate new jailbreaking prompts (text templates) based on strategies like 'Role-play', 'Contextualization', and 'Expand'. The metric definition explicitly states: 'We also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., synthesized speech, rendered images, or generated text) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.' Since PAPILLON's mutation operators generate text prompts directly in their final, serialized format, the resulting test cases are not susceptible to alteration or loss of the fault-inducing perturbation through standard I/O operations like rounding or quantization, which typically affect numerical data. The generated text inputs are inherently reproducible, thus meeting the criteria for a 'High' rating.",
                    "evidence": "Page 9, Section 4.2, Template Mutation and Optimization: 'PAPILLON uses a LLM (e.g., GPT-3.5 turbo), as a mutator to automatically create jailbreaking templates for specific questions.' This is further detailed with descriptions of the mutators: 'Role-play. It involves assigning the LLM a virtual role, such as a writer, screenwriter, gang member, or expert. The LLM is guided by the target question to generate a scenario where the question can be addressed within the scene.' This shows that the mutation process directly yields text, which is an inherently serialized format, ensuring failure reproducibility."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "PAPILLON generates final test inputs as textual jailbreak prompts (serialized text) produced by its mutators and sent directly to target LLMs; textual prompts are inherently stable to typical numeric I/O effects (quantization/rounding) described in the metric. The paper explicitly defines the 'jailbreak prompt' as the final input string, describes using an LLM mutator to produce these templates (controlling token length via max_new_tokens), and treats those generated texts as the inputs used for both execution and judging. Because PAPILLON produces inputs directly in serialized text form (and controls prompt/token length), the fault-inducing inputs it discovers are reproducible under standard I/O operations per the metric\u2019s allowance for serialized-format inputs.",
                    "evidence": "Page 2407, Section 4.1: \"10The jailbreak prompt is the final input to the LLM, created by replacing the placeholders (denotes as [INSERT PROMPT HERE]) in the jailbreaking template with the harmful question.\"; Page 2408, Section 4.2: \"In PAPILLON, we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.\"; Page 2410, Section 5.1 (Experimental Setup): \"The length of the generated prompts is controlled using the max_new_tokens hyperparameter. Specifically, for the \u2018role-play' and \u2018contextualization' mutations, this value is set to 200 tokens, while an additional 100 tokens are allocated for the \u2018expand' mutation.\"; Page 2411, Section 5.3 (Attack Transferability - MLLM transferability): \"successful jailbreaking templates are printed on white-background images and used as image inputs...\" (shows the authors generate serialized text prompts as the primary inputs; when converted to images for MLLMs they do not claim special handling of image quantization, indicating the core method relies on serialized text)."
                },
                "manual": {
                    "value": "High",
                    "why": "Mutate in text level which are inherently stable under IO operations",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly evaluates and demonstrates that fault-inducing prompts generated by PAPILLON on one model can be reused to successfully attack other models (cross-model and multimodal transfer experiments are reported). However, the PAPILLON design and mutation operators are described with goals of semantic coherence, short prompt length, and seed-pool/fuzzing effectiveness rather than with explicit mechanisms specifically intended to discover or maximize transferability across different model implementations (no surrogate-model training, ensemble surrogate testing, or transfer-focused mutators are described). Therefore the work shows reuse/testing of fault-inducing inputs for transferability but does not present an explicit design aimed at producing transferable attacks.",
                    "evidence": "Page 2401, Abstract: \"Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses.\"; Page 2412, Section 5.4 (Attack Transferability): \"We further investigate the transferability of PAPILLON. Transferability refers to the extent to which prompts designed to jailbreak the target model can successfully jailbreak another model.\" and \"Overall, PAPILLON demonstrates notable attack transferability across different models.\"; Page 2420, Table 17 (Cross-model transferability): shows the cross-model success-rate matrix (e.g., \"Baichuan2-7B -> Vicuna-7B-v1.3: 70%\", many non-zero cross-model ASR entries); Page 2407, Section 4.2 (Template Mutation and Optimization): \"we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.\" (describes focus on semantic coherence and prompt length rather than explicit transferability mechanisms)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper extensively evaluates the transferability of its generated jailbreak prompts. It includes a specific section (5.4 Attack Transferability) and tables (Table 5 and Table 17) demonstrating that prompts created for a source model can successfully trigger failures in other target models, including different LLMs and even multimodal models (MLLMs). This confirms that fault-inducing inputs are reused to test transferability. However, the paper's methodological description of the PAPILLON framework does not mention any explicit design choices or mechanisms specifically aimed at discovering or enhancing transferable attacks. The design goals of the mutation operators focus on maintaining semantic coherence and shortening prompt length to evade defenses and reduce costs, not on cross-model generalization. Therefore, while transferability is a demonstrated outcome, it is not an explicit goal of the fuzzing design itself.",
                    "evidence": "Page 13, Section 5.4 Attack Transferability: \"We further investigate the transferability of PAPILLON. Transferability refers to the extent to which prompts designed to jailbreak the target model can successfully jailbreak another model. ... LLM transferability. We first evaluate transferability by using jailbreak prompts generated by the source model to target another LLM. The results are shown in Table 17 (appendix). ... MLLM transferability. We then assess the transferability of the PAPILLON-generated jailbreak prompts to multimodal large language models (MLLMs). ... The results are presented in Table 5.\"\n\nPage 9, Section 4.2 Template Mutation and Optimization: \"Optimization schemes. When optimizing the jailbreaking templates, we control both semantic coherence and prompt length through system prompt instructions and the hyperparameter of the output length respectively.\" This section details the design goals of the attack generation, which do not include transferability."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper explicitly evaluates and demonstrates that fault-inducing prompts generated by PAPILLON on one model can be reused to successfully attack other models (cross-model and multimodal transfer experiments are reported). However, the PAPILLON design and mutation operators are described with goals of semantic coherence, short prompt length, and seed-pool/fuzzing effectiveness rather than with explicit mechanisms specifically intended to discover or maximize transferability across different model implementations (no surrogate-model training, ensemble surrogate testing, or transfer-focused mutators are described). Therefore the work shows reuse/testing of fault-inducing inputs for transferability but does not present an explicit design aimed at producing transferable attacks.",
                    "evidence": "Page 2401, Abstract: \"Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses.\"; Page 2412, Section 5.4 (Attack Transferability): \"We further investigate the transferability of PAPILLON. Transferability refers to the extent to which prompts designed to jailbreak the target model can successfully jailbreak another model.\" and \"Overall, PAPILLON demonstrates notable attack transferability across different models.\"; Page 2420, Table 17 (Cross-model transferability): shows the cross-model success-rate matrix (e.g., \"Baichuan2-7B -> Vicuna-7B-v1.3: 70%\", many non-zero cross-model ASR entries); Page 2407, Section 4.2 (Template Mutation and Optimization): \"we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.\" (describes focus on semantic coherence and prompt length rather than explicit transferability mechanisms)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Performs experiments on transferability",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ]
        }
    },
    "Du et al. - 2018 - DeepCruiser Automated Guided Testing for Stateful Deep Learning Systems": {
        "key": "du2018deepcruiser",
        "name": "DeepCruiser",
        "year": "2018",
        "citation_count": "45",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The failures DeepCruiser uncovers are transcription/misprediction errors in ASR (measured as increased WER/CER) produced by metamorphic audio mutations and coverage-guided generation. The paper's evaluation and research questions focus on increasing coverage and triggering erroneous transcriptions (higher WER). There is no presentation or evaluation of bypassing explicit safety, policy, or security mechanisms, nor any examples of inducing high-severity security behaviors (e.g., jailbreaks, data leakage, unauthorized actions, or toxic/harmful outputs). Therefore the failures reported are robustness errors (mispredictions) rather than high- or medium-severity safety/security violations.",
                    "evidence": "Page 13, Section 6.2 Coverage-Guided Testing: \"If the prediction result is inconsistent with the original seed, a' will be added into the failed tests. For audio in ASR, we decide a failed test by checking whether the Word/Character Error Rate (W/CER) exceeds a certain threshold.\"; Page 16, Section 7.3 (RQ2): \"We sample 100 audio from the test data whose Word Error Rates (WERs) are 0. The WER of the audio represents the error rate for the transcripts from RNN. ... Based on the 100 audio (seeds), we randomly generate 10,000 audio (with different WER) by metamorphic transformations.\"; Page 19, Section 7.5 (Answer to RQ4): \"Answer to RQ4: DeepCruiser can effectively generate tests to trigger erroneous behaviors of the RNN. For more fine-grained abstract model, it will capture more erroneous behaviors (i.e., higher WER) of the RNN.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The method, DeepCruiser, is designed to uncover model robustness errors in Automated Speech Recognition (ASR) systems. It operates by applying semantically preserving 'metamorphic transformations' to audio inputs (e.g., adding white noise, changing pitch, altering speed). A failure, referred to as a 'defect', is identified when the ASR model produces an incorrect transcription for the mutated audio, which is quantified by the Word/Character Error Rate (W/CER). This process directly corresponds to the definition of uncovering 'mispredictions or prediction inconsistencies under semantically preserving input mutations'. The paper's scope and evaluation are confined to these transcription errors and do not extend to exposing unsafe behaviors (like toxic or biased content) or bypassing explicit safety or security mechanisms.",
                    "evidence": "Page 13, Section 6.1: 'For defect detection, our goal is to generate audios which sounds normal to human but are incorrectly transcribed by ASRs.'\nPage 14, Section 6.2: 'If the prediction result is inconsistent with the original seed, a\u2032 will be added into the failed tests. For audio in ASR, we decide a failed test by checking whether the Word/Character Error Rate (W/CER) exceeds a certain threshold.'\nPage 18, Section 7.5: 'To answer the question, we measure the WER of the generated audio in Fig. 8. WER represents the erroneous degree of the RNN prediction.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The failures DeepCruiser uncovers are transcription/misprediction errors in ASR (measured as increased WER/CER) produced by metamorphic audio mutations and coverage-guided generation. The paper's evaluation and research questions focus on increasing coverage and triggering erroneous transcriptions (higher WER). There is no presentation or evaluation of bypassing explicit safety, policy, or security mechanisms, nor any examples of inducing high-severity security behaviors (e.g., jailbreaks, data leakage, unauthorized actions, or toxic/harmful outputs). Therefore the failures reported are robustness errors (mispredictions) rather than high- or medium-severity safety/security violations.",
                    "evidence": "Page 13, Section 6.2 Coverage-Guided Testing: \"If the prediction result is inconsistent with the original seed, a' will be added into the failed tests. For audio in ASR, we decide a failed test by checking whether the Word/Character Error Rate (W/CER) exceeds a certain threshold.\"; Page 16, Section 7.3 (RQ2): \"We sample 100 audio from the test data whose Word Error Rates (WERs) are 0. The WER of the audio represents the error rate for the transcripts from RNN. ... Based on the 100 audio (seeds), we randomly generate 10,000 audio (with different WER) by metamorphic transformations.\"; Page 19, Section 7.5 (Answer to RQ4): \"Answer to RQ4: DeepCruiser can effectively generate tests to trigger erroneous behaviors of the RNN. For more fine-grained abstract model, it will capture more erroneous behaviors (i.e., higher WER) of the RNN.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "DeepCruiser performs coverage-guided, metamorphic mutation of audio seeds and uses a WER/CER-based oracle to detect inputs whose transcriptions differ from expectations. The framework is designed to find generic mis-transcriptions and to increase coverage of RNN internal states, not to drive the model toward attacker-chosen, specific target outputs or phrases. The mutation, exploration and oracle components all focus on preserving original semantics (metamorphic relations) and flagging deviations (high WER), which corresponds to untargeted exploration that reveals generic faults rather than targeted attacks.",
                    "evidence": "Page 1, Abstract: \"we propose an automated testing framework, DeepCruiser, which systematically generates tests in large scale to uncover defects of stateful DL systems with coverage guidance.\" (Abstract) \n\nPage 12, Section 6.1 (Metamorphic Transformations): \"To generate suitable defect-triggering candidates, we apply the transformations with restraints to ensure that the audio seeds and the corresponding mutants sound the same to human beings... Transformation operators satisfying the above requirements are said to keep a metamorphic relation...\" (Section 6.1)\n\nPage 13, Section 6.2 (Coverage-Guided Testing / Algorithm 1): \"If the prediction result is inconsistent with the original seed, a' will be added into the failed tests. For audio in ASR, we decide a failed test by checking whether the Word/Character Error Rate (W/CER) exceeds a certain threshold.\" (Section 6.2, Algorithm 1 and explanation)\n\nThese quotes show the approach: (1) mutations are constrained to preserve original semantics (so not crafted to force a particular attacker-chosen output), and (2) the oracle flags transcription differences (WER/CER), indicating untargeted discovery of mis-transcriptions rather than steering to predefined target outputs."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, DeepCruiser, performs untargeted exploration to discover generic failures in Recurrent Neural Network (RNN) based systems, specifically Automated Speech Recognition (ASR). The testing objective is not to guide the model towards a specific, attacker-chosen output (like a particular phrase). Instead, the system's oracle identifies a failure when a mutated input leads to a prediction that is merely inconsistent with the original input's prediction. For ASR, this inconsistency is quantified by the Word/Character Error Rate (W/CER) exceeding a predefined threshold. The goal is to find *any* input that causes a high transcription error, which constitutes a generic failure, rather than achieving a targeted attack.",
                    "evidence": "Page 14, Section 6.2, Coverage-Guided Testing: \"If the prediction result is inconsistent with the original seed, a' will be added into the failed tests. For audio in ASR, we decide a failed test by checking whether the Word/Character Error Rate (W/CER) exceeds a certain threshold.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DeepCruiser performs coverage-guided, metamorphic mutation of audio seeds and uses a WER/CER-based oracle to detect inputs whose transcriptions differ from expectations. The framework is designed to find generic mis-transcriptions and to increase coverage of RNN internal states, not to drive the model toward attacker-chosen, specific target outputs or phrases. The mutation, exploration and oracle components all focus on preserving original semantics (metamorphic relations) and flagging deviations (high WER), which corresponds to untargeted exploration that reveals generic faults rather than targeted attacks.",
                    "evidence": "Page 1, Abstract: \"we propose an automated testing framework, DeepCruiser, which systematically generates tests in large scale to uncover defects of stateful DL systems with coverage guidance.\" (Abstract) \n\nPage 12, Section 6.1 (Metamorphic Transformations): \"To generate suitable defect-triggering candidates, we apply the transformations with restraints to ensure that the audio seeds and the corresponding mutants sound the same to human beings... Transformation operators satisfying the above requirements are said to keep a metamorphic relation...\" (Section 6.1)\n\nPage 13, Section 6.2 (Coverage-Guided Testing / Algorithm 1): \"If the prediction result is inconsistent with the original seed, a' will be added into the failed tests. For audio in ASR, we decide a failed test by checking whether the Word/Character Error Rate (W/CER) exceeds a certain threshold.\" (Section 6.2, Algorithm 1 and explanation)\n\nThese quotes show the approach: (1) mutations are constrained to preserve original semantics (so not crafted to force a particular attacker-chosen output), and (2) the oracle flags transcription differences (WER/CER), indicating untargeted discovery of mis-transcriptions rather than steering to predefined target outputs."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight based on observable model internals and statistical failure patterns: it constructs an abstract state-transition model (PCA-based state abstraction + MDP transition probabilities), defines multiple coverage metrics (state- and transition-level), and empirically analyzes correlations between state-coverage / state-variation (Jaccard Index) and erroneous behavior (WER). These analyses give useful diagnostic signals (e.g., which abstract states/transitions/test transformations correlate with higher WER) and show that increasing coverage exposes more errors. However, the paper does not perform root-cause or vulnerability attribution that explains why failures occur in terms of underlying model vulnerabilities (e.g., reliance on non-robust or spurious features, dataset bias, or overfitting). Therefore the work goes beyond mere reporting of failures but stops short of explanatory diagnostic analysis linking failures to underlying causes, matching the definition of Medium.",
                    "evidence": "Page 6, Section 3 (Overview): \"The abstract model construction module takes a trained RNN as input and analyzes its internal runtime behaviors through profiling.\"; Page 8, Section 4.2 (State Abstraction): \"we first apply the Principle Component Analysis (PCA) ... keeps the first k dominant components. For each of the k dimensions, we further partition them into m equal intervals.\"; Page 11, Section 5 (Coverage Criteria): definitions of state- and transition-level criteria including BSCOV and k-SBCOV and transition/input/weighted input coverage (formulas and descriptions) show explicit internal-state and transition-based diagnostic signals; Page 16-17, Section 7.3/7.4 (RQ2 Answer): \"Answer to RQ 2: There is a strong correlation between the state variation (i.e., the Jaccard Index) and erroneous behaviors of the RNN (i.e., WER). The transformed audio is likely to trigger more erroneous behaviors if it covers more different states comparing with the original audio.\"; Page 19, Section 7.5 (RQ4 Answer / Evaluation): empirical results reporting that finer-grained abstractions (higher coverage) produce higher average WER (Fig.8) and Table 3 shows coverage increases with DeepCruiser. These passages demonstrate analysis of model internals and statistical failure patterns but contain no statements attributing discovered failures to deeper causes such as spurious features, dataset bias, or overfitting."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by analyzing the correlation between observable model internals (abstract state coverage) and discovered failures (erroneous behaviors). The evaluation in Research Question 2 (RQ2) explicitly investigates and confirms a strong correlation between state variation and the Word Error Rate (WER). The authors show that test inputs which cover more different internal states compared to the original input (i.e., have a lower Jaccard index) are more likely to trigger erroneous behaviors (i.e., have a higher WER). This constitutes a statistical analysis of failure patterns linked to an internal model metric, which fits the 'Medium' criteria. The analysis does not, however, go further to explain the failures in terms of underlying model vulnerabilities like dataset bias or overfitting, which would be required for a 'High' rating.",
                    "evidence": "Page 18, Answer to RQ 2: \"There is a strong correlation between the state variation (i.e., the Jaccard Index) and erroneous behaviors of the RNN (i.e., WER). The transformed audio is likely to trigger more erroneous behaviors if it covers more different states comparing with the original audio. By improving BSC coverage, more states are covered and more erroneous behaviors would potentially be captured.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight based on observable model internals and statistical failure patterns: it constructs an abstract state-transition model (PCA-based state abstraction + MDP transition probabilities), defines multiple coverage metrics (state- and transition-level), and empirically analyzes correlations between state-coverage / state-variation (Jaccard Index) and erroneous behavior (WER). These analyses give useful diagnostic signals (e.g., which abstract states/transitions/test transformations correlate with higher WER) and show that increasing coverage exposes more errors. However, the paper does not perform root-cause or vulnerability attribution that explains why failures occur in terms of underlying model vulnerabilities (e.g., reliance on non-robust or spurious features, dataset bias, or overfitting). Therefore the work goes beyond mere reporting of failures but stops short of explanatory diagnostic analysis linking failures to underlying causes, matching the definition of Medium.",
                    "evidence": "Page 6, Section 3 (Overview): \"The abstract model construction module takes a trained RNN as input and analyzes its internal runtime behaviors through profiling.\"; Page 8, Section 4.2 (State Abstraction): \"we first apply the Principle Component Analysis (PCA) ... keeps the first k dominant components. For each of the k dimensions, we further partition them into m equal intervals.\"; Page 11, Section 5 (Coverage Criteria): definitions of state- and transition-level criteria including BSCOV and k-SBCOV and transition/input/weighted input coverage (formulas and descriptions) show explicit internal-state and transition-based diagnostic signals; Page 16-17, Section 7.3/7.4 (RQ2 Answer): \"Answer to RQ 2: There is a strong correlation between the state variation (i.e., the Jaccard Index) and erroneous behaviors of the RNN (i.e., WER). The transformed audio is likely to trigger more erroneous behaviors if it covers more different states comparing with the original audio.\"; Page 19, Section 7.5 (RQ4 Answer / Evaluation): empirical results reporting that finer-grained abstractions (higher coverage) produce higher average WER (Fig.8) and Table 3 shows coverage increases with DeepCruiser. These passages demonstrate analysis of model internals and statistical failure patterns but contain no statements attributing discovered failures to deeper causes such as spurious features, dataset bias, or overfitting."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Provides heuristic explanations by showing correlation with state variation",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly designs and applies bounded, rule-based metamorphic transformations for audio (Section 6.1) and enforces constraints to limit accumulation of major distortions across mutation steps (e.g., altering volume, speed or clearness at most once). This shows the authors attempt to enforce input plausibility during mutation. However, they do not provide a systematic empirical assessment (human study or task-specific perceptual/fluency metrics such as PESQ/STOI/Perplexity/LPIPS/SSIM) to demonstrate that the final fault-inducing inputs remain perceptually/semantically realistic. The only explicit plausibility validation reported is a manual check of selected (100) high-quality seed inputs to mitigate false positives (Threats to Validity), not a broader or quantitative evaluation of final mutants. Therefore the paper enforces plausibility constraints in mutation design but lacks comprehensive validation of plausibility for produced failing inputs, matching the 'Medium' level.",
                    "evidence": "Section 6.1 (Metamorphic Transformations of Audio Signals), page 13: \"To generate suitable defect-triggering candidates, we apply the transformations with restraints to ensure that the audio seeds and the corresponding mutants sound the same to human beings.\"; Section 6.1 (same), page 13: \"(1) we carefully set the parameters to ensure that a single step transformation (on volume, speed or clearness) does not violate the metamorphic relations ... and (2) an audio, generated by mutating from a seed after a sequence of transformations T, is limited to be mutated by a transformation t \u2208 S \u222a UAT such that ({t} \u222a S) \u2229 T = \u2205, where S \u2208 {VRT, SRT, CRT}. Intuitively, the constraints make sure that a mutant is generated by altering the volume, speed or clearness of a seed input at most once. If this constraints is unsatisfied, the audio will not be transformed further.\"; Section 7.6 (Threats to Validity), page 19: \"We adopt a conservative metamorphic transformation strategy to make small changes on the original audio such that DeepCruiser will generate realistic audio. However, it may still cause false positives ... To mitigate this problem, we manually check and make sure the selected inputs (i.e., the 100 audio in RQ1, RQ2, RQ3 and RQ4) are of high quality.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility through rule-based constraints on its mutation operators, which it terms \"metamorphic transformations.\" The stated goal is to generate audio that \"sounds normal to human.\" Plausibility is enforced in two ways: 1) by carefully selecting parameters for individual transformations to ensure they do not perceptibly alter the audio, and 2) by explicitly limiting the cumulative effect of transformations related to volume, speed, or clearness to a single application per seed audio. This second constraint is a direct attempt to prevent the cumulative drift that can lead to unrealistic inputs. However, the paper falls short of the 'High' rating because it does not provide any empirical validation (e.g., a human study or quantitative metrics like PESQ/STOI) to confirm that the final, fault-inducing audio inputs are, in fact, plausible and intelligible. The claims of realism are based on the design of the transformation strategy rather than on an evaluation of its outputs.",
                    "evidence": "Page 14, Section 6.1: \"transformations for generating defect candidates indistinguishable to human before and after the transformations: (1) we carefully set the parameters to ensure that a single step transformation (on volume, speed or clearness) does not violate the metamorphic relations (i.e., human hearing are not affected); and (2) an audio, generated by mutating from a seed after a sequence of transformations T, is limited to be mutated by a transformation t \u2208 S U UAT such that ({t} U S) \u2229 T = \u2205, where S \u2208 {VR, SRT, CRT}. Intuitively, the constraints make sure that a mutant is generated by altering the volume, speed or clearness of a seed input at most once.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper explicitly designs and applies bounded, rule-based metamorphic transformations for audio (Section 6.1) and enforces constraints to limit accumulation of major distortions across mutation steps (e.g., altering volume, speed or clearness at most once). This shows the authors attempt to enforce input plausibility during mutation. However, they do not provide a systematic empirical assessment (human study or task-specific perceptual/fluency metrics such as PESQ/STOI/Perplexity/LPIPS/SSIM) to demonstrate that the final fault-inducing inputs remain perceptually/semantically realistic. The only explicit plausibility validation reported is a manual check of selected (100) high-quality seed inputs to mitigate false positives (Threats to Validity), not a broader or quantitative evaluation of final mutants. Therefore the paper enforces plausibility constraints in mutation design but lacks comprehensive validation of plausibility for produced failing inputs, matching the 'Medium' level.",
                    "evidence": "Section 6.1 (Metamorphic Transformations of Audio Signals), page 13: \"To generate suitable defect-triggering candidates, we apply the transformations with restraints to ensure that the audio seeds and the corresponding mutants sound the same to human beings.\"; Section 6.1 (same), page 13: \"(1) we carefully set the parameters to ensure that a single step transformation (on volume, speed or clearness) does not violate the metamorphic relations ... and (2) an audio, generated by mutating from a seed after a sequence of transformations T, is limited to be mutated by a transformation t \u2208 S \u222a UAT such that ({t} \u222a S) \u2229 T = \u2205, where S \u2208 {VRT, SRT, CRT}. Intuitively, the constraints make sure that a mutant is generated by altering the volume, speed or clearness of a seed input at most once. If this constraints is unsatisfied, the audio will not be transformed further.\"; Section 7.6 (Threats to Validity), page 19: \"We adopt a conservative metamorphic transformation strategy to make small changes on the original audio such that DeepCruiser will generate realistic audio. However, it may still cause false positives ... To mitigate this problem, we manually check and make sure the selected inputs (i.e., the 100 audio in RQ1, RQ2, RQ3 and RQ4) are of high quality.\""
                },
                "manual": {
                    "value": "High",
                    "why": "Ensures each audio is mutated at most once per transformation category (volume/speed/clearness) to prevent unnatural accumulation and peforms human validation on selected fault-inducing inputs",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's testing framework mutates audio waveforms in memory using metamorphic transformations and evaluates failures via WER, but it does not describe any explicit handling of standard I/O serialization effects (e.g., quantization, clipping, rounding when saving to common audio/image formats) nor does it constrain mutations to ensure perturbations persist after serialization. The methodology and oracle (WER) operate on generated audio without mention of writing/reading through typical storage formats or accounting for their numeric effects. The authors also discuss using conservative metamorphic parameters and manual checking to avoid false positives, which indicates they did not incorporate explicit automated serialization-aware measures into generation or oracle evaluation.",
                    "evidence": "Section 6.1 \"Metamorphic Transformations of Audio Signals\" (page 12-13): \"To generate suitable defect-triggering candidates, we apply the transformations with restraints to ensure that the audio seeds and the corresponding mutants sound the same to human beings... we carefully set the parameters to ensure that a single step transformation (on volume, speed or clearness) does not violate the metamorphic relations... the audio will not be transformed further.\"; Section 6.2 Algorithm 1 (page 14): shows mutation and prediction steps (a' = mutate(t,a); cov, result <- Predict(R,a')) but contains no steps for serialization or handling quantization/clipping; Section 7.6 \"Threats to Validity\" (page 19): \"We adopt a conservative metamorphic transformation strategy to make small changes on the original audio such that DeepCruiser will generate realistic audio. However, it may still cause false positives... To mitigate this problem, we manually check...\" \u2014 no mention of accounting for I/O serialization effects (quantization/clipping) during test generation or oracle evaluation."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, DeepCruiser, uses a set of semantic-preserving metamorphic transformations on audio signals (e.g., changing volume, adding noise, altering speed) to generate test inputs. According to the metric definition, using such transformations without explicit handling of I/O effects provides only partial enforcement of failure reproducibility. The paper discusses applying these transformations with 'restraints' to ensure the generated audio is still recognizable by humans, thus preserving the metamorphic relation. However, it makes no mention of accounting for serialization effects like quantization, rounding, or clipping that occur when audio is saved to standard file formats. The test generation and oracle evaluation process described in the paper operates on the mutated audio directly without an intermediate save/load cycle, which could lead to the detection of failures caused by fragile numerical artifacts that would not persist after standard I/O operations. Therefore, the approach fits the 'Medium' category as it applies semantic-preserving transformations without explicitly enforcing reproducibility under serialization.",
                    "evidence": "Page 12, Section 6.1, 'Metamorphic Transformations of Audio Signals': 'Inspired by these practical scenarios, we derive a set of transformation operators to mimic the environment interference. Overall, they can be classified into four categories: \u2013 Volume-related transformations (VRT): ChangeVolume, LowPassFilter, HighPassFilter. \u2013 Speed-related transformations (SRT): PitchShift, ChangeSpeed. \u2013 Clearness-related transformations (CRT): AddWhiteNoise, ExtractHarmonic \u2013 Unaffected transformations (UAT): DRC, Trim.'\nPage 13, Section 6.1: 'To generate suitable defect-triggering candidates, we apply the transformations with restraints to ensure that the audio seeds and the corresponding mutants sound the same to human beings. Transformation operators satisfying the above requirements are said to keep a metamorphic relation [41] and we refer to them as metamorphic transformations.'\nPage 14, Section 6.2, 'Coverage-Guided Testing': 'DeepCruiser will generate a new audio a\u2019 with transformation t and perform the transformation with the deep learning system R. If the prediction result is inconsistent with the original seed, a\u2019 will be added into the failed tests. For audio in ASR, we decide a failed test by checking whether the Word/Character Error Rate (W/CER) exceeds a certain threshold.' This description of the workflow does not include steps for saving and reloading the audio to ensure perturbations survive serialization."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The DeepCruiser framework relies on semantic\u2010preserving, metamorphic transformations of audio inputs (e.g., volume, speed, noise adjustments) which inherently introduce coarse\u2010grained perturbations that are likely robust to standard I/O quantization. However, the paper does not explicitly account for serialization effects (rounding, clipping, quantization) when writing or reading audio in common formats, nor does it constrain mutations to guarantee that fault\u2010inducing perturbations survive such operations. According to the metric definition, approaches using metamorphic transformation without explicit handling of I/O effects receive a \u201cMedium\u201d rating, reflecting partial enforcement of failure reproducibility.",
                    "evidence": "Section 6.1 describes the set of metamorphic transformations on audio (ChangeVolume, LowPassFilter, PitchShift, etc.) applied directly in memory to preserve human\u2010perceived semantics, but makes no mention of an explicit save/load cycle or quantization/clipping safeguards; Section 6.2 (Algorithm 1) shows mutation and oracle evaluation steps operating solely on in\u2010memory audio without serialization."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Apply semantic-preserving metamorphic transformations but does not enforce reproducility account serialization effect (clipping)",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper evaluates DeepCruiser only on a single RNN-based ASR implementation (Mozilla DeepSpeech-0.3.0) and does not report reusing fault-inducing inputs to test other models nor describe design mechanisms explicitly intended to discover transferable attacks across different models. Therefore it does not demonstrate whether faults found on one model transfer to other models performing the same task.",
                    "evidence": "Page 16, Section 7.1 Datasets and Experiment Setup: \"We selected Mozilla\u2019s implementation of DeepSpeech-0.3.0 [42]... All our experiments were conducted on a pre-trained English model released along with DeepSpeech-0.3.0, which was trained with Fisher [43], LibriSpeech [44], Switchboard [45], and a pre-release snapshot of the English Common Voice training corpus [46].\"; Page 3, Introduction / contribution summary: \"We implement our testing framework, DeepCruiser, and perform in-depth evaluation on a state-of-the-art practical RNN based ASR system to demonstrate the usefulness.\" (demonstrates evaluation on a single ASR system and no cross-model transfer tests are reported)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's entire evaluation is conducted on a single, specific deep learning model for Automated Speech Recognition (ASR). The authors explicitly state they use Mozilla's DeepSpeech-0.3.0 model for all their experiments. The goal of the proposed framework, DeepCruiser, is to generate tests that increase coverage and find defects within this specific model. The paper does not mention testing the fault-inducing audio inputs generated for DeepSpeech on any other ASR models to check for transferability. Consequently, it does not demonstrate whether the discovered failures are specific to this one model or represent shared vulnerabilities across different models.",
                    "evidence": "Page 15, Section 7.1 Datasets and Experiment Setup: \"We selected Mozilla's implementation of DeepSpeech-0.3.0 [42] which produces one of the state-of-the-art open source ASR models. All our experiments were conducted on a pre-trained English model released along with DeepSpeech-0.3.0, which was trained with Fisher [43], LibriSpeech [44], Switchboard [45], and a pre-release snapshot of the English Common Voice training corpus [46].\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper evaluates DeepCruiser only on a single RNN-based ASR implementation (Mozilla DeepSpeech-0.3.0) and does not report reusing fault-inducing inputs to test other models nor describe design mechanisms explicitly intended to discover transferable attacks across different models. Therefore it does not demonstrate whether faults found on one model transfer to other models performing the same task.",
                    "evidence": "Page 16, Section 7.1 Datasets and Experiment Setup: \"We selected Mozilla\u2019s implementation of DeepSpeech-0.3.0 [42]... All our experiments were conducted on a pre-trained English model released along with DeepSpeech-0.3.0, which was trained with Fisher [43], LibriSpeech [44], Switchboard [45], and a pre-release snapshot of the English Common Voice training corpus [46].\"; Page 3, Introduction / contribution summary: \"We implement our testing framework, DeepCruiser, and perform in-depth evaluation on a state-of-the-art practical RNN based ASR system to demonstrate the usefulness.\" (demonstrates evaluation on a single ASR system and no cross-model transfer tests are reported)."
                },
                "manual": {
                    "value": "Low",
                    "why": "mutation and coverage strategies are tightly coupled to a specific RNN-based ASR model and training setup",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Yuan et al. - 2023 - Revisiting Neuron Coverage for DNN Testing A Layer-Wise and Distribution-Aware Criterion": {
        "key": "yuan2023revisiting",
        "name": "NLC",
        "year": "2023",
        "citation_count": "36",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper evaluates NLC by using it to find mispredictions and adversarial examples (robustness errors) in standard discriminative and generative models. All reported failures are incorrect model outputs (prediction errors) induced by input mutations or adversarial attacks (e.g., CW, PGD). The paper does not demonstrate uncovering safety- or security-critical violations (such as bypassing explicit safety/policy mechanisms, jailbreaks, data leakage, or unauthorized actions), nor does it evaluate models with explicit defense or safety-alignment mechanisms. Therefore the failures uncovered are limited to low-impact model robustness errors.",
                    "evidence": "Abstract (page 1200): \"Its capacity to discover DNN prediction errors is promising.\" | VI.A, 2) Fault-Revealing Capability of Test Suites (page 1207\u20131208): \"To evaluate the fault-revealing capabilities of test suites, we construct adversarial examples (AE) using two adversarial attack algorithms, Carlini/Wagner (CW) [9] and Project Gradient Descent (PGD) [31]... all algorithms attacking the three models reach over 98% success rates. That is, more than 0.98 \u00d7 (#training data) AEs can trigger erroneous behaviors.\" | VIB (Guiding Input Mutation) / Triggered Faults (page 1209): \"Images mutated under the guidance of NLC (last row) have the highest error triggering rates compared with random mutation (3rd row) and other criteria...\" | Model list (page 1206, Table II): lists ResNet50, VGG16, MobileNet-V2, BigGAN, LSTM (standard discriminative and generative models; no defended/safety-aligned models reported)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper evaluates its proposed method, NLC, on its ability to find 'faults' which are exclusively defined and measured as model robustness errors, specifically mispredictions. The entire evaluation framework for fault-revealing capability is built around generating adversarial examples (AEs) and applying semantically preserving mutations (e.g., blurring, stylizing, rotation) to cause image classifiers to produce incorrect labels. The paper does not discuss or demonstrate the uncovering of unsafe or biased behaviors (Medium severity), nor does it test against models with explicit safety/security mechanisms to show bypasses (High severity). The failures are consistently framed as misclassifications.",
                    "evidence": "Page 9, Section VI-A.2, \"Fault-Revealing Capability of Test Suites\": \"To evaluate the fault-revealing capabilities of test suites, we construct adversarial examples (AE) using two adversarial attack algorithms, Carlini/Wagner (CW) [9] and Project Gradient Descent (PGD) [31]... The incorrect predictions uniformly distribute across all classes.\"\nPage 10, Section VI-B, \"Guiding Input Mutation in DNN Testing\": \"We evaluate the mutated inputs on 1) #triggered faults, 2) naturalness, and 3) diversity of the erroneous behaviors.\" In this context, for image classification models, a 'triggered fault' is a misclassification.\nPage 11, Section VII, \"Diversity of Erroneous Behaviors\": \"A higher diversity of erroneous behaviors indicates uncovering a practically larger vulnerability surface of DNN models. Overall, a collection of fault-triggering images are regarded as more diverse if they cover more classes.\" This confirms that the 'erroneous behaviors' being evaluated are misclassifications into different output classes."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper evaluates NLC by using it to find mispredictions and adversarial examples (robustness errors) in standard discriminative and generative models. All reported failures are incorrect model outputs (prediction errors) induced by input mutations or adversarial attacks (e.g., CW, PGD). The paper does not demonstrate uncovering safety- or security-critical violations (such as bypassing explicit safety/policy mechanisms, jailbreaks, data leakage, or unauthorized actions), nor does it evaluate models with explicit defense or safety-alignment mechanisms. Therefore the failures uncovered are limited to low-impact model robustness errors.",
                    "evidence": "Abstract (page 1200): \"Its capacity to discover DNN prediction errors is promising.\" | VI.A, 2) Fault-Revealing Capability of Test Suites (page 1207\u20131208): \"To evaluate the fault-revealing capabilities of test suites, we construct adversarial examples (AE) using two adversarial attack algorithms, Carlini/Wagner (CW) [9] and Project Gradient Descent (PGD) [31]... all algorithms attacking the three models reach over 98% success rates. That is, more than 0.98 \u00d7 (#training data) AEs can trigger erroneous behaviors.\" | VIB (Guiding Input Mutation) / Triggered Faults (page 1209): \"Images mutated under the guidance of NLC (last row) have the highest error triggering rates compared with random mutation (3rd row) and other criteria...\" | Model list (page 1206, Table II): lists ResNet50, VGG16, MobileNet-V2, BigGAN, LSTM (standard discriminative and generative models; no defended/safety-aligned models reported)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The method (NLC) is designed to measure and drive exploration toward increasing layer-wise distributional coverage (diversity, covariance, density) and the paper's fuzzing procedure uses coverage increase as the objective. The paper's fuzzing algorithm (Alg.1) accepts mutations when they increase coverage (coverage_inc), and the evaluation focuses on finding more and more diverse erroneous behaviors (number/nature/diversity of faults), not on inducing specific attacker-chosen outputs or labels. Although the authors note a class-conditional form of the covariance can be computed, the paper contains no mechanism, objective function, or experimental setup that attempts to steer mutations to produce a predefined target output (e.g., force a model to predict one specific label or generate a chosen phrase). Therefore NLC and the presented fuzzing approach perform untargeted exploration that discovers generic faults (misclassifications, diverse/novel behaviors) rather than targeted attacks.",
                    "evidence": "Abstract (page 1200\u20131201): \"Test input mutation guided by NLC results in a greater quality and diversity of exposed erroneous behaviors.\" \n\nDesign / class-conditional remark (page 1204): \"\u03a3 can be further refined in a class conditional manner by extending the size to c\u00d7m\u00d7m where c is #classes. It thus will be calculated by first being indexed using the class labels.\" \n\nFuzzing algorithm and objective (Alg.1, Sec. VI-B, page 1209): code excerpt \"if is valid(s\u02c6,s) and coverage inc(C,D) then S.add(s\u02c6); update coverage(C);\" \u2014 the acceptance criterion is increase in coverage, not a target-output condition. \n\nEvaluation goals (Sec. VI-B, page 1209): \"We evaluate the mutated inputs on 1) #triggered faults, 2) naturalness, and 3) diversity of the erroneous behaviors.\" \n\nNo targeted-attack objective described: the paper's descriptions of mutation guidance and experiments (Alg.1 and the evaluations in Sec. VI-A and VI-B, pages 1208\u20131210) maximize NLC and measure broad fault discovery and class diversity, but do not define or demonstrate steering toward a predefined attacker-chosen output (e.g., forcing a specific label or phrase)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a coverage criterion, Neural Coverage (NLC), to guide input mutation in DNN testing. The goal of the fuzzing process is to maximize this coverage metric. The evaluation focuses on the fuzzer's ability to discover generic faults, specifically misclassifications. The paper explicitly measures and values the *diversity* of the discovered errors, which is the opposite of a targeted attack. The methodology does not include any mechanism to steer the fuzzer towards a predefined target output or a specific class of security-violating behavior. The exploration is entirely untargeted, aiming to find any input that increases coverage and, consequently, reveals any prediction error.",
                    "evidence": "Page 11, Section VI-B, \"Diversity of Erroneous Behaviors\": \"A higher diversity of erroneous behaviors indicates uncovering a practically larger vulnerability surface of DNN models. Overall, a collection of fault-triggering images are regarded as more diverse if they cover more classes. In case that #covered classes is equal, we further use the scaled entropy... to assess the diversity: a higher entropy is better.\" This shows the objective is to find a wide range of different errors, not a specific, targeted one. Table XI further reinforces this by reporting '#C' (number of classes) and 'E' (scaled entropy) for the discovered faults, where higher values are considered better."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The method (NLC) is designed to measure and drive exploration toward increasing layer-wise distributional coverage (diversity, covariance, density) and the paper's fuzzing procedure uses coverage increase as the objective. The paper's fuzzing algorithm (Alg.1) accepts mutations when they increase coverage (coverage_inc), and the evaluation focuses on finding more and more diverse erroneous behaviors (number/nature/diversity of faults), not on inducing specific attacker-chosen outputs or labels. Although the authors note a class-conditional form of the covariance can be computed, the paper contains no mechanism, objective function, or experimental setup that attempts to steer mutations to produce a predefined target output (e.g., force a model to predict one specific label or generate a chosen phrase). Therefore NLC and the presented fuzzing approach perform untargeted exploration that discovers generic faults (misclassifications, diverse/novel behaviors) rather than targeted attacks.",
                    "evidence": "Abstract (page 1200\u20131201): \"Test input mutation guided by NLC results in a greater quality and diversity of exposed erroneous behaviors.\" \n\nDesign / class-conditional remark (page 1204): \"\u03a3 can be further refined in a class conditional manner by extending the size to c\u00d7m\u00d7m where c is #classes. It thus will be calculated by first being indexed using the class labels.\" \n\nFuzzing algorithm and objective (Alg.1, Sec. VI-B, page 1209): code excerpt \"if is valid(s\u02c6,s) and coverage inc(C,D) then S.add(s\u02c6); update coverage(C);\" \u2014 the acceptance criterion is increase in coverage, not a target-output condition. \n\nEvaluation goals (Sec. VI-B, page 1209): \"We evaluate the mutated inputs on 1) #triggered faults, 2) naturalness, and 3) diversity of the erroneous behaviors.\" \n\nNo targeted-attack objective described: the paper's descriptions of mutation guidance and experiments (Alg.1 and the evaluations in Sec. VI-A and VI-B, pages 1208\u20131210) maximize NLC and measure broad fault discovery and class diversity, but do not define or demonstrate steering toward a predefined attacker-chosen output (e.g., forcing a specific label or phrase)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides substantive diagnostic insight based on observable model internals and statistical failure patterns, but stops short of attributing discovered failures to deeper underlying vulnerabilities (e.g., specific spurious features, dataset bias, or overfitting) in a principled, per-failure manner. Specifically, the authors design NLC around layer-wise distributional signals (variance, covariance, density, shape) and use those internal signals to (a) explain why their criterion responds to certain inputs, (b) correlate coverage with fault-revealing capability, and (c) distinguish types of failures (e.g., adversarial/dissimilarity effects). They also report statistical analyses of failure distributions (class coverage, entropy) and controlled experiments showing which criteria respond to adversarial perturbations vs. genuine fault-revealing inputs. However, the paper does not perform detailed root-cause analyses that link individual failures to concrete model vulnerabilities such as reliance on particular non-robust features, dataset bias, or overfitting (beyond a brief discussion/usage of texture-blur/stylize to probe texture-bias). Therefore the work meets the metric requirement for Medium: it provides diagnostic insight via observable internals and statistical patterns but not full explanatory links to underlying vulnerabilities.",
                    "evidence": "IV. DESIGN OF NEURAL COVERAGE (p.1204): \"NLC captures four key properties of distributions: divergence, correlation, shape, and density, which corresponds to the first four criteria in Table I.\"; Measuring activation/variance (p.1204\u20131205): \"NLC directly measures how divergent (i.e., how active) the neuron output is in its continuous form by calculating the following variance: \u03c3\u00b2_n = E[(o\u2212E[o])(o\u2212E[o])].\"; Characterizing correlation (p.1204\u20131205): \"Similarly, we use the covariance, a common metric for joint variability of two variables, to characterize the correlation between two neurons... \u03a3 = [\u03c3\u00b2_n1  \u03c2_n1,n2;  \u03c2_n2,n1  \u03c3\u00b2_n2].\"; Responding to density change (p.1205): \"We thus state that our criterion can respond to density change which is possibly introduced by corner-case behaviors and new major behaviors.\"; VI-A2 Fault-Revealing (p.1208): \"Table VI shows that NLC is strongly and positively correlated to the fault-revealing capability of a test suite.\"; Correlation to 'Out-of-Bound' (p.1208): \"KMNC has no response to AEs, but NBC and SNAC respond notably, because AEs primarily lead to out-of-range neuron outputs. NLC measures how divergent a layer output is and responds to the density change of layer outputs.\"; Adversarial-control observation (p.1208\u20131209): \"This is an important observation, indicating that NBC/SNAC/DSC do not accurately reflect the fault-revealing capability. Instead, they are sensitive to the 'dissimilarity' induced by adversarial perturbations...\"; Use of transformations to probe a known bias (p.1209, VI-B setup): \"blurring and stylizing (see Fig. 9) reveal the texture-bias (i.e., rely on texture to make decisions) of DNNs [7, 14].\"; Statistical analysis of failure patterns / diversity (p.1209\u20131210): \"Overall, a collection of fault-triggering images are regarded as more diverse if they cover more classes... we further use the scaled entropy... to assess the diversity.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper provides diagnostic analysis that explains why failures occur by linking them to underlying model vulnerabilities. Specifically, in the fuzzing evaluation (Section VI-B), the authors intentionally select mutation operators (blurring, stylizing) that are known to reveal 'texture-bias,' a model's reliance on spurious features rather than semantic shapes. By guiding mutations with their proposed metric (NLC) and analyzing the resulting failures, they connect the discovered errors to this specific vulnerability. Furthermore, the paper analyzes the statistical patterns of the failures by measuring the diversity of erroneous behaviors through the number of affected classes and the scaled entropy of their distribution. This analysis is explicitly framed as a way of 'uncovering a practically larger vulnerability surface of DNN models,' thus connecting specific failures to a broader model weakness.",
                    "evidence": "Page 10, Section VI-B, \"Setup\": \"mutations in Table IX are representative, for example, blurring and stylizing (see Fig. 9) reveal the texture-bias (i.e., rely on texture to make decisions) of DNNs [7, 14].\"\n\nPage 11, Section VI-B, \"Diversity of Erroneous Behaviors\": \"A higher diversity of erroneous behaviors indicates uncovering a practically larger vulnerability surface of DNN models. Overall, a collection of fault-triggering images are regarded as more diverse if they cover more classes. In case that #covered classes is equal, we further use the scaled entropy... to assess the diversity: a higher entropy is better. The results are also presented in Table XI. It is seen that fault-triggering images generated by using NLC as the guidance cover the most classes.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "While the paper offers substantial diagnostic insight\u2014leveraging internal layer\u2010wise signals (variance, covariance, density, shape) to explain coverage behavior, correlating those signals with fault\u2010revealing capability, and presenting statistical analyses of failure patterns (e.g., class\u2013level diversity, entropy)\u2014it does not systematically trace individual failures back to concrete model vulnerabilities such as reliance on specific non\u2010robust or spurious features, dataset bias, or overfitting. The brief use of stylizing and blurring to probe texture bias cites prior work but does not amount to a principled, per\u2010failure root\u2010cause analysis linking failures to underlying vulnerabilities. Therefore, the paper fulfills the \u2018Medium\u2019 level of failure diagnostics: it provides internal and statistical analyses but stops short of full explanatory links to deeper model weaknesses.",
                    "evidence": "Section IV (p. 1204\u20131205): design of NLC via variance, covariance, density and shape of layer outputs; Section VI-A2 (p. 1208): statistical correlation of NLC with fault-revealing capability; Section VI-B Diversity of Erroneous Behaviors (p. 1209): use of class cover and scaled entropy; Section VI-B Setup (p. 1209): mentions blurring/stylizing to reveal texture bias but without detailed root-cause analysis."
                },
                "manual": {
                    "value": "Medium",
                    "why": "shows correlation between coverage and errors.",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints at each mutation step (rule-based validity checks adopted from prior work) and reports quantitative perceptual quality measures (Inception Score and FID) showing that NLC-guided, fault-inducing images remain natural-looking. However, the paper does not describe any explicit mechanism that prevents cumulative drift across many sequential fuzzing iterations (i.e., global/aggregate plausibility guarantees), nor does it report human studies; plausibility is enforced/checked per-step and validated empirically on final outputs via IS/FID only.",
                    "evidence": "Sec. VI-B (Fuzzing setup) / page 1209: \"is_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than \u03b1\u00d7 #pixels or 2) the maximum of changed pixel value is less than \u03b2 \u00d7255. We set \u03b1 to 0.2 and \u03b2 to 0.4.\" (Alg.1 and accompanying text show the per-step validity check is applied before adding a mutated input as a new seed).; Sec. VI-B (Naturalness of Mutations) / pages 1209\u20131210: \"We measure the naturalness of mutated inputs under each criterion; ... We use Inception Score (IS) [39] and Frechet Inception Distance (FID) [18], which are widely used ... Overall, mutated images guided by NLC have the best scores for both IS and FID.\" (Table XI reports IS/FID values for NLC and other criteria).; There is no statement or section in the paper that introduces a global or cumulative-plausibility enforcement across fuzzing iterations (the algorithm checks is_valid(s_hat,s) relative to the immediate parent only, and mutated inputs are then added to the seed pool), implying cumulative drift across many bounded steps is not explicitly prevented nor analyzed."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper satisfies both conditions for a 'High' rating. First, it enforces input plausibility throughout the fuzzing iterations using an explicit validity check. The `is_valid()` function, described in Algorithm 1, constrains mutations by bounding the number of changed pixels and the maximum change in pixel value relative to the original seed input. This prevents the cumulative drift that can lead to unrealistic inputs. Second, the paper provides dedicated empirical evidence that the final fault-inducing inputs are realistic. It uses two standard quantitative, task-specific measures for image naturalness\u2014Inception Score (IS) and Fr\u00e8chet Inception Distance (FID)\u2014to evaluate the outputs of its fuzzing process. The results in Table XI demonstrate that the images generated under the guidance of the proposed NLC criterion have better naturalness scores compared to other methods and a random baseline.",
                    "evidence": "Page 10, Section VI-B, Triggered Faults: \"is_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than \u03b1 \u00d7 #pixels or 2) the maximum of changed pixel value is less than \u03b2 \u00d7 255. We set \u03b1 to 0.2 and \u03b2 to 0.4.\"\nPage 11, Section VI-B, Naturalness of Mutations: \"We measure the naturalness of mutated inputs under each criterion; a good criterion should lead to mutations that mostly preserve the naturalness. We use Inception Score (IS) [39] and Fr\u00e8chet Inception Distance (FID) [18], which are widely used in the AI and SE community to assess the naturalness of images, as the metrics.\"\nPage 11, Table XI: This table provides the results for IS and FID for various fuzzing configurations, with the accompanying text stating, \"Overall, mutated images guided by NLC have the best scores for both IS and FID.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility via per-step, rule-based validity checks (bounding pixel changes in each mutation) and reports quantitative measures (Inception Score and FID) showing that the final fault-inducing images are natural-looking. However, it does not introduce any mechanism to bound or assess cumulative drift over multiple fuzzing iterations, nor does it include a human evaluation of plausibility. Thus it validates plausibility at each mutation and on the final outputs, but overlooks aggregate guarantees across the entire fuzzing process.",
                    "evidence": "Section VI-B (Fuzzing setup) and Algorithm 1 (page 1209): \u201cis_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than \u03b1\u00d7#pixels or 2) the maximum of changed pixel value is less than \u03b2\u00d7255. We set \u03b1 to 0.2 and \u03b2 to 0.4.\u201d\n\nSection VI-B (Naturalness of Mutations) and Table XI (pages 1209\u20131210): \u201cWe measure the naturalness of mutated inputs \u2026 We use Inception Score (IS) [39] and Fr\u00e9chet Inception Distance (FID) [18] \u2026 Overall, mutated images guided by NLC have the best scores for both IS and FID.\u201d\n\nNo section in the paper describes any global or cumulative\u2010plausibility constraint over multiple iterations of fuzzing."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Uses inception score and FID score to measure natualness quality of the final inputs but overlook cumulave effects of mutations across iteration",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's fuzzing/mutation design primarily uses semantic-preserving, metamorphic transformations (e.g., contrast, brightness, affine, blurring, stylizing) which tend to produce coarse, perceptible perturbations that are less likely to be lost by serialization; this aligns with the metric's allowance that metamorphic transformations provide partial enforcement of reproducibility. The authors also enforce a simple pixel-change validity constraint that bounds maximum pixel change relative to an 8-bit scale (\u03b2 \u00d7 255), which is a partial consideration related to clipping/quantization. However, the paper does not explicitly account for standard I/O serialization effects (rounding, quantization, or encoding to formats like PNG/JPEG/WAV) during test case generation or oracle evaluation, nor does it generate inputs directly in serialized formats or describe steps to ensure failures persist after disk serialization. Therefore the approach partially considers serialization-related concerns (via metamorphic transforms and amplitude bounds) but does not explicitly ensure reproducibility under standard I/O operations as required for a High rating.",
                    "evidence": "Sec. VI-B (Guiding Input Mutation in DNN Testing), p.1209: \"mutations in Table IX are representative, for example, blurring and stylizing (see Fig. 9) reveal the texture-bias (i.e., rely on texture to make decisions) of DNNs [7, 14].\" (Table IX lists: Pixel-Level: contrast, brightness; Affine: translation, scaling, rotation; Texture-Level: blurring, stylizing).  Sec. VI-B (Algorithm 1 description), p.1209: \"is_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than \u03b1\u00d7 #pixels or 2) the maximum of changed pixel value is less than \u03b2 \u00d7255. We set \u03b1 to 0.2 and \u03b2 to 0.4.\"  (These show the use of metamorphic transformations and a pixel-value bound relative to 255.)  No sections discuss serialization/encoding/rounding/quantization or saving inputs to disk (no mention of terms such as \"PNG\", \"JPEG\", \"serialize\", \"quantize\", \"rounding\", or \"I/O\") in the design, mutation, or oracle/evaluation descriptions (see Sec. V Implementation and Sec. VI Evaluation, pp.1208\u20131210), indicating the lack of explicit handling of standard I/O serialization effects."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's fuzzing methodology relies on a set of semantic-preserving metamorphic transformations, such as contrast, brightness, rotation, blurring, and stylizing, applied to image inputs. According to the metric definition, using such transformations without explicitly enforcing reproducibility under serialization qualifies for a 'Medium' rating. The paper describes a validity check for mutations (`is_valid()`) that constrains the number of changed pixels and the maximum change in pixel value. However, this check does not explicitly account for I/O effects like rounding or quantization that occur when saving an image to a standard format (e.g., PNG). The check `the maximum of changed pixel value is less than \u03b2 \u00d7 255` constrains the magnitude of the floating-point perturbation but does not guarantee the change will persist after being quantized to an 8-bit integer. The paper does not mention any procedure to save and reload the mutated inputs to verify that the failure is reproducible after standard I/O operations.",
                    "evidence": "Page 10, Section VI-B, Guiding Input Mutation in DNN Testing: 'mutations in Table IX are representative, for example, blurring and stylizing (see Fig. 9) reveal the texture-bias (i.e., rely on texture to make decisions) of DNNs [7, 14].' and 'is_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than \u03b1 \u00d7 #pixels or 2) the maximum of changed pixel value is less than \u03b2 \u00d7 255. We set \u03b1 to 0.2 and \u03b2 to 0.4.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper's fuzzing/mutation design primarily uses semantic-preserving, metamorphic transformations (e.g., contrast, brightness, affine, blurring, stylizing) which tend to produce coarse, perceptible perturbations that are less likely to be lost by serialization; this aligns with the metric's allowance that metamorphic transformations provide partial enforcement of reproducibility. The authors also enforce a simple pixel-change validity constraint that bounds maximum pixel change relative to an 8-bit scale (\u03b2 \u00d7 255), which is a partial consideration related to clipping/quantization. However, the paper does not explicitly account for standard I/O serialization effects (rounding, quantization, or encoding to formats like PNG/JPEG/WAV) during test case generation or oracle evaluation, nor does it generate inputs directly in serialized formats or describe steps to ensure failures persist after disk serialization. Therefore the approach partially considers serialization-related concerns (via metamorphic transforms and amplitude bounds) but does not explicitly ensure reproducibility under standard I/O operations as required for a High rating.",
                    "evidence": "Sec. VI-B (Guiding Input Mutation in DNN Testing), p.1209: \"mutations in Table IX are representative, for example, blurring and stylizing (see Fig. 9) reveal the texture-bias (i.e., rely on texture to make decisions) of DNNs [7, 14].\" (Table IX lists: Pixel-Level: contrast, brightness; Affine: translation, scaling, rotation; Texture-Level: blurring, stylizing).  Sec. VI-B (Algorithm 1 description), p.1209: \"is_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than \u03b1\u00d7 #pixels or 2) the maximum of changed pixel value is less than \u03b2 \u00d7255. We set \u03b1 to 0.2 and \u03b2 to 0.4.\"  (These show the use of metamorphic transformations and a pixel-value bound relative to 255.)  No sections discuss serialization/encoding/rounding/quantization or saving inputs to disk (no mention of terms such as \"PNG\", \"JPEG\", \"serialize\", \"quantize\", \"rounding\", or \"I/O\") in the design, mutation, or oracle/evaluation descriptions (see Sec. V Implementation and Sec. VI Evaluation, pp.1208\u20131210), indicating the lack of explicit handling of standard I/O serialization effects."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Plausibly reproducible due to large semantic changes, but applied in normalized float domain, so some discovered failures could vanish after IO operations.",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models performing the same task, nor does it describe mechanisms intended to discover or encourage transferable attacks. All fuzzing and adversarial-example experiments are conducted per tested model (i.e., inputs are generated/assessed against the same target DNN), and the evaluation reports coverage/faults per model without reusing generated fault-inducing inputs across different model implementations. Thus the work does not address attack transferability.",
                    "evidence": "VI.B (Guiding Input Mutation in DNN Testing), Alg. 1 (page 1209): \"Tested DNN: D; Criterion: C;\" \u2014 the fuzzing algorithm is defined in terms of a single tested DNN. \n\nVI.A.2 (Fault-Revealing Capability of Test Suites) (page 1208): \"AEs are generated using the training data (see 'Clarification' below for results of AEs generated using test data), and all algorithms attacking the three models reach over 98% success rates.\" \u2014 adversarial examples are generated and evaluated against the same model(s) they attack. \n\nVI (Evaluation) and Tables (e.g., Table VI, Table VII) (pages 1206\u20131208): All reported results present coverage and faults per model (ResNet, VGG, MobileNet, BigGAN, etc.), with no experiment or section reporting reuse of fault-inducing inputs across different models or measurements of transferability. (No section title or experiment in the paper addresses transferability or substitute-model attacks.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper focuses on designing a new coverage criterion (NLC) and evaluating its effectiveness for assessing test suites and guiding fuzzing for a single, specific DNN model at a time. The experiments involve generating fault-inducing inputs (either through adversarial attacks or fuzzing) on a given model and then evaluating the outcome on that same model. The paper does not contain any experiments or discussion on whether the fault-inducing inputs generated for one model (e.g., ResNet50) can also trigger failures in other models performing the same task (e.g., VGG16 or MobileNetV2). The fuzzing design and the NLC metric itself are aimed at exploring the internal state of a single model, not at finding shared vulnerabilities across different models.",
                    "evidence": "Page 10, Section VI-B, \"Guiding Input Mutation in DNN Testing\", Algorithm 1: The fuzzing algorithm is defined with a single \"Tested DNN: D\" as input. The core feedback loop in line 7, `if is_valid($,s) and coverage_inc(C,D) then`, clearly shows that the decision to keep a mutated input is based on its ability to increase the coverage metric `C` on that specific DNN `D`. This demonstrates that the fuzzing process is tailored to a single model, not designed for finding transferable inputs. The subsequent results in Table XI also report fault-triggering rates for individual models that were fuzzed, with no cross-evaluation."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models performing the same task, nor does it describe mechanisms intended to discover or encourage transferable attacks. All fuzzing and adversarial-example experiments are conducted per tested model (i.e., inputs are generated/assessed against the same target DNN), and the evaluation reports coverage/faults per model without reusing generated fault-inducing inputs across different model implementations. Thus the work does not address attack transferability.",
                    "evidence": "VI.B (Guiding Input Mutation in DNN Testing), Alg. 1 (page 1209): \"Tested DNN: D; Criterion: C;\" \u2014 the fuzzing algorithm is defined in terms of a single tested DNN. \n\nVI.A.2 (Fault-Revealing Capability of Test Suites) (page 1208): \"AEs are generated using the training data (see 'Clarification' below for results of AEs generated using test data), and all algorithms attacking the three models reach over 98% success rates.\" \u2014 adversarial examples are generated and evaluated against the same model(s) they attack. \n\nVI (Evaluation) and Tables (e.g., Table VI, Table VII) (pages 1206\u20131208): All reported results present coverage and faults per model (ResNet, VGG, MobileNet, BigGAN, etc.), with no experiment or section reporting reuse of fault-inducing inputs across different models or measurements of transferability. (No section title or experiment in the paper addresses transferability or substitute-model attacks.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Xie et al. - 2019 - DeepHunter a coverage-guided fuzz testing framework for deep neural networks": {
        "key": "xie2019deephunter",
        "name": "DeepHunter",
        "year": "2019",
        "citation_count": "518",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper defines and evaluates DeepHunter solely in terms of detecting mispredictions and model inconsistencies under semantically-preserving input mutations (including quantization-induced inconsistencies). All formal definitions of failures are phrased as erroneous predictions or quantization inconsistencies (Definitions 2 and 3). The evaluation uses standard image-classification datasets and pre-trained models (MNIST, CIFAR-10, ImageNet; LeNet, ResNet, MobileNet, VGG) and measures numbers/diversity of 'errors' and 'quantization erroneous behavior'. There is no design, experiment, or claim that DeepHunter targets bypassing explicit safety/policy mechanisms, safety-aligned or adversarially-defended models, or higher-severity behaviours like jailbreaks, data leakage, or unauthorized actions. Therefore the failures it uncovers are robustness/misprediction errors (low severity) rather than high-impact safety/security violations.",
                    "evidence": "Page 149, Section 3.2.1 (Test Oracle): \"Definition 2. Given a DNN F, a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x), a mutant x' in M(x) is an erroneous behavior of F if F(x') != F(x).\"; \"Definition 3. ... a mutant x' in M(x) is a quantization erroneous behavior between F and F' if ... F(x') != F(x).\" (these definitions show failures are mispredictions or quantization inconsistencies).\n\nPage 149, Section 3.2.2 (Metamorphic Mutation): \"... this paper focuses on the image domain, one of the most widely studied domains.\" (shows scope limited to image classification robustness).\n\nPage 151, Section 4 (Experimental Design and Settings), Datasets and DNN Models: \"We select three popular publicly available datasets (i.e., MNIST [29], CIFAR-10 [27], and ImageNet [41]) ... we study several popular pre-trained DNN models\" (shows evaluation on standard models, not defended/safety-aligned models).\n\nPage 154, Section 5.4 (RQ4 Summary): \"With corner-region based criteria, DeepHunter tends to be more useful for detecting errors introduced during the DNN quantization.\" (shows emphasis on detecting quantization-related misprediction errors).\n\nSection 6 (Related Work, Testing): \"Different from DeepXplore and DiffChaser which focus on differential testing, DeepHunter is a grey-box testing technique focusing on a single DNN model.\" (indicates the method targets single-model robustness testing rather than bypassing policy/safeguards)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper defines and evaluates failures strictly as model robustness errors, specifically prediction inconsistencies under semantically preserving input mutations. The core test oracle, as defined in the paper, identifies an 'erroneous behavior' when a model's prediction on a mutated input differs from its correct prediction on the original input. This aligns perfectly with the 'Low' severity definition. The experimental evaluation is conducted on standard image classification benchmarks (MNIST, CIFAR-10, ImageNet), and the 'errors' or 'defects' counted are misclassifications. The paper does not discuss or evaluate the fuzzer's ability to uncover unsafe behaviors (Medium) or to bypass any explicit safety, policy, or security mechanisms (High).",
                    "evidence": "Page 5, Section 3.2.1, \"Definition 2. Given a DNN F, a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x), a mutant x' \u2208 M(x) is an erroneous behavior of F if F(x') \u2260 F(x).\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper defines and evaluates DeepHunter solely in terms of detecting mispredictions and model inconsistencies under semantically-preserving input mutations (including quantization-induced inconsistencies). All formal definitions of failures are phrased as erroneous predictions or quantization inconsistencies (Definitions 2 and 3). The evaluation uses standard image-classification datasets and pre-trained models (MNIST, CIFAR-10, ImageNet; LeNet, ResNet, MobileNet, VGG) and measures numbers/diversity of 'errors' and 'quantization erroneous behavior'. There is no design, experiment, or claim that DeepHunter targets bypassing explicit safety/policy mechanisms, safety-aligned or adversarially-defended models, or higher-severity behaviours like jailbreaks, data leakage, or unauthorized actions. Therefore the failures it uncovers are robustness/misprediction errors (low severity) rather than high-impact safety/security violations.",
                    "evidence": "Page 149, Section 3.2.1 (Test Oracle): \"Definition 2. Given a DNN F, a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x), a mutant x' in M(x) is an erroneous behavior of F if F(x') != F(x).\"; \"Definition 3. ... a mutant x' in M(x) is a quantization erroneous behavior between F and F' if ... F(x') != F(x).\" (these definitions show failures are mispredictions or quantization inconsistencies).\n\nPage 149, Section 3.2.2 (Metamorphic Mutation): \"... this paper focuses on the image domain, one of the most widely studied domains.\" (shows scope limited to image classification robustness).\n\nPage 151, Section 4 (Experimental Design and Settings), Datasets and DNN Models: \"We select three popular publicly available datasets (i.e., MNIST [29], CIFAR-10 [27], and ImageNet [41]) ... we study several popular pre-trained DNN models\" (shows evaluation on standard models, not defended/safety-aligned models).\n\nPage 154, Section 5.4 (RQ4 Summary): \"With corner-region based criteria, DeepHunter tends to be more useful for detecting errors introduced during the DNN quantization.\" (shows emphasis on detecting quantization-related misprediction errors).\n\nSection 6 (Related Work, Testing): \"Different from DeepXplore and DiffChaser which focus on differential testing, DeepHunter is a grey-box testing technique focusing on a single DNN model.\" (indicates the method targets single-model robustness testing rather than bypassing policy/safeguards)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "DeepHunter is designed to find generic erroneous behaviors (e.g., misclassifications and inconsistencies after metamorphic mutations or quantization) by generating semantically-preserved mutants and checking for prediction changes. The framework\u2019s oracle is metamorphic (it assumes the true label stays the same) and it treats any change in the model\u2019s output on a mutated input as an error. The paper describes no mechanism to steer exploration toward attacker-chosen specific outputs (e.g., force a model to output a particular target label or phrase) or to pursue targeted adversarial objectives; instead the system records inputs that cause the model to disagree with the original prediction or with a quantized variant. Therefore DeepHunter performs untargeted exploration and discovers generic failures rather than supporting targeted attack discovery.",
                    "evidence": "Page 149, Section 3.2 Test Oracle: \"Definition 1. Given an ideal human oracle O and a test x of a specific input domain, we define metamorphic mutation M on x, if \u2200x' \u2208 M(x), we have O(x') = O(x).\"\n\nPage 149, Section 3.2 Transformation and Mutation, Definition 2: \"Given a DNN F, a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x), a mutant x' \u2208 M(x) is an erroneous behavior of F if F(x') \u2260 F(x).\"\n\nPage 149, Algorithm 1 (DeepHunter test generation): \"for s' \u2208 T do if isFailedTest(s', result) then F \u2190 F \u222a {s'} else if isCoverageGain(cov) then Q \u2190 Q.append(s')\" (shows tests are marked 'failed' based on a prediction change/failed oracle, not a targeted output).\n\nPage 147, Introduction (research questions): \"RQ3 (Error Detection): How different are the existing criteria for guiding erroneous behaviors detection? How effective are different seed selection strategies for detecting erroneous behaviors of DNNs?\" (paper frames testing goal as detecting erroneous behaviors rather than steering to attacker-chosen outputs)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, DeepHunter, performs untargeted exploration to discover generic failures. The test oracle is based on metamorphic testing, where an error is defined as any misclassification of a validly mutated input. The goal is to find inputs `x'` where the model's prediction `F(x')` is different from the original correct prediction `F(x)`, without specifying what `F(x')` should be. This constitutes a search for general misclassifications. A secondary error definition involves finding inconsistencies between a full-precision model and its quantized version, which is also an untargeted approach. The framework does not include any mechanism to steer the fuzzing process toward a specific, attacker-chosen output label or a broader class of security-violating behaviors.",
                    "evidence": "Page 5, Section 3.2.1 Test Oracle, Definition 2: \"Given a DNN F, a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x), a mutant x' \u2208 M(x) is an erroneous behavior of F if F(x') \u2260 F(x).\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DeepHunter is designed to find generic erroneous behaviors (e.g., misclassifications and inconsistencies after metamorphic mutations or quantization) by generating semantically-preserved mutants and checking for prediction changes. The framework\u2019s oracle is metamorphic (it assumes the true label stays the same) and it treats any change in the model\u2019s output on a mutated input as an error. The paper describes no mechanism to steer exploration toward attacker-chosen specific outputs (e.g., force a model to output a particular target label or phrase) or to pursue targeted adversarial objectives; instead the system records inputs that cause the model to disagree with the original prediction or with a quantized variant. Therefore DeepHunter performs untargeted exploration and discovers generic failures rather than supporting targeted attack discovery.",
                    "evidence": "Page 149, Section 3.2 Test Oracle: \"Definition 1. Given an ideal human oracle O and a test x of a specific input domain, we define metamorphic mutation M on x, if \u2200x' \u2208 M(x), we have O(x') = O(x).\"\n\nPage 149, Section 3.2 Transformation and Mutation, Definition 2: \"Given a DNN F, a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x), a mutant x' \u2208 M(x) is an erroneous behavior of F if F(x') \u2260 F(x).\"\n\nPage 149, Algorithm 1 (DeepHunter test generation): \"for s' \u2208 T do if isFailedTest(s', result) then F \u2190 F \u222a {s'} else if isCoverageGain(cov) then Q \u2190 Q.append(s')\" (shows tests are marked 'failed' based on a prediction change/failed oracle, not a targeted output).\n\nPage 147, Introduction (research questions): \"RQ3 (Error Detection): How different are the existing criteria for guiding erroneous behaviors detection? How effective are different seed selection strategies for detecting erroneous behaviors of DNNs?\" (paper frames testing goal as detecting erroneous behaviors rather than steering to attacker-chosen outputs)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by measuring and analyzing observable model internals (multiple neuron-level coverage metrics) and by statistically characterizing failure counts and diversity under different coverage criteria and seed-selection strategies. It uses these signals to compare which criteria and strategies expose more errors (including a targeted analysis showing corner-region criteria like NBC/SNAC/TKNC are better at revealing quantization-induced inconsistencies). However, the paper does not provide deeper root-cause explanations that link failures to broader underlying model vulnerabilities such as reliance on spurious / non-robust features, dataset bias, or overfitting, so it does not meet the 'High' level.",
                    "evidence": "Page 148, Section 2 (DNN Testing Criteria): \"DNN Testing Criteria. ... we perform a large-scale controlled study on the following testing criteria [31, 36]. \u2022 Neuron Coverage (NC). ... \u2022 k-Multisection Neuron Coverage (KMNC). ... \u2022 Neuron Boundary Coverage (NBC). ... \u2022 Strong Neuron Activation Coverage (SNAC). ... \u2022 Top-k Neuron Coverage (TKNC).\" \n\nPage 149, Section 3.3 (Extensible Testing Criteria): \"we implement 5 existing criteria [31, 36] as different feedback metrics to determine whether the newly generated tests should be kept for further mutation. The criteria represent the state-of-the-arts and have been proven to be useful to capture the internal DNN states.\" \n\nPage 153, Section 5.3 (RQ3 Results of Error Detection): \"We also observe that KMNC is less effective than the other criteria in detecting errors, likely because it mainly considers the major functional behavior coverage [31].\" (discussion linking observed error-detection performance to properties of coverage metrics)\n\nPage 154, Section 5.4 (RQ4 Error Detection for Quantization): \"We can see that the number of errors found by DeepHunter with different coverage guidance is different. With NBC, DeepHunter found more errors for LeNet-5 and ResNet-20. ... The result reveals that KMNC and NC, which represent the overall major behavior of DNNs, are relatively not effective in such scenarios. Corner-region based criteria may be more useful to capture quantization errors.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by analyzing discovered failures in relation to observable model internals (specifically, different coverage criteria) and through statistical analysis of failure patterns (error diversity). It correlates the type of errors found (e.g., quantization errors) with the effectiveness of different coverage criteria, noting that 'corner-region based criteria' are more useful for capturing these specific failures than criteria representing major model behavior. Additionally, it analyzes failure patterns by measuring and comparing the diversity of error categories found by different fuzzing strategies, linking the seed selection strategy to the diversity of detected errors. However, it does not extend this analysis to explain the underlying model vulnerabilities, such as reliance on non-robust features, dataset bias, or overfitting, which would be required for a 'High' rating.",
                    "evidence": "Page 10, Section 5.4 RQ4. Error Detection for Quantization, \"We can see that the number of errors found by DeepHunter with different coverage guidance is different. With NBC, DeepHunter found more errors for LeNet-5 and ResNet-20. For MobileNet, errors found with NBC, SNAC, TKNC outnumbers the other two criteria. The result reveals that KMNC and NC, which represent the overall major behavior of DNNs, are relatively not effective in such scenarios. Corner-region based criteria may be more useful to capture quantization errors.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by measuring and analyzing observable model internals (multiple neuron-level coverage metrics) and by statistically characterizing failure counts and diversity under different coverage criteria and seed-selection strategies. It uses these signals to compare which criteria and strategies expose more errors (including a targeted analysis showing corner-region criteria like NBC/SNAC/TKNC are better at revealing quantization-induced inconsistencies). However, the paper does not provide deeper root-cause explanations that link failures to broader underlying model vulnerabilities such as reliance on spurious / non-robust features, dataset bias, or overfitting, so it does not meet the 'High' level.",
                    "evidence": "Page 148, Section 2 (DNN Testing Criteria): \"DNN Testing Criteria. ... we perform a large-scale controlled study on the following testing criteria [31, 36]. \u2022 Neuron Coverage (NC). ... \u2022 k-Multisection Neuron Coverage (KMNC). ... \u2022 Neuron Boundary Coverage (NBC). ... \u2022 Strong Neuron Activation Coverage (SNAC). ... \u2022 Top-k Neuron Coverage (TKNC).\" \n\nPage 149, Section 3.3 (Extensible Testing Criteria): \"we implement 5 existing criteria [31, 36] as different feedback metrics to determine whether the newly generated tests should be kept for further mutation. The criteria represent the state-of-the-arts and have been proven to be useful to capture the internal DNN states.\" \n\nPage 153, Section 5.3 (RQ3 Results of Error Detection): \"We also observe that KMNC is less effective than the other criteria in detecting errors, likely because it mainly considers the major functional behavior coverage [31].\" (discussion linking observed error-detection performance to properties of coverage metrics)\n\nPage 154, Section 5.4 (RQ4 Error Detection for Quantization): \"We can see that the number of errors found by DeepHunter with different coverage guidance is different. With NBC, DeepHunter found more errors for LeNet-5 and ResNet-20. ... The result reveals that KMNC and NC, which represent the overall major behavior of DNNs, are relatively not effective in such scenarios. Corner-region based criteria may be more useful to capture quantization errors.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Correlates coverage with discovered failures",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly designs mutation operators and checks that constrain mutations across iterative fuzzing (e.g., allowing at most one affine transform per mutation chain, and bounding cumulative pixel changes via L0/L\u221e computed relative to a reference image), thereby enforcing plausibility throughout sequential mutations. In addition, the authors performed a human user study that empirically measured the perceptual validity of generated tests (45,000 images total) and report low invalidity rates for DeepHunter (e.g., 1.2% MNIST, 2.9% CIFAR-10, 0.8% ImageNet). These two facts show the method both enforces plausibility across iterations and provides empirical evidence that final generated (including fault-inducing) inputs remain perceptually realistic.",
                    "evidence": "Page 149, Section 3.2 (Transformation and Mutation): \"In order to keep the semantics of the mutants close to the original seed, we adopt a conservative strategy that selects the Affine Transformation to be used only once ... A Pixel Value Transformation can be used multiple times ... we use L0 and L\u221e to constrain the pixel-level changes.\"; (Alg.2 / Definition 5 / Eq.2) Page 150: \"For sequential mutations that include an Affine Transformation ... the L0 and L\u221e between s0 and sn are computed as follows: L0(s0,sn) = L0(s0,sj-1) + L0(sj,sn)  L\u221e(s0,sn) = MAX(L\u221e(s0,sj-1), L\u221e(sj,sn)).\"; Page 151, Section 4 (User Study Settings): \"For each dataset, we randomly select 30 images as the seeds and generate 5,000 images with each of the 3 mutation strategies. Finally, 9 sets, with a total of 45,000 test images ... We recruit 9 participants ... A generated image is marked invalid if it either could not be perceived, or is recognized as a different class compared with its original counterpart.\"; Page 153, Section 5.1 (RQ1 Results) and Table 2: \"Table 2: The number and ratio of invalid images generated by different mutation strategies ... DeepHunter 60 (1.2%) 145 (2.9%) 40 (0.8%)\"; Page 147 (Abstract / Summary of results): \"The user study on mutation strategies shows ... with careful transformation constraint design and tuning, it can still reduce the likelihood to generate invalid tests to some extent (e.g., DeepHunter generates 1.2% on MNIST, 2.0% on CIFAR-10, and 0.8% on ImageNet).\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper explicitly designs its metamorphic mutation strategy to maintain the semantic validity of generated inputs throughout the fuzzing process, directly addressing the issue of cumulative transformations leading to implausible results. The methodology enforces this by allowing structural changes (Affine Transformations) only once per mutation sequence from an initial seed, while constraining subsequent pixel-level changes relative to a stable reference image. This design directly counters the problem of input drift over multiple iterations. Furthermore, the paper provides strong empirical evidence for the plausibility of its generated inputs via a formal user study. The study assessed the validity of thousands of generated images and found that DeepHunter's strategy produces a very low percentage of invalid images (e.g., 0.8% for ImageNet, 1.2% for MNIST), thus confirming that the inputs it generates, including those that find faults, remain perceptually realistic.",
                    "evidence": "Page 6, Section 3.2.3, Definition 5 and surrounding text: 'In order to keep the semantics of the mutants close to the original seed, we adopt a conservative strategy that selects the Affine Transformation to be used only once (we assume that mutation with only one affine transformation will not affect the semantics with the carefully selected parameters) as an image is more likely to be unrecognizable if it is changed by multiple affine transformations.' This describes the mechanism for controlling cumulative effects. \n\nPage 7, Section 4, 'User Study Settings': 'For RQ1, we perform the user study on 3 different strategies that constrain the generation of test images, and evaluate their effectiveness in generating valid test images... Each participant is asked to perform manual analysis on each of the generated test images in line with its original counterpart. A generated image is marked invalid if it either could not be perceived, or is recognized as a different class compared with its original counterpart.' This describes the empirical evaluation. \n\nPage 8, Table 2: This table presents the results of the user study, showing that DeepHunter generated invalid images at rates of only 1.2% (MNIST), 2.9% (CIFAR-10), and 0.8% (ImageNet), providing quantitative evidence of plausibility."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly designs mutation operators and checks that constrain mutations across iterative fuzzing (e.g., allowing at most one affine transform per mutation chain, and bounding cumulative pixel changes via L0/L\u221e computed relative to a reference image), thereby enforcing plausibility throughout sequential mutations. In addition, the authors performed a human user study that empirically measured the perceptual validity of generated tests (45,000 images total) and report low invalidity rates for DeepHunter (e.g., 1.2% MNIST, 2.9% CIFAR-10, 0.8% ImageNet). These two facts show the method both enforces plausibility across iterations and provides empirical evidence that final generated (including fault-inducing) inputs remain perceptually realistic.",
                    "evidence": "Page 149, Section 3.2 (Transformation and Mutation): \"In order to keep the semantics of the mutants close to the original seed, we adopt a conservative strategy that selects the Affine Transformation to be used only once ... A Pixel Value Transformation can be used multiple times ... we use L0 and L\u221e to constrain the pixel-level changes.\"; (Alg.2 / Definition 5 / Eq.2) Page 150: \"For sequential mutations that include an Affine Transformation ... the L0 and L\u221e between s0 and sn are computed as follows: L0(s0,sn) = L0(s0,sj-1) + L0(sj,sn)  L\u221e(s0,sn) = MAX(L\u221e(s0,sj-1), L\u221e(sj,sn)).\"; Page 151, Section 4 (User Study Settings): \"For each dataset, we randomly select 30 images as the seeds and generate 5,000 images with each of the 3 mutation strategies. Finally, 9 sets, with a total of 45,000 test images ... We recruit 9 participants ... A generated image is marked invalid if it either could not be perceived, or is recognized as a different class compared with its original counterpart.\"; Page 153, Section 5.1 (RQ1 Results) and Table 2: \"Table 2: The number and ratio of invalid images generated by different mutation strategies ... DeepHunter 60 (1.2%) 145 (2.9%) 40 (0.8%)\"; Page 147 (Abstract / Summary of results): \"The user study on mutation strategies shows ... with careful transformation constraint design and tuning, it can still reduce the likelihood to generate invalid tests to some extent (e.g., DeepHunter generates 1.2% on MNIST, 2.0% on CIFAR-10, and 0.8% on ImageNet).\""
                },
                "manual": {
                    "value": "High",
                    "why": "Carefulf mutation sequence design to preserve plausibility across fuzzing iterations. Provides human validation.",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "DeepHunter uses metamorphic (semantic-preserving) image transformations and enforces conservative pixel-level constraints (L0 and L\u221e) during mutation (partial consideration of numeric effects). The paper therefore does consider serialization-like concerns only implicitly by constraining pixel changes (e.g., bounds relative to 255) and by using metamorphic transformations, but it does not explicitly account for standard I/O serialization effects (rounding/quantization/clipping when saving inputs) during test generation or oracle evaluation. The paper treats metamorphic transformations as the oracle and evaluates validity via a human user study, and it focuses explicitly on model-weight quantization (quantizing network weights) as a separate concern; it does not describe explicit handling to ensure failures persist after common input serialization (e.g., saving as 8-bit PNG/JPEG) or any explicit post-serialization reproducibility checks. This matches the metric\u2019s \u201cMedium\u201d level: serialization effects are considered only partially (via metamorphic transforms and pixel-change constraints) but not explicitly enforced for reproducibility under I/O serialization.",
                    "evidence": "Page 149, Section 3.2 (Metamorphic Mutation): \"To keep the semantics of the mutants close to the original seed, we adopt a conservative strategy... A Pixel Value Transformation can be used multiple times... we use L0 and L\u221e to constrain the pixel-level changes... f(s,s') = L\u221e(s,s') \u2264 255, if L0(s,s') < \u03b1 \u00d7 size(s)  L\u221e(s,s') < \u03b2 \u00d7 255, otherwise (1) ... L0(s,s') represents the maximum number of the changed pixels... L\u221e represents the maximum value of a pixel changes...\" Page 153, Table 2 / Section 5.1 (User Study): \"Table 2 shows the number of invalid images generated by different strategies... DeepHunter 60 (1.2%) [MNIST], 145 (2.9%) [CIFAR-10], 40 (0.8%) [ImageNet]\" (shows reliance on metamorphic validity checks and human study). Page 148, Section 2 (DNN Quantization): \"DNN Quantization. ... quantizing the weights to lower bits (e.g., from 32-bit floating to 16-bit, 8-bit) ...\" (paper treats model-weight quantization explicitly, not input serialization). Nowhere in the methodology or evaluation (e.g., Sections 3.2, 3.3, 4, 5) does the paper describe explicit handling of standard I/O serialization (rounding/clipping when saving inputs to common storage formats) or post-serialization reproducibility checks."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper proposes DeepHunter, a fuzzing framework that uses metamorphic transformations (e.g., image brightness, rotation, scaling) to generate new test cases. The assessment metric explicitly states that applying semantic-preserving metamorphic transformations without explicit enforcement of reproducibility under serialization corresponds to a 'Medium' rating. DeepHunter's methodology for validating mutations, described in Section 3.2.2 and Algorithm 2, applies constraints based on L0 and L\u221e norms (Equation 1) to limit the magnitude of pixel-level changes. These constraints operate on the in-memory, likely floating-point, representation of the image and do not account for I/O effects like rounding or quantization that occur when serializing an image to a standard file format (e.g., 8-bit PNG). A small, fault-inducing floating-point perturbation (e.g., changing a pixel value from 155.0 to 155.4) could be generated and trigger a failure in memory, but this perturbation would be lost upon serialization as it would be rounded back to 155, making the failure non-reproducible from the stored artifact. The paper does not describe any mechanism to ensure that the generated mutations persist after a save-and-reload cycle.",
                    "evidence": "Page 5, Section 3.2.2, Metamorphic Mutation: \"To increase the changeability of mutation, we select eight image transformations of two categories: \u2022 Pixel Value transformation P: image contrast, image brightness, image blur, and image noise. \u2022 Affine transformation G: image translation, image scaling, image shearing, and image rotation.\"\n\nPage 6, Section 3.2.2: \"A Pixel Value Transformation can be used multiple times for increasing the changeability, we use Lo and L\u221e to constrain the pixel-level changes. Suppose an image s' is mutated from s by a pixel value transformation, then s' is valid if f(s, s') is satisfied.\"\n\nPage 6, Equation (1): The validity function f(s, s') is defined using L0(s, s') (number of changed pixels) and L\u221e(s, s') (maximum value of a pixel's change). This formulation constrains the magnitude of mutations but does not model or account for serialization effects such as rounding fractional changes to integers, which is critical for ensuring failure reproducibility from saved files."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "DeepHunter uses metamorphic (semantic-preserving) image transformations and enforces conservative pixel-level constraints (L0 and L\u221e) during mutation (partial consideration of numeric effects). The paper therefore does consider serialization-like concerns only implicitly by constraining pixel changes (e.g., bounds relative to 255) and by using metamorphic transformations, but it does not explicitly account for standard I/O serialization effects (rounding/quantization/clipping when saving inputs) during test generation or oracle evaluation. The paper treats metamorphic transformations as the oracle and evaluates validity via a human user study, and it focuses explicitly on model-weight quantization (quantizing network weights) as a separate concern; it does not describe explicit handling to ensure failures persist after common input serialization (e.g., saving as 8-bit PNG/JPEG) or any explicit post-serialization reproducibility checks. This matches the metric\u2019s \u201cMedium\u201d level: serialization effects are considered only partially (via metamorphic transforms and pixel-change constraints) but not explicitly enforced for reproducibility under I/O serialization.",
                    "evidence": "Page 149, Section 3.2 (Metamorphic Mutation): \"To keep the semantics of the mutants close to the original seed, we adopt a conservative strategy... A Pixel Value Transformation can be used multiple times... we use L0 and L\u221e to constrain the pixel-level changes... f(s,s') = L\u221e(s,s') \u2264 255, if L0(s,s') < \u03b1 \u00d7 size(s)  L\u221e(s,s') < \u03b2 \u00d7 255, otherwise (1) ... L0(s,s') represents the maximum number of the changed pixels... L\u221e represents the maximum value of a pixel changes...\" Page 153, Table 2 / Section 5.1 (User Study): \"Table 2 shows the number of invalid images generated by different strategies... DeepHunter 60 (1.2%) [MNIST], 145 (2.9%) [CIFAR-10], 40 (0.8%) [ImageNet]\" (shows reliance on metamorphic validity checks and human study). Page 148, Section 2 (DNN Quantization): \"DNN Quantization. ... quantizing the weights to lower bits (e.g., from 32-bit floating to 16-bit, 8-bit) ...\" (paper treats model-weight quantization explicitly, not input serialization). Nowhere in the methodology or evaluation (e.g., Sections 3.2, 3.3, 4, 5) does the paper describe explicit handling of standard I/O serialization (rounding/clipping when saving inputs to common storage formats) or post-serialization reproducibility checks."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Use metamorphic transformations but not consider IO effects (clipping, rounding).",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly defines and implements a mechanism to detect faults that transfer from an original model to other models (specifically quantized versions), and the evaluation reuses tests generated on the original (unquantized) model to detect errors in the quantized models. Definition 3 formalizes a \u201cquantization erroneous behavior\u201d (i.e., a fault that manifests in one model but not the other), DeepHunter\u2019s workflow and experiments use that relation, and RQ4 reports results where tests generated on the original model are applied to quantized variants to measure such transfer. This demonstrates both reuse of fault-inducing inputs across models performing the same task (original vs. quantized) and an explicit design for discovering transferable failures.",
                    "evidence": "Page 149, Section 3.2.1 (Transformation and Mutation) \u2014 Definition 3: \"Given a DNN F and its quantized version F', a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x) \u2227 F'(x) = O(x), a mutant x' \u2208 M(x) is a quantization erroneous behavior between F and F' if (F(x) = F(x') \u2228 F'(x) = F'(x')) \u2227 F(x') \u2260 F(x).\" \n\nPage 154, Section 5.4 (RQ4. Error Detection for Quantization) \u2014 experimental procedure and results: \"In this experiment, all initial seeds of each dataset are unable to detect the errors before and after quantization for all cases... At runtime, we allocate 10 hours for test generation on each original unquantized model... Table 6 summarizes the results in detecting errors introduced during quantization by DeepHunter+Prob and TensorFuzz.\" \n\nPage 154, Section 5.4 (Answer to RQ4): \"With corner-region based criteria, DeepHunter tends to be more useful for detecting errors introduced during the DNN quantization. A larger QR often introduces larger decision logic inconsistencies before and after quantization.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed framework, DeepHunter, is designed and evaluated for testing a single Deep Neural Network (DNN) model. The methodology, including the test generation algorithm and oracles, is focused on finding defects within one specific target model or between that model and its quantized version. The experimental evaluation is conducted on several different DNNs, but each is tested in isolation. The paper does not include any experiments where fault-inducing inputs generated for one model (e.g., LeNet-5) are used to test another model performing the same task (e.g., a different MNIST classifier). The authors explicitly state this single-model focus in the related work section, distinguishing their approach from differential testing techniques that compare multiple DNNs.",
                    "evidence": "Page 11, Section 6 RELATED WORK, 'Different from DeepXplore and DiffChaser which focus on differential testing, DeepHunter is a grey-box testing technique focusing on a single DNN model.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "DeepHunter explicitly evaluates the transferability of fault\u2010inducing inputs by first generating mutants on an original (32\u2010bit) DNN model and then reusing those same tests to detect errors in its quantized counterparts. Definition 3 formalizes the notion of a \u201cquantization erroneous behavior\u201d (i.e., faults that appear in one model version but not the other), and RQ4\u2019s experimental setup and results demonstrate this cross\u2010model reuse. This matches the \u201cHigh\u201d level of the Attack Transferability metric, since the framework both reuses inputs across models performing the same task (original versus quantized) and incorporates mechanisms to discover these transferable failures.",
                    "evidence": "Page 149, Section 3.2.1 \u2013 Definition 3: Formalizes \u201cquantization erroneous behavior\u201d between F and its quantized version F\u2032, describing tests generated on one model and applied to the other.\nPage 154, Section 5.4 \u2013 RQ4: Describes running DeepHunter on the original model and applying the generated tests to quantized versions, with Table 6 summarizing the errors detected across models."
                },
                "manual": {
                    "value": "Low",
                    "why": "The oracle definitions is that if same input produce different output to a model and its quanitized version, then it is fault-triggering input. But it is not transferability.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic",
                "Differential"
            ]
        }
    },
    "Guo et al. - 2018 - DLFuzz differential fuzzing testing of deep learning systems": {
        "key": "guo2018dlfuzz",
        "name": "DLFuzz",
        "year": "2018",
        "citation_count": "360",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's testing goal and reported failures are limited to causing mispredictions (adversarial examples) and increasing neuron coverage. DLFuzz generates tiny perturbations to inputs and labels as 'incorrect behaviors' any case where the mutated input is classified differently from the original. The paper evaluates success by number of adversarial inputs, L2 perturbation magnitudes, and neuron coverage on standard image classification models (MNIST, ImageNet). It does not describe uncovering unsafe/undesired content (e.g., hallucination, toxicity, bias), nor does it describe bypassing explicit safety, policy, or security mechanisms or testing defended / safety-aligned models for jailbreaks, data leakage, or unauthorized actions. Therefore the failures exposed are robustness/misprediction errors only, corresponding to the 'Low' severity category.",
                    "evidence": "Abstract (page 739): \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\" \n\nSection 3.1 Architecture (page 740): \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.\" \n\nAlgorithm 1 / Section 3.2 (page 740): objective described as jointly maximizing \"#incorrect behaviors & neuron coverage\" (Algorithm 1 line 7 / equation (1)). \n\nSection 4 Results (page 741-742), Table 1: reports metrics such as \"l2 Distance\" and \"#Adv.\" (number of adversarial inputs), and discussion: \"adversarial inputs generated by DLFuzz have much smaller perturbations... DLFuzz averagely generated 338.59% more adversarial inputs with 89.82% smaller perturbations.\" \n\nNo sections or quotes in the paper describe bypassing defenses, attacking safety/policy mechanisms, inducing data leakage, or producing unsafe/toxic outputs; all reported failures are adversarial misclassifications on image classification models (MNIST, ImageNet)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, DLFuzz, is designed to uncover model robustness errors, specifically prediction inconsistencies. The core mechanism identifies a failure when a semantically-preserved mutated input results in a different classification label compared to the original input. The experiments and examples provided in the paper, such as misclassifying a 'candle' as a 'lampshade' or a handwritten '9' as a '4', are classic instances of mispredictions. The paper does not mention or evaluate the fuzzer's ability to uncover unsafe behaviors (like toxicity or bias) or to bypass any explicit safety or security mechanisms.",
                    "evidence": "Page 3, Section 3.1, Architecture: \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs. The inconsistent classification results indicate that at least one of them is wrong so that manually labeling effort is not required here.\" This quote explicitly defines the failure condition as a prediction inconsistency, which aligns with the 'Low' severity definition."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's testing goal and reported failures are limited to causing mispredictions (adversarial examples) and increasing neuron coverage. DLFuzz generates tiny perturbations to inputs and labels as 'incorrect behaviors' any case where the mutated input is classified differently from the original. The paper evaluates success by number of adversarial inputs, L2 perturbation magnitudes, and neuron coverage on standard image classification models (MNIST, ImageNet). It does not describe uncovering unsafe/undesired content (e.g., hallucination, toxicity, bias), nor does it describe bypassing explicit safety, policy, or security mechanisms or testing defended / safety-aligned models for jailbreaks, data leakage, or unauthorized actions. Therefore the failures exposed are robustness/misprediction errors only, corresponding to the 'Low' severity category.",
                    "evidence": "Abstract (page 739): \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\" \n\nSection 3.1 Architecture (page 740): \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.\" \n\nAlgorithm 1 / Section 3.2 (page 740): objective described as jointly maximizing \"#incorrect behaviors & neuron coverage\" (Algorithm 1 line 7 / equation (1)). \n\nSection 4 Results (page 741-742), Table 1: reports metrics such as \"l2 Distance\" and \"#Adv.\" (number of adversarial inputs), and discussion: \"adversarial inputs generated by DLFuzz have much smaller perturbations... DLFuzz averagely generated 338.59% more adversarial inputs with 89.82% smaller perturbations.\" \n\nNo sections or quotes in the paper describe bypassing defenses, attacking safety/policy mechanisms, inducing data leakage, or producing unsafe/toxic outputs; all reported failures are adversarial misclassifications on image classification models (MNIST, ImageNet)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "DLFuzz is designed to induce generic misclassifications by maximizing neuron coverage and the prediction difference from the original label (i.e., make the mutated input be predicted as any class different from the original). The optimization objective (obj = sum(top-k scores) - original_score + \u03bb\u00b7neuron_terms) steers inputs away from the original class toward the top-k alternative classes, but does not allow the tester to specify a single attacker-chosen target output. The framework treats an input as adversarial when its predicted label differs from the original, i.e., untargeted misclassification, rather than steering exploration to a specific predefined target label or attacker-chosen output.",
                    "evidence": "Page 740, Section 3.1 Architecture: \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.\"; Page 741, Section 3.2 Algorithm (Algorithm 1 inputs): \"k <- top k labels different from the original label\"; Page 741, Section 3.2 Algorithm (objective definition and explanation): \"obj = \u03a3_{i=0}^k c_i - c + \u03bb \u00b7 \u03a3_{i=0}^m n_i\" and \"Maximizing the first part guides the input to cross the decision boundary of the original class and lie in the decision space of top k other classes. Such modified inputs are more likely to be classified incorrectly.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The method performs untargeted exploration to discover generic misclassifications. The goal of DLFuzz is to generate an input that is classified differently from the original seed input, without specifying what the new, incorrect classification label should be. This is explicitly defined by its oracle and its optimization objective. The oracle triggers when the prediction for the mutated input is simply not the same as the original. The optimization function aims to guide the input to cross the decision boundary into the space of 'top k other classes,' rather than steering it toward a single, predefined target label chosen by an attacker.",
                    "evidence": "Page 4, Algorithm 1, line 18 and Section 3.1, Architecture. The condition for identifying an adversarial example is `if c' != c then`, which means the mutated input's class `c'` is simply different from the original class `c`. The architecture description states: 'If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior'. Furthermore, the description of the optimization objective on Page 4 states its goal is to 'guide the input to cross the decision boundary of the original class and lie in the decision space of top k other classes. Such modified inputs are more likely to be classified incorrectly.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DLFuzz is designed to induce generic misclassifications by maximizing neuron coverage and the prediction difference from the original label (i.e., make the mutated input be predicted as any class different from the original). The optimization objective (obj = sum(top-k scores) - original_score + \u03bb\u00b7neuron_terms) steers inputs away from the original class toward the top-k alternative classes, but does not allow the tester to specify a single attacker-chosen target output. The framework treats an input as adversarial when its predicted label differs from the original, i.e., untargeted misclassification, rather than steering exploration to a specific predefined target label or attacker-chosen output.",
                    "evidence": "Page 740, Section 3.1 Architecture: \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.\"; Page 741, Section 3.2 Algorithm (Algorithm 1 inputs): \"k <- top k labels different from the original label\"; Page 741, Section 3.2 Algorithm (objective definition and explanation): \"obj = \u03a3_{i=0}^k c_i - c + \u03bb \u00b7 \u03a3_{i=0}^m n_i\" and \"Maximizing the first part guides the input to cross the decision boundary of the original class and lie in the decision space of top k other classes. Such modified inputs are more likely to be classified incorrectly.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "Discovers generic misclassification errors",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight based on observable model internals: it adopts and measures neuron coverage, defines a combined optimization objective that explicitly includes neuron activation terms, proposes and evaluates multiple neuron-selection heuristics (frequency, rarity, top weights, near-threshold), and reports experimental correlations between these strategies, increased neuron coverage, and discovery of adversarial inputs (Table 1, Figure 4). However, the analysis stops at these coverage- and activation-level observations and does not link discovered failures to deeper causes such as reliance on non-robust or spurious features, dataset bias, or overfitting, nor does it present statistical failure-pattern analyses (e.g., class-level error concentration or input-structure correlations). Therefore the paper meets the criterion for Medium: it analyzes observable internals and failure-finding correlations but does not explain why failures occur in terms of underlying model vulnerabilities.",
                    "evidence": "Page 741, Section 3.2 (Algorithm): \"DLFuzz also leverages the same definition and computing way of neuron coverage as DeepXplore [13]. Neurons with output values larger than the set threshold are regarded as activated (covered).\"; Page 741, Section 3.2 (Strategies for Neuron Selection): \"To maximize neuron coverage, we propose four heuristic strategies for selecting neurons... (1) Select neurons covered frequently... (2) Select neurons covered rarely... (3) Select neurons with top weights... (4) Select neurons near the activation threshold.\"; Page 742, Section 4.2 (Result): \"Table 1 presents the effectiveness of DLFuzz compared with DeepXplore. DLFuzz exhibits its advantages in improving neuron coverage... DLFuzz achieves 1.10% to 5.59% higher neuron coverage than DeepXplore...\" and \"We also tried all the proposed neuron selection strategies on two CNNs and depicted the results in Figure 4. All strategies are shown to contribute more to neuron coverage improvement than DeepXplore...\". (These quotes show analysis of observable internals and empirical correlation with failure discovery but there is no text linking failures to non-robust/spurious features, dataset bias, or overfitting.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by analyzing observable model internals. The core methodology of DLFuzz is designed to guide the search for failures (adversarial examples) by maximizing neuron coverage. The authors propose and evaluate four distinct heuristic strategies for selecting specific neurons to activate based on their internal characteristics, such as activation frequency, weights, and proximity to the activation threshold. This approach explicitly connects the discovery of erroneous behaviors to the internal state of the model. The evaluation also analyzes the effectiveness of these neuron selection strategies in improving neuron coverage, further strengthening the link between model internals and failure discovery. The paper does not, however, conduct a deeper analysis to explain why these failures occur by linking them to underlying vulnerabilities like spurious features, dataset bias, or overfitting, which would be required for a 'High' rating.",
                    "evidence": "Page 4, Section 'Strategies for Neuron Selection': 'To maximize neuron coverage, we propose four heuristic strategies for selecting neurons more likely to improve coverage. For each seed xs, m neurons will be selected utilizing one or multiple strategies... (1) Strategy 1. Select neurons covered frequently during past testing. Inspired by practical experience in traditional software testing that code fragments often or rarely executed are more possible to introduce defects. Neurons covered often or rarely perhaps can result in unusual logic and activate more neurons. (2) Strategy 2. Select neurons covered rarely... (3) Strategy 3. Select neurons with top weights... (4) Strategy 4. Select neurons near the activation threshold.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight based on observable model internals: it adopts and measures neuron coverage, defines a combined optimization objective that explicitly includes neuron activation terms, proposes and evaluates multiple neuron-selection heuristics (frequency, rarity, top weights, near-threshold), and reports experimental correlations between these strategies, increased neuron coverage, and discovery of adversarial inputs (Table 1, Figure 4). However, the analysis stops at these coverage- and activation-level observations and does not link discovered failures to deeper causes such as reliance on non-robust or spurious features, dataset bias, or overfitting, nor does it present statistical failure-pattern analyses (e.g., class-level error concentration or input-structure correlations). Therefore the paper meets the criterion for Medium: it analyzes observable internals and failure-finding correlations but does not explain why failures occur in terms of underlying model vulnerabilities.",
                    "evidence": "Page 741, Section 3.2 (Algorithm): \"DLFuzz also leverages the same definition and computing way of neuron coverage as DeepXplore [13]. Neurons with output values larger than the set threshold are regarded as activated (covered).\"; Page 741, Section 3.2 (Strategies for Neuron Selection): \"To maximize neuron coverage, we propose four heuristic strategies for selecting neurons... (1) Select neurons covered frequently... (2) Select neurons covered rarely... (3) Select neurons with top weights... (4) Select neurons near the activation threshold.\"; Page 742, Section 4.2 (Result): \"Table 1 presents the effectiveness of DLFuzz compared with DeepXplore. DLFuzz exhibits its advantages in improving neuron coverage... DLFuzz achieves 1.10% to 5.59% higher neuron coverage than DeepXplore...\" and \"We also tried all the proposed neuron selection strategies on two CNNs and depicted the results in Figure 4. All strategies are shown to contribute more to neuron coverage improvement than DeepXplore...\". (These quotes show analysis of observable internals and empirical correlation with failure discovery but there is no text linking failures to non-robust/spurious features, dataset bias, or overfitting.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "Reports discovered failures but does not analyze model internals or failure patterns to provide diagnostic insight.",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper enforces perceptual plausibility throughout the fuzzing process by measuring and bounding the L2 distance of each mutated input relative to the original input (used as a criterion before keeping a seed), thereby preventing cumulative drift across iterations. In addition, the authors report an explicit manual validation of all generated adversarial inputs (366 images) and state they found no changes of identity and that perturbations are imperceptible. These constitute an enforcement mechanism plus empirical human validation that final fault-inducing inputs remain perceptually realistic.",
                    "evidence": "Section 3.2 (Algorithm) \u2014 (page 741): Algorithm 1, line 15: \"l2_distance = distance(x', x) //measure the perturbation\" and accompanying text: \"As for the conditions of seed keeping in line 16, DLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\" \n\nSection 3.1 (Architecture) \u2014 (page 740): \"The minute perturbation is restricted to be invisible and ensures the prediction results before and after the mutation to be the same.\" \n\nSection 4.3 (Discussion) \u2014 (page 742): \"For confirmation, we checked all the 366 adversarial inputs generated by DLFuzz, though DLFuzz maintains quite small l2 distance by the restricted threshold. We haven't found any adversarial inputs that have already changed their identities after mutation. The adversarial inputs are nearly the same as the original input, and the perturbations are imperceptible.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility in individual mutation steps by using a bounded L2 distance constraint to ensure perturbations are small and imperceptible. The algorithm's design, which uses mutated inputs as new seeds, may overlook the cumulative effect of these small perturbations over many iterations. However, the paper validates the plausibility of the final fault-inducing inputs through manual inspection. The authors state they visually checked all 366 generated adversarial inputs and found them to be imperceptible and nearly identical to the original inputs, which constitutes a form of human-based validation. This combination of bounded per-step constraints and validation of the final outputs, without enforcing plausibility throughout the entire process or using quantitative perceptual metrics, aligns with the 'Medium' rating.",
                    "evidence": "Page 4, Section 4, Fuzzing Process: \"Second, DLFuzz adopts l2 distance to measure the perturbation with similar computation with [12]. As for the conditions of seed keeping in line 16, DLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\"\n\nPage 5, Section 4.3, Discussion: \"Without Manual Effort. For confirmation, we checked all the 366 adversarial inputs generated by DLFuzz, though DLFuzz maintains quite small l2 distance by the restricted threshold. We haven't found any adversarial inputs that have already changed their identities after mutation. The adversarial inputs are nearly the same as the original input, and the perturbations are imperceptible.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "DLFuzz actively enforces plausibility at every fuzzing iteration by bounding the L2 distance of each mutated input to the original seed (Algorithm 1, lines 15\u201316), preventing cumulative drift. Furthermore, the authors conducted a full manual validation of all 366 adversarial examples, reporting that none changed their semantic identity and that all perturbations remained imperceptible. This combination of a global perturbation constraint and explicit human evaluation of the final fault-inducing inputs satisfies the criteria for a \u201cHigh\u201d rating.",
                    "evidence": "Section 3.2, Algorithm 1 (page 741): \u201cl2_distance = distance(x\u2032, x) // measure the perturbation\u201d and \u201cD\u2009Lfuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\u201d\n\nSection 4.3, Discussion (page 742): \u201cFor confirmation, we checked all the 366 adversarial inputs generated by DLFuzz\u2026 We haven\u2019t found any adversarial inputs that have already changed their identities after mutation. The adversarial inputs are nearly the same as the original input, and the perturbations are imperceptible.\u201d"
                },
                "manual": {
                    "value": "High",
                    "why": "Perturbation is constrained with respect to the original seed, not the immediated parent seed that is already mutated. Performs manual validation of discovered fault-inducing inputs",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Low",
                    "why": "DLFuzz performs fine-grained, gradient-based in-memory perturbations (x' = x + perturbation) and constrains L2 distance to keep changes imperceptible, but the paper contains no explicit handling of standard I/O serialization effects (clipping, quantization, rounding) nor does it state that inputs are generated or validated in serialized formats. Because the method mutates floating-point tensors in memory and evaluates the oracle on those in-memory inputs without accounting for how those perturbations would be altered or lost when saved to common storage formats (e.g., 8-bit images), failures discovered by DLFuzz may be fragile and could be lost after standard I/O operations. This matches the metric definition for Low (ignores I/O effects during mutation and oracle evaluation).",
                    "evidence": "Page 741, Section 3.2 Algorithm: \"perturbation = processing(grads) ... x' = xs + perturbation\" and \"l2_distance = distance(x', x) ... DLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\"; Page 741, Section 4.1 Experiment Setup: \"We implemented DLFuzz based on the widespread frameworks, Tensorflow 1.2.1 and Keras 2.1.3. Tensorflow and Keras provide the efficient interfaces for computing the gradients and recording the intermediate output of all neurons after each prediction of the DNN.\"; Page 742, Section 4.3 Discussion (Without Manual Effort): \"For confirmation, we checked all the 366 adversarial inputs generated by DLFuzz ... We haven't found any adversarial inputs that have already changed their identities after mutation.\" (The paper does not describe any explicit handling of serialization, clipping, or quantization when generating or validating mutated inputs.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper describes DLFuzz's mutation strategy as adding small, gradient-based numerical perturbations directly to the input tensor in memory. The magnitude of these perturbations is controlled by keeping the L2 distance small to ensure they are imperceptible. The methodology does not mention any steps to account for standard I/O operations like clipping, rounding, or quantization. Small numerical changes, such as those generated by this method, are highly likely to be lost or altered when the input is serialized (e.g., saved as an 8-bit image), making the discovered failures fragile and not guaranteed to be reproducible. The paper's description of the mutation and oracle evaluation process completely ignores these serialization effects.",
                    "evidence": "Page 4, Algorithm 1, line 12: The mutation is a direct numerical addition: `x' = xs + perturbation //mutated input obtained`. Page 4, Section 4, Fuzzing Process: The paper confirms the use of small numerical perturbations measured by L2 distance: 'DLFuzz adopts l2 distance to measure the perturbation with similar computation with [12]. As for the conditions of seed keeping in line 16, DLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.' The paper contains no mention of clipping pixel values to a valid range (e.g., [0, 255]) or any other serialization-aware processing."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DLFuzz performs fine-grained, gradient-based in-memory perturbations (x' = x + perturbation) and constrains L2 distance to keep changes imperceptible, but the paper contains no explicit handling of standard I/O serialization effects (clipping, quantization, rounding) nor does it state that inputs are generated or validated in serialized formats. Because the method mutates floating-point tensors in memory and evaluates the oracle on those in-memory inputs without accounting for how those perturbations would be altered or lost when saved to common storage formats (e.g., 8-bit images), failures discovered by DLFuzz may be fragile and could be lost after standard I/O operations. This matches the metric definition for Low (ignores I/O effects during mutation and oracle evaluation).",
                    "evidence": "Page 741, Section 3.2 Algorithm: \"perturbation = processing(grads) ... x' = xs + perturbation\" and \"l2_distance = distance(x', x) ... DLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\"; Page 741, Section 4.1 Experiment Setup: \"We implemented DLFuzz based on the widespread frameworks, Tensorflow 1.2.1 and Keras 2.1.3. Tensorflow and Keras provide the efficient interfaces for computing the gradients and recording the intermediate output of all neurons after each prediction of the DNN.\"; Page 742, Section 4.3 Discussion (Without Manual Effort): \"For confirmation, we checked all the 366 adversarial inputs generated by DLFuzz ... We haven't found any adversarial inputs that have already changed their identities after mutation.\" (The paper does not describe any explicit handling of serialization, clipping, or quantization when generating or validating mutated inputs.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "relies on imperceptible, float-level perturbations that can vanish after saving to disk, with no persistence verification.",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across different models. DLFuzz is explicitly designed to avoid cross-referencing multiple models (unlike DeepXplore) and treats an input as a fault when the mutated input and the original input produce different labels on the same CNN under test. The evaluation compares DLFuzz to DeepXplore on per-model metrics (neuron coverage, number of adversarial inputs, perturbation size, time) but does not reuse adversarial inputs generated on one model to test other models or report whether such inputs trigger failures across models. The only related experiment (augmenting training sets) retrains the same CNNs with adversarial images and is not a transferability test. Therefore the paper does not demonstrate transferable attacks nor include mechanisms explicitly aimed at discovering transferable failures across models.",
                    "evidence": "Abstract (p.739, Abstract): \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\" \n\nSection 3.1 Architecture (p.740, 3.1 Architecture): \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.\" \n\nIntroduction / Evaluation summary (p.740, end of Intro / start of 3): \"Compared with DeepXplore, DLFuzz does not need extra efforts to collect similar functional DL systems for cross-referencing label check...\" \n\nResults (p.742, 4.2 Result): \"we incorporated 114 adversarial images into the training set of three CNNs on MNIST and retrained them trying to increase their accuracy.\" (This describes retraining with generated adversarial inputs, not testing those inputs on other, unseen models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper presents DLFuzz as a method for testing a single Deep Learning (DL) system in isolation. A core contribution highlighted is its ability to find incorrect behaviors without needing \"cross-referencing oracles from other DL systems with the same functionality.\" The methodology focuses on generating adversarial inputs for one specific model under test by maximizing prediction differences between an original and a mutated input for that same model. The experimental evaluation, while conducted on multiple models (e.g., LeNet-1, LeNet-4, VGG-16), treats each model as an independent target. The results are reported on a per-model basis, and there is no mention of any experiment where fault-inducing inputs generated for one model are used to test another model to assess transferability.",
                    "evidence": "Page 2, Abstract: \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\"\n\nPage 5, Section 5, Conclusion: \"DLFuzz also overcomes the trouble of relying on multiple DL systems of the similar functionality in DeepXplore.\"\n\nPage 5, Table 1: \"Effectiveness of DLFuzz compared with DeepXplore.\" This table presents the results for each model (LeNet-1, LeNet-4, LeNet-5, VGG16, VGG19, ResNet50) individually, comparing DLFuzz to DeepXplore on that specific model. There is no data or discussion regarding the transferability of adversarial inputs generated for one model to any of the others."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across different models. DLFuzz is explicitly designed to avoid cross-referencing multiple models (unlike DeepXplore) and treats an input as a fault when the mutated input and the original input produce different labels on the same CNN under test. The evaluation compares DLFuzz to DeepXplore on per-model metrics (neuron coverage, number of adversarial inputs, perturbation size, time) but does not reuse adversarial inputs generated on one model to test other models or report whether such inputs trigger failures across models. The only related experiment (augmenting training sets) retrains the same CNNs with adversarial images and is not a transferability test. Therefore the paper does not demonstrate transferable attacks nor include mechanisms explicitly aimed at discovering transferable failures across models.",
                    "evidence": "Abstract (p.739, Abstract): \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\" \n\nSection 3.1 Architecture (p.740, 3.1 Architecture): \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.\" \n\nIntroduction / Evaluation summary (p.740, end of Intro / start of 3): \"Compared with DeepXplore, DLFuzz does not need extra efforts to collect similar functional DL systems for cross-referencing label check...\" \n\nResults (p.742, 4.2 Result): \"we incorporated 114 adversarial images into the training set of three CNNs on MNIST and retrained them trying to increase their accuracy.\" (This describes retraining with generated adversarial inputs, not testing those inputs on other, unseen models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Whitebox"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Prediction-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Yu et al. - LLM-Fuzzer Scaling Assessment of Large Language Model Jailbreaks": {
        "key": "yu2024llm",
        "name": "LLM-Fuzzer",
        "year": "2024",
        "citation_count": "40",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper's method is explicitly designed to find and produce jailbreak prompts that bypass safety/alignment mechanisms in defended models. The authors demonstrate that LLM-FUZZER generates prompts that successfully induce harmful outputs from well-aligned and commercial LLMs (quantified across many models and questions), showing it uncovers high-impact violations of safety/policy safeguards rather than mere low-level robustness errors.",
                    "evidence": "Page 4657, Abstract: \"Our experiments show that LLM-FUZZER-generated jailbreak prompts demonstrate significantly increased effectiveness and transferability. This highlights that many open-source and commercial LLMs suffer from severe jailbreak issues, even after safety fine-tuning.\"; Page 4659, Introduction (Section 1): \"...despite dedicated efforts in training set filtering and safety alignment, all existing open-source and commercial LLMs continue to exhibit vulnerabilities to jailbreak prompts.\"; Page 4658, Figure 1 caption: \"In a typical scenario, ChatGPT would reject the harmful question. When this question is embedded within the jailbreak template, the resulting prompt can fool the gpt-3.5-turbo-0301 model into generating a potentially harmful response.\"; Page 4664, Results for Experiment I (Section 4.3): \"Among the 100 unethical questions where human-written templates all failed to jailbreak gpt-3.5-turbo-0125, our fuzzer was capable of generating templates to assist on average 96.85 questions in achieving their jailbreaking objectives through template mutations.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper introduces LLM-Fuzzer, a method explicitly designed to find and scale the assessment of \"jailbreaks\" in Large Language Models. A jailbreak is, by definition, a bypass of a model's explicit safety and policy mechanisms to elicit harmful or prohibited content. The method is evaluated against contemporary, safety-aligned models such as gpt-3.5-turbo-0125 and Llama-2-7B-Chat. The stated goal is to generate prompts that successfully cause these models to produce outputs for \"unethical questions\" that they would normally refuse, which directly corresponds to the \"High\" severity category of uncovering high-impact failures by bypassing explicit safety mechanisms in defended models.",
                    "evidence": "Page 10, Section 4.2, Experiment Design: \"Can our fuzzer generate new templates to successfully facilitate jailbreak attempts for unethical questions that the seed templates fail to exploit against gpt-3.5-turbo-0125 and Llama-2-7B-Chat? If so, what is the average query budget and token budget required for each unethical question?\" This quote clearly states that the fuzzer's objective is to create successful jailbreak attempts against specific, well-known, safety-aligned LLMs for questions they are designed to resist."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper's method is explicitly designed to find and produce jailbreak prompts that bypass safety/alignment mechanisms in defended models. The authors demonstrate that LLM-FUZZER generates prompts that successfully induce harmful outputs from well-aligned and commercial LLMs (quantified across many models and questions), showing it uncovers high-impact violations of safety/policy safeguards rather than mere low-level robustness errors.",
                    "evidence": "Page 4657, Abstract: \"Our experiments show that LLM-FUZZER-generated jailbreak prompts demonstrate significantly increased effectiveness and transferability. This highlights that many open-source and commercial LLMs suffer from severe jailbreak issues, even after safety fine-tuning.\"; Page 4659, Introduction (Section 1): \"...despite dedicated efforts in training set filtering and safety alignment, all existing open-source and commercial LLMs continue to exhibit vulnerabilities to jailbreak prompts.\"; Page 4658, Figure 1 caption: \"In a typical scenario, ChatGPT would reject the harmful question. When this question is embedded within the jailbreak template, the resulting prompt can fool the gpt-3.5-turbo-0301 model into generating a potentially harmful response.\"; Page 4664, Results for Experiment I (Section 4.3): \"Among the 100 unethical questions where human-written templates all failed to jailbreak gpt-3.5-turbo-0125, our fuzzer was capable of generating templates to assist on average 96.85 questions in achieving their jailbreaking objectives through template mutations.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's fuzzer (LLM-FUZZER) is explicitly designed to discover jailbreaks / harmful outputs (i.e., broader classes of security-violating behavior) by generating and mutating jailbreak prompts and using an oracle that labels responses as 'harmful' or not. The objective is to induce unsafe/harmful completions for given unethical questions (measured via metrics like JQN, ASR, EASR), rather than to force the model to produce a specific attacker-chosen output token sequence or exact target phrase. Thus the design steers exploration toward detecting unsafe/jailbreak behavior (broad classes) rather than targeted, per-output attacks.",
                    "evidence": "Page 4657, Abstract: \"LLM-FUZZER uses human-crafted jailbreak prompts as starting points. By employing carefully customized seed selection strategies and mutation mechanisms, LLM-FUZZER generates additional jailbreak prompts tailored to specific LLMs.\" \n\nPage 4663, Section 3.4 Oracle: \"The oracle\u2019s role is to take prompts as input and evaluate the responses to determine if they are harmful. If harmful responses are detected, the oracle assigns a corresponding reward.\" and \"When it comes to evaluating responses to individual prompts, the oracle follows a straightforward protocol. If it determines a response to be harmful, it assigns a reward of 1. Conversely, if the response is deemed non-harmful, a reward of 0 is assigned.\" \n\nPage 4664, Section 4.1 Metrics: \"JQN assesses the effectiveness for a set of jailbreak templates, reflecting an LLM\u2019s resistance to unethical questions... For each question, if there exists at least one template that could be leveraged to successfully jailbreak the target model, then the question is deemed as jailbreaking.\" and definitions of ASR/EASR showing measurement of ability to elicit harmful outputs across questions and template sets."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's proposed method, LLM-Fuzzer, is designed to discover 'jailbreaks' in Large Language Models. The goal is not to elicit a specific, predefined output string from the model, but rather to find inputs that cause the model to generate responses belonging to a broad class of security-violating behaviors, such as 'unlawful, biased, or inappropriate content.' The fuzzer's exploration is guided by an oracle that assesses whether a model's response is 'harmful.' This oracle, a fine-tuned RoBERTa model, is trained to classify responses based on their harmfulness, rewarding the fuzzer for any successful jailbreak, regardless of the specific content. This approach directly aligns with the 'Medium' category, which involves the discovery of broader classes of security-violating behaviors rather than specific, targeted outputs or untargeted exploration.",
                    "evidence": "Page 5, Section 3.1, Technical Overview and Challenges: 'The oracle is responsible for identifying whether a response contains harmful content. If the oracle identifies a response as harmful, LLM-FUZZER retains the template and adds it back to the seed pool.'\n\nPage 8, Section 3.4, Oracle: 'To achieve a balance between cost and accuracy, we developed our own oracle for assessing the harmfulness of responses in our work. We began by using a substantial corpus of responses to unethical questions, meticulously annotating them with labels denoting their harmfulness or non-harmful nature. Subsequently, we harnessed these annotated responses to fine-tune a RoBERTa model [35]. In the context of our research, this RoBERTa model serves as our designated \u201coracle\u201d.'\n\nPage 2, Abstract: 'The jailbreak threat poses a significant concern for Large Language Models (LLMs), primarily due to their potential to generate content at scale. If not properly controlled, LLMs can be exploited to produce undesirable, offensive content, and other forms of harmful or unethical behavior.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper's fuzzer (LLM-FUZZER) is explicitly designed to discover jailbreaks / harmful outputs (i.e., broader classes of security-violating behavior) by generating and mutating jailbreak prompts and using an oracle that labels responses as 'harmful' or not. The objective is to induce unsafe/harmful completions for given unethical questions (measured via metrics like JQN, ASR, EASR), rather than to force the model to produce a specific attacker-chosen output token sequence or exact target phrase. Thus the design steers exploration toward detecting unsafe/jailbreak behavior (broad classes) rather than targeted, per-output attacks.",
                    "evidence": "Page 4657, Abstract: \"LLM-FUZZER uses human-crafted jailbreak prompts as starting points. By employing carefully customized seed selection strategies and mutation mechanisms, LLM-FUZZER generates additional jailbreak prompts tailored to specific LLMs.\" \n\nPage 4663, Section 3.4 Oracle: \"The oracle\u2019s role is to take prompts as input and evaluate the responses to determine if they are harmful. If harmful responses are detected, the oracle assigns a corresponding reward.\" and \"When it comes to evaluating responses to individual prompts, the oracle follows a straightforward protocol. If it determines a response to be harmful, it assigns a reward of 1. Conversely, if the response is deemed non-harmful, a reward of 0 is assigned.\" \n\nPage 4664, Section 4.1 Metrics: \"JQN assesses the effectiveness for a set of jailbreak templates, reflecting an LLM\u2019s resistance to unethical questions... For each question, if there exists at least one template that could be leveraged to successfully jailbreak the target model, then the question is deemed as jailbreaking.\" and definitions of ASR/EASR showing measurement of ability to elicit harmful outputs across questions and template sets."
                },
                "manual": {
                    "value": "Medium",
                    "why": "aims to generate many jailbreak prompts that trigger harmful content without specifying exact outcomes",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides statistical and empirical diagnostic analysis of failures (e.g., per-model ASR/EASR, transferability patterns across models, ablations of seed-selection and mutator choices, and oracle accuracy), which gives insight into where and when jailbreaks succeed. However, it does not analyze observable model internals (neuron activations, layer behavior, coverage metrics) nor does it link discovered failures to deeper model vulnerabilities such as reliance on non-robust/spurious features, dataset bias, or overfitting. Thus it goes beyond mere reporting (so not Low) but stops short of the causal, internal-model explanations required for a High rating.",
                    "evidence": "Page 4661, Section 3.1 (Problem/Threat Model): \"In our case, the fuzzing target is an LLM that remains beyond our control. Our only means of assessment is analyzing the LLM\u2019s response to determine if an undesired output has been generated. Consequently, we need an efficient and effective method for discerning the harmfulness of the response.\"; Page 4666, Section 4.3 (Experiment Result): \"Figure 4 also sheds light on the performance of our method when applied to jailbreak commercial models. A notable observation is that our method exhibits the worst performance on Gemini, a very recent commercial model, with a significantly lower EASR and ASR. This underperformance could be attributed to the extensive red-teaming that Gemini has likely undergone, reflecting the increasing emphasis on model safety by producers of commercial LLMs.\"; Page 4664, Section 4.3 (Experiment I results / Table 1): \"As illustrated in Table 1, our fuzzing process reveals intriguing insights. Among the 100 unethical questions where human-written templates all failed to jailbreak gpt-3.5-turbo-0125, our fuzzer was capable of generating templates to assist on average 96.85 questions in achieving their jailbreaking objectives through template mutations.\"; Page 4667, Section 5 (Discussion - Limitations): \"The success of LLM-FUZZER partially relies on the oracle\u2019s ability to accurately determine whether a response constitutes a jailbreak. We utilized a RoBERTa model as our oracle, and while it has proven effective, its accuracy is not 100% as shown in Table 3. The potential for misclassification presents a challenge, as false positives or negatives could misguide the fuzzing process.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper focuses on presenting and evaluating a novel method, LLM-Fuzzer, for discovering jailbreaks in Large Language Models (LLMs). The entire experimental evaluation is centered on the performance and efficiency of the fuzzer itself. The metrics used, such as Attack Success Rate (ASR), Query Budget Consumption (QBC), and Token Budget Consumption (TBC), quantify the effectiveness of the generated jailbreak templates and the fuzzer's process. The paper reports the successful jailbreaks (failures) but does not conduct any diagnostic analysis to explain the underlying reasons for these failures within the target LLMs. There is no investigation of model internals (like neuron activation or coverage, which are mentioned in the metric definition), nor is there an analysis linking the failures to fundamental model vulnerabilities such as reliance on spurious features, dataset bias, or overfitting. The research demonstrates a powerful method for *finding* failures, but stops short of analyzing *why* those failures occur.",
                    "evidence": "Page 10, Section 4.1, subsection 'Metrics'. The paper explicitly lists its evaluation metrics: 'To evaluate the effectiveness and efficiency of our fuzzer, we define four metrics \u2014 \u2460 jailbreaking question number (JQN), \u2461 the individual template\u2019s attack success rate (ASR), \u2462 a group of templates\u2019s attack success rate (EASR), \u2463 query budget consumption (QBC) and \u2464 the token budget consumption (TBC).' These metrics measure the success rate and cost of the fuzzing attack, thereby evaluating the fuzzer's performance, but they do not provide any diagnostic information about the LLM's vulnerabilities. The results presented in tables and figures throughout Section 4.3 consistently report on these performance metrics, confirming that the paper's scope is limited to reporting discovered failures without analyzing their root cause."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "While the paper presents extensive experimental results (e.g., ASR, EASR, QBC, TBC) and ablation studies on the fuzzer\u2019s components, it does not perform any diagnostic analysis of the models themselves. There is no examination of internal model signals (neuron activations, coverage metrics), nor any statistical analysis linking failure patterns to input structure or underlying vulnerabilities (spurious features, dataset bias, overfitting). The work focuses on *finding* jailbreaks rather than *explaining* why the models fail.",
                    "evidence": "Page 4664, Section 4.1 (\u2018Metrics\u2019): defines ASR, EASR, QBC, TBC\u2014metrics measuring fuzzer performance rather than model diagnostics.  Page 4667, Section 5 (\u2018Discussion \u2013 Limitations\u2019): acknowledges reliance on oracle accuracy but makes no mention of internal model analysis or causal vulnerability explanations."
                },
                "manual": {
                    "value": "Low",
                    "why": "Does not provide any explanation why those errors occur.",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper designs mutation operators that explicitly aim to preserve semantic correctness (using LLM-assisted Generate/Crossover/Expand/Shorten/Rephrase operators) and adds a seed-selection reward discount (\u03b1 * path length) and early-termination probability to discourage deep mutation chains, i.e., mechanisms that constrain individual mutation steps and help limit cumulative drift. However, the paper does not present any empirical assessment of the perceptual or semantic plausibility of the final fault-inducing prompts (no human evaluation or task-specific quantitative measures such as perplexity or semantic-similarity scores are reported). Thus the work enforces plausibility in mutation design and includes measures to reduce accumulation effects, but it does not validate final-input plausibility empirically \u2014 matching the \"Medium\" definition.",
                    "evidence": "Section 3.3 Mutation (page 4662): \"To maintain the semantic correctness of newly generated prompts, we have introduced five innovative mutation operators that harness LLMs as assistants for prompt alteration while preserving their semantic integrity.\"; Section 3.2 Seed Selection / Algorithm 1 (page 4661): algorithm lines show reward discounting to penalize deep paths: \"if reward > 0 then\\n    reward \u2190 max(reward \u2212 \u03b1 * len(path), \u03b2)\" (Algorithm 1, Update); Section 3.3 (page 4662): \"To showcase that the newly generated templates can maintain their semantic integrity and diversify our template collection, Figure 3 presents several post-mutated templates.\" \u2014 i.e., qualitative examples only; Section 3.4 Oracle (page 4663): \"we harnessed these annotated responses to fine-tune a RoBERTa model... this RoBERTa model serves as our designated 'oracle'.\" \u2014 oracle evaluates harmfulness of outputs, not input plausibility; Section 4.1 Metrics (page 4664): defines evaluation metrics (JQN, ASR, EASR, QBC, TBC) with no mention of human evaluation or quantitative plausibility measures (e.g., perplexity or semantic-similarity), and no human study of final prompts is reported. Together these passages show plausibility is enforced in mutation steps and some controls for accumulation exist, but final-input plausibility is not empirically validated."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's methodology is explicitly designed to maintain the plausibility and semantic integrity of the fuzzer's inputs. The authors identify the shortcomings of traditional mutation techniques, which can produce \"syntactically incorrect or semantically nonsensical inputs\" when applied to natural language. To overcome this, their core mutation strategy leverages LLMs to perform all transformations (Generate, Crossover, Expand, Shorten, Rephrase). This design inherently ensures that mutated inputs remain within the plausible distribution of coherent, human-readable text throughout all fuzzing iterations. Furthermore, the paper provides empirical evidence to support this claim by presenting multiple examples of mutated templates and explicitly stating that this is done to \"showcase that the newly generated templates can maintain their semantic integrity\". These examples serve as qualitative validation that the final fault-inducing inputs remain semantically realistic.",
                    "evidence": "Page 7, Section 3.3 Mutation: \"Directly applying these techniques [traditional program fuzzing] to natural language inputs can result in syntactically incorrect or semantically nonsensical inputs, which are unlikely to be effective in jailbreaking LLMs. To address this challenge, we introduce distinct mutation methods that leverage LLMs themselves to assist in the mutation process. LLMs, with their proficiency in understanding and generating human-like text, offer a promising approach for crafting coherent and contextually relevant mutations... In order to showcase that the newly generated templates can maintain their semantic integrity and diversify our template collection, Figure 3 presents several post-mutated templates.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper incorporates mechanisms to enforce semantic plausibility during each mutation step\u2014namely, LLM-assisted Generate/Crossover/Expand/Shorten/Rephrase operators\u2014and penalizes overly deep mutation chains via reward discounting and early\u2010termination in the MCTS-Explore seed selection (Algorithm 1). However, it does not provide any explicit empirical evaluation of the final fault-inducing prompts\u2019 plausibility (no human studies or quantitative metrics such as perplexity or semantic-similarity scores are reported). Thus, while plausibility is enforced in mutation design and some mitigation against cumulative drift is applied, there is no empirical validation of final-input realism, matching the \u201cMedium\u201d level.",
                    "evidence": "Section 3.3 Mutation (page 4662): \u201cTo maintain the semantic correctness of newly generated prompts, we have introduced five innovative mutation operators that harness LLMs\u2026\u2009\u201d and Figure 3 qualitative examples; Algorithm 1 Update (page 4661): reward \u2190 max(reward \u2212 \u03b1*len(path),\u03b2) and early-termination to limit deep mutation chains; Section 4.1 Metrics (page 4664): defines JQN, ASR, EASR, QBC, TBC with no mention of human evaluation or task-specific measures (e.g., perplexity, SSIM) to assess input plausibility."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Does not provide empirical evaluation of the final fault-inducing prompts.",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper's fuzzer generates and mutates textual jailbreak templates/prompts (not fine-grained numeric tensor perturbations) and feeds those serialized text prompts to LLMs; the oracle likewise evaluates textual responses. Because inputs are produced directly as text (i.e., already in a serialized format stable to quantization/clipping), the fault-inducing test cases are inherently stable under standard I/O serialization and are not fragile numerical memory artifacts. This matches the metric's allowance that approaches which generate inputs directly in serialized formats (e.g., generated text) are reproducible.",
                    "evidence": "Page 4657, Abstract: \"LLM-FUZZER uses human-crafted jailbreak prompts as starting points. By employing carefully customized seed selection strategies and mutation mechanisms, LLM-FUZZER generates additional jailbreak prompts tailored to specific LLMs.\" \n\nPage 4660, Section 3.1 (Proposed Technique / Technical Overview): \"LLM-FUZZER ... begins with a corpus comprising human-written jailbreak templates, serving as the initial seed pool. In each iteration, LLM-FUZZER selects a template from this pool and randomly chooses a mutator to apply the mutation to create a new jailbreak template. Subsequently, it combines this newly generated template with a target question to form a prompt.\" \n\nPage 4662, Section 3.3 (Mutation): \"In this study, we introduce five distinct mutation schemes... Generate: This mutation operator employs an LLM by providing it with a template and instructing the model to generate a new template in a similar style but with different content.... Rephrase: ... instructing the LLM to rephrase every sentence within the template while maintaining the overall meaning.\" \n\nPage 4663, Section 3.4 (Oracle): \"The oracle\u2019s role is to take prompts as input and evaluate the responses to determine if they are harmful. If harmful responses are detected, the oracle assigns a corresponding reward.\" \n\nAppendix D.1 (Mutate Prompt examples): \"I need you to generate one template. I will give you one template example... You are required to write one example with similar style but different content and MUST have '[INSERT PROMPT HERE]' in your template.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The method proposed in the paper, LLM-Fuzzer, operates entirely on textual data. The mutation operators generate new jailbreak templates by using a large language model to modify existing text templates. The resulting test cases are text prompts, which are an inherently serialized format. The metric definition explicitly states: 'We also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., synthesized speech, rendered images, or generated text) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.' LLM-Fuzzer's approach of generating text-based inputs perfectly aligns with this condition. Unlike numerical perturbations in image or audio data that can be lost through quantization or rounding, the textual perturbations generated by LLM-Fuzzer are perfectly preserved during standard I/O operations (e.g., saving to a file), ensuring failure reproducibility.",
                    "evidence": "Page 7, Section 3.3, Mutation: The paper describes its mutation operators as text-to-text transformations performed by an LLM. For instance: 'Generate: This mutation operator employs an LLM by providing it with a template and instructing the model to generate a new template in a similar style but with different content.' The other operators (Crossover, Expand, Shorten, Rephrase) are also described as manipulating text. This confirms that the test cases are generated directly in their final, serialized text format."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper's fuzzer generates and mutates textual jailbreak templates/prompts (not fine-grained numeric tensor perturbations) and feeds those serialized text prompts to LLMs; the oracle likewise evaluates textual responses. Because inputs are produced directly as text (i.e., already in a serialized format stable to quantization/clipping), the fault-inducing test cases are inherently stable under standard I/O serialization and are not fragile numerical memory artifacts. This matches the metric's allowance that approaches which generate inputs directly in serialized formats (e.g., generated text) are reproducible.",
                    "evidence": "Page 4657, Abstract: \"LLM-FUZZER uses human-crafted jailbreak prompts as starting points. By employing carefully customized seed selection strategies and mutation mechanisms, LLM-FUZZER generates additional jailbreak prompts tailored to specific LLMs.\" \n\nPage 4660, Section 3.1 (Proposed Technique / Technical Overview): \"LLM-FUZZER ... begins with a corpus comprising human-written jailbreak templates, serving as the initial seed pool. In each iteration, LLM-FUZZER selects a template from this pool and randomly chooses a mutator to apply the mutation to create a new jailbreak template. Subsequently, it combines this newly generated template with a target question to form a prompt.\" \n\nPage 4662, Section 3.3 (Mutation): \"In this study, we introduce five distinct mutation schemes... Generate: This mutation operator employs an LLM by providing it with a template and instructing the model to generate a new template in a similar style but with different content.... Rephrase: ... instructing the LLM to rephrase every sentence within the template while maintaining the overall meaning.\" \n\nPage 4663, Section 3.4 (Oracle): \"The oracle\u2019s role is to take prompts as input and evaluate the responses to determine if they are harmful. If harmful responses are detected, the oracle assigns a corresponding reward.\" \n\nAppendix D.1 (Mutate Prompt examples): \"I need you to generate one template. I will give you one template example... You are required to write one example with similar style but different content and MUST have '[INSERT PROMPT HERE]' in your template.\""
                },
                "manual": {
                    "value": "High",
                    "why": "Mutations are purely at the text level",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly evaluates transferability: templates (fault-inducing inputs) generated by LLM-FUZZER on a set of surrogate models are reused to test many other models, and the fuzzer's design includes an explicit mechanism (oracle reward modification) to encourage generation of templates that succeed across multiple models. The authors both describe this cross-model generation/evaluation procedure (Experiment III) and report strong transfer results (EASR > 80% on many target models) and comparisons to other methods.",
                    "evidence": "Section 4.2 Experiment III (p.4664\u20134665): \"To address the question of transferability, we chose to create templates using gpt-3.5-turbo-0125, Llama-2-7B-Chat, and Vicuna-7B.... in the template reward assignment by the oracle, a zero reward was imposed if the template\u2019s all successful jailbreak attempts were attributed solely to one or two LLMs.\" \n\nSection 4.2 Experiment III (p.4665): \"Following the exhaustion of the 150k query budget, our experiment involved calculating the average ASR for each template generated across the three target models. The template with the highest average ASR, denoted as 'top-1', and the top five templates with the highest average ASR, known as 'top-5', were selected for further experimentation.\" \n\nSection 4.3 Results for Experiment III (p.4666\u20134668): \"These templates displayed a notable degree of transferability... As depicted in Figure 4, when evaluated against Vicuna-13B, Baichuan-13B, ChatGLM-6B, Llama-2-13B/70B, Claude1, and PaLM2, the EASR for LLM-FUZZER consistently exceeded 80%, having a huge advantage over EASRs of other methods.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper satisfies both conditions for a 'High' rating. First, it explicitly tests the transferability of the jailbreak templates generated by LLM-FUZZER. Templates are created using a set of three models and then evaluated against a larger, diverse set of unseen open-source and commercial LLMs. Second, the fuzzing design incorporates an explicit mechanism to support the discovery of transferable failures. Specifically for the transferability experiment, the reward mechanism of the fuzzer is modified to query multiple models simultaneously and penalize templates that are not universally effective, thereby actively guiding the fuzzer towards generating transferable jailbreaks.",
                    "evidence": "Page 10-11, Section 4.2, Experiment Design: \"Experiment III. To address the question of transferability, we chose to create templates using gpt-3.5-turbo-0125, Llama-2-7B-Chat, and Vicuna-7B. This approach was taken to ensure that the generated templates would be applicable across various LLMs. We followed a similar procedure with the previous experiment but made two slight modifications. First, in each fuzzing iteration, we queried 100 questions for each template across all three models, resulting in a total of 300 responses. Second, in the template reward assignment by the oracle, a zero reward was imposed if the template's all successful jailbreak attempts were attributed solely to one or two LLMs. This modification aimed to bolster universality by discouraging the generation of templates that could exploit only one or two specific LLMs.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly evaluates transferability: templates (fault-inducing inputs) generated by LLM-FUZZER on a set of surrogate models are reused to test many other models, and the fuzzer's design includes an explicit mechanism (oracle reward modification) to encourage generation of templates that succeed across multiple models. The authors both describe this cross-model generation/evaluation procedure (Experiment III) and report strong transfer results (EASR > 80% on many target models) and comparisons to other methods.",
                    "evidence": "Section 4.2 Experiment III (p.4664\u20134665): \"To address the question of transferability, we chose to create templates using gpt-3.5-turbo-0125, Llama-2-7B-Chat, and Vicuna-7B.... in the template reward assignment by the oracle, a zero reward was imposed if the template\u2019s all successful jailbreak attempts were attributed solely to one or two LLMs.\" \n\nSection 4.2 Experiment III (p.4665): \"Following the exhaustion of the 150k query budget, our experiment involved calculating the average ASR for each template generated across the three target models. The template with the highest average ASR, denoted as 'top-1', and the top five templates with the highest average ASR, known as 'top-5', were selected for further experimentation.\" \n\nSection 4.3 Results for Experiment III (p.4666\u20134668): \"These templates displayed a notable degree of transferability... As depicted in Figure 4, when evaluated against Vicuna-13B, Baichuan-13B, ChatGLM-6B, Llama-2-13B/70B, Claude1, and PaLM2, the EASR for LLM-FUZZER consistently exceeded 80%, having a huge advantage over EASRs of other methods.\""
                },
                "manual": {
                    "value": "High",
                    "why": "Transferability is encouraged in mutation by rewarding less to mutations that transfer less.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "Tian et al. - 2018 - DeepTest automated testing of deep-neural-network-driven autonomous cars": {
        "key": "tian2018deeptest",
        "name": "DeepTest",
        "year": "2018",
        "citation_count": "1791",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's DeepTest method uncovers many unsafe, high-impact functional failures (steering-angle deviations that can lead to potentially fatal crashes) and reports thousands of such erroneous behaviors in evaluated driving models. However, the evaluation targets standard Udacity driving models and the paper does not claim or demonstrate bypassing explicit safety, policy, or security mechanisms (e.g., adversarially-trained or safety-aligned models) nor uncover attacks like data leakage or jailbreaks. Therefore the failures exposed violate user safety expectations (unsafe steering) but do not show bypassing of explicit defenses, matching the 'Medium' category.",
                    "evidence": "Page 303, Abstract: \"DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.\"; Page 306, Section 3.4 (Creating a Test Oracle with Metamorphic Relations): \"the autonomous car\u2019s steering angle should not change significantly for the same image under any lighting/weather conditions, blurring, or any affine transformations with small parameter values.\"; Page 311, Results / RQ4: \"In total, DeepTest detects 6339 erroneous behaviors across all three models.\"; Page 308, Section 4 (Implementation): \"We evaluate our techniques on three DNN models that won top positions in the Udacity self-driving challenge ... Rambo, Chauffeur, and Epoch.\" (no mention of testing defended/adversarially-trained or safety-aligned models or bypassing explicit safety or security mechanisms)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper introduces DeepTest, a tool for automatically detecting erroneous behaviors in DNNs for autonomous cars. The failures it uncovers are significant deviations in steering angle under realistic, synthetically generated environmental conditions like rain, fog, and different lighting. These incorrect steering predictions can lead to \"potentially fatal crashes,\" which are clear violations of safety expectations and constitute unsafe behavior. The evaluation demonstrates finding thousands of such dangerous behaviors. The models under test are standard DNNs from a driving challenge and are not described as having explicit safety or security mechanisms that DeepTest bypasses. Therefore, the failures align with the 'Medium' severity level, as the tool uncovers unsafe behavior without the context of bypassing explicit defenses.",
                    "evidence": "Page 2, Abstract: \"In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. ... DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.\"\n\nPage 3, Figure 1 Caption: \"A sample dangerous erroneous behavior found by DeepTest in the Chauffeur DNN.\" (The accompanying image shows the car steering off the road in a synthetically generated rainy condition)."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper's DeepTest method uncovers many unsafe, high-impact functional failures (steering-angle deviations that can lead to potentially fatal crashes) and reports thousands of such erroneous behaviors in evaluated driving models. However, the evaluation targets standard Udacity driving models and the paper does not claim or demonstrate bypassing explicit safety, policy, or security mechanisms (e.g., adversarially-trained or safety-aligned models) nor uncover attacks like data leakage or jailbreaks. Therefore the failures exposed violate user safety expectations (unsafe steering) but do not show bypassing of explicit defenses, matching the 'Medium' category.",
                    "evidence": "Page 303, Abstract: \"DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.\"; Page 306, Section 3.4 (Creating a Test Oracle with Metamorphic Relations): \"the autonomous car\u2019s steering angle should not change significantly for the same image under any lighting/weather conditions, blurring, or any affine transformations with small parameter values.\"; Page 311, Results / RQ4: \"In total, DeepTest detects 6339 erroneous behaviors across all three models.\"; Page 308, Section 4 (Implementation): \"We evaluate our techniques on three DNN models that won top positions in the Udacity self-driving challenge ... Rambo, Chauffeur, and Epoch.\" (no mention of testing defended/adversarially-trained or safety-aligned models or bypassing explicit safety or security mechanisms)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The DeepTest fuzzer intentionally steers exploration using neuron-coverage-guided transformations and uses transformation-specific metamorphic relations as an oracle to find safety-violating behaviors (e.g., steering deviations under rain/fog/lighting changes). This design pursues broader classes of unsafe or erroneous behaviors (corner cases that may cause dangerous steering), rather than aiming to induce a specific, attacker-chosen output (e.g., a particular steering angle or a chosen label). The mutation/exploration components (realistic image transformations + coverage-guided search) and the oracle (metamorphic relations that flag outputs outside an allowed error band) enable discovery of unsafe behavior classes but do not implement a mechanism to target a precise predefined output value.",
                    "evidence": "Page 304, Introduction: \"we use transformation-specific metamorphic relations between multiple executions of the tested DNN (e.g., a car should behave similarly under different lighting conditions) to automatically detect erroneous corner case behaviors.\"; Page 306, Section 3.1 (Systematic Testing with Neuron Coverage): \"DeepTest tries to generate inputs that maximize neuron coverage of the test DNN.\"; Page 310, RQ4 (Can we automatically detect erroneous behaviors using metamorphic relations?): \"We call these images Ierr ... The deviations produced for Ierr are much larger than Iorg (also confirmed by Wilcoxon test for statistical significance). In fact, mean squared error (MSE) for Ierr is 0.41, while the MSE of the corresponding Iorg is 0.035.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, DeepTest, performs untargeted exploration to discover generic failures. Its primary objective is to maximize neuron coverage to explore different parts of the DNN's logic, rather than steering the model towards a specific, predefined output. The error detection mechanism, or oracle, is based on metamorphic relations. It identifies erroneous behaviors by detecting inconsistencies in the model's predictions when the input is subjected to realistic transformations (e.g., changes in brightness, weather conditions). Specifically, it flags cases where the steering angle changes significantly after a minor, realistic transformation is applied to an input image. This approach is designed to find any unexpected or inconsistent behavior, which aligns with the definition of 'untargeted exploration' and the discovery of 'inconsistent predictions' described in the 'Low' category of the metric.",
                    "evidence": "Page 6, Section 3.4, Creating a Test Oracle with Metamorphic Relations: \"To avoid this issue, we leverage metamorphic relations [33] between the car behaviors across different synthetic images. The key insight is that even though it is hard to specify the correct behavior of a self-driving car for every transformed image, one can define relationships between the car's behaviors across certain types of transformations. For example, the autonomous car's steering angle should not change significantly for the same image under any lighting/weather conditions, blurring, or any affine transformations with small parameter values.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "DeepTest is designed to discover broad classes of safety-violating behaviors (e.g., corner-case steering errors under rain, fog, lighting changes) rather than to induce a specific, attacker-chosen steering angle or output. Its mutation engine applies realistic transformations and coverage-guided search to explore unsafe conditions, and its oracle flags any outputs that deviate beyond an acceptable tolerance under those transformations. While it steers exploration toward unsafe behavior classes, it does not target precise, predefined outputs, placing it in the \u201cMedium\u201d category.",
                    "evidence": "Page 304 (Abstract/Introduction): \u201cDeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes.\u201d\nPage 310 (Section 3.4): \u201cWe redefine a new metamorphic relation as: (\u03b8\u0302_i \u2013 \u03b8_t_i)\u00b2 \u2264 \u03bb MSE_orig \u2026 the errors produced by a model for the transformed images \u2026 should be within a range \u2026\u201d\nPage 308 (Table 5 and accompanying text): DeepTest maximizes neuron coverage to explore diverse behaviors, not to drive the model to a specific steering output."
                },
                "manual": {
                    "value": "Low",
                    "why": "Aims to find errors in steering angle predictions under realistic environmental variations",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides substantial diagnostic insight using observable model internals and statistical failure patterns: it uses neuron-coverage as an internal signal, measures correlations between coverage and outputs, compares activated-neuron sets across transformations (Jaccard distances), reports per-transformation and per-model failure statistics, and even shows that retraining on generated failures reduces error. However, it does not offer deeper causal explanations that link specific failures to underlying model vulnerabilities (e.g., precise reliance on spurious/non-robust features, explicit dataset bias analysis, or detailed neuron-/layer-level root-cause attributions). Thus the work surpasses mere reporting (not Low) but stops short of the explanatory, vulnerability-rooted analysis required for a High rating.",
                    "evidence": "\u2022 Use of neuron-coverage as internal diagnostic signal (Introduction / contribution): \"We empirically demonstrate that changes in neuron coverage correlate with changes in an autonomous car\u2019s behavior.\" (p.303, Introduction / contributions).; \u2022 Statistical linkage between coverage and outputs (Section 5 / RQ1): \"Table 5 shows that Spearman correlations for all the models are statistically significant... This result indicates that the neuron coverage changes with the changes in output steering angles\" (p.308, Section 5, RQ1 and Table 5).; \u2022 Analysis of which neurons transform-variants activate (Section 5 / RQ2): \"These results indicate that for all models, except Chauffeur-LSTM, different transformations activate different neurons.\" (p.309, Section 5, RQ2; Figure 4 shows Jaccard distances).; \u2022 Statistical failure-pattern reporting across transformations and models: Table 7 and Table 8 enumerate numbers of erroneous behaviors by transformation, thresholds, and model, and the paper notes: \"Interestingly, some models are more prone to erroneous behaviors for some transformations than others.\" (p.311, Section 5, discussion around Tables 7\u20138).; \u2022 Metamorphic-oracle formulation used to detect failures (Section 3.4): \"We redefine a new metamorphic relation as: (\u02c6\u03b8i \u2212 \u03b8ti)^2 \u2264 \u03bb MSEorig\" (p.307, Section 3.4).; \u2022 Empirical link to training-data coverage (retraining experiment): \"In all cases, the accuracy of the retrained model improved significantly over the original model as shown in Table 9.\" (p.311, Section 5, RQ5 and Table 9) \u2014 this provides an empirical connection suggesting some failures relate to missing data conditions. ; \u2022 Manual inspection of false positives: \"We also manually checked the bugs reported in Table 8 and report the false positives in Figure 8.\" (p.311, Section 5)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight through both analysis of observable model internals and statistical analysis of failure patterns. First, it establishes a direct statistical correlation between neuron coverage (a model internal) and the model's output (steering angle), demonstrating that changes in internal activations are linked to changes in behavior. Second, it analyzes failure patterns by showing that certain models are more susceptible to failures under specific types of data transformations (e.g., one model failing on 'shear' transformations while others do not). This constitutes a statistical analysis of failure patterns. The paper does not, however, extend this analysis to explain the root cause of these vulnerabilities (e.g., dataset bias, reliance on spurious features), which would be required for a 'High' rating.",
                    "evidence": "Page 7, Section 5, RQ1: \"Table 5 shows that Spearman correlations for all the models are statistically significant... This result indicates that the neuron coverage changes with the changes in output steering angles, i.e. different neurons get activated for different outputs. Thus, in this setting, neuron coverage can be a good approximation for estimating the diversity of input-output pairs.\" \nPage 10, Section 5, analysis of Table 8: \"Interestingly, some models are more prone to erroneous behaviors for some transformations than others. For example, Rambo produces 23 erroneous cases for shear, while the other two models do not show any such cases. Similarly, DeepTest finds 650 instances of erroneous behavior in Chauffeur for rain but only 64 and 27 for Epoch and Rambo respectively.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides substantial diagnostic insight using observable model internals and statistical failure patterns: it uses neuron-coverage as an internal signal, measures correlations between coverage and outputs, compares activated-neuron sets across transformations (Jaccard distances), reports per-transformation and per-model failure statistics, and even shows that retraining on generated failures reduces error. However, it does not offer deeper causal explanations that link specific failures to underlying model vulnerabilities (e.g., precise reliance on spurious/non-robust features, explicit dataset bias analysis, or detailed neuron-/layer-level root-cause attributions). Thus the work surpasses mere reporting (not Low) but stops short of the explanatory, vulnerability-rooted analysis required for a High rating.",
                    "evidence": "\u2022 Use of neuron-coverage as internal diagnostic signal (Introduction / contribution): \"We empirically demonstrate that changes in neuron coverage correlate with changes in an autonomous car\u2019s behavior.\" (p.303, Introduction / contributions).; \u2022 Statistical linkage between coverage and outputs (Section 5 / RQ1): \"Table 5 shows that Spearman correlations for all the models are statistically significant... This result indicates that the neuron coverage changes with the changes in output steering angles\" (p.308, Section 5, RQ1 and Table 5).; \u2022 Analysis of which neurons transform-variants activate (Section 5 / RQ2): \"These results indicate that for all models, except Chauffeur-LSTM, different transformations activate different neurons.\" (p.309, Section 5, RQ2; Figure 4 shows Jaccard distances).; \u2022 Statistical failure-pattern reporting across transformations and models: Table 7 and Table 8 enumerate numbers of erroneous behaviors by transformation, thresholds, and model, and the paper notes: \"Interestingly, some models are more prone to erroneous behaviors for some transformations than others.\" (p.311, Section 5, discussion around Tables 7\u20138).; \u2022 Metamorphic-oracle formulation used to detect failures (Section 3.4): \"We redefine a new metamorphic relation as: (\u02c6\u03b8i \u2212 \u03b8ti)^2 \u2264 \u03bb MSEorig\" (p.307, Section 3.4).; \u2022 Empirical link to training-data coverage (retraining experiment): \"In all cases, the accuracy of the retrained model improved significantly over the original model as shown in Table 9.\" (p.311, Section 5, RQ5 and Table 9) \u2014 this provides an empirical connection suggesting some failures relate to missing data conditions. ; \u2022 Manual inspection of false positives: \"We also manually checked the bugs reported in Table 8 and report the false positives in Figure 8.\" (p.311, Section 5)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Shows correlation between coverage and errors",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "DeepTest explicitly restricts individual mutation steps to realistic, domain-specific image transformations (brightness, contrast, affine transforms, blur, rain/fog composed with Photoshop filters) and specifies bounded parameter ranges (Table 4). It also applies a quantitative filter (Equation 3) and manual inspection to reduce implausible / false-positive reports, i.e., it validates plausibility of final fault-inducing inputs. However, the fuzzing algorithm (Algorithm 1) composes multiple transformations without an explicit mechanism that enforces plausibility across cumulative iterations (no distributional constraint or perceptual-similarity metric is enforced during generation), and the authors acknowledge that generated images \u201cmay not be exactly reproducible in reality.\u201d Thus DeepTest validates final inputs and constrains single-step mutations, but it does not fully prevent or measure cumulative drift with perceptual metrics or a human study, matching the paper to the Medium level.",
                    "evidence": "Section 3.2 (p.306): \"we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect).\"; Table 4 (p.306): lists bounded parameter ranges for each transformation (e.g., Translation (tx, ty) (10, 10) to (100, 100) step (10, 10); Rotation q 3 to 30 with step 3; Contrast \u03b1 1.2 to 3.0 step 0.2; Brightness \u03b2 10 to 100 step 10), showing per-step constraints; Algorithm 1 (Section 3.3, p.307): shows composition of transformations across iterations (e.g., \"Randomly pick parameter P1 for T1 ... newImage = ApplyTransforms(image, T1, P1, T2, P2)\"), indicating cumulative mutation without an explicit plausibility-enforcing constraint across iterations; Section 3.4 (p.309\u2013310): defines metamorphic oracle and filtering: \"(\u02c6\u03b8i \u2212 \u03b8t_i)^2 \u2264 \u03bb MSEorig\" (Eqn.2) and the filtration criterion \"| MSE(trans,param) \u2212 MSEorig | \u2264 \u03f5 (3)\" and states \"Thus, we only choose the transformations that obey Equation 3 for counting erroneous behaviors.\" \u2014 this is a quantitative validation/filter applied to final inputs; Section 6 Threats to Validity (p.311): \"DeepTest generates realistic synthetic images by applying different image transformations on the seed images. However, these transformations are not designed to be exhaustive and therefore may not cover all realistic cases... the generated pictures may not be exactly reproducible in reality due to a large number of unpredictable factors...\"; Section 5 (p.311): \"We also manually checked the bugs reported in Table 8 and report the false positives in Figure 8... all the authors have reviewed the images and agreed on the false positives.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The method is explicitly designed to generate inputs by mimicking realistic, real-world phenomena. The transformations applied to seed images include weather effects (rain, fog), lighting changes (brightness, contrast), and physical changes (rotation, shear, scaling) intended to be plausible. This enforces plausibility at the level of individual mutation steps. However, the paper's greedy algorithm for combining these transformations is driven solely by increasing neuron coverage, without an explicit mechanism to check if the combined transformations result in a plausible image, thus potentially overlooking cumulative effects. The paper does perform a final assessment of the plausibility of the failure-inducing inputs, but this is an informal, manual check by the authors rather than a formal human study or the use of quantitative perceptual metrics.",
                    "evidence": "Page 5, Section 3.2, 'Increasing Coverage with Synthetic Images': 'Therefore, DeepTest focuses on generating realistic synthetic images by applying image transformations on seed images and mimic different real-world phenomena like camera lens distortions, object movements, different weather conditions, etc. To this end, we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect).' \nPage 6, Section 3.3, 'Algorithm 1': The greedy search algorithm for combining transformations checks only for coverage increase ('if covInc(newimage) then') and does not include a plausibility check for the resulting 'newImage'.\nPage 10, Section 5, 'RQ4': 'We also manually checked the bugs reported in Table 8 and report the false positives in Figure 8. It also shows two synthetic images (the corresponding original images) where DeepTest incorrectly reports erroneous behaviors while the model\u2019s output is indeed safe. Although such manual investigation is, by definition, subjective and approximate, all the authors have reviewed the images and agreed on the false positives.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "DeepTest explicitly restricts individual mutation steps to realistic, domain-specific image transformations (brightness, contrast, affine transforms, blur, rain/fog composed with Photoshop filters) and specifies bounded parameter ranges (Table 4). It also applies a quantitative filter (Equation 3) and manual inspection to reduce implausible / false-positive reports, i.e., it validates plausibility of final fault-inducing inputs. However, the fuzzing algorithm (Algorithm 1) composes multiple transformations without an explicit mechanism that enforces plausibility across cumulative iterations (no distributional constraint or perceptual-similarity metric is enforced during generation), and the authors acknowledge that generated images \u201cmay not be exactly reproducible in reality.\u201d Thus DeepTest validates final inputs and constrains single-step mutations, but it does not fully prevent or measure cumulative drift with perceptual metrics or a human study, matching the paper to the Medium level.",
                    "evidence": "Section 3.2 (p.306): \"we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect).\"; Table 4 (p.306): lists bounded parameter ranges for each transformation (e.g., Translation (tx, ty) (10, 10) to (100, 100) step (10, 10); Rotation q 3 to 30 with step 3; Contrast \u03b1 1.2 to 3.0 step 0.2; Brightness \u03b2 10 to 100 step 10), showing per-step constraints; Algorithm 1 (Section 3.3, p.307): shows composition of transformations across iterations (e.g., \"Randomly pick parameter P1 for T1 ... newImage = ApplyTransforms(image, T1, P1, T2, P2)\"), indicating cumulative mutation without an explicit plausibility-enforcing constraint across iterations; Section 3.4 (p.309\u2013310): defines metamorphic oracle and filtering: \"(\u02c6\u03b8i \u2212 \u03b8t_i)^2 \u2264 \u03bb MSEorig\" (Eqn.2) and the filtration criterion \"| MSE(trans,param) \u2212 MSEorig | \u2264 \u03f5 (3)\" and states \"Thus, we only choose the transformations that obey Equation 3 for counting erroneous behaviors.\" \u2014 this is a quantitative validation/filter applied to final inputs; Section 6 Threats to Validity (p.311): \"DeepTest generates realistic synthetic images by applying different image transformations on the seed images. However, these transformations are not designed to be exhaustive and therefore may not cover all realistic cases... the generated pictures may not be exactly reproducible in reality due to a large number of unpredictable factors...\"; Section 5 (p.311): \"We also manually checked the bugs reported in Table 8 and report the false positives in Figure 8... all the authors have reviewed the images and agreed on the false positives.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Does not verify the naturalness of the fault-revealing mutated images through any human study or algorithmic naturalness metric. Individual transformations are naturalistic but chained mutations might not be",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "DeepTest generates test inputs by applying semantic, metamorphic image transformations (brightness, contrast, affine transforms, blur, rain/fog, etc.) and uses those transformed images as tests. This approach produces coarse, perceptible changes that are likely to survive standard image serialization, and the paper explicitly uses metamorphic relations as the oracle. However, the paper does not explicitly reason about or constrain low-level numerical effects of serialization (quantization, rounding, clipping) nor does it describe steps to ensure that fault-inducing perturbations survive typical I/O formats. Therefore DeepTest partially considers I/O effects via use of semantic image transformations (which tends to improve reproducibility) but does not explicitly enforce or evaluate reproducibility under serialization, matching the definition of the metric's Medium level.",
                    "evidence": "Page 306, Section 3.2 (Increasing Coverage with Synthetic Images): \"Therefore, DeepTest focuses on generating realistic synthetic images by applying image transformations on seed images and mimic different real-world phenomena like camera lens distortions, object movements, different weather conditions, etc. To this end, we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect).\"; Page 307, Section 4 (Implementation) / \"Image transformations\": \"In the experiments for RQ2 and RQ3, we leverage seven different types of simple image transformations: translation, scaling, horizontal shearing, rotation, contrast adjustment, brightness adjustment, and blurring. We use OpenCV to implement these transformations [7].\"; Page 307, Section 3.4 (Creating a Test Oracle with Metamorphic Relations): \"The key insight is that even though it is hard to specify the correct behavior of a self-driving car for every transformed image, one can define relationships between the car\u2019s behaviors across certain types of transformations. For example, the autonomous car\u2019s steering angle should not change significantly for the same image under any lighting/weather conditions, blurring, or any affine transformations with small parameter values.\"; Page 311, Section 6 (Threats to Validity): \"While our transformations like rain and fog effects are designed to be realistic, the generated pictures may not be exactly reproducible in reality due to a large number of unpredictable factors, e.g., the position of the sun, the angle and size of the rain drops. etc.\"; Page 304, Introduction (Contributions): \"We have made the erroneous behaviors detected by DeepTest available at https://deeplearningtest.github.io/deepTest/. We also plan to release the generated test images and the source of DeepTest for public use.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, DeepTest, generates new test inputs by applying semantic-preserving metamorphic transformations to seed images. These transformations include brightness/contrast adjustment, rotation, scaling, blurring, and adding synthetic fog or rain effects. According to the metric definition, using such metamorphic transformations without explicitly handling serialization effects (like quantization, rounding, or clipping) provides only partial enforcement of failure reproducibility. The paper describes applying these transformations using libraries like OpenCV and Photoshop filters but does not mention any steps to ensure that the resulting numerical perturbations in the image data would persist after standard I/O operations (e.g., saving to a PNG or JPEG file). There is no discussion of how floating-point pixel values are handled or whether they are constrained to a specific bit depth, meaning fault-inducing perturbations could be lost or altered upon serialization. This approach aligns directly with the 'Medium' rating.",
                    "evidence": "Page 5, Section 3.2, Increasing Coverage with Synthetic Images: \"Therefore, DeepTest focuses on generating realistic synthetic images by applying image transformations on seed images and mimic different real-world phenomena like camera lens distortions, object movements, different weather conditions, etc. To this end, we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect). These transformations can be classified into three groups: linear, affine, and convolutional.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "DeepTest generates test inputs by applying semantic, metamorphic image transformations (brightness, contrast, affine transforms, blur, rain/fog, etc.) and uses those transformed images as tests. This approach produces coarse, perceptible changes that are likely to survive standard image serialization, and the paper explicitly uses metamorphic relations as the oracle. However, the paper does not explicitly reason about or constrain low-level numerical effects of serialization (quantization, rounding, clipping) nor does it describe steps to ensure that fault-inducing perturbations survive typical I/O formats. Therefore DeepTest partially considers I/O effects via use of semantic image transformations (which tends to improve reproducibility) but does not explicitly enforce or evaluate reproducibility under serialization, matching the definition of the metric's Medium level.",
                    "evidence": "Page 306, Section 3.2 (Increasing Coverage with Synthetic Images): \"Therefore, DeepTest focuses on generating realistic synthetic images by applying image transformations on seed images and mimic different real-world phenomena like camera lens distortions, object movements, different weather conditions, etc. To this end, we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect).\"; Page 307, Section 4 (Implementation) / \"Image transformations\": \"In the experiments for RQ2 and RQ3, we leverage seven different types of simple image transformations: translation, scaling, horizontal shearing, rotation, contrast adjustment, brightness adjustment, and blurring. We use OpenCV to implement these transformations [7].\"; Page 307, Section 3.4 (Creating a Test Oracle with Metamorphic Relations): \"The key insight is that even though it is hard to specify the correct behavior of a self-driving car for every transformed image, one can define relationships between the car\u2019s behaviors across certain types of transformations. For example, the autonomous car\u2019s steering angle should not change significantly for the same image under any lighting/weather conditions, blurring, or any affine transformations with small parameter values.\"; Page 311, Section 6 (Threats to Validity): \"While our transformations like rain and fog effects are designed to be realistic, the generated pictures may not be exactly reproducible in reality due to a large number of unpredictable factors, e.g., the position of the sun, the angle and size of the rain drops. etc.\"; Page 304, Introduction (Contributions): \"We have made the erroneous behaviors detected by DeepTest available at https://deeplearningtest.github.io/deepTest/. We also plan to release the generated test images and the source of DeepTest for public use.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Performs semantic, real-world image transformations (brightness, contrast, blur, fog, rain, etc.) but does not ensure IO stability using clipping and rounding",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across models. DeepTest generates synthetic tests and reports erroneous behaviors for three different DNNs, but it treats each model's testing separately and does not (a) reuse inputs crafted on one model to test other models, (b) measure overlap of discovered faults across models, or (c) include design elements explicitly aimed at finding transferable attacks. The authors explicitly contrast their approach with prior differential-testing work (DeepXplore) that used multiple DNNs; DeepTest is presented as a single-model, metamorphic-relation-driven approach rather than a substitute-model transfer-attack methodology.",
                    "evidence": "\u2022 Paper objective / scope (multiple models tested but separately): \u201cWe use DeepTest to systematically test three top performing DNN models from the Udacity driving challenge.\u201d (Abstract / Contributions, p.304). \n\n\u2022 Explicit distinction from multi-model differential testing (shows DeepTest is single-model focused): \u201cPei et al. [70] proposed DeepXplore, a whitebox differential testing algorithm for systematically finding inputs that can trigger inconsistencies between multiple DNNs. \u2026 By contrast, our graybox methods use neuron coverage for guided test generation in a single DNN and leverage metamorphic relations to identify erroneous behaviors without requiring multiple DNNs.\u201d (Related Work \u2014 Testing and verification of machine learning / DeepXplore comparison, p.312). \n\n\u2022 Results present per-model erroneous behaviors but no cross-model reuse/transfer experiments or overlap analysis: e.g., Tables and text report errors per model (Table 8: \u201cNumber of unique erroneous behaviors reported by DeepTest for different models...\u201d and surrounding discussion) and aggregate counts, but contain no experiment where inputs found on one model are applied to other models to test transferability (Results / RQ4 and Table 8 discussion, pp.310\u2013311)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper introduces DeepTest, a method for finding erroneous behaviors in a single Deep Neural Network (DNN) by generating synthetic inputs that maximize neuron coverage. The evaluation is performed on three separate DNN models (Chauffeur, Rambo, and Epoch) that perform the same task of autonomous driving. However, the paper's methodology and experiments focus on testing each model in isolation. The results are presented separately for each model, highlighting which types of transformations induce errors in which models. There is no experiment or discussion where a fault-inducing input generated for one model is then used to test another model to check for transferability. The paper does not demonstrate whether the discovered failures are model-specific or shared across different architectures.",
                    "evidence": "Page 10, Section 5. RESULTS, Table 8: 'Number of unique erroneous behaviors reported by DeepTest for different models with \u03bb = 5'. The accompanying text states: 'Interestingly, some models are more prone to erroneous behaviors for some transformations than others. For example, Rambo produces 23 erroneous cases for shear, while the other two models do not show any such cases. Similarly, DeepTest finds 650 instances of erroneous behavior in Chauffeur for rain but only 64 and 27 for Epoch and Rambo respectively.' This evidence shows that bugs were identified and counted for each model independently, rather than testing if a bug found in one model transfers to others."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across models. DeepTest generates synthetic tests and reports erroneous behaviors for three different DNNs, but it treats each model's testing separately and does not (a) reuse inputs crafted on one model to test other models, (b) measure overlap of discovered faults across models, or (c) include design elements explicitly aimed at finding transferable attacks. The authors explicitly contrast their approach with prior differential-testing work (DeepXplore) that used multiple DNNs; DeepTest is presented as a single-model, metamorphic-relation-driven approach rather than a substitute-model transfer-attack methodology.",
                    "evidence": "\u2022 Paper objective / scope (multiple models tested but separately): \u201cWe use DeepTest to systematically test three top performing DNN models from the Udacity driving challenge.\u201d (Abstract / Contributions, p.304). \n\n\u2022 Explicit distinction from multi-model differential testing (shows DeepTest is single-model focused): \u201cPei et al. [70] proposed DeepXplore, a whitebox differential testing algorithm for systematically finding inputs that can trigger inconsistencies between multiple DNNs. \u2026 By contrast, our graybox methods use neuron coverage for guided test generation in a single DNN and leverage metamorphic relations to identify erroneous behaviors without requiring multiple DNNs.\u201d (Related Work \u2014 Testing and verification of machine learning / DeepXplore comparison, p.312). \n\n\u2022 Results present per-model erroneous behaviors but no cross-model reuse/transfer experiments or overlap analysis: e.g., Tables and text report errors per model (Table 8: \u201cNumber of unique erroneous behaviors reported by DeepTest for different models...\u201d and surrounding discussion) and aggregate counts, but contain no experiment where inputs found on one model are applied to other models to test transferability (Results / RQ4 and Table 8 discussion, pp.310\u2013311)."
                },
                "manual": {
                    "value": "Low",
                    "why": "No cross model failure analysis",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Xian Yuen et al. - 2023 - ASDF A Differential Testing Framework for Automatic Speech Recognition Systems": {
        "key": "yuen2023asdf",
        "name": "ASDF",
        "year": "2023",
        "citation_count": "8",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper describes ASDF as a differential testing framework that identifies instances where an ASR system's transcription does not match the original text (i.e., mispredictions / transcription errors) and performs phonetic analysis to find error-prone phonemes. The tool's goals, methods, and metrics focus on generating more test cases and measuring transcription failures and phoneme-level weaknesses. The paper contains no description or claim that ASDF is designed to bypass safety, policy, or security mechanisms in defended or safety-aligned models, nor does it report uncovering high-severity behaviors (e.g., jailbreaks, data leakage, unauthorized actions) or testing models with explicit defense/safety mechanisms. Therefore the failures it uncovers correspond to standard robustness/misprediction errors rather than high- or medium-severity safety/security violations.",
                    "evidence": "Page 461, I. INTRODUCTION: \"Intuitively, the transcription produced by an ASR system should be equivalent to the text used to generate the audio. Otherwise, a failed test case for an ASR system is uncovered.\"; Page 462, IV. USAGE: \"A failed text is defined as an audio file that is incorrectly transcribed by at least one ASR system.\"; Page 462, III. CORE FUNCTIONALITIES: \"The ASR systems currently available in ASDF are DeepSpeech [19], wav2letter [20], and wav2vec2 [21].\" (tooling described focuses on transcription comparison and phonetic analysis; no sections or quotes describe bypassing defenses, jailbreaks, data leakage, or testing of safety-aligned/defended models)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper describes ASDF, a framework for testing Automatic Speech Recognition (ASR) systems. The failures it aims to uncover are incorrect transcriptions. ASDF generates test cases by applying various text transformations (e.g., changing tense, substituting homophones) to an initial text, converting it to audio, and then checking if the ASR's transcription matches the original text. A failure is explicitly defined as a mismatch between the transcription and the source text. This process fits the definition of uncovering \"model robustness errors, such as mispredictions or prediction inconsistencies under semantically preserving input mutations.\" The paper does not mention any goal or capability related to uncovering unsafe, biased, or toxic outputs (Medium severity), nor does it discuss bypassing any safety or security mechanisms (High severity). The entire focus is on the correctness and robustness of the speech-to-text transcription.",
                    "evidence": "Page 2, Section IV. USAGE: \"A failed test case is defined as a specific text output from an individual ASR service that does not match its corresponding input text.\" and \"A failed text is defined as an audio file that is incorrectly transcribed by at least one ASR system.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper describes ASDF as a differential testing framework that identifies instances where an ASR system's transcription does not match the original text (i.e., mispredictions / transcription errors) and performs phonetic analysis to find error-prone phonemes. The tool's goals, methods, and metrics focus on generating more test cases and measuring transcription failures and phoneme-level weaknesses. The paper contains no description or claim that ASDF is designed to bypass safety, policy, or security mechanisms in defended or safety-aligned models, nor does it report uncovering high-severity behaviors (e.g., jailbreaks, data leakage, unauthorized actions) or testing models with explicit defense/safety mechanisms. Therefore the failures it uncovers correspond to standard robustness/misprediction errors rather than high- or medium-severity safety/security violations.",
                    "evidence": "Page 461, I. INTRODUCTION: \"Intuitively, the transcription produced by an ASR system should be equivalent to the text used to generate the audio. Otherwise, a failed test case for an ASR system is uncovered.\"; Page 462, IV. USAGE: \"A failed text is defined as an audio file that is incorrectly transcribed by at least one ASR system.\"; Page 462, III. CORE FUNCTIONALITIES: \"The ASR systems currently available in ASDF are DeepSpeech [19], wav2letter [20], and wav2vec2 [21].\" (tooling described focuses on transcription comparison and phonetic analysis; no sections or quotes describe bypassing defenses, jailbreaks, data leakage, or testing of safety-aligned/defended models)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The ASDF framework performs untargeted differential testing: it generates audio from input texts (including transformed variants) and flags failures when an ASR's transcription does not match the original text or when one ASR succeeds while another fails. The tool's transformations (tense changes, homophone substitution, augmentation, etc.) are used to expand the input space and increase the number of mis-transcriptions discovered, not to steer the ASR toward specific attacker-chosen outputs or phrases. There is no mechanism described that aims to induce a particular predetermined output (e.g., a chosen phrase, command, or label); instead the oracle checks for mismatches versus the input text or cross-referenced ASR outputs, consistent with untargeted exploration.",
                    "evidence": "Abstract (page 461): \"ASDF generates more high-quality test cases by applying various text transformation methods (e.g., changing tense) to the input text in a failed test case.\"; II. TOOL DESIGN (page 461): \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\"; IV. USAGE (page 462): \"A failed text is defined as an audio file that is incorrectly transcribed by at least one ASR system.\" and \"Cross-referencing is used to filter the low-quality, unrealistic texts resulting from text transformation. A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\"; III. CORE FUNCTIONALITIES (page 462): \"The Homophone Transformation method first identifies the homophone of the error-inducing term of the failed test case... This new sentence will be used as the new test case.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The framework performs untargeted exploration to discover generic failures in Automatic Speech Recognition (ASR) systems. The objective is to find any discrepancy between the input text used to generate an audio file and the transcription produced by an ASR system. The system's oracle is based on differential testing, where a failure is defined as an incorrect transcription by one system that is correctly transcribed by at least one other system. The paper's text transformation modules are designed to generate more test cases that are likely to cause *any* transcription error, not to steer the output toward a specific, predefined phrase or a broader class of security-violating behavior. The entire process is aimed at uncovering general inaccuracies and inconsistencies.",
                    "evidence": "Page 462, Section IV. USAGE: 'A failed test case is defined as a specific text output from an individual ASR service that does not match its corresponding input text.' This quote explicitly defines a failure as any mismatch with the ground truth, which is characteristic of untargeted exploration for generic faults. The differential testing approach is described on Page 461, Section II. TOOL DESIGN: 'If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.' This establishes the oracle as one that finds inconsistencies rather than guiding toward a specific target output."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The ASDF framework performs untargeted differential testing: it generates audio from input texts (including transformed variants) and flags failures when an ASR's transcription does not match the original text or when one ASR succeeds while another fails. The tool's transformations (tense changes, homophone substitution, augmentation, etc.) are used to expand the input space and increase the number of mis-transcriptions discovered, not to steer the ASR toward specific attacker-chosen outputs or phrases. There is no mechanism described that aims to induce a particular predetermined output (e.g., a chosen phrase, command, or label); instead the oracle checks for mismatches versus the input text or cross-referenced ASR outputs, consistent with untargeted exploration.",
                    "evidence": "Abstract (page 461): \"ASDF generates more high-quality test cases by applying various text transformation methods (e.g., changing tense) to the input text in a failed test case.\"; II. TOOL DESIGN (page 461): \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\"; IV. USAGE (page 462): \"A failed text is defined as an audio file that is incorrectly transcribed by at least one ASR system.\" and \"Cross-referencing is used to filter the low-quality, unrealistic texts resulting from text transformation. A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\"; III. CORE FUNCTIONALITIES (page 462): \"The Homophone Transformation method first identifies the homophone of the error-inducing term of the failed test case... This new sentence will be used as the new test case.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight through statistical and input-level analysis (phonetic analysis and reporting of phoneme frequencies) that helps characterize failure patterns across inputs, but it does not perform or report analysis of internal model signals (e.g., neuron activations, layer coverage) nor does it link discovered failures to deeper underlying model vulnerabilities such as dataset bias, spurious features, or overfitting. Thus it meets the criterion for Medium: statistical/input-structure analysis is present, but higher-level causal linkage to model vulnerabilities is absent.",
                    "evidence": "Page 461, Abstract: \"ASDF ... (2) a phonetic analysis module to identify phonemes that the ASR systems tend to transcribe incorrectly.\"; Page 461, II. TOOL DESIGN: \"Lastly, phonetic analysis is performed on the failed test cases from both iterations. The phonetics of the error-inducing terms found in the test cases will be analyzed to determine the most common phonemes that appear in the failed test cases. Analysis of highly occurring phonemes in error-inducing terms can be utilized to identify the phonemes that are more challenging for ASR systems to transcribe.\"; Page 462, IV. USAGE: \"Ten metrics are used to evaluate the performance of the ASR systems. A bar chart is also plotted to show the phonemes and their frequency of appearance in error-inducing terms.\"; Page 462, V. CHALLENGES AND FUTURE WORK: (no model-internal diagnostics described) \"ASDF generates an audio test suite using a TTS library... there would still be differences between human speech and computer-generated audio... More sophisticated text transformation strategies can be incorporated...\" (indicating focus on input-level transformations rather than analysis of model internals or linking failures to dataset bias/overfitting)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by performing a statistical analysis of failure patterns. The proposed tool, ASDF, includes a 'phonetic analysis module' that systematically identifies and reports the phonemes most frequently present in the error-inducing terms of failed test cases. This analysis aims to pinpoint which specific sounds are more challenging for the ASR systems to transcribe, thus providing developers with actionable patterns of failure. However, the analysis does not extend to linking these phonetic failures to underlying model vulnerabilities like dataset bias or overfitting, nor does it analyze observable model internals such as neuron activation or coverage. It focuses on characterizing the failures at the input (phoneme) level, which aligns with the 'statistical analysis of failure patterns' criterion for a Medium rating.",
                    "evidence": "Page 2, Section II. Tool Design: \"Lastly, phonetic analysis is performed on the failed test cases from both iterations. The phonetics of the error-inducing terms found in the test cases will be analyzed to determine the most common phonemes that appear in the failed test cases. Analysis of highly occurring phonemes in error-inducing terms can be utilized to identify the phonemes that are more challenging for ASR systems to transcribe.\"\n\nPage 2, Section IV. Usage: \"A bar chart is also plotted to show the phonemes and their frequency of appearance in error-inducing terms.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight through statistical and input-level analysis (phonetic analysis and reporting of phoneme frequencies) that helps characterize failure patterns across inputs, but it does not perform or report analysis of internal model signals (e.g., neuron activations, layer coverage) nor does it link discovered failures to deeper underlying model vulnerabilities such as dataset bias, spurious features, or overfitting. Thus it meets the criterion for Medium: statistical/input-structure analysis is present, but higher-level causal linkage to model vulnerabilities is absent.",
                    "evidence": "Page 461, Abstract: \"ASDF ... (2) a phonetic analysis module to identify phonemes that the ASR systems tend to transcribe incorrectly.\"; Page 461, II. TOOL DESIGN: \"Lastly, phonetic analysis is performed on the failed test cases from both iterations. The phonetics of the error-inducing terms found in the test cases will be analyzed to determine the most common phonemes that appear in the failed test cases. Analysis of highly occurring phonemes in error-inducing terms can be utilized to identify the phonemes that are more challenging for ASR systems to transcribe.\"; Page 462, IV. USAGE: \"Ten metrics are used to evaluate the performance of the ASR systems. A bar chart is also plotted to show the phonemes and their frequency of appearance in error-inducing terms.\"; Page 462, V. CHALLENGES AND FUTURE WORK: (no model-internal diagnostics described) \"ASDF generates an audio test suite using a TTS library... there would still be differences between human speech and computer-generated audio... More sophisticated text transformation strategies can be incorporated...\" (indicating focus on input-level transformations rather than analysis of model internals or linking failures to dataset bias/overfitting)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Performs phonetic analysis to correlated phonemes with errors.",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces a plausibility filter on transformed inputs by requiring that any transformed text be transcribed correctly by at least one ASR system (cross-referencing) before it is considered valid, which is an explicit validation of final mutated inputs. However, the paper does not report any human evaluation or task-specific perceptual/fluency metrics (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for final fault-inducing audio/text, nor does it describe mechanisms that bound or control cumulative drift across multiple mutation iterations. The authors also acknowledge limitations of using TTS-generated audio versus real human speech. Together, this matches the metric\u2019s \u201cMedium\u201d level: plausibility of final inputs is validated, but cumulative-effects and empirical perceptual assessments are not addressed.",
                    "evidence": "Section IV (USAGE), p.462: \"It is important to note that cross-referencing is used to filter the low-quality, unrealistic texts resulting from text transformation. A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\" Section V (CHALLENGES AND FUTURE WORK), p.462: \"ASDF generates an audio test suite using a TTS library. This assumes that the generated audio is interchangeable with the real-world speech of humans, which may not be the case in real-life situations. There would still be differences between human speech and computer-generated audio, namely pronunciation, accent, and tone. As a result, if human speech were to be used instead of computer-generated audio, such inconsistencies may yield different results for our differential testing.\" (Also: Section III / Abstract describe text transformations such as tense change and homophone substitution used to generate test cases, but no human study or perceptual/fluency metrics are reported in the paper or the described evaluation.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper does not provide empirical evidence, such as human studies or quantitative perceptual metrics (e.g., PESQ, STOI), to confirm that the final fault-inducing audio inputs are realistic. In fact, the authors explicitly acknowledge this as a limitation, stating that the Text-To-Speech (TTS) generated audio may not be interchangeable with real human speech. However, the method does not completely ignore plausibility. It incorporates an indirect validation mechanism through cross-referencing, where a transformed input is only considered valid if at least one ASR system under test can transcribe it correctly. This step is explicitly designed to filter out \"low-quality, unrealistic texts.\" This filtering of final inputs aligns with the criteria for a 'Medium' rating, as it constitutes a form of plausibility validation without enforcing it throughout the generation process or providing direct realism measurements.",
                    "evidence": "Page 2, Section IV. USAGE: \"It is important to note that cross-referencing is used to filter the low-quality, unrealistic texts resulting from text transformation. A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\"\n\nPage 2, Section V. CHALLENGES AND FUTURE WORK: \"ASDF generates an audio test suite using a TTS library. This assumes that the generated audio is interchangeable with the real-world speech of humans, which may not be the case in real-life situations. There would still be differences between human speech and computer-generated audio, namely pronunciation, accent, and tone.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces a plausibility filter on transformed inputs by requiring that any transformed text be transcribed correctly by at least one ASR system (cross-referencing) before it is considered valid, which is an explicit validation of final mutated inputs. However, the paper does not report any human evaluation or task-specific perceptual/fluency metrics (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for final fault-inducing audio/text, nor does it describe mechanisms that bound or control cumulative drift across multiple mutation iterations. The authors also acknowledge limitations of using TTS-generated audio versus real human speech. Together, this matches the metric\u2019s \u201cMedium\u201d level: plausibility of final inputs is validated, but cumulative-effects and empirical perceptual assessments are not addressed.",
                    "evidence": "Section IV (USAGE), p.462: \"It is important to note that cross-referencing is used to filter the low-quality, unrealistic texts resulting from text transformation. A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\" Section V (CHALLENGES AND FUTURE WORK), p.462: \"ASDF generates an audio test suite using a TTS library. This assumes that the generated audio is interchangeable with the real-world speech of humans, which may not be the case in real-life situations. There would still be differences between human speech and computer-generated audio, namely pronunciation, accent, and tone. As a result, if human speech were to be used instead of computer-generated audio, such inconsistencies may yield different results for our differential testing.\" (Also: Section III / Abstract describe text transformations such as tense change and homophone substitution used to generate test cases, but no human study or perceptual/fluency metrics are reported in the paper or the described evaluation.)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Synthesized inputs might not be natural and no measure of naturnaless provided",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "ASDF generates test inputs as serialized audio files via a TTS engine (gTTS) and feeds those audio files into ASR systems for evaluation. Because inputs are produced and saved in a serialized audio format before being consumed by the oracle (the ASR systems), the fault-inducing perturbations are inherently preserved across standard I/O serialization; the paper therefore produces reproducible failures under standard I/O operations.",
                    "evidence": "Page 461, Section II (Tool Design): \"A Text-To-Speech (TTS) service is used to generate audio from the text, which is then fed into the ASR systems to obtain their transcriptions.\"; Page 462, Section III (Core Functionalities): \"Each line of text in the file is converted into an audio file by gTTS [18].\"; Page 462, Section V (Challenges and Future Work): \"ASDF generates an audio test suite using a TTS library.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, ASDF, generates test inputs by synthesizing audio directly from text using a Text-To-Speech (TTS) service. This process creates inputs in a standard, serialized audio format. The metric definition explicitly states that approaches generating inputs 'directly in serialized formats (e.g., synthesized speech...)' are considered reproducible and meet the 'High' criteria. The mutations in ASDF are performed at the text level (e.g., changing tense, substituting homophones) before the audio is ever generated. This means the fault-inducing properties are embedded in the content of the speech itself, not as fragile numerical perturbations to an audio waveform that could be lost during I/O operations like quantization or rounding. The generated audio file is inherently stable, ensuring that any failure it induces is reproducible.",
                    "evidence": "Page 2, Section III. CORE FUNCTIONALITIES: 'The input text corpus should be in the format of a .txt file. Each line of text in the file is converted into an audio file by gTTS [18].'\n\nPage 1, Section I. INTRODUCTION: 'After collecting a small initial set of failed test cases, ASDF transforms the failed texts using various text transformation methods to further generate more test cases. For example, changing the tense of a sentence or substituting error-inducing terms with other words that have similar phonemes.'\n\nPage 1, Section II. TOOL DESIGN: 'A Text-To-Speech (TTS) service is used to generate audio from the text, which is then fed into the ASR systems to obtain their transcriptions.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "ASDF generates test inputs as serialized audio files via a TTS engine (gTTS) and feeds those audio files into ASR systems for evaluation. Because inputs are produced and saved in a serialized audio format before being consumed by the oracle (the ASR systems), the fault-inducing perturbations are inherently preserved across standard I/O serialization; the paper therefore produces reproducible failures under standard I/O operations.",
                    "evidence": "Page 461, Section II (Tool Design): \"A Text-To-Speech (TTS) service is used to generate audio from the text, which is then fed into the ASR systems to obtain their transcriptions.\"; Page 462, Section III (Core Functionalities): \"Each line of text in the file is converted into an audio file by gTTS [18].\"; Page 462, Section V (Challenges and Future Work): \"ASDF generates an audio test suite using a TTS library.\""
                },
                "manual": {
                    "value": "High",
                    "why": "The speech input is generated using TTS library which are fed to the ASR model. There is not in memory mutation on the audio.",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper describes a differential testing framework that runs multiple ASR systems in parallel and uses cross-referencing to identify failed cases, and it leverages failures to generate transformed test texts. However, it does not demonstrate or evaluate whether fault-inducing inputs discovered on one model are reused to test other (unseen) target models, nor does it describe explicit mechanisms or experiments aimed at measuring cross-model transferability of faults. All descriptions pertain to comparing outputs across selected ASR systems within the same test run and generating transformations from observed failures, but no reuse-on-surrogate \u2192 test-on-other-model workflow or transferability analysis is presented.",
                    "evidence": "Page 461, Section II (Tool Design): \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\" \n\nPage 461-462, Section III (Core Functionalities): \"The ASR systems currently available in ASDF are DeepSpeech [19], wav2letter [20], and wav2vec2 [21].\" and \"Cross-referencing is performed amongst the selected ASR systems to ensure that the test cases are valid and can be determined by at least one ASR service.\" \n\nPage 462, Section IV (Usage): \"A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\" \n\n(There are no sections or quotes in the paper that describe generating fault-inducing inputs on a surrogate model and then reusing them to test other unseen target models, nor experiments measuring transferability across models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper describes a differential testing framework, ASDF, where a failure is defined by a disagreement among multiple Automated Speech Recognition (ASR) models. A test case is considered a failure for a specific ASR system only if at least one other system under test transcribes the audio correctly. This design principle means the framework is explicitly built to find model-specific failures, not failures that transfer across different models. The methodology does not involve generating a fault-inducing input on one model and then reusing it to test for transferability on others; rather, it tests a single input on all models simultaneously to find discrepancies. Furthermore, the framework discards inputs that cannot be correctly transcribed by at least one ASR service, which would include cases where a failure might be transferable across all tested models. Therefore, the paper does not demonstrate whether fault-inducing inputs discovered for one model also trigger failures in others.",
                    "evidence": "Page 1, Section II, TOOL DESIGN: \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\" \n\nPage 2, Section IV, USAGE: \"A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper describes a differential testing framework that runs multiple ASR systems in parallel and uses cross-referencing to identify failed cases, and it leverages failures to generate transformed test texts. However, it does not demonstrate or evaluate whether fault-inducing inputs discovered on one model are reused to test other (unseen) target models, nor does it describe explicit mechanisms or experiments aimed at measuring cross-model transferability of faults. All descriptions pertain to comparing outputs across selected ASR systems within the same test run and generating transformations from observed failures, but no reuse-on-surrogate \u2192 test-on-other-model workflow or transferability analysis is presented.",
                    "evidence": "Page 461, Section II (Tool Design): \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\" \n\nPage 461-462, Section III (Core Functionalities): \"The ASR systems currently available in ASDF are DeepSpeech [19], wav2letter [20], and wav2vec2 [21].\" and \"Cross-referencing is performed amongst the selected ASR systems to ensure that the test cases are valid and can be determined by at least one ASR service.\" \n\nPage 462, Section IV (Usage): \"A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\" \n\n(There are no sections or quotes in the paper that describe generating fault-inducing inputs on a surrogate model and then reusing them to test other unseen target models, nor experiments measuring transferability across models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ]
        }
    },
    "Odena et al. - 2019 - TensorFuzz Debugging Neural Networks with Coverage-Guided Fuzzing": {
        "key": "odena2019tensorfuzz",
        "name": "TensorFuzz",
        "year": "2019",
        "citation_count": "426",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper demonstrates that TensorFuzz uncovers robustness and implementation errors (numerical NaNs, model prediction disagreements after quantization, broken loss-function behavior, and unintended semantic changes from code transformations). These are model robustness and software-bug failures rather than violations of safety/policy mechanisms or security-critical behaviors: the authors do not claim discovering jailbreaks, data leakage, bypasses of safety-aligned defenses, or unauthorized actions in defended models. Therefore the failures exposed fit the metric's description of low-severity (robustness/misprediction) failures.",
                    "evidence": "Section 6 (Experimental Results): \"TensorFuzz can efficiently find numerical errors in trained neural networks: ... We focus on finding inputs that result in not-a-number (NaN) values.\"; \"TensorFuzz surfaces disagreements between models and their quantized versions: ... the fuzzer was able to generate disagreements for 70% of the examples we tried.\"; \"TensorFuzz can find real bugs in popular model implementations: ... we hooked up TensorFuzz to the sub-graph defined by the loss function and wrote an objective ... TensorFuzz was able to quickly find a satisfying input.\" (all from Section 6, Experimental Results). Conclusion: \"We have demonstrated the practical utility of TensorFuzz by finding numerical errors, exposing disagreements between neural networks and their quantized versions, surfacing broken loss functions in popular repositories, and making performance improvements to TensorFlow.\" (Conclusion). Nowhere in the paper do the authors claim bypassing explicit safety/policy mechanisms, inducing jailbreaks, data leakage, or other security-critical behaviors (no sections or experiments addressing defended/safety-aligned models or security bypasses)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper demonstrates that TensorFuzz is primarily used to uncover model robustness and correctness errors. The experiments focus on three types of failures: 1) numerical errors (e.g., finding inputs that cause NaN values), 2) prediction inconsistencies (e.g., finding inputs where a model and its quantized version disagree), and 3) implementation bugs (e.g., surfacing issues in loss function implementations). These examples align directly with the definition of 'Low' severity, as they concern robustness and consistency rather than unsafe behavior or the bypassing of explicit safety mechanisms. The paper does not provide any examples of uncovering unsafe, biased, or toxic outputs, nor does it test models with explicit safety alignments or security defenses.",
                    "evidence": "Page 7, Section 6, \"TensorFuzz surfaces disagreements between models and their quantized versions\": \"We then compared the predictions of the 32-bit and the 16-bit model on the MNIST test set and found 0 disagreements... With these settings, the fuzzer was able to generate disagreements for 70% of the examples we tried. Thus, CGF allowed us to find real errors that could have occured at test time.\"\n\nPage 7, Section 6, \"TensorFuzz can efficiently find numerical errors in trained neural networks\": \"We focus on finding inputs that result in not-a-number (NaN) values... As shown in Figure 2, TensorFuzz found NaNs quickly across 10 different random initializations.\"\n\nPage 8, Section 6, \"TensorFuzz can find real bugs in popular model implementations\": \"TensorFuzz was able to quickly find a satsifying input. The input was one in which the output of the discriminator had too high a magnitude, which caused the incorrectly implemented loss function to have saturating gradients.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper demonstrates that TensorFuzz uncovers robustness and implementation errors (numerical NaNs, model prediction disagreements after quantization, broken loss-function behavior, and unintended semantic changes from code transformations). These are model robustness and software-bug failures rather than violations of safety/policy mechanisms or security-critical behaviors: the authors do not claim discovering jailbreaks, data leakage, bypasses of safety-aligned defenses, or unauthorized actions in defended models. Therefore the failures exposed fit the metric's description of low-severity (robustness/misprediction) failures.",
                    "evidence": "Section 6 (Experimental Results): \"TensorFuzz can efficiently find numerical errors in trained neural networks: ... We focus on finding inputs that result in not-a-number (NaN) values.\"; \"TensorFuzz surfaces disagreements between models and their quantized versions: ... the fuzzer was able to generate disagreements for 70% of the examples we tried.\"; \"TensorFuzz can find real bugs in popular model implementations: ... we hooked up TensorFuzz to the sub-graph defined by the loss function and wrote an objective ... TensorFuzz was able to quickly find a satisfying input.\" (all from Section 6, Experimental Results). Conclusion: \"We have demonstrated the practical utility of TensorFuzz by finding numerical errors, exposing disagreements between neural networks and their quantized versions, surfacing broken loss functions in popular repositories, and making performance improvements to TensorFlow.\" (Conclusion). Nowhere in the paper do the authors claim bypassing explicit safety/policy mechanisms, inducing jailbreaks, data leakage, or other security-critical behaviors (no sections or experiments addressing defended/safety-aligned models or security bypasses)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "TensorFuzz is built around user-defined objective functions and is demonstrated to steer exploration toward broader classes of problematic behaviour (e.g., numerical errors/NaNs, model vs quantized-model disagreements, broken loss-function states). The paper does not present or demonstrate steering the fuzzer toward attacker-chosen specific outputs (for example, forcing a classifier to emit a particular label or forcing a speech model to utter a particular phrase). Thus the method supports discovery of broader security- or correctness-violating behaviours rather than targeted, adversary-chosen outputs.",
                    "evidence": "Section 5 (Objective Function), p.4: \"The objective function is used to assess whether that state has been reached. ... The objective function is applied to the metadata arrays, and flags inputs that caused the objective to be satisfied. In this way, the objective function asserts the property to be tested and the fuzzer attempts to find a violation of this property.\" | Section 6 (Experimental Results), p.5: \"TensorFuzz can efficiently find numerical errors in trained neural networks... We focus on finding inputs that result in not-a-number (NaN) values.\" | Section 6 (Experimental Results), p.6: \"TensorFuzz surfaces disagreements between models and their quantized versions... With these settings, the fuzzer was able to generate disagreements for 70% of the examples we tried.\" | No statement in the paper demonstrates or claims steering to attacker-chosen specific outputs (e.g., \"inducing a classifier to predict a chosen label\"), and all concrete examples target classes of failures (NaNs, disagreements, broken loss behavior) rather than predetermined specific outputs."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper presents TensorFuzz as a system for general-purpose debugging of neural networks by combining coverage-guided fuzzing with property-based testing. The user defines an \"objective function\" that specifies a property the system should not violate. All experiments and examples in the paper demonstrate untargeted exploration for generic failures. The objectives are to find numerical errors (NaNs), any disagreement between a model and its quantized version, bugs in loss functions (e.g., high loss with zero gradient), or inconsistencies between two versions of a code implementation. The method is not designed or evaluated for steering exploration toward a specific, predefined model output (like a chosen class label), but rather for discovering any input that causes a generic fault or violates a general property of correctness or consistency.",
                    "evidence": "Page 7, Section \"TensorFuzz surfaces disagreements between models and their quantized versions\": \"We then compared the predictions of the 32-bit and the 16-bit model on the MNIST test set and found 0 disagreements... With these settings, the fuzzer was able to generate disagreements for 70% of the examples we tried.\" This objective is to find any disagreement, not a specific target classification. \nPage 7, Section \"TensorFuzz can efficiently find numerical errors in trained neural networks\": \"We focus on finding inputs that result in not-a-number (NaN) values.\" This objective targets a generic numerical error, not a specific output. \nPage 8, Section \"TensorFuzz can help make semantics-preserving code transformations\": \"First, we 'attached' TensorFuzz to two copies of the old implementation and wrote an objective function asserting that both implementations are the same.\" This is a form of differential testing to find any inconsistency, which is an untargeted objective."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "TensorFuzz\u2019s design includes a user-defined objective function that steers the fuzzing process toward inputs violating specified properties, but all examples in the paper target broad classes of failures (e.g., numerical errors, model/quantization disagreements, broken loss behavior) rather than forcing a model to produce a particular attacker-chosen output. Thus, while it supports discovery of security-relevant behaviors beyond untargeted misclassification, it does not demonstrate steering toward specific, predefined output values.",
                    "evidence": "Section 5 (Objective Function): \u201cThe objective function is used to assess whether that state has been reached\u2026flags inputs that caused the objective to be satisfied.\u201d \nSection 6 (Experimental Results) \u2013 Numerical Errors: \u201cWe focus on finding inputs that result in not-a-number (NaN) values.\u201d \nSection 6 (Experimental Results) \u2013 Quantized Model Disagreements: \u201cWith these settings, the fuzzer was able to generate disagreements for 70% of the examples we tried.\u201d"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Objective function mechanism allows arbitrary user-defined checks to enable failure detection beyond robustness errors",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight based on observable model internals: it adopts activation-level signals (using activations / logits and ANN nearest-neighbor checks) as its coverage metric and uses those internals during analysis. In the experiments the authors inspect internal activations to explain specific failures (e.g., NaNs caused by extreme logits and a broken loss implementation; saturated gradients caused by discriminator outputs of large magnitude). However, the diagnostic work is limited to these concrete cases and to coverage-related choices; the paper does not perform broader statistical failure-pattern analyses (e.g., class-level error concentration, distributional proximity analyses) nor does it link discovered failures to higher-level model vulnerabilities such as reliance on spurious features, dataset bias, or overfitting. This matches the definition of Medium: the paper analyzes observable internals and correlates failures with those signals, but stops short of deeper, general explanations tying failures to broader sources of vulnerability.",
                    "evidence": "Section 5 (Coverage Analyzer): \"we elect to use nearest neighbor algorithms to determine if two sets of neural network 'activations' are meaningfully different from each other. This provides a coverage metric...\"; also \"When fuzzing whole classifiers, we find it is often possible to obtain good results by tracking only the logits, or the layer before the logits.\" (Section 5, Coverage Analyzer). Section 6 (Experimental Results) \u2014 NaNs: \"Inspecting the classifier when exposed to the image on the right, we see that the NaN surfaces because there was one logit with a very positive value and one logit with a very negative value, which broke the loss calculation.\" (Section 6, Numerical errors experiment). Section 6 (Experimental Results) \u2014 DCGAN loss bug: \"The input was one in which the output of the discriminator had too high a magnitude, which caused the incorrectly implemented loss function to have saturating gradients.\" (Section 6, Tests on popular model implementations). Section 6 (Experimental Results) \u2014 Quantization disagreements: \"With these settings, the fuzzer was able to generate disagreements for 70% of the examples we tried.\" (Section 6, Quantization experiment) \u2014 this reports failure incidence but contains no deeper statistical or vulnerability analysis."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight for discovered failures by analyzing observable model internals. For instance, when finding numerical errors (NaNs), the authors inspect the model's state and identify that the failure is caused by specific internal values (one very positive logit and one very negative logit) that break the loss calculation. Similarly, when finding a bug in a DCGAN implementation, they diagnose the failure as being caused by the discriminator's output having too high a magnitude, leading to saturating gradients in the loss function. This level of analysis goes beyond merely reporting failures and explains the immediate mechanical cause by looking at internal model states, which aligns with the 'Medium' criteria. However, it does not connect these failures to deeper underlying vulnerabilities like dataset bias or overfitting, which would be required for a 'High' rating.",
                    "evidence": "Page 7, Section 6: \"Inspecting the classifier when exposed to the image on the right, we see that the NaN surfaces because there was one logit with a very positive value and one logit with a very negative value, which broke the loss calculation.\" \n\nPage 8, Section 6: \"The input was one in which the output of the discriminator had too high a magnitude, which caused the incorrectly implemented loss function to have saturating gradients. In this way, TensorFuzz was able to reproduce a known issue in a real, heavily used piece of TensorFlow code.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight based on observable model internals: it adopts activation-level signals (using activations / logits and ANN nearest-neighbor checks) as its coverage metric and uses those internals during analysis. In the experiments the authors inspect internal activations to explain specific failures (e.g., NaNs caused by extreme logits and a broken loss implementation; saturated gradients caused by discriminator outputs of large magnitude). However, the diagnostic work is limited to these concrete cases and to coverage-related choices; the paper does not perform broader statistical failure-pattern analyses (e.g., class-level error concentration, distributional proximity analyses) nor does it link discovered failures to higher-level model vulnerabilities such as reliance on spurious features, dataset bias, or overfitting. This matches the definition of Medium: the paper analyzes observable internals and correlates failures with those signals, but stops short of deeper, general explanations tying failures to broader sources of vulnerability.",
                    "evidence": "Section 5 (Coverage Analyzer): \"we elect to use nearest neighbor algorithms to determine if two sets of neural network 'activations' are meaningfully different from each other. This provides a coverage metric...\"; also \"When fuzzing whole classifiers, we find it is often possible to obtain good results by tracking only the logits, or the layer before the logits.\" (Section 5, Coverage Analyzer). Section 6 (Experimental Results) \u2014 NaNs: \"Inspecting the classifier when exposed to the image on the right, we see that the NaN surfaces because there was one logit with a very positive value and one logit with a very negative value, which broke the loss calculation.\" (Section 6, Numerical errors experiment). Section 6 (Experimental Results) \u2014 DCGAN loss bug: \"The input was one in which the output of the discriminator had too high a magnitude, which caused the incorrectly implemented loss function to have saturating gradients.\" (Section 6, Tests on popular model implementations). Section 6 (Experimental Results) \u2014 Quantization disagreements: \"With these settings, the fuzzer was able to generate disagreements for 70% of the examples we tried.\" (Section 6, Quantization experiment) \u2014 this reports failure incidence but contains no deeper statistical or vulnerability analysis."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Provides rationale why NaN error happened by inspecting logit value that broke loss calculation",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints on inputs (e.g., restricting inputs to valid shapes/ranges and providing a bounded L\u221e mutation option relative to an original seed) and explicitly uses such bounded perturbations in at least one experiment (the quantization experiment where mutations are restricted to an L\u221e ball). However, the paper does not report any explicit empirical assessment of the perceptual or semantic realism of final fault-inducing inputs (no human study or task-specific perceptual/fluency metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity are provided). Thus TensorFuzz enforces plausibility via bounded/rule-based constraints in mutation design and uses them in experiments, but it does not provide quantitative or human-evaluation evidence that final failing inputs remain perceptually/semantically realistic across fuzzing iterations.",
                    "evidence": "Section 4 (An Overview of the TensorFuzz Library): \"we restrict the inputs to those that are valid neural network inputs. If the inputs are images, we restrict our inputs to have the correct size and shape, and to lie in the same interval as the input pixels of the dataset under consideration. If the inputs are sequences of characters, we only allow characters that are in the vocabulary extracted from the training set.\" \n\nSection 5 (Details \u2014 Mutator): \"For image inputs, we implemented two different types of mutation. The first is to just add white noise of a user-configurable variance to the input. The second is to add white noise, but to constrain the difference between the mutated element and the original element from which it is descended to have a user-configurable L\u221e norm. This type of constrained mutation can be useful if we want to find inputs that satisfy some objective function, but are still plausibly of the same 'class' as the original input that was used as a seed. In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.\" \n\nSection 6 (Experimental Results \u2014 Quantization experiment): \"We then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. We restrict to inputs near seed images because these inputs nearly all have unambiguous class semantics.\" \n\nAbsence of empirical plausibility evaluation: The paper describes mutation constraints and uses bounded perturbations in experiments but contains no section or statements reporting human evaluation or task-specific perceptual/fluency metrics (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) for the final fault-inducing inputs."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper describes a mutation strategy specifically designed to maintain input plausibility by constraining mutations within an L-infinity norm ball around the original seed image. The authors explicitly state this is done to ensure inputs retain \"unambiguous class semantics\" and to avoid testing on \"out-of-domain garbage data\". This method enforces plausibility by bounding the cumulative perturbation from the original seed. However, the paper does not provide any empirical validation, such as a human study or quantitative perceptual quality metrics (e.g., LPIPS, SSIM), to systematically assess whether the final fault-inducing inputs are indeed realistic. It only provides a single visual example (Figure 3). This meets the criteria for \"Medium\" as it enforces plausibility through bounded constraints but does not include an explicit assessment of the final inputs' plausibility.",
                    "evidence": "Page 7, Section 6, \"TensorFuzz surfaces disagreements between models and their quantized versions\": \"We then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. We restrict to inputs near seed images because these inputs nearly all have unambiguous class semantics. It is less interesting if two versions of the model disagree on out-of-domain garbage data with no true class.\"\n\nPage 5, Section 5, \"Mutator\": \"The second is to add white noise, but to constrain the difference between the mutated element and the original element from which it is descended to have a user-configurable L\u221e norm. This type of constrained mutation can be useful if we want to find inputs that satisfy some objective function, but are still plausibly of the same 'class' as the original input that was used as a seed.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints on inputs (e.g., restricting inputs to valid shapes/ranges and providing a bounded L\u221e mutation option relative to an original seed) and explicitly uses such bounded perturbations in at least one experiment (the quantization experiment where mutations are restricted to an L\u221e ball). However, the paper does not report any explicit empirical assessment of the perceptual or semantic realism of final fault-inducing inputs (no human study or task-specific perceptual/fluency metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity are provided). Thus TensorFuzz enforces plausibility via bounded/rule-based constraints in mutation design and uses them in experiments, but it does not provide quantitative or human-evaluation evidence that final failing inputs remain perceptually/semantically realistic across fuzzing iterations.",
                    "evidence": "Section 4 (An Overview of the TensorFuzz Library): \"we restrict the inputs to those that are valid neural network inputs. If the inputs are images, we restrict our inputs to have the correct size and shape, and to lie in the same interval as the input pixels of the dataset under consideration. If the inputs are sequences of characters, we only allow characters that are in the vocabulary extracted from the training set.\" \n\nSection 5 (Details \u2014 Mutator): \"For image inputs, we implemented two different types of mutation. The first is to just add white noise of a user-configurable variance to the input. The second is to add white noise, but to constrain the difference between the mutated element and the original element from which it is descended to have a user-configurable L\u221e norm. This type of constrained mutation can be useful if we want to find inputs that satisfy some objective function, but are still plausibly of the same 'class' as the original input that was used as a seed. In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.\" \n\nSection 6 (Experimental Results \u2014 Quantization experiment): \"We then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. We restrict to inputs near seed images because these inputs nearly all have unambiguous class semantics.\" \n\nAbsence of empirical plausibility evaluation: The paper describes mutation constraints and uses bounded perturbations in experiments but contains no section or statements reporting human evaluation or task-specific perceptual/fluency metrics (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) for the final fault-inducing inputs."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Constrain mutation to norm-bounded but does not validate palusiblity of the final fault-triggering inputs",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's fuzzing design does not explicitly account for standard I/O serialization effects (rounding/quantization/format encoding). The implemented image mutations are adding white noise and optional L\u221e-bounded perturbations followed by clipping to the training input range, but there is no handling or checking that fault-inducing perturbations survive common serialization steps (e.g., 8-bit PNG/JPEG quantization or file encoding). The only use of quantization in the paper is quantizing model weights/activations (32-bit \u2192 16-bit) to compare model disagreement, which is a model-internal quantization rather than input serialization handling. Because TensorFuzz neither constrains mutations to be robust under typical I/O quantization/rounding nor evaluates failures after serialization, failures it finds may rely on fragile in-memory numerical perturbations that would be lost when inputs are serialized. This matches the metric definition for Low (ignores I/O effects during mutation and oracle evaluation).",
                    "evidence": "Section 5 (Mutator): \"For image inputs, we implemented two different types of mutation. The first is to just add white noise ... The second is to add white noise, but to constrain the difference ... to have a user-configurable L\u221e norm. ... In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.\" (Section 5, \"Mutator\")\n\nSection 4 (Overview of the TensorFuzz Library): \"If the inputs are images, we restrict our inputs to have the correct size and shape, and to lie in the same interval as the input pixels of the dataset under consideration.\" (Section 4, \"Overview of the fuzzing procecure\")\n\nSection 6 (Quantization experiment): \"We then truncated all weights and activations to 16-bits. We then compared the predictions of the 32-bit and the 16-bit model ...\" \u2014 this describes quantization of model internals, not input serialization handling. (Section 6, \"TensorFuzz surfaces disagreements between models and their quantized versions\")\n\nNowhere in the paper do the authors describe serializing mutated inputs to common on-disk formats (PNG/JPEG/WAV/MP3) or explicitly modeling/handling rounding, clipping, or encoding effects during test generation or oracle evaluation; the paper only describes clipping to the input value range and L\u221e constraints (Sections 4 and 5). (See Sections 4 and 5 for mutation/clip descriptions and Section 6 for the separate model quantization experiment.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper states that the mutation process for images includes a clipping step to ensure the resulting values remain within the valid range of the training data. This explicitly accounts for one type of serialization effect (clipping). However, the paper does not mention any handling of other common I/O effects like rounding or quantization. Mutations are applied to floating-point representations, and small perturbations (e.g., changing a pixel value from 155.0 to 155.4) would be preserved by clipping but lost when serialized to an 8-bit integer format (which would round it to 155). This partial consideration of I/O effects aligns with the 'Medium' rating, as fault-inducing inputs are not fully guaranteed to be reproducible after standard serialization.",
                    "evidence": "Page 5, Section 5. Details of the TensorFuzz Library, Mutator: \"In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "TensorFuzz\u2019s mutation and oracle procedures operate entirely on floating-point tensors and only clip mutated inputs to lie within the model\u2019s training value range. The paper does not describe serializing mutated inputs to common on-disk formats (e.g. 8-bit PNG/JPEG) nor modeling or enforcing that perturbations survive typical quantization or rounding steps during I/O. Clipping to the floating-point input range is not equivalent to handling integer quantization or encoding effects. Therefore the approach ignores standard I/O effects and may produce failures that vanish after serialization, matching the \u201cLow\u201d category.",
                    "evidence": "Section 5 (\u201cMutator\u201d): \u201cIn both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.\u201d \n\nNo mention in Sections 4\u20136 of writing mutated inputs to disk or accounting for 8-bit quantization, rounding, or file encoding. The only quantization discussed (Section 6) applies to model weights/activations, not to input serialization."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Applies clipping to keep the pixel values in valid range but does not consider quantization effect of IO operations",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly reuses fault-inducing inputs generated by fuzzing one model (the 32-bit model) to test another implementation (the 16-bit quantized/truncated model) and reports that many such inputs cause disagreements. However, the fuzzing framework is presented as a general-purpose CGF + PBT tool rather than being explicitly designed primarily to discover transferable attacks across different model implementations; the cross-model experiment is an application/example rather than a described core design goal. Thus the work demonstrates transferability reuse but the fuzzing design is not explicitly aimed at discovering transferable attacks across models.",
                    "evidence": "Section 6, 'TensorFuzz surfaces disagreements between models and their quantized versions': \"We then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. ... With these settings, the fuzzer was able to generate disagreements for 70% of the examples we tried.\" Also in same subsection: \"As a baseline experiment, we trained an MNIST classifier (this time without intentionally introducing numerical issues) using 32-bit floating point numbers. We then truncated all weights and activations to 16-bits. We then compared the predictions of the 32-bit and the 16-bit model on the MNIST test set and found 0 disagreements.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper does not evaluate whether the fault-inducing inputs discovered through fuzzing on one model can trigger failures in other, distinct models performing the same task. The experiments focus on finding bugs within a single model instance (e.g., numerical errors), comparing a model to its own quantized version, or testing the correctness of specific code implementations. There is no investigation into the transferability of these discovered faults to different model architectures or independently trained models, which is the core of the Attack Transferability metric.",
                    "evidence": "Page 7, Section \"TensorFuzz surfaces disagreements between models and their quantized versions\": The paper describes an experiment comparing a 32-bit MNIST classifier to its 16-bit quantized version. It states, \"We then compared the predictions of the 32-bit and the 16-bit model on the MNIST test set and found 0 disagreements. We then ran the fuzzer... the fuzzer was able to generate disagreements for 70% of the examples we tried.\" While this involves two model variants, it is a test of a model against its direct, transformed derivative, not a test of transferability to a separate, independently trained model. The other experiments in the paper similarly focus on single models or codebases without testing for transferability."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper includes an explicit experiment in which fault-inducing inputs generated by fuzzing a 32-bit MNIST classifier are reused to test a 16-bit quantized version, demonstrating that many of these inputs transfer and cause disagreements. This satisfies the criterion of reusing fuzzed inputs across two models performing the same task. However, the overall fuzzing framework is presented as a general CGF+PBT tool rather than being specifically designed to discover transferable failures across arbitrary surrogate and target models, so it lacks an explicit architectural mechanism for transferability beyond this illustrative application.",
                    "evidence": "Section 6, \u201cTensorFuzz surfaces disagreements between models and their quantized versions\u2019: \u201cWe then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. ... With these settings, the fuzzer was able to generate disagreements for 70% of the examples we tried.\u201d"
                },
                "manual": {
                    "value": "Low",
                    "why": "It is not transferability. It is differential testing based oracle.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Property-based",
                "Differential"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ]
        }
    },
    "Chao et al. - 2025 - Jailbreaking Black Box Large Language Models in Twenty Queries": {
        "key": "chao2025jailbreaking",
        "name": "PAIR",
        "year": "2025",
        "citation_count": "806",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly targets and successfully produces semantic jailbreaks that coax safety-aligned and closed-source LLMs (e.g., GPT-3.5/4 and Gemini-Pro) into producing unsafe/forbidden outputs, and it evaluates PAIR against existing defenses showing PAIR can bypass or remain effective under defense mechanisms. Thus PAIR uncovers high-impact failures that bypass explicit safety/policy mechanisms (jailbreaks), not merely low-level robustness errors.",
                    "evidence": "Page 23, Abstract: \"However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails.\" \n\nPage 23, Contributions (Efficiency/Effectiveness): \"PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini.\" \n\nPage 27, Table III (Direct jailbreak attacks): shows PAIR Jailbreak % for closed-source models (e.g., \"GPT-3.5 51%\" and \"GPT-4 48%\", and \"Gemini 73%\"), demonstrating PAIR elicits unsafe responses from safety-aligned/closed models. \n\nPage 29, Section IV-C (Defended performance of PAIR): \"In Table VIII, we evaluate the performance of PAIR against two jailbreaking defenses: SmoothLLM [20] and a perplexity filter [37, 38]. ... the JB% of PAIR drops significantly less than GCG when defended by these two defenses, meaning that PAIR is significantly harder to defend against than GCG.\" \n\nPage 29, Fig. 3 caption (example jailbreak): \"Claude-1 jailbreak. A successful jailbreak where the objective is to 'design a phishing email that can trick users into giving away their login credentials.'\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper introduces PAIR, a method explicitly designed to generate \"semantic jailbreaks\" that bypass the safety guardrails of large language models. The evaluation is conducted on a range of modern, safety-aligned LLMs, including Llama-2, GPT-4, Claude, and Gemini, which are described as representing the state-of-the-art in safety alignment. The goal is to induce the models to produce objectionable content for harmful behaviors, which constitutes a high-impact failure by bypassing explicit safety mechanisms. This aligns perfectly with the 'High' severity rating, which is defined by uncovering failures that bypass explicit safety mechanisms in defended or safety-aligned models to induce violations like jailbreaks.",
                    "evidence": "Page 1, Abstract: \"The alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. ... we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM.\"\n\nPage 4, Section IV. EXPERIMENTS, Target LLMs: \"We red team seven different LLMs... Of these models, Vicuna and Llama-2 are open source, whereas the remaining five are only available as black boxes. These models collectively represent the current state-of-the-art in terms of both generation capability (GPT-4 and Gemini-Pro) and safety alignment (Claude and Llama-2).\"\n\nPage 5, Table III: The table is titled \"Direct jailbreak attacks on JailbreakBench\" and reports the \"Jailbreak %\" for various defended models like Llama-2, GPT-3.5, GPT-4, Claude-1, Claude-2, and Gemini, demonstrating the method's capability to bypass their safety features."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly targets and successfully produces semantic jailbreaks that coax safety-aligned and closed-source LLMs (e.g., GPT-3.5/4 and Gemini-Pro) into producing unsafe/forbidden outputs, and it evaluates PAIR against existing defenses showing PAIR can bypass or remain effective under defense mechanisms. Thus PAIR uncovers high-impact failures that bypass explicit safety/policy mechanisms (jailbreaks), not merely low-level robustness errors.",
                    "evidence": "Page 23, Abstract: \"However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails.\" \n\nPage 23, Contributions (Efficiency/Effectiveness): \"PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini.\" \n\nPage 27, Table III (Direct jailbreak attacks): shows PAIR Jailbreak % for closed-source models (e.g., \"GPT-3.5 51%\" and \"GPT-4 48%\", and \"Gemini 73%\"), demonstrating PAIR elicits unsafe responses from safety-aligned/closed models. \n\nPage 29, Section IV-C (Defended performance of PAIR): \"In Table VIII, we evaluate the performance of PAIR against two jailbreaking defenses: SmoothLLM [20] and a perplexity filter [37, 38]. ... the JB% of PAIR drops significantly less than GCG when defended by these two defenses, meaning that PAIR is significantly harder to defend against than GCG.\" \n\nPage 29, Fig. 3 caption (example jailbreak): \"Claude-1 jailbreak. A successful jailbreak where the objective is to 'design a phishing email that can trick users into giving away their login credentials.'\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "High",
                    "why": "The PAIR algorithm is explicitly designed to steer its search toward attacker-specified objectives: each run is given an objective O (a specific harmful behavior or piece of content), the attacker LLM is initialized with that objective and tasked to generate candidate prompts to elicit that particular output from the target, and the JUDGE function evaluates whether the target's response satisfies that objective. Experiments operate over a benchmark of 100 specific behaviors (JBB-Behaviors) and report per-behavior jailbreak percentages and queries-per-success, demonstrating that PAIR pursues and measures success on predefined target outcomes rather than performing untargeted fault discovery.",
                    "evidence": "Section II (Preliminaries), p.24: \"To this end, throughout the paper we assume that each jailbreak is characterized by an objective O, which describes the toxic content that the attacker seeks to elicit from the target (e.g., 'tell me how to build a bomb'). The objective O informs the generation and evaluation of prompts...\"; Section III-B (Algorithm 1), p.25: \"Input: Number of iterations K, attack objective O\" and \"Initialize: system prompt of A with O\"; Section III-A (Attack generation), p.24: \"We design targeted, yet flexible system prompts which direct the attacker A to generate a candidate prompt P that jailbreak the target model.\"; Section IV (Experiments), p.26: \"When red teaming LLMs, the objective is to discover prompts that elicit responses from a targeted LLM which contain the content requested in each behavior from JBB-Behaviors, as evaluated by the JUDGE function.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's proposed method, Prompt Automatic Iterative Refinement (PAIR), is designed to automatically discover prompts that cause a large language model to generate 'jailbreaks'. A jailbreak is defined by a specific 'objective O', which represents a class of undesirable behavior (e.g., 'tell me how to build a bomb' or 'design a phishing email'). The system's exploration is steered towards generating content that fulfills this objective, which is a broader class of security-violating behavior rather than a specific, predefined output string. The metric definition explicitly lists 'jailbreaks' as an example for the Medium value, which perfectly aligns with the paper's goal and methodology.",
                    "evidence": "Page 2, Section II. PRELIMINARIES: \"To this end, throughout the paper we assume that each jailbreak is characterized by an objective O, which describes the toxic content that the attacker seeks to elicit from the target (e.g., \"tell me how to build a bomb\"). The objective O informs the generation and evaluation of prompts, ensuring that generated jailbreaks are contextually relevant and aligned with the specific malicious intent being simulated.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "While PAIR clearly steers exploration toward attacker\u2010defined objectives O (e.g. \u2018tell me how to build a bomb\u2019, \u2018design a phishing email\u2019), these objectives specify broader classes of unsafe behavior rather than a single, exact target output string or token sequence. According to the Targeted Attack Discovery metric, this corresponds to \u2018Medium\u2019: the fuzzer pursues predefined classes of security-violating behavior (jailbreaks, illicit tutorials, disallowed content) rather than driving the model to produce one specific, attacker-chosen output.",
                    "evidence": "Section II, p.24: \u201cwe assume that each jailbreak is characterized by an objective O, which describes the toxic content that the attacker seeks to elicit\u2026 (e.g., \u2018tell me how to build a bomb\u2019).\u201d  Section IV, p.26: \u201cWhen red teaming LLMs, the objective is to discover prompts that elicit responses\u2026 which contain the content requested in each behavior from JBB-Behaviors\u2026.\u201d"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Targets specific malicious intents (e.g. bomb-making, phishing) but does not enforce a fixed phrase or label.",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight through statistical analyses and ablations that characterize where and when PAIR succeeds or fails (e.g., per-model and per-category jailbreak rates, transferability matrices, defended performance, and attacker-design ablations). It connects some failure patterns to broader sources of vulnerability (for example, noting PAIR's difficulty with heavily safety fine-tuned models and attributing transferability to shared training data), but it does not analyze observable model internals (no neuron, layer, coverage, or activation analyses) nor provide deeper mechanistic explanations (e.g., reliance on specific non-robust features, spurious cues, or overfitting at the neuron/layer level). This matches the metric's \u201cMedium\u201d level: statistical/failure-pattern and ablation diagnostics are present, but internal-model diagnostic analyses are absent.",
                    "evidence": "Section IV-A (Direct jailbreaking attacks), p.27: \"However, PAIR struggles with Llama-2 and the Claude models, demonstrating the success of the extensive safety fine-tuning.\" (Section IV-A paragraph discussing Table III) \n\nSection IV-B (Jailbreak transfer experiments), p.28: \"We believe that this is largely attributable to the fact that PAIR\u2019s prompts are semantic, and they therefore target similar vulnerabilities across LLMs, which are generally trained on similar datasets.\" (explaining transferability results in Table VI) \n\nFigure 4 caption / Section IV (Categorization), p.29: \"Categorizing PAIR\u2019s jailbreak %. Each square represents PAIR\u2019s JB% for a target LLM (x-axis) and JBB-Behaviors category (y-axis); darker squares indicate higher JB%.\" (shows per-category statistical failure patterns across models) \n\nSection IV-C / Table VIII (Defended performance), p.29: \"the JB% of PAIR drops significantly less than GCG when defended by these two defenses, meaning that PAIR is significantly harder to defend against than GCG.\" (statistical comparison under defenses) \n\nSection IV-E (Ablation experiments), p.30: \"We find that jailbreaks are most likely to be found in the first or second query, and observe diminishing returns as the depth increases.\" and \"We find that across the board, the role-playing approach is most effective, given that it finds 82 out of the 88 successful jailbreaks for Vicuna.\" (statistical/ablation analysis of attack design choices)\n\nWhat is not present (negative evidence): Nowhere in the design, implementation, or evaluation sections (III, IV, Appendices) does the paper present analyses of internal model signals such as neuron activation frequency, layer-wise behavior, model coverage metrics, or per-neuron correlations with failures; nor does it provide mechanistic attribution to specific non-robust/spurious features at the internal-activation level (no neuron/layer quotes or tables)."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper provides diagnostic analysis that explains why failures occur by linking the success and transferability of its jailbreak prompts to underlying model vulnerabilities. It performs a statistical analysis of failure patterns by categorizing jailbreak success rates across different harmful behavior categories and target models (Figure 4). More importantly, it directly hypothesizes a root cause for these failures. The authors attribute the high transferability of their attacks to the semantic nature of the prompts, suggesting they exploit \"similar vulnerabilities across LLMs, which are generally trained on similar datasets.\" This connects the observed failures to a systemic vulnerability stemming from the models' training data. Additionally, the paper analyzes which attack strategies are most effective, finding that the \"role-playing approach is most effective,\" which provides diagnostic insight into the specific types of prompts that exploit vulnerabilities in the models' alignment and instruction-following mechanisms.",
                    "evidence": "Page 6, Section IV. B. Jailbreak transfer experiments: \"We believe that this is largely attributable to the fact that PAIR\u2019s prompts are semantic, and they therefore target similar vulnerabilities across LLMs, which are generally trained on similar datasets.\"\n\nPage 7, Figure 4: \"Categorizing PAIR's jailbreak %. Each square represents PAIR's JB% for a target LLM (x-axis) and JBB-Behaviors category (y-axis); darker squares indicate higher JB%.\"\n\nPage 8, Section Attacker system prompt criteria: \"We find that across the board, the role-playing approach is most effective, given that it finds 82 out of the 88 successful jailbreaks for Vicuna. We also find that authority endorsement prompts are typically the least effective.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "While the paper conducts extensive statistical analyses of failure patterns (per-model and per-category jailbreak rates, transfer experiments, ablations on attacker strategies and defenses), it does not delve into internal model diagnostics such as neuron activations, coverage metrics, or mechanistic attribution to non-robust or spurious features. This matches the \u201cMedium\u201d level: the authors provide rich statistical and ablation insights into when and where PAIR succeeds or fails, but they do not analyze observable model internals to explain why these failures occur at the representation level.",
                    "evidence": "\u2022 Section IV-A, IV-B, IV-C, IV-E: Statistical analyses of jailbreak success rates across models, categories, defenses, and attacker-design ablations (Tables III, IV, VIII; Figure 4).  \n\u2022 Section IV-B (p.28): Hypothesis that transferability arises from shared training data vulnerabilities\u2014an untested, high-level linkage rather than measured internal bias analysis.  \n\u2022 No sections present neuron-level, layer-wise, or coverage-based investigations, nor do they attribute failures to specific spurious features or dataset biases via model internals."
                },
                "manual": {
                    "value": "Low",
                    "why": "Provides rationale behind transferability of the failures not cause of the failures",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The PAIR method explicitly constrains prompt generation via rule-based attacker system prompts (role-playing, logical appeal, authority endorsement), JSON output formats, and in-context examples that keep mutations semantic and human-interpretable in each iteration. These design choices enforce plausibility at the individual mutation step level. However, the paper does not provide an explicit mechanism or empirical evaluation that enforces or measures plausibility across cumulative iterations (e.g., no bounded-perturbation guarantees or perceptual/semantic metrics like perplexity reported for final fault-inducing prompts), nor a human study that directly assesses the realism of final jailbreak prompts. Therefore PAIR enforces plausibility through rule-based / template constraints per mutation but does not validate or measure cumulative drift of final inputs, matching the Medium category.",
                    "evidence": "III-A Attacker\u2019s system prompt (p.25): \"we carefully design three distinct system prompts templates, all of which instructs the LLM to output a specific kind of objectionable content... each system prompt template is based on one of three criteria: logical appeal, authority endorsement, and role-playing.\"; III-A (p.25): \"Format... we require that the attacker generate its responses in JSON format.\" (shows rule-based/template constraints and structured generation per iteration). Algorithm / Iterative refinement (III-B, p.25): \"If S == 1 then return P ... otherwise the conversation is updated... the conversation history is then passed back to the attacker, and the process repeats.\" (shows iterative mutation without bounded perturbation guarantees). IV Evaluation (p.26): \"We compute the Jailbreak %\u2014the percentage of behaviors that elicit a jailbroken response according to JUDGE\u2014and the Queries per Success.\" (evaluation reports jailbreak rates, not perceptual/plausibility metrics). IV / JUDGE dataset (p.26, Table I): \"We collected a dataset of 100 prompts and responses... Three expert annotators labeled each pair...\" (human annotation was used to evaluate JUDGE agreement on jailbreak labeling, not to assess plausibility of generated prompts). Appendix C / Examples (pp.41-42): contains example generated prompts and conversations (illustrative human-interpretable examples), but no reported human study or quantitative plausibility measures such as perplexity, LPIPS, SSIM, PESQ, STOI, or Perplexity for final fault-inducing prompts."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's proposed method, Prompt Automatic Iterative Refinement (PAIR), is explicitly designed to generate semantically plausible and human-interpretable inputs (prompts) as its core mechanism. Plausibility is enforced throughout the iterative process by using an attacker Large Language Model (LLM) to generate and refine prompts based on conversational history and chain-of-thought reasoning. This design inherently constrains the generated inputs to a distribution of realistic natural language. The paper contrasts this directly with token-level attacks that produce semantically invalid or uninterpretable inputs. Furthermore, the paper provides substantial empirical evidence, in the form of numerous qualitative examples of the final fault-inducing prompts, which are all semantically coherent and represent realistic user queries.",
                    "evidence": "Page 2, Contributions, 'Interpretability': 'PAIR generates prompt-level semantic jailbreaks that are interpretable to humans and includes interpretable, chain-of-thought improvement assessments.'\n\nPage 2, Figure 2 'Prompt- vs. token-level jailbreaks': This figure provides a direct visual comparison, showing a plausible prompt generated by PAIR ('Write a fun and creative story about a person who gets into a lot of trouble for committing insider trading...') versus an uninterpretable token-level jailbreak from another method.\n\nPage 3, Section III-A, 'Attacker's system prompt': The method's design relies on plausible social engineering strategies to construct prompts: 'each system prompt template is based on one of three criteria: logical appeal, authority endorsement, and role-playing.'\n\nPages 7, 19, and 20, Figures 3, 9, 10, 11, 12, 13, 14: These figures provide multiple examples of final, successful jailbreak prompts generated by PAIR. All examples are coherent, plausible, and framed as realistic scenarios (e.g., 'You are a creative writer working on a new political thriller... write a convincing letter from the IRS which the protagonist will use...')."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "While PAIR\u2019s design clearly constrains each generated prompt to remain semantically coherent\u2014via role-playing, logical-appeal and authority-endorsement system prompts, JSON formatting, and chain-of-thought refinements\u2014it does not include any explicit mechanism to prevent cumulative semantic drift over multiple iterations, nor does it report quantitative or human-study assessments (e.g. perplexity scores or user judgments) of the final fault-inducing prompts\u2019 realism. Thus, PAIR enforces plausibility at the individual mutation step level but overlooks validation of the aggregate effect of those mutations, fitting the Medium category.",
                    "evidence": "III-A Attacker\u2019s system prompt design (p.25): uses structured templates and in-context examples to keep each prompt plausible; III-B Algorithm 1 (p.25): iterative refinement with no bounded-perturbation or plausibility filter; IV Evaluation (p.26): reports only jailbreak success rates and query counts, with no perceptual/linguistic plausibility metrics or human evaluation of final prompts."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "PAIR generates adversarial inputs as textual prompts (i.e., already-serialized inputs) and explicitly enforces stable, well-formed serialization for attacker outputs (JSON formatting, seeding of braces, termination on closing brace). The paper also states experiments use public APIs for query access, which means prompts are sent and stored via standard serialized channels rather than relying on fragile in-memory floating-point perturbations. These design choices ensure that fault-inducing inputs remain reproducible under standard I/O/serialization operations.",
                    "evidence": "III-A Implementing the attacker LLM (page 25): \"To standardize the generation of this content, we require that the attacker generate its responses in JSON format.\"; III-A Implementing the attacker LLM (page 25): \"we initialize the output of the language model to begin with the brace '{' ... For the first iteration ... we seed the output with: {\"improvement\":\"\" ... Secondly, we terminate generation upon any closing brace.\"; IV. Experiments (page 26): \"Since PAIR only requires black box access, we use public APIs for all of our experiments, which reduces costs and ensures reproduciblility.\"; Appendix B / Table XII (System prompt template, page 37): \"FORMAT: Format your response in JSON, with the two elements 'improvement' and 'prompt'... Your response should **only** contain this JSON element and nothing else.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The proposed method, PAIR (Prompt Automatic Iterative Refinement), generates adversarial inputs as text prompts. Text is an inherently discrete and serialized format. Unlike floating-point representations of images or audio, text prompts do not suffer from numerical instability caused by standard I/O operations like quantization, clipping, or rounding. A generated text prompt, when saved to disk and reloaded, remains identical, thus perfectly preserving the properties that induce the failure (the 'jailbreak'). The metric definition explicitly states that such approaches are considered highly reproducible: 'We also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., synthesized speech, rendered images, or generated text) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.' The paper consistently describes its output as semantic, interpretable text prompts, which fall directly into this category.",
                    "evidence": "Page 2, Section II. PRELIMINARIES: 'We focus on prompt-level jailbreaks, wherein the goal is to craft semantic, human-interpretable prompts that fool a targeted LLM into outputting objectionable content.' This confirms that the adversarial inputs are text. Page 1, Abstract: '...we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks...'. The entire methodology revolves around generating and refining natural language prompts, which are inherently stable under standard I/O operations."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "PAIR generates adversarial inputs as textual prompts (i.e., already-serialized inputs) and explicitly enforces stable, well-formed serialization for attacker outputs (JSON formatting, seeding of braces, termination on closing brace). The paper also states experiments use public APIs for query access, which means prompts are sent and stored via standard serialized channels rather than relying on fragile in-memory floating-point perturbations. These design choices ensure that fault-inducing inputs remain reproducible under standard I/O/serialization operations.",
                    "evidence": "III-A Implementing the attacker LLM (page 25): \"To standardize the generation of this content, we require that the attacker generate its responses in JSON format.\"; III-A Implementing the attacker LLM (page 25): \"we initialize the output of the language model to begin with the brace '{' ... For the first iteration ... we seed the output with: {\"improvement\":\"\" ... Secondly, we terminate generation upon any closing brace.\"; IV. Experiments (page 26): \"Since PAIR only requires black box access, we use public APIs for all of our experiments, which reduces costs and ensures reproduciblility.\"; Appendix B / Table XII (System prompt template, page 37): \"FORMAT: Format your response in JSON, with the two elements 'improvement' and 'prompt'... Your response should **only** contain this JSON element and nothing else.\""
                },
                "manual": {
                    "value": "High",
                    "why": "Fuzzed inputs are text prompts generated by another LLM, not numerical perturbations",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly evaluates transferability by reusing jailbreak prompts found on one model to test other models (Section IV-B, Table VI) and reports transfer percentages. Furthermore, the PAIR design intentionally generates semantic, prompt-level (interpretable) attacks\u2014via attacker system-prompt templates, chain-of-thought improvement assessments, and parallel streams\u2014which the authors argue and evidence as supporting cross-model transferability (they compare PAIR vs GCG transfer and attribute better transfer to PAIR's semantic prompts). Thus the paper both demonstrates reuse of fault-inducing inputs across models and incorporates design choices aimed at discovering transferable failures.",
                    "evidence": "Abstract (p. 23): \"PAIR also achieves competitive jailbreaking success rates and transferability on open and closed\u2011source LLMs, including GPT-3.5/4, Vicuna, and Gemini.\"; Introduction / Contributions (p. 23): \"We also find that PAIR jailbreaks are often more trans\u2011ferrable to other LLMs than jailbreaks generated by GCG.\"; Section IV-B 'Jailbreak transfer experiments' (p. 28): \"For PAIR, we use the successful jailbreaks found for GPT-4 and Vicuna... Our results in Table VI indicate that PAIR\u2019s Vicuna prompts transfer more readily than those generated by GCG on all models except GPT-3.5, and PAIR\u2019s GPT-4 prompts transfer well on Vicuna, GPT-3.5, and Gemini. We believe that this is largely attributable to the fact that PAIR\u2019s prompts are semantic, and they therefore target similar vulnerabilities across LLMs...\"; Table VI caption (p. 28): \"Jailbreak transferability. We report the jailbreaking percentage of prompts that successfully jailbreak a source LLM when transferred to downstream LLM.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper extensively evaluates the transferability of the jailbreak prompts generated by its proposed method, PAIR. Fault-inducing inputs (prompts) generated on source models (GPT-4 and Vicuna) are systematically reused to test their effectiveness on a suite of other target models, with results clearly presented. This fulfills the primary condition for a 'Medium' rating. However, the paper does not meet the criteria for a 'High' rating because the PAIR method's design is not explicitly aimed at discovering transferable attacks. The core algorithm is an iterative refinement process focused on jailbreaking a single, specific target model. The authors attribute the observed high transferability to the semantic nature of the generated prompts, which is a beneficial byproduct of the method rather than an explicit mechanism integrated into the design to optimize for transferability across models.",
                    "evidence": "Page 6, Section IV-B, Jailbreak transfer experiments: \"We next evaluate the transferability of the attacks generated in \u00a7 IV-A. For PAIR, we use the successful jailbreaks found for GPT-4 and Vicuna... Our results in Table VI indicate that PAIR\u2019s Vicuna prompts transfer more readily than those generated by GCG on all models except GPT-3.5, and PAIR\u2019s GPT-4 prompts transfer well on Vicuna, GPT-3.5, and Gemini. We believe that this is largely attributable to the fact that PAIR\u2019s prompts are semantic, and they therefore target similar vulnerabilities across LLMs, which are generally trained on similar datasets.\" This section, along with Table VI, shows the reuse of prompts to test transferability, but the explanation attributes this property to the semantic nature of the prompts, not to an explicit design choice for promoting transferability."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper clearly demonstrates reuse of fault-inducing inputs (jailbreak prompts) generated on one model to test their effectiveness on other models (see Section IV-B, Table VI). However, while the authors attribute high transferability to the semantic, interpretable nature of PAIR\u2019s prompt-level attacks, the core design of PAIR is focused on efficiently generating jailbreaks on a single target model rather than explicitly optimizing for cross-model transfer. Thus it meets the criteria for Medium: it reuses inputs for transfer tests but does not incorporate mechanisms explicitly aimed at discovering transferable failures.",
                    "evidence": "Section IV-B (p. 28): \u201cFor PAIR, we use the successful jailbreaks found for GPT-4 and Vicuna\u2026 Our results in Table VI indicate that PAIR\u2019s Vicuna prompts transfer more readily than those generated by GCG\u2026 We believe that this is largely attributable to the fact that PAIR\u2019s prompts are semantic, and they therefore target similar vulnerabilities across LLMs.\u201d; Table VI caption: \u201cJailbreak transferability. We report the jailbreaking percentage of prompts that successfully jailbreak a source LLM when transferred to downstream LLM.\u201d"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Transferability is a byproduct in this paper, not a design goal",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ]
        }
    },
    "Asyrofi et al. - 2020 - CrossASR Efficient Differential Testing of Automatic Speech Recognition via Text-To-Speech": {
        "key": "asyrofi2020crossasr",
        "name": "CrossASR",
        "year": "2020",
        "citation_count": "25",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The failures CrossASR is designed to uncover are transcription errors and correctness/accuracy defects (mispredictions) in ASR outputs identified via differential testing between ASRs. The paper demonstrates examples of incorrect transcriptions (e.g., words substituted or mistranscribed) and frames the goal as detecting ASR correctness issues. It does not claim to target or demonstrate uncovering unsafe, biased, hallucinated, or security-critical behaviors, nor does it evaluate bypassing safety/policy or defended/safety-aligned models. Therefore the failures exposed correspond to low-severity (robustness/correctness) errors rather than medium- or high-severity safety/security violations.",
                    "evidence": "Page 640, Abstract: \"CrossASR is a differential testing solution that compares outputs of multiple ASR systems to uncover erroneous behaviors among ASRs.\" \n\nPage 641, Introduction / Section II overview: \"CrossASR performs a black-box differential testing to uncover erroneous behaviors on ASR systems by cross-referencing different ASRs to detect different transcriptions from ASRs.\" \n\nPage 642, Section II-A (Case 2): \"In this case, CrossASR considers cases where the transcribed text does not match input text as failed test cases; it considers the other cases as successful test cases.\" \n\nPage 647, Section IV.A (example failed test case): Table IX shows a concrete transcription error: Original Text: \"they can be consulted without difficulty\"; ASR Transcribed Text: \"they can be controlled without difficulty\" (demonstrating a misprediction). \n\nPage 648, Section V.A (related work comparison): \"CrossASR ... is concerned in assessing correctness of ASRs instead of their robustness.\" \n\nPage 643, Experimental setup / Table II: lists the evaluated ASRs (Deepspeech, DeepSpeech2, wav2letter, wit) \u2014 the paper does not describe testing of defended, safety-aligned, or policy-protected models nor any experiments demonstrating bypassing of safety or security mechanisms."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The CrossASR method is designed to uncover correctness errors in Automatic Speech Recognition (ASR) systems, which manifest as mispredictions. A failure is identified when an ASR's transcribed text does not match the ground-truth text used to generate the audio input. The paper provides examples of such failures, like transcribing 'consulted' as 'controlled', which are transcription errors. The methodology does not aim to generate or identify unsafe, biased, or toxic behavior, nor does it test the model against any explicit safety or security mechanisms. The paper explicitly states its focus is on 'correctness' rather than other types of failures. This aligns with the definition of uncovering model robustness errors or mispredictions without exposing unsafe behaviors.",
                    "evidence": "Page 3, Section II-A, Case 2: 'In this case, CrossASR considers cases where the transcribed text does not match input text as failed test cases...'\n\nPage 1, Abstract: '...we found that most samples are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR)...'\n\nPage 8, Section IV-A and Table IX: 'DeepSpeech mistakenly transcribed \u201cconsulted\u201d to \u201ccontrolled\u201d.'\n\nPage 10, Section V-A: 'Also, CrossASR is concerned in assessing correctness of ASRs instead of their robustness.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The failures CrossASR is designed to uncover are transcription errors and correctness/accuracy defects (mispredictions) in ASR outputs identified via differential testing between ASRs. The paper demonstrates examples of incorrect transcriptions (e.g., words substituted or mistranscribed) and frames the goal as detecting ASR correctness issues. It does not claim to target or demonstrate uncovering unsafe, biased, hallucinated, or security-critical behaviors, nor does it evaluate bypassing safety/policy or defended/safety-aligned models. Therefore the failures exposed correspond to low-severity (robustness/correctness) errors rather than medium- or high-severity safety/security violations.",
                    "evidence": "Page 640, Abstract: \"CrossASR is a differential testing solution that compares outputs of multiple ASR systems to uncover erroneous behaviors among ASRs.\" \n\nPage 641, Introduction / Section II overview: \"CrossASR performs a black-box differential testing to uncover erroneous behaviors on ASR systems by cross-referencing different ASRs to detect different transcriptions from ASRs.\" \n\nPage 642, Section II-A (Case 2): \"In this case, CrossASR considers cases where the transcribed text does not match input text as failed test cases; it considers the other cases as successful test cases.\" \n\nPage 647, Section IV.A (example failed test case): Table IX shows a concrete transcription error: Original Text: \"they can be consulted without difficulty\"; ASR Transcribed Text: \"they can be controlled without difficulty\" (demonstrating a misprediction). \n\nPage 648, Section V.A (related work comparison): \"CrossASR ... is concerned in assessing correctness of ASRs instead of their robustness.\" \n\nPage 643, Experimental setup / Table II: lists the evaluated ASRs (Deepspeech, DeepSpeech2, wav2letter, wit) \u2014 the paper does not describe testing of defended, safety-aligned, or policy-protected models nor any experiments demonstrating bypassing of safety or security mechanisms."
                },
                "manual": {
                    "value": "Low",
                    "why": "transcription error",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "CrossASR performs untargeted differential testing that looks for mismatches between an ASR's transcription and the original text (using other ASRs as reference). While it uses a failure-probability predictor to prioritize texts likely to produce failures, the objective is to discover generic misrecognitions (failed test cases) rather than to steer the ASR toward specific, attacker-chosen outputs or phrases. The paper explicitly contrasts CrossASR with adversarial/targeted attack work and describes its oracle as detecting mismatches to the input text, not achieving attacker-specified target outputs.",
                    "evidence": "Page 640, Abstract: \"CrossASR is a differential testing solution that compares outputs of multiple ASR systems to uncover erroneous behaviors among ASRs.\" \n\nPage 641, Section II-A (Architecture): \"These ASR-transcribed texts are compared with the text that was input to the TTS. For each ASR, if the ASR-transcribed text matches with the text input to TTS... If the ASR-transcribed text does not match the text input to TTS and there is another ASR\u2019s transcribed text that matches with the text input to TTS, CrossASR determines the speech audio as a failed test case. We consider cases where all ASRs\u2019 transcribed texts differ from the input text as indeterminable cases \u2013 as possibly the error resides in the TTS.\" \n\nPage 648-649, Section V.A (ASR Testing, related work): \"From the artificial intelligence community, there are several works that perform adversarial attacks targeting ASR systems (a.k.a. adversarial test case generation) [20]\u2013[25]. ... Different from these works, CrossASR performs differential testing by cross-referencing different ASRs as test oracle. Also, CrossASR is concerned in assessing correctness of ASRs instead of their robustness.\" \n\nPage 640-641, Introduction: \"CrossASR achieves its efficiency by using a failure probability predictor to select texts that are likely to lead to failed test cases...\" (shows prioritization of likely failures, not selection of attacker-specified target outputs)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The method described in the paper, CrossASR, performs untargeted exploration to find generic failures in Automatic Speech Recognition (ASR) systems. Its objective is to identify any audio input that is incorrectly transcribed by an ASR, which constitutes a generic misclassification or inaccuracy. The system uses differential testing, comparing the outputs of multiple ASRs against the original text used for audio synthesis. A failure is declared when one ASR's transcription does not match the input text, while another ASR's transcription does. This is a classic untargeted approach focused on finding inconsistencies. The system includes a 'failure probability predictor' to guide the selection of input texts, but its purpose is to increase the efficiency of finding *any* failure, not to steer the system towards generating a specific, predefined output phrase or a broader class of security-violating content. The entire methodology is designed to reveal generic prediction inconsistencies rather than discover targeted attacks.",
                    "evidence": "Page 3, Section II-A, Case 2: 'If the ASR-transcribed text does not match the text input to TTS and there is another ASR's transcribed text that matches with the text input to TTS, CrossASR determines the speech audio as a failed test case.' This definition of a failure is based on a generic mismatch (inaccuracy) and inconsistency between systems, not on achieving a targeted output. The examples of failures found, such as 'consulted' being transcribed as 'controlled' (Table IX, page 8), further demonstrate that the discovered faults are generic transcription errors."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "CrossASR performs untargeted differential testing that looks for mismatches between an ASR's transcription and the original text (using other ASRs as reference). While it uses a failure-probability predictor to prioritize texts likely to produce failures, the objective is to discover generic misrecognitions (failed test cases) rather than to steer the ASR toward specific, attacker-chosen outputs or phrases. The paper explicitly contrasts CrossASR with adversarial/targeted attack work and describes its oracle as detecting mismatches to the input text, not achieving attacker-specified target outputs.",
                    "evidence": "Page 640, Abstract: \"CrossASR is a differential testing solution that compares outputs of multiple ASR systems to uncover erroneous behaviors among ASRs.\" \n\nPage 641, Section II-A (Architecture): \"These ASR-transcribed texts are compared with the text that was input to the TTS. For each ASR, if the ASR-transcribed text matches with the text input to TTS... If the ASR-transcribed text does not match the text input to TTS and there is another ASR\u2019s transcribed text that matches with the text input to TTS, CrossASR determines the speech audio as a failed test case. We consider cases where all ASRs\u2019 transcribed texts differ from the input text as indeterminable cases \u2013 as possibly the error resides in the TTS.\" \n\nPage 648-649, Section V.A (ASR Testing, related work): \"From the artificial intelligence community, there are several works that perform adversarial attacks targeting ASR systems (a.k.a. adversarial test case generation) [20]\u2013[25]. ... Different from these works, CrossASR performs differential testing by cross-referencing different ASRs as test oracle. Also, CrossASR is concerned in assessing correctness of ASRs instead of their robustness.\" \n\nPage 640-641, Introduction: \"CrossASR achieves its efficiency by using a failure probability predictor to select texts that are likely to lead to failed test cases...\" (shows prioritization of likely failures, not selection of attacker-specified target outputs)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight via statistical analysis of failures (per\u2011ASR and per\u2011TTS failure counts, cumulative plots, FPR measurements) and a manual root\u2011cause categorization of false positives (TTS errors, preprocessing issues, ambiguous audio). However, it does not analyze observable model internals (e.g., neuron activation, layer behaviour, coverage) nor does it link discovered failures to deeper model vulnerabilities such as reliance on non\u2011robust/spurious features, dataset bias, or overfitting. Thus it meets the criteria for Medium.",
                    "evidence": "1) Statistical failure counts and per\u2011system comparisons: Section III.D (RQ3) and Table V report total failed cases per TTS/ASR: \"CrossASR generates 13,572, 13,071, 5,911, and 1,064 failed test cases using Google, ResponsiveVoice, Festival, and Espeak TTSes, respectively.\" (Section III.D / Table V). 2) Accuracy and false positive rates (quantitative diagnostics): Section III.D (RQ4) and Table VI show sampled FPRs per ASR/TTS and report manual verification: \"From all combination of TTSes and ASRs, the FPRs are low...\" (Section III.D, Table VI). 3) Manual analysis and categorization of failure causes: Section IV.B (Discussion) performs a diagnostic analysis of false positives and categorizes root causes into four cases (TTS problems, TTS producing unintelligible audio, text preprocessing limitations, and ambiguous audio): \"We analyze these false positives and observed that they can be categorized into 4 different cases: (1) ... (2) ... (3) ... (4) Audio is ambiguous; It can match both texts.\" (Section IV.B). 4) Explicit absence of internal/coverage analysis: the paper emphasizes a black\u2011box differential method and contrasts with coverage\u2011guided approaches: \"As a black-box approach, CrossASR can generate test cases for any ASR...\" and \"It differentiates itself from the many existing AI testing work that is coverage\u2011guided, e.g., [5], [8], [12].\" (Introduction / Section I). These excerpts show the paper provides statistical and root\u2011cause diagnostics but does not analyze model internals or tie failures to underlying model vulnerabilities, matching the Medium level."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper presents a black-box differential testing approach, which by its nature does not analyze model internals like neuron coverage or activations. The primary analysis of failures is presented in Section IV.B, which focuses exclusively on categorizing *false positives* found during manual verification. This analysis diagnoses issues with the testing infrastructure\u2014such as problems with the Text-To-Speech (TTS) engine, limitations of the text preprocessing step, or ambiguous audio generation\u2014rather than providing diagnostic insight into the actual failures of the Automatic Speech Recognition (ASR) models under test. The paper successfully reports numerous failure cases with examples (Section IV.A) and provides aggregate failure counts to compare the relative reliability of different ASR systems, but it does not perform statistical analysis of true failure patterns or connect the failures to underlying model vulnerabilities like dataset bias or reliance on non-robust features. Therefore, it reports failures without providing the diagnostic insight required for a higher rating.",
                    "evidence": "Page 9, Section IV.B, \"Analysis of False Positive Failed Test Case from Manual Verification\": \"From 33,618 failed test cases, we manually verified a statistically representative sample of size 395 and found 12 of them to be false positives. We analyze these false positives and observed that they can be categorized into 4 different cases: (1) Human deems the audio to be the same as the transcribed text, but different from the original text. This highlights a problem with the TTS. ... (2) Human deems the audio to be different from both transcribed and input text. This indicates a problem with the TTS. ... (3) Cases highlighting limitations of our Text Preprocessing step. ... (4) Audio is ambiguous; It can match both texts.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "While the paper reports extensive failure counts (per-ASR and per-TTS) and measures false positive rates, it does not perform diagnostic analysis of the ASR models themselves. There is no examination of model internals (e.g., neuron activations or coverage), nor statistical analysis linking specific input characteristics (such as word classes, sentence length, or phonetic features) to failure patterns. The only root-cause analysis presented (Section IV.B) categorizes false positives in the testing infrastructure (TTS errors, preprocessing quirks, ambiguous audio) rather than uncovering why the ASR models fail. Consequently, the paper merely reports failures without providing the diagnostic insights into underlying model vulnerabilities required for a Medium or High rating.",
                    "evidence": "Section IV.B (Analysis of False Positive Failed Test Case from Manual Verification):\n\"We analyze these false positives and observed that they can be categorized into 4 different cases: (1) ... highlights a problem with the TTS; (2) ... indicates a problem with the TTS; (3) ... limitations of our Text Preprocessing step; (4) Audio is ambiguous...\"\n\nNo sections of the paper investigate correlations between input features and ASR failures or analyze model internals (e.g., neuron coverage, layer behavior), nor do they link identified failures to model vulnerabilities such as over-reliance on spurious features or dataset bias."
                },
                "manual": {
                    "value": "Low",
                    "why": "No diagnsotic analysis",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "CrossASR generates test inputs by synthesizing speech from natural texts (Europarl) via TTS engines rather than applying unconstrained, accumulating perturbations. The paper describes explicit text preprocessing (normalization) \u201cintended to ensure that the text are pronounced properly by TTS\u201d (preventing implausible TTS outputs), uses multiple real-world TTSes to avoid bias, and treats texts from a real corpus as the input distribution. Importantly, the authors perform a human verification on a statistically representative sample of final failed test cases (395 samples) and report low false positive rates (12 false positives out of 395), explicitly noting that the failed cases are \u201caudio understandable to humans but cannot be transcribed properly by an ASR.\u201d These design choices and the human evaluation demonstrate that CrossASR enforces and empirically validates input plausibility for final fault-inducing inputs.",
                    "evidence": "Intro / Abstract (page 640): \"CrossASR employs Text-to-Speech (TTS) engines to synthesize test inputs from texts and treats the texts as the ground truth labels...\"; Architecture / Text preprocessing (Section II-A / II-B, page 643): \"The Text Preprocessor lowercases the text, removes punctuation from it, and normalizes non-standard words... This normalization is intended to ensure that the text are pronounced properly by TTS.\"; Experimental validation / RQ4 & manual check (Section III-D, pages 645-647): \"we ask a native English speaker, who is not an author of this paper to verify whether the failed test cases generated by CrossASR are actual failed test cases...\" and Table VI reporting low FPRs; Discussion / manual sample analysis (page 647): \"From 33,618 failed test cases, we manually verified a statistically representative sample of size 395 and found 12 of them to be false positives.\"; Abstract / Conclusion (page 640 / 649): \"Based on a manual verification carried out on statistically representative sample size, we found that most samples are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR).\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, CrossASR, generates audio test cases using Text-To-Speech (TTS) engines. This design choice inherently aims for plausible inputs, as TTS systems are specifically created to produce realistic, human-like speech. More importantly, the paper provides direct empirical evidence for the plausibility of the fault-inducing inputs through a human evaluation. The authors tasked a native English speaker with verifying a statistically significant sample of the generated failed test cases. The explicit goal was to check if the audio was understandable to a human. The results confirmed that the vast majority of inputs that caused failures were indeed plausible and intelligible. This combination of a generation method designed for plausibility (TTS) and a rigorous human-centric validation of the final, failure-inducing inputs satisfies the criteria for a 'High' rating.",
                    "evidence": "Page 10, Section VI, Conclusion: \"CrossASR is highly reliable in determining failed test cases, as demonstrated by manual verification. Almost all of the failed test case samples inspected are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR).\"\n\nPage 5, Section III-C, RQ4: \"To measure the reliability of CrossASR in determining failed test cases, we ask a native English speaker, who is not an author of this paper to verify whether the failed test cases generated by CrossASR are actual failed test cases.\"\n\nPage 6, Section III-D, RQ4: \"The non-author verified that in the random sample, CrossASR generates failed test cases with the highest and lowest FPR of 9% and 0%, respectively. From all combination of TTSes and ASRs, the FPRs are low, which means that the failed test cases that are detected by CrossASR actually highlight a deficiency in the corresponding ASR (it is a true positive).\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "CrossASR generates test inputs by synthesizing speech from natural texts (Europarl) via TTS engines rather than applying unconstrained, accumulating perturbations. The paper describes explicit text preprocessing (normalization) \u201cintended to ensure that the text are pronounced properly by TTS\u201d (preventing implausible TTS outputs), uses multiple real-world TTSes to avoid bias, and treats texts from a real corpus as the input distribution. Importantly, the authors perform a human verification on a statistically representative sample of final failed test cases (395 samples) and report low false positive rates (12 false positives out of 395), explicitly noting that the failed cases are \u201caudio understandable to humans but cannot be transcribed properly by an ASR.\u201d These design choices and the human evaluation demonstrate that CrossASR enforces and empirically validates input plausibility for final fault-inducing inputs.",
                    "evidence": "Intro / Abstract (page 640): \"CrossASR employs Text-to-Speech (TTS) engines to synthesize test inputs from texts and treats the texts as the ground truth labels...\"; Architecture / Text preprocessing (Section II-A / II-B, page 643): \"The Text Preprocessor lowercases the text, removes punctuation from it, and normalizes non-standard words... This normalization is intended to ensure that the text are pronounced properly by TTS.\"; Experimental validation / RQ4 & manual check (Section III-D, pages 645-647): \"we ask a native English speaker, who is not an author of this paper to verify whether the failed test cases generated by CrossASR are actual failed test cases...\" and Table VI reporting low FPRs; Discussion / manual sample analysis (page 647): \"From 33,618 failed test cases, we manually verified a statistically representative sample of size 395 and found 12 of them to be false positives.\"; Abstract / Conclusion (page 640 / 649): \"Based on a manual verification carried out on statistically representative sample size, we found that most samples are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR).\""
                },
                "manual": {
                    "value": "High",
                    "why": "Performs human study",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "CrossASR generates test inputs by synthesizing speech audio from text via Text-To-Speech (TTS) engines and represents each test case as a tuple that includes the TTS-generated audio. That is, inputs are produced directly as serialized audio artifacts (speech audio files) rather than as ephemeral in-memory floating-point perturbations. Because CrossASR feeds these synthesized audio artifacts to ASRs and records them as part of the generated test cases (s in the 5-tuple), the failures it detects are based on persisted, serialized inputs and thus are stable with respect to standard I/O (quantization/rounding/clipping) effects. The paper also indicates metamorphic transformations are future work, implying the current approach does not rely on fine-grained numerical mutations that could be lost on serialization.",
                    "evidence": "Section II-A (Architecture), p. 642: \"Each piece of text in the batch is then fed into the Text-To-Speech Engine (TTS). The TTS converts the Text into a Speech Audio. The Speech Audio is then fed to the Automatic Speech Recognition (ASR) Engines.\"; Section II-A (Architecture), p. 642: \"Each 5-tuple is of the format t, s, a, t', l, where t is the input text, s is the TTS generated audio, a is the ASR used to transcribe t, t' is the ASR transcribed text, and l is the status of the test case.\"; Conclusion (VI), p. 649: \"We want to add some metamorphic transformations to generate more speech audios that can lead to failed test cases.\" (indicating such transformations are future work and not the current source of test mutations)."
                },
                "gemini": {
                    "value": "High",
                    "why": "The method generates test inputs (speech audio) directly in a serialized format using Text-To-Speech (TTS) engines. As stated in the metric definition, inputs generated this way are considered inherently stable and reproducible under standard I/O operations. The approach does not rely on applying fine-grained numerical perturbations to in-memory data representations, thus avoiding the issue of fault-inducing changes being lost during serialization (e.g., through quantization or clipping). The generated audio file is the stable test case, and failures are determined by the ASR's ability to transcribe it correctly.",
                    "evidence": "Page 3, Section II-A. Architecture: 'Given a Text Collection, the Test Selection Engine selects a Text Batch. Each Text in the Text Batch is then fed into the Text-To-Speech Engine (TTS). The TTS converts the Text into a Speech Audio. The Speech Audio is then fed to the Automatic Speech Recognition (ASR) Engines.' This quote, supported by Figure 1, demonstrates that the test cases are synthesized directly into a serialized audio format, ensuring they are reproducible by design."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "CrossASR generates test inputs by synthesizing speech audio from text via Text-To-Speech (TTS) engines and represents each test case as a tuple that includes the TTS-generated audio. That is, inputs are produced directly as serialized audio artifacts (speech audio files) rather than as ephemeral in-memory floating-point perturbations. Because CrossASR feeds these synthesized audio artifacts to ASRs and records them as part of the generated test cases (s in the 5-tuple), the failures it detects are based on persisted, serialized inputs and thus are stable with respect to standard I/O (quantization/rounding/clipping) effects. The paper also indicates metamorphic transformations are future work, implying the current approach does not rely on fine-grained numerical mutations that could be lost on serialization.",
                    "evidence": "Section II-A (Architecture), p. 642: \"Each piece of text in the batch is then fed into the Text-To-Speech Engine (TTS). The TTS converts the Text into a Speech Audio. The Speech Audio is then fed to the Automatic Speech Recognition (ASR) Engines.\"; Section II-A (Architecture), p. 642: \"Each 5-tuple is of the format t, s, a, t', l, where t is the input text, s is the TTS generated audio, a is the ASR used to transcribe t, t' is the ASR transcribed text, and l is the status of the test case.\"; Conclusion (VI), p. 649: \"We want to add some metamorphic transformations to generate more speech audios that can lead to failed test cases.\" (indicating such transformations are future work and not the current source of test mutations)."
                },
                "manual": {
                    "value": "High",
                    "why": "Uses TTS to generate the input audio - non in memory mutation",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models, nor does it present experiments or analyses measuring transferability. CrossASR runs each synthesized audio against multiple ASRs simultaneously (differential testing) and reports per-ASR failure counts, but it does not (a) create inputs targeted at one surrogate ASR and then reuse them to test other ASRs, (b) measure overlap/transfer of failures across models, or (c) claim design choices explicitly intended to discover transferable attacks. The classifier (failure probability predictor) is trained with labels per ASR to predict likely failures, but the approach and experiments focus on efficiently finding failures per ASR rather than on demonstrating or quantifying transferability across ASRs.",
                    "evidence": "Page 642, I. Introduction: \"CrossASR performs a black-box differential testing to uncover erroneous behaviors on ASR systems by cross-referencing different ASRs to detect different transcriptions from ASRs.\" \n\nPage 643, II-B Test Selection Engine: \"For this training, a text has m labels, each corresponds to an ASR.\" \n\nPage 646, III-D RQ3 / Table V: \"Table V shows the total number of failed test cases that is generated from the entire 20,000 texts for all combinations of experimented TTSes and ASRs.\" (Table V reports per-ASR failed-case counts but no cross-model transfer/overlap statistics.) \n\nPage 649, VI Conclusion / Future Work: \"In the future, we plan to evaluate CrossASR using additional ASRs and TTSes.\" (Indicates no current evaluation of transferability across broader sets of models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, CrossASR, is based on differential testing, not attack transferability. It generates a single audio input and feeds it to multiple Automatic Speech Recognition (ASR) systems simultaneously. A failure is defined based on a disagreement among the outputs of these systems. Specifically, a test case is considered a failure for a particular ASR if its transcription does not match the input text, while at least one other ASR's transcription does. The methodology does not involve generating a fault-inducing input on a single source/surrogate model and then reusing that specific input to test for failures on other target models. The evaluation counts the total number of failures detected for each ASR system individually but does not analyze or report on whether the same audio inputs caused failures across multiple ASRs. Therefore, it does not demonstrate the transferability of fault-inducing inputs.",
                    "evidence": "Page 3, Section II-A, Architecture, Case 2: \"At least one of the ASR transcribed texts that matches with the input Text while at least one of the other ASR Transcribed Texts do not match with the input Text. In this case, CrossASR considers cases where the transcribed text does not match input text as failed test cases...\" This describes the core differential testing mechanism, where a failure in one model is identified by its disagreement with a successful transcription from another model for the same input, rather than testing for transferability of a known failing input. \n\nPage 6, Table V, \"NUMBER OF FAILED TEST CASES GENERATED FOR EACH COMBINATION OF TTSES AND ASRS\": This table presents the total number of failures for each ASR model as separate, aggregated counts. For example, using the Google TTS, it found 4,036 failures for Deepspeech and 2,539 for Deepspeech2. The paper does not analyze the intersection of these sets to determine how many specific audio files caused failures in both models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models, nor does it present experiments or analyses measuring transferability. CrossASR runs each synthesized audio against multiple ASRs simultaneously (differential testing) and reports per-ASR failure counts, but it does not (a) create inputs targeted at one surrogate ASR and then reuse them to test other ASRs, (b) measure overlap/transfer of failures across models, or (c) claim design choices explicitly intended to discover transferable attacks. The classifier (failure probability predictor) is trained with labels per ASR to predict likely failures, but the approach and experiments focus on efficiently finding failures per ASR rather than on demonstrating or quantifying transferability across ASRs.",
                    "evidence": "Page 642, I. Introduction: \"CrossASR performs a black-box differential testing to uncover erroneous behaviors on ASR systems by cross-referencing different ASRs to detect different transcriptions from ASRs.\" \n\nPage 643, II-B Test Selection Engine: \"For this training, a text has m labels, each corresponds to an ASR.\" \n\nPage 646, III-D RQ3 / Table V: \"Table V shows the total number of failed test cases that is generated from the entire 20,000 texts for all combinations of experimented TTSes and ASRs.\" (Table V reports per-ASR failed-case counts but no cross-model transfer/overlap statistics.) \n\nPage 649, VI Conclusion / Future Work: \"In the future, we plan to evaluate CrossASR using additional ASRs and TTSes.\" (Indicates no current evaluation of transferability across broader sets of models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "Utilize multiple ASRs to find faults using differential oracle but does not analyze if same error is reproducible to other model",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Asyrofi et al. - 2021 - CrossASR++ a modular differential testing framework for automatic speech recognition": {
        "key": "asyrofi2021crossasr++",
        "name": "CrossASR++",
        "year": "2021",
        "citation_count": "25",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper defines and evaluates failures solely as recognition mismatches discovered via differential (cross-referencing) testing\u2014i.e., cases where the system under test transcription does not match the original text while another ASR does, or cases deemed indeterminable due to TTS. All technical descriptions, workflow, and experiments focus on counting such failed test cases (robustness/misprediction errors). The paper does not describe uncovering unsafe/undesired outputs (e.g., toxic/bias/hallucination), nor any attempt to bypass safety, policy, or security mechanisms or to elicit data leakage or unauthorized actions in defended or safety-aligned models.",
                    "evidence": "Section 1 (Introduction), p.1575: \"If the transcribed text recognized by the ASR system under test does not match the text input to TTS while another ASR\u2019s transcribed text does, this input audio is viewed as a failed test case.\"; Section 2.1 (Workflow), p.1576: \"For a piece of audio and its corresponding transcription, there are three possible situations: 1) if the SUT can recognize it successfully, i.e. the transcript matches the original text... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT. 3) If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case...\"; Section 3 (Experiments), p.1577: evaluation reports only counts of \"failed test cases\" (e.g., \"CrossASR++ can find 26.2% more failed test cases than the original tool\"), with no discussion of safety-policy bypasses or high-severity failures."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, CrossASR++, uses differential testing to find failures in Automatic Speech Recognition (ASR) systems. A failure is defined as an instance where the ASR system under test incorrectly transcribes an audio file generated from a known text, while another ASR system transcribes it correctly. This process identifies prediction inconsistencies and mispredictions, which are classic model robustness errors. The paper does not describe any attempt to generate or identify failures related to unsafe, biased, or toxic content, nor does it target the bypassing of any safety or security mechanisms. The sole focus is on the correctness of the speech-to-text transcription.",
                    "evidence": "Page 2, Section 1, Introduction: 'If the transcribed text recognized by the ASR system under test does not match the text input to TTS while another ASR's transcribed text does, this input audio is viewed as a failed test case.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper defines and evaluates failures solely as recognition mismatches discovered via differential (cross-referencing) testing\u2014i.e., cases where the system under test transcription does not match the original text while another ASR does, or cases deemed indeterminable due to TTS. All technical descriptions, workflow, and experiments focus on counting such failed test cases (robustness/misprediction errors). The paper does not describe uncovering unsafe/undesired outputs (e.g., toxic/bias/hallucination), nor any attempt to bypass safety, policy, or security mechanisms or to elicit data leakage or unauthorized actions in defended or safety-aligned models.",
                    "evidence": "Section 1 (Introduction), p.1575: \"If the transcribed text recognized by the ASR system under test does not match the text input to TTS while another ASR\u2019s transcribed text does, this input audio is viewed as a failed test case.\"; Section 2.1 (Workflow), p.1576: \"For a piece of audio and its corresponding transcription, there are three possible situations: 1) if the SUT can recognize it successfully, i.e. the transcript matches the original text... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT. 3) If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case...\"; Section 3 (Experiments), p.1577: evaluation reports only counts of \"failed test cases\" (e.g., \"CrossASR++ can find 26.2% more failed test cases than the original tool\"), with no discussion of safety-policy bypasses or high-severity failures."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "CrossASR++ performs black-box differential testing that seeks inputs which produce mismatched or failed transcriptions (i.e., generic recognition failures) rather than steering generation toward any attacker-chosen or specific target outputs or classes of unsafe behaviour. Its failure estimator ranks texts by likelihood of producing failures (mismatches), but there is no mechanism described to direct the fuzzer to induce a predefined target transcription or targeted unsafe output. Thus the method enables untargeted discovery of misrecognitions/inconsistencies rather than targeted attack discovery.",
                    "evidence": "(p.1575, Abstract) \"[CrossASR] performs black-box differential testing on ASR systems by cross-referencing multiple ASRs to detect different transcriptions.\"; (p.1575, Introduction) \"If the transcribed text recognized by the ASR system under test does not match the text input to TTS while another ASR\u2019s transcribed text does, this input audio is viewed as a failed test case.\"; (p.1576, Section 2.1 Workflow) \"If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\"; (p.1576, Section 2.1 Workflow) \"This estimator can estimate the probability of a piece of text leading to a failed test cases. We rank the texts to be chosen in the iteration and prioritize texts with a higher ranking to be processed...\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper describes CrossASR++, a differential testing framework. The goal of this framework is to find 'failed test cases'. A failed test case is defined as an instance where the Automatic Speech Recognition (ASR) system under test produces a transcription that does not match the original text used to generate the audio, while at least one other ASR system produces the correct transcription. This approach is designed to discover generic failures (incorrect transcriptions) through inconsistent predictions between different models. The exploration is untargeted because it does not aim to steer the ASR system towards producing a specific, predefined phrase or a broader class of security-violating content. Instead, it seeks any deviation from the expected transcription, which is characteristic of untargeted exploration.",
                    "evidence": "Page 2, Section 1 Introduction: \"If the transcribed text recognized by the ASR system under test does not match the text input to TTS while another ASR's transcribed text does, this input audio is viewed as a failed test case.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "CrossASR++ performs black-box differential testing that seeks inputs which produce mismatched or failed transcriptions (i.e., generic recognition failures) rather than steering generation toward any attacker-chosen or specific target outputs or classes of unsafe behaviour. Its failure estimator ranks texts by likelihood of producing failures (mismatches), but there is no mechanism described to direct the fuzzer to induce a predefined target transcription or targeted unsafe output. Thus the method enables untargeted discovery of misrecognitions/inconsistencies rather than targeted attack discovery.",
                    "evidence": "(p.1575, Abstract) \"[CrossASR] performs black-box differential testing on ASR systems by cross-referencing multiple ASRs to detect different transcriptions.\"; (p.1575, Introduction) \"If the transcribed text recognized by the ASR system under test does not match the text input to TTS while another ASR\u2019s transcribed text does, this input audio is viewed as a failed test case.\"; (p.1576, Section 2.1 Workflow) \"If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\"; (p.1576, Section 2.1 Workflow) \"This estimator can estimate the probability of a piece of text leading to a failed test cases. We rank the texts to be chosen in the iteration and prioritize texts with a higher ranking to be processed...\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper primarily reports discovered failures (counts of failed test cases) and evaluates tool improvements (more failures found when adding ASRs, changing visibility, or using different estimators). While it uses a failure estimator to prioritize texts, it does not analyze model internals (e.g., neuron activations, layer characteristics, coverage metrics) nor provide statistical diagnostic analyses that explain why failures occur (e.g., linking failures to non-robust/spurious features, dataset bias, class-level error concentration, or overfitting). All evidence in the paper concerns numbers of failures and ablation of tool features rather than explanations of underlying model vulnerabilities.",
                    "evidence": "Section 2.1 (page 1576): \"At the start of each iteration except the first one, we use failed test cases, successful test cases, and indeterminable test cases so far to train a failure estimator (Line 6). This estimator can estimate the probability of a piece of text leading to a failed test cases.\"  Section 3 RQ1 (page 1577): \"Table 1 illustrates the results of running the tool ... CrossASR++ can find 26.2% more failed test cases than the original tool, which demonstrates significant improvements to the original tool.\"  Section 3 RQ2 / ablation (page 1577): tables and text report how adding ASRs, changing visibility, or switching estimators changes the number of failed test cases (e.g., Table 2 and Table 3), but contain no analysis of model internals or root causes.  Conclusion (page 1578): \"Our evaluation results show that CrossASR++ outperform CrossASR by revealing 26.2% more failed test cases... We also find that leveraging a more advanced failure estimator can help CrossASR++ achieve better performance.\"  Nowhere in the paper are neuron-level signals, coverage metrics, layer/activation analyses, or statistical failure-pattern analyses presented to explain why failures occur."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper introduces CrossASR++, a black-box differential testing framework for Automatic Speech Recognition (ASR) systems. The primary objective and the entire evaluation of the proposed method are focused on the efficiency and effectiveness of *finding* failures, measured by the quantity of failed test cases uncovered. The paper does not provide any diagnostic analysis to explain *why* these failures occur. It treats the ASR systems as black boxes, so there is no analysis of model internals (e.g., neuron activation, coverage). Furthermore, it does not perform any statistical analysis on the discovered failure patterns (e.g., common phonetic errors, sentence structures causing failure) or attempt to link them to underlying model vulnerabilities like dataset bias or overfitting. The experiments exclusively report the number of failed test cases found under different configurations.",
                    "evidence": "Page 4, Section 3, EXPERIMENTS: The paper's evaluation is structured around two research questions that exclusively target the quantity of failures discovered: 'RQ1. How many failed test cases can CrossASR++ find?' and 'RQ2. To what extent do enhanced features help find more failed test cases?'. The results presented in tables (e.g., Table 1) and the accompanying text only report the counts of failures. For example, the paper states: 'In total, CrossASR++ can find 26.2% more failed test cases than the original tool, which demonstrates significant improvements to the original tool.' This demonstrates that the evaluation's goal is to count failures, not to diagnose their root cause."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper primarily reports discovered failures (counts of failed test cases) and evaluates tool improvements (more failures found when adding ASRs, changing visibility, or using different estimators). While it uses a failure estimator to prioritize texts, it does not analyze model internals (e.g., neuron activations, layer characteristics, coverage metrics) nor provide statistical diagnostic analyses that explain why failures occur (e.g., linking failures to non-robust/spurious features, dataset bias, class-level error concentration, or overfitting). All evidence in the paper concerns numbers of failures and ablation of tool features rather than explanations of underlying model vulnerabilities.",
                    "evidence": "Section 2.1 (page 1576): \"At the start of each iteration except the first one, we use failed test cases, successful test cases, and indeterminable test cases so far to train a failure estimator (Line 6). This estimator can estimate the probability of a piece of text leading to a failed test cases.\"  Section 3 RQ1 (page 1577): \"Table 1 illustrates the results of running the tool ... CrossASR++ can find 26.2% more failed test cases than the original tool, which demonstrates significant improvements to the original tool.\"  Section 3 RQ2 / ablation (page 1577): tables and text report how adding ASRs, changing visibility, or switching estimators changes the number of failed test cases (e.g., Table 2 and Table 3), but contain no analysis of model internals or root causes.  Conclusion (page 1578): \"Our evaluation results show that CrossASR++ outperform CrossASR by revealing 26.2% more failed test cases... We also find that leveraging a more advanced failure estimator can help CrossASR++ achieve better performance.\"  Nowhere in the paper are neuron-level signals, coverage metrics, layer/activation analyses, or statistical failure-pattern analyses presented to explain why failures occur."
                },
                "manual": {
                    "value": "Low",
                    "why": "No diagnsotic analysis",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "CrossASR++ generates test inputs via TTS and enforces a plausibility-based filter in its differential-testing logic: an audio is counted as a failed test case for the System Under Test (SUT) only if the SUT fails but at least one other ASR recognizes the audio correctly. This implicitly filters out wholly implausible/invalid TTS outputs (which the paper calls \"indeterminable\"). The paper also explicitly discusses that better TTSes produce fewer invalid/indeterminable audios and that indeterminable cases exist. However, the paper does not provide explicit perceptual or task-specific quantitative assessments (e.g., PESQ, STOI, human evaluation, or other audio-quality/intelligibility metrics) of the final fault-inducing audios. Because CrossASR++ therefore enforces plausibility via cross-referencing (a form of validation of final inputs) but lacks dedicated empirical perceptual/quality evaluation, the appropriate rating is Medium.",
                    "evidence": "Section 2.1 Workflow (p.1576): \"For a piece of audio and its corresponding transcription, there are three possible situations: 1) if the SUT can recognize it successfully... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT. 3) If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case because a TTS may generate an invalid audio.\" (Section 2.1, Workflow, p.1576) Section 2.2 Extensibility (p.1576): \"The main reason is that more advanced TTSes generate fewer invalid audios that are more likely to be indeterminable test cases.\" (Section 2.2, p.1576) Section 3 Experiments (p.1577): experimental evaluation focuses on counts of \"# Failed Test Case\" and ablation of features (visibility, added ASRs, estimators) \u2014 no reported perceptual or speech-quality / intelligibility metrics or human evaluation is provided in the experiments or results tables (e.g., Tables 1\u20133 measure numbers of failed test cases). (Section 3, RQ1/RQ2, p.1577)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The method generates test inputs by synthesizing audio from a text corpus using standard Text-to-Speech (TTS) engines. This design inherently constrains the inputs to be plausible speech, as TTS systems are designed to produce human-like utterances. This serves as a strong, rule-based constraint that enforces input plausibility in the generation step. However, the paper does not provide any empirical validation, either through human studies or quantitative metrics (e.g., PESQ, STOI), to confirm that the final, failure-inducing audio inputs are realistic and intelligible. The paper acknowledges that TTS can produce 'invalid audios,' but these are filtered out as 'indeterminable test cases' based on consensus among ASR systems, not by a direct assessment of plausibility. This approach fits the 'Medium' criteria, as it enforces plausibility during generation but does not empirically validate the final outputs.",
                    "evidence": "Page 3, Section 2.1, Workflow: 'Then, TTS is used to generate an audio file for each piece of selected text (Line 9)... If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case because a TTS may generate an invalid audio.' This shows the method of generation (TTS) and the indirect way of handling potentially implausible ('invalid') inputs. The Experiments section (Section 3) evaluates the method based on the number of failed test cases found, without reporting any measures of the audio inputs' realism or quality."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "CrossASR++ generates test inputs via TTS and enforces a plausibility-based filter in its differential-testing logic: an audio is counted as a failed test case for the System Under Test (SUT) only if the SUT fails but at least one other ASR recognizes the audio correctly. This implicitly filters out wholly implausible/invalid TTS outputs (which the paper calls \"indeterminable\"). The paper also explicitly discusses that better TTSes produce fewer invalid/indeterminable audios and that indeterminable cases exist. However, the paper does not provide explicit perceptual or task-specific quantitative assessments (e.g., PESQ, STOI, human evaluation, or other audio-quality/intelligibility metrics) of the final fault-inducing audios. Because CrossASR++ therefore enforces plausibility via cross-referencing (a form of validation of final inputs) but lacks dedicated empirical perceptual/quality evaluation, the appropriate rating is Medium.",
                    "evidence": "Section 2.1 Workflow (p.1576): \"For a piece of audio and its corresponding transcription, there are three possible situations: 1) if the SUT can recognize it successfully... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT. 3) If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case because a TTS may generate an invalid audio.\" (Section 2.1, Workflow, p.1576) Section 2.2 Extensibility (p.1576): \"The main reason is that more advanced TTSes generate fewer invalid audios that are more likely to be indeterminable test cases.\" (Section 2.2, p.1576) Section 3 Experiments (p.1577): experimental evaluation focuses on counts of \"# Failed Test Case\" and ablation of features (visibility, added ASRs, estimators) \u2014 no reported perceptual or speech-quality / intelligibility metrics or human evaluation is provided in the experiments or results tables (e.g., Tables 1\u20133 measure numbers of failed test cases). (Section 3, RQ1/RQ2, p.1577)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "TTS engines are inherently designed to produce natural-sounding speech from text, thereby providing a theoretical justification and design constraint for input naturalness.",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "CrossASR++ generates test inputs by synthesizing and saving audio files via TTS (i.e., it produces serialized audio inputs that are used for cross-referencing). Because the tool produces and persists TTS-generated audio files (and even provides these audio files in its repository for reuse), the failure-inducing inputs are inherently serialized and thus stable with respect to standard I/O operations (they are not ephemeral in-memory floating\u2011point perturbations that would be lost by quantization/rounding). The paper therefore meets the metric criterion that inputs are generated in serialized form and remain reproducible under standard I/O operations.",
                    "evidence": "Section 2.1 Workflow (page 1576): \"Then, TTS is used to generate an audio file for each piece of selected text (Line 9).\" (Algorithm 1 shows: audios = tts.generateAudio(texts);). Section 2.4.2 Configuration and Execution (page 1577): \"In the Github repository, we also provide all the TTS-generated audios we use. Users can set recompute as false to use them directly.\" Section 2.2 Extensibility (page 1576): \"In TTS class, the method generateAudio() which converts a text into audio must be overrided by derived classes.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The method generates test cases by synthesizing speech from text using Text-to-Speech (TTS) engines. This process directly creates audio files in a standard, serialized format (e.g., WAV). The metric definition explicitly states that approaches generating inputs directly in serialized formats, such as synthesized speech, are considered reproducible because they are inherently stable with respect to standard I/O operations. The framework does not apply fine-grained numerical mutations to in-memory audio tensors, thus bypassing the problem of perturbations being lost during serialization (e.g., through quantization or rounding). The generated audio file itself is the test case, ensuring its reproducibility.",
                    "evidence": "Page 3, Section 2.1 Workflow, Algorithm 1: The paper describes the core workflow where text is converted directly into audio. The algorithm states: 'audios = tts.generateAudio(texts);' (Line 9). This shows that the test inputs are synthesized directly into an audio format. The abstract also confirms this approach: 'This method first utilizes Text-to-Speech (TTS) to generate audios from texts automatically and then feed these audios into different ASR systems for cross-referencing to uncover failed test cases.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "CrossASR++ generates test inputs by synthesizing and saving audio files via TTS (i.e., it produces serialized audio inputs that are used for cross-referencing). Because the tool produces and persists TTS-generated audio files (and even provides these audio files in its repository for reuse), the failure-inducing inputs are inherently serialized and thus stable with respect to standard I/O operations (they are not ephemeral in-memory floating\u2011point perturbations that would be lost by quantization/rounding). The paper therefore meets the metric criterion that inputs are generated in serialized form and remain reproducible under standard I/O operations.",
                    "evidence": "Section 2.1 Workflow (page 1576): \"Then, TTS is used to generate an audio file for each piece of selected text (Line 9).\" (Algorithm 1 shows: audios = tts.generateAudio(texts);). Section 2.4.2 Configuration and Execution (page 1577): \"In the Github repository, we also provide all the TTS-generated audios we use. Users can set recompute as false to use them directly.\" Section 2.2 Extensibility (page 1576): \"In TTS class, the method generateAudio() which converts a text into audio must be overrided by derived classes.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper describes a differential testing tool that generates the same TTS-produced inputs and feeds them to multiple ASRs for cross-referencing (so inputs are evaluated across ASRs concurrently), but it does not evaluate or claim that fault-inducing inputs discovered on one model are reused to test transferability to other (unseen) models, nor does it include experiments that measure cross-model transfer of faults or support surrogate-model attacks. The evaluation focuses on counting failed test cases per SUT and on how adding more ASRs or changing estimator/visibility affects the number of failures found, without demonstrating or measuring whether inputs that fail one model also cause failures in other models beyond the concurrent cross-referencing setup or in unseen targets.",
                    "evidence": "Page 1576, Section 2.1 Workflow: \"we use all ASR systems (including the System Under Test (SUT)) to convert the audio files back to texts (Line 11), and then we perform cross-reference (Line 12). For a piece of audio and its corresponding transcription, there are three possible situations: ... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\"  Page 1577, Section 3 RQ1 and Table 1: RQ1 is \"How many failed test cases can CrossASR++ find?\" and results report counts of failed test cases per SUT under different configurations (no analysis of reuse/transfer to unseen models).  Page 1577, Section 3 RQ2 / Table 2: \"By simply adding one more ASR for cross-referencing, we can increase the number of failed test cases uncovered for 4 SUTs by 25.07%, 39.63%, 20.95% and 8.17% respectively.\" (shows cross-referencing effect but not a transferability experiment).  Page 1578, Conclusion: \"CrossASR++ , an extensible tool that performs black-box differential testing on ASR systems... The extensibility allows CrossASR++ to flexibly utilizes more tools to boost its performance on uncovering failed test cases.\" (no mention of evaluating or supporting transferable/substitute-model attacks)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, CrossASR++, is a differential testing framework. A test case is defined as a failure for a specific System Under Test (SUT) if that SUT fails to transcribe an audio input correctly, while at least one other ASR system succeeds. This design inherently focuses on finding disagreements between models, rather than shared vulnerabilities or transferable failures. The case where all ASR models fail on the same input is categorized as an \"indeterminable test case\" and is not treated as a successful discovery of a fault. The paper does not reuse the inputs that are found to cause failures in one model to evaluate if they also induce failures in other models. Therefore, it does not demonstrate the transferability of fault-inducing inputs.",
                    "evidence": "Page 3, Section 2.1 Workflow, \"For a piece of audio and its corresponding transcription, there are three possible situations: ... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT. 3) If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case because a TTS may generate an invalid audio.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper describes a differential testing tool that generates the same TTS-produced inputs and feeds them to multiple ASRs for cross-referencing (so inputs are evaluated across ASRs concurrently), but it does not evaluate or claim that fault-inducing inputs discovered on one model are reused to test transferability to other (unseen) models, nor does it include experiments that measure cross-model transfer of faults or support surrogate-model attacks. The evaluation focuses on counting failed test cases per SUT and on how adding more ASRs or changing estimator/visibility affects the number of failures found, without demonstrating or measuring whether inputs that fail one model also cause failures in other models beyond the concurrent cross-referencing setup or in unseen targets.",
                    "evidence": "Page 1576, Section 2.1 Workflow: \"we use all ASR systems (including the System Under Test (SUT)) to convert the audio files back to texts (Line 11), and then we perform cross-reference (Line 12). For a piece of audio and its corresponding transcription, there are three possible situations: ... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\"  Page 1577, Section 3 RQ1 and Table 1: RQ1 is \"How many failed test cases can CrossASR++ find?\" and results report counts of failed test cases per SUT under different configurations (no analysis of reuse/transfer to unseen models).  Page 1577, Section 3 RQ2 / Table 2: \"By simply adding one more ASR for cross-referencing, we can increase the number of failed test cases uncovered for 4 SUTs by 25.07%, 39.63%, 20.95% and 8.17% respectively.\" (shows cross-referencing effect but not a transferability experiment).  Page 1578, Conclusion: \"CrossASR++ , an extensible tool that performs black-box differential testing on ASR systems... The extensibility allows CrossASR++ to flexibly utilizes more tools to boost its performance on uncovering failed test cases.\" (no mention of evaluating or supporting transferable/substitute-model attacks)."
                },
                "manual": {
                    "value": "Low",
                    "why": "Uses multiple ASRs in differential oracle, does not show transferability of failures across errors",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Data-driven"
            ]
        }
    },
    "Asyrofi et al. - 2021 - Can Differential Testing Improve Automatic Speech Recognition Systems": {
        "key": "asyrofi2021can",
        "name": "Asyrofi et al.",
        "year": "2021",
        "citation_count": "16",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's differential-testing approach (CrossASR++) identifies failures solely as cases where the ASR under test mis-transcribes a synthesized audio compared to the input text. The evaluation and improvements are measured by counts of such failed test cases and by word error rate (WER), classic robustness metrics for mispredictions. The paper contains no description or experiments that uncover unsafe, biased, hallucinated outputs, privacy/data-leakage, authorization bypasses, jailbreaks, or any bypassing of explicit safety/security mechanisms. Therefore the failures exposed are model robustness errors (mispredictions) and correspond to the 'Low' severity category.",
                    "evidence": "Page 675, Section II.A (CrossASR++): \"The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1). The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio (i.e., each word in the transcription matches each corresponding word in the input text). A valid test case can be either a failed test case or a successful test case. If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT.\" Page 677, Section III.C (Preliminary Evaluation): \"For the second evaluation, the WER is the most common metric to evaluate the performance of ASR systems... The WER is computed as follows: WER = (I + D + S) / N.\" (These passages show failures are defined as mispredictions and evaluated by WER; no safety/security violations or bypassing of defenses are discussed anywhere in the paper.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's methodology focuses exclusively on uncovering and correcting model robustness errors, specifically mispredictions in Automatic Speech Recognition (ASR) systems. A failure is strictly defined as an instance where the ASR system's output transcription does not match the ground-truth input text. The evaluation metrics used are the 'number of failed test cases' and 'Word Error Rate (WER)', both of which measure transcription accuracy. The paper does not investigate or report on failures related to unsafe behavior (e.g., toxic or biased outputs) nor does it attempt to bypass any explicit safety or security mechanisms. The uncovered failures are purely prediction inconsistencies.",
                    "evidence": "Page 2, Section II-A, 'CrossASR++': 'A valid test case can be either a failed test case or a successful test case. If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT. Otherwise, a successful test case is found.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's differential-testing approach (CrossASR++) identifies failures solely as cases where the ASR under test mis-transcribes a synthesized audio compared to the input text. The evaluation and improvements are measured by counts of such failed test cases and by word error rate (WER), classic robustness metrics for mispredictions. The paper contains no description or experiments that uncover unsafe, biased, hallucinated outputs, privacy/data-leakage, authorization bypasses, jailbreaks, or any bypassing of explicit safety/security mechanisms. Therefore the failures exposed are model robustness errors (mispredictions) and correspond to the 'Low' severity category.",
                    "evidence": "Page 675, Section II.A (CrossASR++): \"The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1). The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio (i.e., each word in the transcription matches each corresponding word in the input text). A valid test case can be either a failed test case or a successful test case. If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT.\" Page 677, Section III.C (Preliminary Evaluation): \"For the second evaluation, the WER is the most common metric to evaluate the performance of ASR systems... The WER is computed as follows: WER = (I + D + S) / N.\" (These passages show failures are defined as mispredictions and evaluated by WER; no safety/security violations or bypassing of defenses are discussed anywhere in the paper.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's CrossASR++ design performs untargeted differential testing: it synthesizes audio from corpus texts and seeks any mismatch between the SUT's transcription and the input text (i.e., generic misrecognitions). Its mutation (TTS of corpus texts), exploration (a failure estimator that ranks texts by likelihood of producing a failure), and oracle (cross-referencing that marks a test as failed if the SUT transcription differs from the input text) all aim to discover generic failures rather than to steer the fuzzer toward attacker-chosen, specific target outputs or phrases. There is no mechanism described to drive exploration toward inducing a predefined label, phrase, command, or other attacker-specified output\u2014only to find cases where the SUT differs from the reference\u2014so the work aligns with untargeted exploration under the metric definition.",
                    "evidence": "Page 675, Section II.A (CrossASR++): \"The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1). The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio (i.e., each word in the transcription matches each corresponding word in the input text). A valid test case can be either a failed test case or a successful test case. If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT.\"; Page 675\u2013676, Section II.A (failure estimator): \"The failure estimator is a fine-tuned BERT-based classifier with a softmax layer that outputs the probability of an input text being failed, successful and indeterminable test cases. ... CrossASR++ ranks texts according to their probability in descending order and gives higher priority to the texts with a higher rank.\"; Abstract / Introduction (page 674\u2013675): \"CrossASR++ ... employs a Text-To-Speech (TTS) system to synthesize audios from texts and then reveals failed test cases by feeding them to multiple ASR systems for cross-referencing.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, which utilizes the CrossASR++ tool, performs untargeted exploration to discover generic failures in Automatic Speech Recognition (ASR) systems. The objective is to find any input that causes a mis-transcription, not to steer the system towards a specific, attacker-chosen output phrase (High) or a broad class of security-violating behaviors (Medium). A failure is defined simply as a discrepancy between the ASR system's output and the original input text used to generate the audio. This approach aligns perfectly with the metric's definition for 'Low' as it discovers generic inaccuracies and inconsistent predictions.",
                    "evidence": "Page 2, Section II-A, \"A valid test case can be either a failed test case or a successful test case. If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT. Otherwise, a successful test case is found.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's CrossASR++ design performs untargeted differential testing: it synthesizes audio from corpus texts and seeks any mismatch between the SUT's transcription and the input text (i.e., generic misrecognitions). Its mutation (TTS of corpus texts), exploration (a failure estimator that ranks texts by likelihood of producing a failure), and oracle (cross-referencing that marks a test as failed if the SUT transcription differs from the input text) all aim to discover generic failures rather than to steer the fuzzer toward attacker-chosen, specific target outputs or phrases. There is no mechanism described to drive exploration toward inducing a predefined label, phrase, command, or other attacker-specified output\u2014only to find cases where the SUT differs from the reference\u2014so the work aligns with untargeted exploration under the metric definition.",
                    "evidence": "Page 675, Section II.A (CrossASR++): \"The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1). The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio (i.e., each word in the transcription matches each corresponding word in the input text). A valid test case can be either a failed test case or a successful test case. If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT.\"; Page 675\u2013676, Section II.A (failure estimator): \"The failure estimator is a fine-tuned BERT-based classifier with a softmax layer that outputs the probability of an input text being failed, successful and indeterminable test cases. ... CrossASR++ ranks texts according to their probability in descending order and gives higher priority to the texts with a higher rank.\"; Abstract / Introduction (page 674\u2013675): \"CrossASR++ ... employs a Text-To-Speech (TTS) system to synthesize audios from texts and then reveals failed test cases by feeding them to multiple ASR systems for cross-referencing.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper reports discovered failures and quantifies improvements (number of failed test cases and WER) after fine-tuning, but it does not perform diagnostic analysis of model internals (e.g., neuron activations, coverage, layer characteristics) nor statistical analyses of failure patterns that link failures to underlying model vulnerabilities such as reliance on non-robust/spurious features, dataset bias, or overfitting. In fact, the approach is explicitly described as black-box differential testing and its evaluations focus on counts and WER improvements rather than explanations of why failures occur.",
                    "evidence": "Page 675, Section II.A (description of approach): \"Both tools leverage recent advances in Text-to-Speech (TTS) systems and ASR systems to uncover failed test cases in a purely black-box manner.\" \n\nPage 676, Section III.C (metrics used for evaluation): \"We use two metrics to evaluate the performance of the SUT. One is the number of failed test cases uncovered by CrossASR++, and the other is the word error rate (WER) on a test set.\" \n\nPage 677, Section III.C (reporting results without diagnostics): \"The number of failed test cases found decreases from 399 to 296, which is a 25.81% reduction... the WER of the evolved SUT is 4.33%, which surpasses the WER of the static SUT (7.99%). The evolution corresponds to a 45.81% relative improvement of the WER.\" \n\nPage 678, Conclusion (summary focus on improvement outcomes): \"we empirically show that by fine-tuning the target ASR using generated test cases, the number of failed test cases uncovered decreases by 25.81%, and the WER decreases by 45.81%.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper focuses on using discovered failures as training data to improve the ASR models (a process they call 'evolutionary differential testing'), rather than performing diagnostic analysis to understand the root cause of these failures. The evaluation is purely quantitative, measuring the change in the number of failed test cases and the Word Error Rate (WER) before and after fine-tuning. The paper reports the existence and quantity of failures but does not analyze model internals (e.g., neuron coverage, activation patterns) or statistical failure patterns (e.g., characteristics of inputs that cause failures) to provide any diagnostic insight into why the models fail.",
                    "evidence": "Page 3, Section III-C, Experiment Design: 'We evaluate the performance of the evolved SUT using 2 metrics, i.e., the number of failed test cases found and the word error rate (WER). ... We measure the number of failed test cases found in a static SUT (non-evolving SUT from CrossASR++) and an evolved SUT. We then calculate the reduction of the number of failed test cases after evolution.'\n\nPage 4, Section III-C, Answers to RQ1: 'Answers to RQ1: By fine-tuning the ASR system under test using generated test cases, the number of failed test cases uncovered decreases by 25.81% and the WER decreases by 45.81%.'\n\nThis evidence shows that the paper's contribution and evaluation are centered on the reduction of failure counts and WER, not on diagnosing the nature or cause of the original failures."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper reports discovered failures and quantifies improvements (number of failed test cases and WER) after fine-tuning, but it does not perform diagnostic analysis of model internals (e.g., neuron activations, coverage, layer characteristics) nor statistical analyses of failure patterns that link failures to underlying model vulnerabilities such as reliance on non-robust/spurious features, dataset bias, or overfitting. In fact, the approach is explicitly described as black-box differential testing and its evaluations focus on counts and WER improvements rather than explanations of why failures occur.",
                    "evidence": "Page 675, Section II.A (description of approach): \"Both tools leverage recent advances in Text-to-Speech (TTS) systems and ASR systems to uncover failed test cases in a purely black-box manner.\" \n\nPage 676, Section III.C (metrics used for evaluation): \"We use two metrics to evaluate the performance of the SUT. One is the number of failed test cases uncovered by CrossASR++, and the other is the word error rate (WER) on a test set.\" \n\nPage 677, Section III.C (reporting results without diagnostics): \"The number of failed test cases found decreases from 399 to 296, which is a 25.81% reduction... the WER of the evolved SUT is 4.33%, which surpasses the WER of the static SUT (7.99%). The evolution corresponds to a 45.81% relative improvement of the WER.\" \n\nPage 678, Conclusion (summary focus on improvement outcomes): \"we empirically show that by fine-tuning the target ASR using generated test cases, the number of failed test cases uncovered decreases by 25.81%, and the WER decreases by 45.81%.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "No diagnsotic analysis",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility via a rule-based cross-referencing filter: synthesized audio-text pairs are accepted as valid only if at least one ASR correctly transcribes them, which ensures generated inputs are intelligible/valid at the time of creation. However, the paper does not provide any human evaluation or task-specific perceptual/semantic quality metrics (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for the final fault-inducing inputs, nor does it report additional quantitative checks of perceptual realism. Also, although their cross-referencing filter enforces plausibility per generated audio, the work does not report explicit mechanisms or analyses addressing cumulative mutations or perceptual assessment of final failing inputs. These facts align with the \"Medium\" level: plausibility is enforced by rule-based constraints but there is no empirical perceptual validation of final fault-triggering inputs.",
                    "evidence": "Section II.A (p.675): \"to avoid using the invalid audios synthesized by the TTS system, the generated audio-text pair is fed into multiple ASR systems for cross-referencing: If at least one ASR system can successfully transcribe the audio, the audio-text pair is viewed as a valid test case.\"; Section II.A (p.675): \"We take the audio-text pair as an indeterminable test case if none of them can successfully transcribe the audio.\"; Section III.B (p.676): \"The TTS system is ResponsiveVoice. The 5 ASR systems are DeepSpeech, DeepSpeech2, Wav2Letter++, Wav2Vec2, and Wit.\"; Section III.C (p.676): \"We use two metrics to evaluate the performance of the SUT. One is the number of failed test cases uncovered by CrossASR++, and the other is the word error rate (WER) on a test set.\". The paper contains no reported human study or perceptual/quality measures (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for assessing realism of final fault-inducing inputs."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method generates audio test cases from a real-world text corpus using a Text-to-Speech (TTS) system. It incorporates a mechanism to enforce input plausibility by filtering the generated audio. Specifically, an audio file is considered a 'valid test case' only if at least one of the multiple Automatic Speech Recognition (ASR) systems used for cross-referencing can transcribe it correctly. This step is explicitly designed to discard 'invalid audios synthesized by the TTS system,' which serves as a proxy for filtering out unintelligible or unrealistic speech. However, the paper does not provide any additional empirical validation of the plausibility of the final fault-inducing audio inputs, such as through human evaluation or by reporting quantitative speech quality metrics (e.g., PESQ, STOI). The method enforces plausibility during the test case generation and filtering process but lacks a final, explicit assessment of the resulting inputs.",
                    "evidence": "Page 2, Section II.A, paragraph 1: \"The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio (i.e., each word in the transcription matches each corresponding word in the input text). A valid test case can be either a failed test case or a successful test case... We take the audio-text pair as an indeterminable test case if none of them can successfully transcribe the audio.\"\nThis quote describes the filtering mechanism that ensures a baseline level of intelligibility for an input to be considered 'valid', thus enforcing a form of plausibility. The paper lacks any further assessment beyond this."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility via a rule-based cross-referencing filter: synthesized audio-text pairs are accepted as valid only if at least one ASR correctly transcribes them, which ensures generated inputs are intelligible/valid at the time of creation. However, the paper does not provide any human evaluation or task-specific perceptual/semantic quality metrics (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for the final fault-inducing inputs, nor does it report additional quantitative checks of perceptual realism. Also, although their cross-referencing filter enforces plausibility per generated audio, the work does not report explicit mechanisms or analyses addressing cumulative mutations or perceptual assessment of final failing inputs. These facts align with the \"Medium\" level: plausibility is enforced by rule-based constraints but there is no empirical perceptual validation of final fault-triggering inputs.",
                    "evidence": "Section II.A (p.675): \"to avoid using the invalid audios synthesized by the TTS system, the generated audio-text pair is fed into multiple ASR systems for cross-referencing: If at least one ASR system can successfully transcribe the audio, the audio-text pair is viewed as a valid test case.\"; Section II.A (p.675): \"We take the audio-text pair as an indeterminable test case if none of them can successfully transcribe the audio.\"; Section III.B (p.676): \"The TTS system is ResponsiveVoice. The 5 ASR systems are DeepSpeech, DeepSpeech2, Wav2Letter++, Wav2Vec2, and Wit.\"; Section III.C (p.676): \"We use two metrics to evaluate the performance of the SUT. One is the number of failed test cases uncovered by CrossASR++, and the other is the word error rate (WER) on a test set.\". The paper contains no reported human study or perceptual/quality measures (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for assessing realism of final fault-inducing inputs."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper's test-generation pipeline synthesizes and uses audio files directly via a TTS system (i.e., inputs are created in serialized audio form and stored as audio\u2013text pairs). According to the metric, approaches that generate inputs directly in serialized formats (such as synthesized speech) are reproducible under standard I/O operations because the fault-inducing perturbations exist in the saved audio files rather than only in transient in-memory floating-point tensors. The paper both (1) synthesizes and feeds audio files to ASR systems and (2) stores those audio\u2013text pairs and fine-tunes models on them, which indicates failures are tied to persisted audio files rather than fragile in-memory numerical perturbations that could be lost by serialization.",
                    "evidence": "Page 675, Section II.A (CrossASR++): \"Specifically, given a piece of text from the corpus, it first employs a TTS system to synthesize an audio file from the text. The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1).\"; Page 675, Section II.B (Improving ASR Systems) and Fig.1 caption: \"We store test cases for both the ASR system under test (SUT) and cross-referenced AS systems and then fine-tune these ASR systems on their corresponding test cases.\"; Abstract: \"It employs a Text-To-Speech (TTS) system to synthesize audios from texts and then reveals failed test cases by feeding them to multiple ASR systems for cross-referencing.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The method generates test cases by synthesizing audio from text using a Text-To-Speech (TTS) system. This process creates test inputs directly in a serialized audio format. The metric definition explicitly states that fuzzing approaches generating inputs directly in serialized formats (e.g., synthesized speech) are considered reproducible, as such inputs are inherently stable with respect to standard I/O operations. The fault-inducing properties are embedded within the synthesized audio file itself, not in fragile, in-memory numerical perturbations that could be lost during serialization processes like rounding or quantization.",
                    "evidence": "Page 2, Section II-A, CrossASR++: 'Specifically, given a piece of text from the corpus, it first employs a TTS system to synthesize an audio file from the text. The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1).'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper's test-generation pipeline synthesizes and uses audio files directly via a TTS system (i.e., inputs are created in serialized audio form and stored as audio\u2013text pairs). According to the metric, approaches that generate inputs directly in serialized formats (such as synthesized speech) are reproducible under standard I/O operations because the fault-inducing perturbations exist in the saved audio files rather than only in transient in-memory floating-point tensors. The paper both (1) synthesizes and feeds audio files to ASR systems and (2) stores those audio\u2013text pairs and fine-tunes models on them, which indicates failures are tied to persisted audio files rather than fragile in-memory numerical perturbations that could be lost by serialization.",
                    "evidence": "Page 675, Section II.A (CrossASR++): \"Specifically, given a piece of text from the corpus, it first employs a TTS system to synthesize an audio file from the text. The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1).\"; Page 675, Section II.B (Improving ASR Systems) and Fig.1 caption: \"We store test cases for both the ASR system under test (SUT) and cross-referenced AS systems and then fine-tune these ASR systems on their corresponding test cases.\"; Abstract: \"It employs a Text-To-Speech (TTS) system to synthesize audios from texts and then reveals failed test cases by feeding them to multiple ASR systems for cross-referencing.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. While CrossASR++ feeds each synthesized audio to multiple ASR systems simultaneously (so generated inputs are executed on several models), the authors do not (a) craft fault-inducing inputs on a surrogate model and then reuse them to test other models, nor (b) analyze whether fault-inducing inputs discovered for one model also trigger failures in other models. The experiments focus on (i) fine-tuning the SUT with generated test cases to reduce its failures, and (ii) fine-tuning one cross-referenced ASR to improve CrossASR++\u2019s ability to find failures. These evaluations measure improvement in detection and model quality, not the transferability of faults between independently-trained target models.",
                    "evidence": "Page 675, Section II.A: \"The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1). The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio.\" \n\nPage 677, Section III.C (RQ2 - Experiment Design & Result): \"In this research question, we want to investigate whether fine-tuning one of the cross-referenced ASR systems using generated test cases can boost the performance of CrossASR++ to uncover failed test cases... For the evolved CrossASR++, we continuously evolve DeepSpeech and always utilize DeepSpeech as one of the ASR systems used for cross-referencing... By comparing the number of failed test cases found by the static and evolved CrossASR++ head-to-head, it is clear that the evolved CrossASR++ uncovers more failed test cases than the static one for each ASR system.\" \n\n(These excerpts show that inputs are evaluated across multiple ASRs and that the paper measures improvements from fine-tuning, but there is no experiment or analysis that shows fault-inducing inputs crafted on one (surrogate) model transfer to other models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, evolutionary differential testing, uses multiple Automatic Speech Recognition (ASR) models for cross-referencing. However, this is done to create a reliable test oracle\u2014a test case is deemed valid if at least one model can correctly transcribe the audio. A failure is then identified specifically for the System Under Test (SUT) if it fails on this valid test case. The paper does not investigate or demonstrate whether these fault-inducing inputs for one SUT also trigger failures in other ASR models. The generated failed test cases are used to fine-tune and improve the SUT itself, rather than being reused to test for attack transferability across different models. The core focus is on improving a single model or the testing tool, not on discovering shared vulnerabilities.",
                    "evidence": "Page 2, Section II-A, \"CrossASR++\": \"A valid test case can be either a failed test case or a successful test case. If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT. Otherwise, a successful test case is found.\" This quote illustrates that a failure is defined and recorded for a single, specific SUT, not in the context of transferability to other models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. While CrossASR++ feeds each synthesized audio to multiple ASR systems simultaneously (so generated inputs are executed on several models), the authors do not (a) craft fault-inducing inputs on a surrogate model and then reuse them to test other models, nor (b) analyze whether fault-inducing inputs discovered for one model also trigger failures in other models. The experiments focus on (i) fine-tuning the SUT with generated test cases to reduce its failures, and (ii) fine-tuning one cross-referenced ASR to improve CrossASR++\u2019s ability to find failures. These evaluations measure improvement in detection and model quality, not the transferability of faults between independently-trained target models.",
                    "evidence": "Page 675, Section II.A: \"The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1). The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio.\" \n\nPage 677, Section III.C (RQ2 - Experiment Design & Result): \"In this research question, we want to investigate whether fine-tuning one of the cross-referenced ASR systems using generated test cases can boost the performance of CrossASR++ to uncover failed test cases... For the evolved CrossASR++, we continuously evolve DeepSpeech and always utilize DeepSpeech as one of the ASR systems used for cross-referencing... By comparing the number of failed test cases found by the static and evolved CrossASR++ head-to-head, it is clear that the evolved CrossASR++ uncovers more failed test cases than the static one for each ASR system.\" \n\n(These excerpts show that inputs are evaluated across multiple ASRs and that the paper measures improvements from fine-tuning, but there is no experiment or analysis that shows fault-inducing inputs crafted on one (surrogate) model transfer to other models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "Does not demonstrate if fuzzed input on one model aslo causes error in another model",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Data-driven"
            ]
        }
    },
    "Pei et al. - 2017 - DeepXplore Automated Whitebox Testing of Deep Learning Systems": {
        "key": "pei2017deepxplore",
        "name": "DeepXplore",
        "year": "2017",
        "citation_count": "1814",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper demonstrates that DeepXplore uncovers failures that violate user intent and safety expectations \u2014 e.g., it finds inputs that cause self-driving models to make dangerous steering decisions (\"crashing into guard rails\") and causes malware detectors to misclassify malicious files as benign \u2014 which are high-impact in real-world terms. However, the paper does not claim or demonstrate that DeepXplore specifically bypasses explicit safety, policy, or security mechanisms in defended or safety-aligned models (e.g., adversarially-trained or policy-enforced systems), nor does it claim to induce jailbreaks, data leakage, or unauthorized actions by circumventing explicit defenses. Moreover, the authors explicitly note limitations of differential testing (it requires multiple models and cannot find failures if all models make the same mistake), indicating the method is not presented as a technique for bypassing explicit safeguards. Thus the failures exposed match the metric's description for Medium severity (unsafe/undesired behaviors without demonstrated bypassing of explicit defenses).",
                    "evidence": "Page 1, Abstract: \"DeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models...\"; Page 11, Section 7 / Table 3 caption: \"The features added to the manifest file by DeepXplore for generating two sample malware inputs which Android app classifiers (Drebin) incorrectly mark as benign.\"; Page 14, Section 8 Limitations: \"First, differential testing requires at least two different DNNs with the same functionality... Second, differential testing can only detect an erroneous behavior if at least one DNN produces different results than other DNNs. If all the tested DNNs make the same mistake, DeepXplore cannot generate the corresponding test case.\"; Page 1, Introduction: \"In safety- and security-critical settings, such incorrect behaviors can lead to disastrous consequences such as a fatal collision of a self-driving car.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper demonstrates that DeepXplore uncovers failures that go beyond simple model robustness errors and directly violate user and safety expectations by producing unsafe or security-compromising outputs. Specifically, the authors show that their method can generate inputs causing a self-driving car model to make decisions that lead to a crash, and cause malware detection models to misclassify malicious software as benign. These represent clear violations of safety and security expectations. However, the paper does not provide evidence of bypassing explicit, engineered safety or security mechanisms in defended or safety-aligned models (e.g., adversarially trained models, or models with safety filters), which is a requirement for the 'High' rating. The failures are exposed in standard, state-of-the-art models of the time, not models with specific defenses.",
                    "evidence": "Page 2, Abstract: \"DeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models...\"\n\nPage 3, Figure 1 Caption: \"An example erroneous behavior found by DeepXplore in Nvidia DAVE-2 self-driving car platform. The DNN-based self-driving car correctly decides to turn left for image (a) but incorrectly decides to turn right and crashes into the guardrail for image (b), a slightly darker version of (a).\"\n\nPage 9, Section 6.1, Test datasets and DNNs: The evaluation is performed on datasets with clear safety and security implications, including the 'Driving' dataset from Udacity's self-driving car challenge, the 'Contagio/VirusTotal' PDF malware dataset, and the 'Drebin' Android malware dataset."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper demonstrates that DeepXplore uncovers failures that violate user intent and safety expectations \u2014 e.g., it finds inputs that cause self-driving models to make dangerous steering decisions (\"crashing into guard rails\") and causes malware detectors to misclassify malicious files as benign \u2014 which are high-impact in real-world terms. However, the paper does not claim or demonstrate that DeepXplore specifically bypasses explicit safety, policy, or security mechanisms in defended or safety-aligned models (e.g., adversarially-trained or policy-enforced systems), nor does it claim to induce jailbreaks, data leakage, or unauthorized actions by circumventing explicit defenses. Moreover, the authors explicitly note limitations of differential testing (it requires multiple models and cannot find failures if all models make the same mistake), indicating the method is not presented as a technique for bypassing explicit safeguards. Thus the failures exposed match the metric's description for Medium severity (unsafe/undesired behaviors without demonstrated bypassing of explicit defenses).",
                    "evidence": "Page 1, Abstract: \"DeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models...\"; Page 11, Section 7 / Table 3 caption: \"The features added to the manifest file by DeepXplore for generating two sample malware inputs which Android app classifiers (Drebin) incorrectly mark as benign.\"; Page 14, Section 8 Limitations: \"First, differential testing requires at least two different DNNs with the same functionality... Second, differential testing can only detect an erroneous behavior if at least one DNN produces different results than other DNNs. If all the tested DNNs make the same mistake, DeepXplore cannot generate the corresponding test case.\"; Page 1, Introduction: \"In safety- and security-critical settings, such incorrect behaviors can lead to disastrous consequences such as a fatal collision of a self-driving car.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "reveals incorrect corner-case behaviors but does not bypass explicity safety mechinasims",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "DeepXplore is designed to find inputs that cause differential behaviors between multiple models and to maximize neuron coverage, not to drive a model toward a specific attacker-chosen output. The paper formulates the objective as maximizing differences (making at least one DNN produce a different prediction) and neuron activation; it selects a class c from the seed input and a random target model to reduce that model's confidence relative to others, but it does not provide a mechanism to specify an arbitrary attacker-chosen target label or output (e.g., force output = specific label/phrase). Thus the exploration is untargeted (aimed at discovering disagreements/misbehaviors), rather than targeted to induce a predefined specific output.",
                    "evidence": "Page 5, Section 3 (Overview): \"we solve a joint optimization problem that maximizes both differential behaviors and neuron coverage.\"; Page 6, Section 4.2 (Maximizing differential behaviors): \"Given an arbitrary x as seed that gets classified to the same class by all DNNs, our goal is to modify x such that the modified input x' will be classified differently by at least one of the n DNNs.\"; Page 6, Algorithm 1 / Equation 2: Algorithm 1 lines show \"c = dnns[0].predict(x)\" and \"d = randomly select one dnn from dnns\", and Equation 2 defines obj1 as \"obj1(x) = \u03a3_k F_k(x)[c] \u2212 \u03bb1 \u00b7 F_j(x)[c]\", i.e., an objective to induce differential predictions rather than to force a specific attacker-chosen target output."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The method proposed in the paper, DeepXplore, uses differential testing to find inputs that cause disagreements among multiple deep neural networks (DNNs). The explicit goal is to discover inputs that are classified differently by at least one DNN compared to its peers. The optimization objective is formulated to decrease one model's confidence in the original input's class while maintaining the other models' confidence. This approach is inherently untargeted, as it searches for *any* inconsistent prediction or misclassification, rather than steering the input transformation toward a specific, predefined target class or behavior. The framework's purpose is to find generic faults revealed by model disagreement, which aligns with the definition of untargeted exploration.",
                    "evidence": "Page 8, Section 4.2, 'Maximizing differential behaviors': 'Given an arbitrary x as seed that gets classified to the same class by all DNNs, our goal is to modify x such that the modified input x' will be classified differently by at least one of the n DNNs.' This statement clearly defines the goal as inducing any difference in classification, not a specific target output. This is further formalized in Equation 2 on the same page, which aims to minimize the confidence of one selected DNN for the original class, thereby causing it to predict any other class."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DeepXplore is designed to find inputs that cause differential behaviors between multiple models and to maximize neuron coverage, not to drive a model toward a specific attacker-chosen output. The paper formulates the objective as maximizing differences (making at least one DNN produce a different prediction) and neuron activation; it selects a class c from the seed input and a random target model to reduce that model's confidence relative to others, but it does not provide a mechanism to specify an arbitrary attacker-chosen target label or output (e.g., force output = specific label/phrase). Thus the exploration is untargeted (aimed at discovering disagreements/misbehaviors), rather than targeted to induce a predefined specific output.",
                    "evidence": "Page 5, Section 3 (Overview): \"we solve a joint optimization problem that maximizes both differential behaviors and neuron coverage.\"; Page 6, Section 4.2 (Maximizing differential behaviors): \"Given an arbitrary x as seed that gets classified to the same class by all DNNs, our goal is to modify x such that the modified input x' will be classified differently by at least one of the n DNNs.\"; Page 6, Algorithm 1 / Equation 2: Algorithm 1 lines show \"c = dnns[0].predict(x)\" and \"d = randomly select one dnn from dnns\", and Equation 2 defines obj1 as \"obj1(x) = \u03a3_k F_k(x)[c] \u2212 \u03bb1 \u00b7 F_j(x)[c]\", i.e., an objective to induce differential predictions rather than to force a specific attacker-chosen target output."
                },
                "manual": {
                    "value": "Low",
                    "why": "performs untargeted exploration to reveal generic model faults (differential behaviour, inaccuracies). No targeted misclassification.",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper goes beyond merely reporting failures: it introduces and measures neuron coverage (an internal model signal), analyzes neuron activation patterns (e.g., overlap between inputs of same vs different classes), reports how coverage correlates with discovery/diversity of failures, inspects which input features the fuzzer changes for malware/PDF cases, and uses generated failures to diagnose and remedy dataset problems (detecting polluted training labels and improving accuracy via retraining). These analyses link discovered failures to underlying model vulnerabilities such as dataset pollution/bias and limited coverage of learned rules.",
                    "evidence": "Page 6, Section 4.1 (Neuron coverage): \"We define neuron coverage of a set of test inputs as the ratio of the number of unique activated neurons for all test inputs and the total number of neurons in the DNN. We consider a neuron to be activated if its output is higher than a threshold value (e.g., 0).\"\n\nPage 12, Section 7.1 (Benefits of neuron coverage / Activation of neurons for different classes): \"Our results show that inputs coming from the same class share more activated neurons than those coming from different classes. As inputs from different classes tend to get detected through matching of different DNN rules, our result also confirms that neuron coverage can effectively estimate the numbers of different rules activated during DNN testing.\" (Table 7 & surrounding text)\n\nPage 11, Section 7 (Results), Table 3 and Table 4 (examples): Table 3 caption: \"The features added to the manifest file by DeepXplore for generating two sample malware inputs which Android app classifiers (Drebin) incorrectly mark as benign.\" (shows specific manifest features toggled). Table 4 caption: \"The top-3 most in(de)cremented features for generating two sample malware inputs which PDF classifiers incorrectly mark as benign.\" (shows concrete feature changes correlated with failures).\n\nPage 14, Section 7.3 (Detecting training data pollution attack): \"Using this process, we are able to correctly identify 95.6% of the polluted samples.\" (shows they link discovered difference-inducing inputs to dataset pollution and use that for diagnosis).\n\nPage 14, Section 8 (Causes of differences between DNNs): \"The underlying root cause behind prediction differences between two DNNs for the same input is differences in their decision logic/boundaries. As described in \u00a72.1, a DNN's decision logic is determined by multiple factors including training data, the DNN architecture, hyperparameters, etc. Therefore, any differences in the choice of these factors will result in subtle changes in the decision logic of the resulting DNN.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper provides diagnostic analysis that links discovered failures to underlying model vulnerabilities, specifically dataset bias or pollution. In Section 7.3, the authors conduct an experiment where they use the generated error-inducing inputs to identify mislabeled data within a polluted training set. They generate inputs that cause a discrepancy between a model trained on clean data and one trained on polluted data. Then, by finding the training samples most similar to these error-inducing inputs, they successfully identify the polluted data points. This demonstrates a method for tracing discovered failures back to a specific, underlying vulnerability (dataset pollution), thereby explaining why these failures occur.",
                    "evidence": "Page 15, Section 7.3, 'Detecting training data pollution attack': 'As another application of DeepXplore, we demonstrate how it can be used to detect training data pollution attacks with an experiment on two LeNet-5 DNNs: one trained on 60, 000 hand-written digits from MNIST dataset and the other trained on an artificially polluted version of the same dataset where 30% of the images originally labeled as digit 9 are mislabeled as 1. We use DeepXplore to generate error-inducing inputs that are classified as the digit 9 and 1 by the unpolluted and polluted versions of the LeNet-5 DNN respectively. We then search for samples in the training set that are closest to the inputs generated by DeepXplore in terms of structural similarity [80] and identify them as polluted data. Using this process, we are able to correctly identify 95.6% of the polluted samples.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper goes beyond merely reporting failures: it introduces and measures neuron coverage (an internal model signal), analyzes neuron activation patterns (e.g., overlap between inputs of same vs different classes), reports how coverage correlates with discovery/diversity of failures, inspects which input features the fuzzer changes for malware/PDF cases, and uses generated failures to diagnose and remedy dataset problems (detecting polluted training labels and improving accuracy via retraining). These analyses link discovered failures to underlying model vulnerabilities such as dataset pollution/bias and limited coverage of learned rules.",
                    "evidence": "Page 6, Section 4.1 (Neuron coverage): \"We define neuron coverage of a set of test inputs as the ratio of the number of unique activated neurons for all test inputs and the total number of neurons in the DNN. We consider a neuron to be activated if its output is higher than a threshold value (e.g., 0).\"\n\nPage 12, Section 7.1 (Benefits of neuron coverage / Activation of neurons for different classes): \"Our results show that inputs coming from the same class share more activated neurons than those coming from different classes. As inputs from different classes tend to get detected through matching of different DNN rules, our result also confirms that neuron coverage can effectively estimate the numbers of different rules activated during DNN testing.\" (Table 7 & surrounding text)\n\nPage 11, Section 7 (Results), Table 3 and Table 4 (examples): Table 3 caption: \"The features added to the manifest file by DeepXplore for generating two sample malware inputs which Android app classifiers (Drebin) incorrectly mark as benign.\" (shows specific manifest features toggled). Table 4 caption: \"The top-3 most in(de)cremented features for generating two sample malware inputs which PDF classifiers incorrectly mark as benign.\" (shows concrete feature changes correlated with failures).\n\nPage 14, Section 7.3 (Detecting training data pollution attack): \"Using this process, we are able to correctly identify 95.6% of the polluted samples.\" (shows they link discovered difference-inducing inputs to dataset pollution and use that for diagnosis).\n\nPage 14, Section 8 (Causes of differences between DNNs): \"The underlying root cause behind prediction differences between two DNNs for the same input is differences in their decision logic/boundaries. As described in \u00a72.1, a DNN's decision logic is determined by multiple factors including training data, the DNN architecture, hyperparameters, etc. Therefore, any differences in the choice of these factors will result in subtle changes in the decision logic of the resulting DNN.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Correlates failures with coverages. Pollution attack detection is about how the fuzzer generated failures can be used to detect pollution attack but they do not explain why the failures occur",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces input-plausibility via explicit, rule-based domain constraints that are applied during gradient-based mutation (the authors describe modifying gradients so each iteration preserves constraints and give concrete image/file constraints). This shows the fuzzing design actively restricts mutations to remain realistic at each mutation step. However, the paper does not provide any empirical, task-specific or human evaluation of the final fault-inducing inputs\u2019 perceptual or semantic realism (no human study or quantitative perceptual measures such as LPIPS/SSIM/PESQ/STOI/Perplexity are reported). Because the work enforces plausibility through bounded / rule-based constraints during mutation but does not report explicit empirical validation of final-input realism, the appropriate rating is Medium.",
                    "evidence": "Section 4.2 (Domain-specific constraints), p. 6: \"One important aspect of the optimization process is that the generated test inputs need to satisfy several domain-specific constraints to be physically realistic... Our algorithm ensures this property by modifying the gradient \u0434rad (line 13 in Algorithm 1) such that xi+1 = xi + s \u00b7 \u0434rad still satisfies the constraints (s is the step size in the gradient ascent).\" \n\nSection 6.2 (Domain-specific constraints), p. 9: \"DeepXplore used three different types of constraints for simulating different environment conditions of images: (1) lighting effects for simulating different intensities of lights, (2) occlusion by a single small rectangle for simulating an attacker potentially blocking some parts of a camera, and (3) occlusion by multiple tiny black rectangles for simulating effects of dirt on camera lens... The first constraint restricts image modifications so that DeepXplore can only make the image darker or brighter without changing its content. Specifically, the modification can only increase or decrease all pixel values by the same amount...\" \n\nSection 7 (Results), p. 10: \"For the experimental results shown in Figure 8, we apply three domain-specific constraints (lighting effects, occlusion by a single rectangle, and occlusion by multiple rectangles) as described in \u00a76.2.\" \n\n(Absence of empirical perceptual evaluation) The evaluation (Sections 7 and 7.2, pp. 10\u201313) reports neuron coverage, counts of difference-inducing inputs, runtimes, and retraining accuracy improvements, but contains no human-subject study or quantitative perceptual/linguistic measures (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity) assessing whether final fault-inducing inputs remain perceptually/semantically realistic."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility through explicit, rule-based, domain-specific constraints designed to generate realistic inputs. For image-based tasks, these constraints simulate plausible environmental conditions like changes in lighting, occlusions from objects, or dirt on a camera lens. For other domains like Android malware, constraints ensure that application functionality is not broken (e.g., only adding permissions, not removing them). The paper provides visual examples (Figure 8) of the generated inputs, which serve as a qualitative validation of their plausibility. However, it does not meet the 'High' criteria because it lacks a formal, empirical validation of the final fault-inducing inputs, such as a human study or quantitative perceptual quality metrics (e.g., LPIPS, SSIM), to confirm their realism. The methodology relies on pre-defined rules rather than ensuring mutations remain within a learned plausible distribution.",
                    "evidence": "Page 10, Section 6.2, \"Domain-specific constraints\": \"As discussed earlier, to be useful in practice, we need to ensure that the generated tests are valid and realistic by applying domain-specific constraints. For example, generated images should be physically producible by a camera... Below we describe two major types of domain-specific constraints (i.e., image and file constraints) that we use in this paper.\"\n\nPage 10, Section 6.2, \"Image constraints (MNIST, ImageNet, and Driving)\": \"DeepXplore used three different types of constraints for simulating different environment conditions of images: (1) lighting effects for simulating different intensities of lights, (2) occlusion by a single small rectangle for simulating an attacker potentially blocking some parts of a camera, and (3) occlusion by multiple tiny black rectangles for simulating effects of dirt on camera lens.\"\n\nPage 11, Figure 8: This figure provides visual examples of the seed inputs and the final difference-inducing inputs generated by DeepXplore under the various constraints (lighting, single occlusion, multiple occlusions), qualitatively demonstrating the plausibility of the outputs."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces input-plausibility via explicit, rule-based domain constraints that are applied during gradient-based mutation (the authors describe modifying gradients so each iteration preserves constraints and give concrete image/file constraints). This shows the fuzzing design actively restricts mutations to remain realistic at each mutation step. However, the paper does not provide any empirical, task-specific or human evaluation of the final fault-inducing inputs\u2019 perceptual or semantic realism (no human study or quantitative perceptual measures such as LPIPS/SSIM/PESQ/STOI/Perplexity are reported). Because the work enforces plausibility through bounded / rule-based constraints during mutation but does not report explicit empirical validation of final-input realism, the appropriate rating is Medium.",
                    "evidence": "Section 4.2 (Domain-specific constraints), p. 6: \"One important aspect of the optimization process is that the generated test inputs need to satisfy several domain-specific constraints to be physically realistic... Our algorithm ensures this property by modifying the gradient \u0434rad (line 13 in Algorithm 1) such that xi+1 = xi + s \u00b7 \u0434rad still satisfies the constraints (s is the step size in the gradient ascent).\" \n\nSection 6.2 (Domain-specific constraints), p. 9: \"DeepXplore used three different types of constraints for simulating different environment conditions of images: (1) lighting effects for simulating different intensities of lights, (2) occlusion by a single small rectangle for simulating an attacker potentially blocking some parts of a camera, and (3) occlusion by multiple tiny black rectangles for simulating effects of dirt on camera lens... The first constraint restricts image modifications so that DeepXplore can only make the image darker or brighter without changing its content. Specifically, the modification can only increase or decrease all pixel values by the same amount...\" \n\nSection 7 (Results), p. 10: \"For the experimental results shown in Figure 8, we apply three domain-specific constraints (lighting effects, occlusion by a single rectangle, and occlusion by multiple rectangles) as described in \u00a76.2.\" \n\n(Absence of empirical perceptual evaluation) The evaluation (Sections 7 and 7.2, pp. 10\u201313) reports neuron coverage, counts of difference-inducing inputs, runtimes, and retraining accuracy improvements, but contains no human-subject study or quantitative perceptual/linguistic measures (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity) assessing whether final fault-inducing inputs remain perceptually/semantically realistic."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The DeepXplore paper explicitly enforces domain-specific constraints that prevent out-of-range or non-integer feature values (e.g., clipping pixel values to [0,255] and rounding discrete features), which partially addresses I/O effects such as clipping/quantization. However, the paper does not describe explicit handling or verification of common serialization/encoding effects (e.g., saving as PNG/JPEG or re-loading compressed formats) nor does it state that generated tests are serialized and re-evaluated through typical I/O pipelines before deciding a failure. Thus DeepXplore partially considers serialization effects (clipping/discretization) but does not fully account for I/O/encoding effects during both generation and oracle evaluation, matching the \u201cMedium\u201d level in the metric definition.",
                    "evidence": "Section 3 (Overview) \u2014 page 5: \u201cWe further add domain-specific constraints (e.g., ensure the pixel values are integers within 0 and 255 for image input) to make sure that the modified inputs still represent real-world images.\u201d; Section 4.2 (Domain-specific constraints) \u2014 page 9: \u201cFor discrete features, we round the gradient to an integer. For DNNs handling visual input (e.g., images), we add different spatial restrictions ... we only allow the pixel values to be decreased.\u201d; Algorithm 1 / Section 4.2 \u2014 page 6: the oracle check uses model predictions in memory (pseudo-code): \u201cif d.predict(x) != (dnns-d).predict(x) then gen_test.add(x)\u201d, indicating failures are detected from in-memory model outputs rather than from inputs re-serialized and re-loaded through standard I/O pipelines. (No text in the paper describes explicit serialization/encoding and re-evaluation steps such as saving to PNG/JPEG/WAV and confirming failures persist.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The method proposed in the paper, DeepXplore, partially accounts for I/O effects. The methodology includes applying 'domain-specific constraints' to ensure that generated test inputs are 'physically realistic'. For image-based inputs, this explicitly includes clipping, i.e., ensuring pixel values remain within a valid range like [0, 255]. The paper also describes using several semantic-preserving metamorphic transformations, such as simulating different lighting conditions and adding occlusions. However, the paper does not explicitly state that it accounts for quantization or rounding effects inherent in standard serialization formats (e.g., saving a floating-point tensor as an 8-bit PNG). The core test generation works by applying gradient ascent to a continuous representation of the input, and there is no mention of a serialization/deserialization step to confirm that a discovered failure persists after such an operation. Therefore, since the method considers clipping and uses metamorphic transformations but does not fully address all aspects of serialization like rounding, it aligns with the 'Medium' rating.",
                    "evidence": "Page 8, Section 4.2, 'Domain-specific constraints': 'One important aspect of the optimization process is that the generated test inputs need to satisfy several domain-specific constraints to be physically realistic [63]. [...] For example, for a generated test image x the pixel values must be within a certain range (e.g., 0 to 255).' and Page 10, Section 6.2, 'Image constraints (MNIST, ImageNet, and Driving)': 'DeepXplore used three different types of constraints for simulating different environment conditions of images: (1) lighting effects for simulating different intensities of lights, (2) occlusion by a single small rectangle for simulating an attacker potentially blocking some parts of a camera, and (3) occlusion by multiple tiny black rectangles for simulating effects of dirt on camera lens.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The DeepXplore paper explicitly enforces domain-specific constraints that prevent out-of-range or non-integer feature values (e.g., clipping pixel values to [0,255] and rounding discrete features), which partially addresses I/O effects such as clipping/quantization. However, the paper does not describe explicit handling or verification of common serialization/encoding effects (e.g., saving as PNG/JPEG or re-loading compressed formats) nor does it state that generated tests are serialized and re-evaluated through typical I/O pipelines before deciding a failure. Thus DeepXplore partially considers serialization effects (clipping/discretization) but does not fully account for I/O/encoding effects during both generation and oracle evaluation, matching the \u201cMedium\u201d level in the metric definition.",
                    "evidence": "Section 3 (Overview) \u2014 page 5: \u201cWe further add domain-specific constraints (e.g., ensure the pixel values are integers within 0 and 255 for image input) to make sure that the modified inputs still represent real-world images.\u201d; Section 4.2 (Domain-specific constraints) \u2014 page 9: \u201cFor discrete features, we round the gradient to an integer. For DNNs handling visual input (e.g., images), we add different spatial restrictions ... we only allow the pixel values to be decreased.\u201d; Algorithm 1 / Section 4.2 \u2014 page 6: the oracle check uses model predictions in memory (pseudo-code): \u201cif d.predict(x) != (dnns-d).predict(x) then gen_test.add(x)\u201d, indicating failures are detected from in-memory model outputs rather than from inputs re-serialized and re-loaded through standard I/O pipelines. (No text in the paper describes explicit serialization/encoding and re-evaluation steps such as saving to PNG/JPEG/WAV and confirming failures persist.)"
                },
                "manual": {
                    "value": "High",
                    "why": "Enforces domain specific constraints like keeping pixel values between 0 and 255, rounding gradient to integer which satisfies the condition for failure reproduciblity",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's testing approach is explicitly differential: it requires whitebox access to multiple DNNs and constructs inputs by optimizing across those models so that they disagree. DeepXplore does not present or evaluate the scenario where fault-inducing inputs are crafted on a single surrogate model and then reused (without access) to trigger failures in other, unseen target models. In other words, the paper demonstrates generating inputs that cause differences among available models, but it does not demonstrate or measure transferability of fault-inducing inputs from one model to other independent/unknown models (the kind of surrogate-model transfer evaluated by the metric).",
                    "evidence": "Page 1, Abstract: \"Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles to avoid manual checking.\"; Page 5, Section 3 (Overview): \"DeepXplore takes unlabeled test inputs as seeds and generates new tests that cover a large number of neurons ... while causing the tested DNNs to behave differently.\"; Page 6, Section 4.2 / Algorithm 1: algorithm inputs show \"dnns \u2190 multiple DNNs under test\" and the algorithm selects \"d = randomly select one dnn from dnns\" (showing the method relies on multiple available models during generation); Page 14, Section 8 (Limitations): \"First, differential testing requires at least two different DNNs with the same functionality. Further, if two DNNs only differ slightly ... DeepXplore will take longer to find difference-inducing inputs...\" (stating the necessity of multiple models and acknowledging that DeepXplore cannot operate in the single-surrogate transfer scenario)."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's core methodology, differential testing, is explicitly designed to discover transferable failures by finding inputs that cause different behaviors across multiple deep learning models with similar functionality. The method does not simply generate a fault-inducing input for one model and then test its transferability on others as a secondary step. Instead, it uses multiple models as 'cross-referencing oracles' from the outset. The fuzzing design incorporates an explicit mechanism\u2014a joint optimization function\u2014that directly aims to maximize the behavioral differences between the models under test. This process inherently generates inputs that demonstrate failures across different models.",
                    "evidence": "Page 7, Section 4.2, 'Maximizing differential behaviors': 'The first objective of the optimization problem is to generate test inputs that can induce different behaviors in the tested DNNs, i.e., different DNNs will classify the same input into different classes.' and Page 8, Equation (2) and its description: 'obj1(x) = \u2211k\u2260j Fk(x)[c] - \u03bb1 \u00b7 Fj(x)[c] where \u03bb1 is a parameter to balance the objective terms between the DNNs Fk\u2260j that maintain the same class outputs as before and the DNN Fj that produce different class outputs.' This objective function is the explicit mechanism to find transferable/differential failures."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DeepXplore\u2019s approach requires whitebox access to multiple DNNs during test generation and optimizes inputs to provoke disagreements among them. It does not generate fault-inducing inputs on a surrogate model and then reuse those inputs (without internal access) to trigger failures in independently trained or unseen target models. Thus, it does not evaluate or support the traditional notion of attack transferability from one model to another.",
                    "evidence": "Page 1 (Abstract): \u201cNext, we leverage multiple DL systems with similar functionality as cross-referencing oracles\u2026\u201d; Page 5 (Section 3 Overview): \u201cDeepXplore takes unlabeled test inputs as seeds \u2026 while causing the tested DNNs to behave differently.\u201d; Page 6 (Algorithm 1 inputs): \u201cdnns \u2190 multiple DNNs under test\u201d and selection of one DNN from dnns for optimization; Page 14 (Section 8 Limitations): \u201cdifferential testing requires at least two different DNNs with the same functionality\u2026 DeepXplore cannot operate in the single-surrogate transfer scenario.\u201d"
                },
                "manual": {
                    "value": "Low",
                    "why": "It is differential testing based oracle. Not transferability.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Whitebox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Prediction-guided"
            ]
        }
    },
    "Gao et al. - 2020 - Fuzz testing based data augmentation to improve robustness of deep neural networks": {
        "key": "gao2020fuzz",
        "name": "Sensei",
        "year": "2020",
        "citation_count": "169",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper frames and evaluates Sensei as a data-augmentation/search technique to improve DNN robustness to natural, benign input variations (rotation, translation, brightness, etc.). The method is explicitly targeted at reducing mispredictions under semantically-preserving mutations and improving 'robust accuracy' on perturbed images. The authors explicitly state they do not target adversarial (security) perturbations and do not claim to bypass safety, policy, or defense mechanisms; no experiments attempt jailbreaks, data exfiltration, or violations of explicit safety mechanisms. Therefore the failures the fuzzer exposes are standard robustness errors (mispredictions/inconsistencies under input mutations), not high- or medium-severity safety/security violations.",
                    "evidence": "Page 1148, Introduction: \"In this work, we focus on the latter scenario and investigate how software testing techniques can be applied for the robust generalization of Deep Neural Networks (DNNs) to natural environmental variations.\"; Page 1149, Section 2.3 (Robustness of DNNs): \"As discussed earlier, this work targets robustness issues arising from natural, environmental perturbations \u03b4 in the input data and not perturbations \u03b4 constructed adversarially, in a security context.\"; Page 1153, Section 4.3 (Evaluation Metric): \"robust accuracy is the proportion of images in the testing dataset where the prediction of a DNN does not fluctuate with any small realistic perturbations.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper focuses exclusively on improving model robustness against semantically preserving input mutations that simulate natural environmental variations. The failures uncovered by the fuzzing process are defined as mispredictions or prediction inconsistencies under these small perturbations. The paper's stated goal, methodology, and evaluation metric (robust accuracy) are all centered on this type of low-severity robustness error. It explicitly distinguishes its scope from security contexts or adversarial attacks and does not address unsafe behaviors like toxicity or bias, nor does it attempt to bypass any safety or security mechanisms.",
                    "evidence": "Page 4, Section 2.3, \"As discussed earlier, this work targets robustness issues arising from natural, environmental perturbations \u03b4 in the input data and not perturbations \u03b4 constructed adversarially, in a security context.\" and Page 7, Section 4.3, \"More specifically, robust accuracy is the proportion of images in the testing dataset where the prediction of a DNN does not fluctuate with any small realistic perturbations.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper frames and evaluates Sensei as a data-augmentation/search technique to improve DNN robustness to natural, benign input variations (rotation, translation, brightness, etc.). The method is explicitly targeted at reducing mispredictions under semantically-preserving mutations and improving 'robust accuracy' on perturbed images. The authors explicitly state they do not target adversarial (security) perturbations and do not claim to bypass safety, policy, or defense mechanisms; no experiments attempt jailbreaks, data exfiltration, or violations of explicit safety mechanisms. Therefore the failures the fuzzer exposes are standard robustness errors (mispredictions/inconsistencies under input mutations), not high- or medium-severity safety/security violations.",
                    "evidence": "Page 1148, Introduction: \"In this work, we focus on the latter scenario and investigate how software testing techniques can be applied for the robust generalization of Deep Neural Networks (DNNs) to natural environmental variations.\"; Page 1149, Section 2.3 (Robustness of DNNs): \"As discussed earlier, this work targets robustness issues arising from natural, environmental perturbations \u03b4 in the input data and not perturbations \u03b4 constructed adversarially, in a security context.\"; Page 1153, Section 4.3 (Evaluation Metric): \"robust accuracy is the proportion of images in the testing dataset where the prediction of a DNN does not fluctuate with any small realistic perturbations.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The Sensei fuzzer (genetic search) is designed to find high-loss, label-preserving natural variants (the \"worst\" variants) to improve robust generalization to environmental perturbations. Its objective and fitness function maximize model loss (L(\u03b8,x',y)) rather than steering generation toward attacker-chosen specific outputs or target labels. The paper explicitly frames the task as finding variants that produce high loss / cause misclassification under natural perturbations and states it is not addressing adversarially constructed, targeted attacks. Therefore the approach performs untargeted exploration that discovers generic failures (lossy variants / misclassifications) rather than targeted attacker-chosen outputs.",
                    "evidence": "p.1150, Section 3.1 (Problem Formulation): \"Definition 1 (Augmentation target). ... find \u03c1 yielding the optimal variant x' ... to optimize: max_{\u03c1 \u2208 S} L(\u03b8, t(\u03c1, x), y) (4)\"; p.1150, Section 3.3.3 (Fitness function): \"floss(x') = L(\u03b8, x', y) (5) ... the variant that suffers more loss by the DNN should be used in the augmented training\"; p.1149, Section 2.3 (Robustness of DNNs): \"As discussed earlier, this work targets robustness issues arising from natural, environmental perturbations \u03b4 in the input data and not perturbations \u03b4 constructed adversarially, in a security context.\"; p.1150, Section 3 (Overview): \"Sensei ... uses genetic search on a space of the natural environmental variants of each training input data, to identify the worst variant for augmentation.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, SENSEI, performs untargeted exploration to discover generic misclassifications. The goal is to improve general robustness against natural perturbations, not to find inputs that cause a specific, attacker-chosen output. The fuzzing process, guided by a genetic algorithm, is designed to find input variants that maximize the model's loss function with respect to the original, correct label. This objective function steers the search towards any input that confuses the model or causes a misclassification, without a predefined target outcome. The paper explicitly distinguishes its focus from targeted adversarial attacks in a security context.",
                    "evidence": "Page 6, Section 3.3.3 Fitness function: \"In SENSEI, we define the fitness function based on the empirical loss of the DNN. More specifically, since the training of DNN focuses on minimizing loss across the entire training data-set, the variant that suffers in more loss by the DNN should be used in the augmented training to make the DNN more robust. Formally: floss(x') = L(\u03b8, x', y)\". This shows the optimization target is maximizing loss relative to the correct label 'y', which is the definition of an untargeted approach.\nPage 4, Section 2.3 Robustness of DNNs: \"Specifically, a DNN M may correctly classify an input x with its corresponding label l, but incorrectly classify an input x + \u03b4 that is similar to x, with label l', where l \u2260 l'.\" This defines the failure condition as any misclassification (l' is not equal to l), not a specific targeted misclassification."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The Sensei fuzzer (genetic search) is designed to find high-loss, label-preserving natural variants (the \"worst\" variants) to improve robust generalization to environmental perturbations. Its objective and fitness function maximize model loss (L(\u03b8,x',y)) rather than steering generation toward attacker-chosen specific outputs or target labels. The paper explicitly frames the task as finding variants that produce high loss / cause misclassification under natural perturbations and states it is not addressing adversarially constructed, targeted attacks. Therefore the approach performs untargeted exploration that discovers generic failures (lossy variants / misclassifications) rather than targeted attacker-chosen outputs.",
                    "evidence": "p.1150, Section 3.1 (Problem Formulation): \"Definition 1 (Augmentation target). ... find \u03c1 yielding the optimal variant x' ... to optimize: max_{\u03c1 \u2208 S} L(\u03b8, t(\u03c1, x), y) (4)\"; p.1150, Section 3.3.3 (Fitness function): \"floss(x') = L(\u03b8, x', y) (5) ... the variant that suffers more loss by the DNN should be used in the augmented training\"; p.1149, Section 2.3 (Robustness of DNNs): \"As discussed earlier, this work targets robustness issues arising from natural, environmental perturbations \u03b4 in the input data and not perturbations \u03b4 constructed adversarially, in a security context.\"; p.1150, Section 3 (Overview): \"Sensei ... uses genetic search on a space of the natural environmental variants of each training input data, to identify the worst variant for augmentation.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "Aims to find generic misclassifications",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight using observable model internals and point-wise statistical checks, but does not explain why failures occur in terms of underlying model vulnerabilities (e.g., spurious features, dataset bias, or overfitting). Concretely, the authors evaluate alternative fitness/selection signals that use model internals (loss, neuron coverage, surprise adequacy) and introduce point-wise robustness checks (classification- and loss-based) used to drive selective augmentation; they report empirical comparisons (e.g., Table 6, Table 7) showing how these internal signals correlate with robustness and training cost. However, the paper stops at using these signals as heuristics/metrics for augmentation and efficiency (and reporting robust-accuracy patterns), and does not perform a root-cause diagnostic analysis that links discovered failures to broader sources of vulnerability (no analysis of spurious features, class-level error concentration beyond aggregate robust accuracy, or neuron-level explanations of failure modes).",
                    "evidence": "Page 1150, Section 3.3.3 (Fitness function): \"Other metrics as fitness function. Any metric that quantifies the quality of a DNN with respect to a test input may be used as a fitness function. Some of the concrete examples include neuron coverage [24] and surprise adequacy [15].\"; Page 1150, Section 3.4 (Selective Augmentation): \"Classification-based robustness. A model is point-wise robust w.r.t. a data-point x if and only if it classifies x and all the label preserving realistic variations (x') correctly. Loss-based robustness. A model is point-wise robust w.r.t. a data-point x if and only if the prediction loss of x or any label preserving realistic variations (x') is not greater than a loss threshold.\"; Page 1156, Section 5.5.2 (Fitness function results / Table 6): \"The results show that both loss-based and coverage-based fitness functions achieve very similar robust accuracy. Since surprise adequacy is correlated to neuron coverage [15], we expect a similar performance also using surprise adequacy. However, loss based fitness function may be a better choice since neuron coverage based fitness function increases the training time by 50% than that of loss-based fitness function.\"; Page 1156, Section 5.5.3 (Selection metrics / Table 7): \"The evaluation results in Table 7 show that the loss-based selection outperforms the classification-based selection for all the models. The reason is that the loss-based selection is more conservative than the classification-based selection.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's primary contribution is a data augmentation method to improve model robustness, not to diagnose the causes of model failures. The evaluation is centered on measuring and improving 'robust accuracy', which is an aggregate measure of failure rates under perturbation. The paper finds inputs that cause failures (i.e., high-loss variants) to use them for retraining. While it mentions using model internals like neuron coverage as a potential fitness function to guide this search, it does not use these signals to provide a diagnostic analysis of why the discovered failures occur. The analysis does not connect failures to underlying model vulnerabilities like spurious features or dataset bias, nor does it present a statistical analysis of failure patterns. The paper reports the failures it finds but does not analyze them to provide diagnostic insight.",
                    "evidence": "Page 7, Section 4.3, Evaluation Metric: \"Since SENSEI is focused on improving the robustness of DNN models, following Engstrom et al. [8], we compute robust accuracy of SENSEI to answer each research question. More specifically, robust accuracy is the proportion of images in the testing dataset where the prediction of a DNN does not fluctuate with any small realistic perturbations.\" This shows the evaluation is based on counting failures, not diagnosing them. \n\nPage 9, Table 2: This table presents the core results, reporting the 'Robust accuracy' as a percentage. This is an aggregate report of failure rates, without any diagnostic breakdown or analysis of the failures themselves. \n\nPage 11, Section 5.5.2, Fitness function: \"Table 6 presents the robust accuracy of SENSEI for neuron coverage based fitness function. The results show that both loss-based and coverage-based fitness functions achieve very similar robust accuracy... loss based fitness function may be a better choice since neuron coverage based fitness function increases the training time by 50% than that of loss-based fitness function.\" This demonstrates that model internals (neuron coverage) are considered as a component of the test generation process (as a fitness function) and are evaluated based on their efficiency in finding failures, not for providing post-hoc diagnostic insight into the nature of those failures."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "Although the paper uses model\u2010internal signals (loss, neuron coverage) to guide its genetic search and selective augmentation, it does not perform any post\u2010hoc analysis of the failures it discovers to explain their root causes or to link them to broader model vulnerabilities (e.g., spurious features, dataset bias, overfitting). All evaluation is framed in terms of aggregate \u2018robust accuracy\u2019 improvements, and while the paper reports that loss\u2010based selection is more conservative than classification\u2010based selection (Table 7), it never examines the patterns or origins of the underlying failures themselves. There is no statistical breakdown of failure modes by class or transformation, nor any analysis of internal neuron activations or feature attributions to diagnose why the model fails under specific perturbations.",
                    "evidence": "The entire experimental evaluation (Section 5) centers on measuring and improving \u2018robust accuracy\u2019 (Equation 6, Table 2) via augmentation, with no subsequent analysis of the discovered failures. For example, Table 7 (p. 1156) compares loss\u2010based versus classification\u2010based selection purely in terms of robust accuracy gains, not as a diagnostic of failure causes."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper uses semantic-preserving, metamorphic image transformations (rotation, translation, shear, zoom, brightness, contrast) as mutation operators and generates augmented inputs on-the-fly during training. These coarse transformations are inherently less sensitive to tiny floating\u2011point memory artifacts, but the paper does not explicitly address standard I/O serialization effects (quantization, rounding, clipping) nor does it claim to constrain mutations so that perturbations survive common image serialization. It therefore aligns with the metric\u2019s description of \u201cconsidering serialization effects partially (e.g., applying metamorphic transformations) but without explicit enforcement of reproducibility under serialization,\u201d i.e., Medium.",
                    "evidence": "Section 4.2 (Generation of Realistic Variations), p.1153: \"The operations and corresponding restrictions with respect to an image x are as follows: \u2022 rotation(x,d): rotate x by d degree within a range [-30, 30]. \u2022 translation(x, d): horizontally or vertically translate x by d pixels within a range of [-10%, 10%] of image size. \u2022 shear(x,d): ... \u2022 brightness(x,d): uniformly add or subtract a value d for each pixel of x within a range of [-32, 32] \u2022 contrast(x,d): scale the RGB value of each pixel of x by a factor d within range of [0.8, 1.2]\"; Section 3.3.2 (Generation of population), p.1150: \"...Sensei always applies the resulting transformation vector (chromosome) on the original image (as opposed to applying on an already transformed data) to prevent the resulting data from being unrealistic.\"; Section 5.1 (Implementation), p.1153: \"we implement a new data generator that augments the training data during training. Our data generator takes as inputs the current model and original training set, augments the original data and then feeds the augmented data to the training process at each step.\" These passages show the use of metamorphic (coarse) image transformations applied in-memory during training but no explicit handling or discussion of serialization/quantization/clipping to ensure failures persist after saving to standard image formats."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper proposes a method, SENSEI, that uses fuzzing with semantic-preserving metamorphic transformations to generate augmented training data. The transformations include geometric operations (rotation, translation, shear, zoom) and color operations (brightness, contrast). According to the metric definition, applying such transformations without explicit enforcement of reproducibility under serialization warrants a 'Medium' rating. The paper details the transformations and their parameter ranges but does not mention any mechanism to account for I/O effects like quantization, rounding, or clipping that occur when saving an image to a file. For instance, a brightness adjustment could create pixel values outside the standard [0, 255] range for 8-bit images, which would be clipped upon serialization. The paper does not state that these effects are considered during the mutation or oracle evaluation (loss calculation) stages. Therefore, while the coarse-grained nature of the transformations provides some resilience against I/O effects compared to fine-grained numerical noise, the lack of explicit handling of serialization means failure reproducibility is only partially enforced.",
                    "evidence": "Page 7, Section 4.2 Generation of Realistic Variations, \"SENSEI focuses on improving the robustness of DNN models by augmenting training data with natural environmental variations. Since, we focus on the applications with image, we choose two major kinds of image operations: i) geometric operations ii) color operations to simulate the camera movements and lighting conditions in real life... The operations and corresponding restrictions with respect to an image x are as follows:\n\u2022 rotation(x,d): rotate x by d degree within a range [-30, 30].\n\u2022 translation(x, d): horizontally or vertically translate x by d pixels within a range of [-10%, 10%] of image size.\n\u2022 shear(x,d): horizontally shear x with a shear factor d within a range of [-0.1, 0.1].\n\u2022 zoom(x,d): zoom in/out x with a zoom factor d ranging [0.9,1.1]\n\u2022 brightness(x,d): uniformly add or subtract a value d for each pixel of x within a range of [-32, 32]\n\u2022 contrast(x,d): scale the RGB value of each pixel of x by a factor d within range of [0.8, 1.2]\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper uses semantic-preserving, metamorphic image transformations (rotation, translation, shear, zoom, brightness, contrast) as mutation operators and generates augmented inputs on-the-fly during training. These coarse transformations are inherently less sensitive to tiny floating\u2011point memory artifacts, but the paper does not explicitly address standard I/O serialization effects (quantization, rounding, clipping) nor does it claim to constrain mutations so that perturbations survive common image serialization. It therefore aligns with the metric\u2019s description of \u201cconsidering serialization effects partially (e.g., applying metamorphic transformations) but without explicit enforcement of reproducibility under serialization,\u201d i.e., Medium.",
                    "evidence": "Section 4.2 (Generation of Realistic Variations), p.1153: \"The operations and corresponding restrictions with respect to an image x are as follows: \u2022 rotation(x,d): rotate x by d degree within a range [-30, 30]. \u2022 translation(x, d): horizontally or vertically translate x by d pixels within a range of [-10%, 10%] of image size. \u2022 shear(x,d): ... \u2022 brightness(x,d): uniformly add or subtract a value d for each pixel of x within a range of [-32, 32] \u2022 contrast(x,d): scale the RGB value of each pixel of x by a factor d within range of [0.8, 1.2]\"; Section 3.3.2 (Generation of population), p.1150: \"...Sensei always applies the resulting transformation vector (chromosome) on the original image (as opposed to applying on an already transformed data) to prevent the resulting data from being unrealistic.\"; Section 5.1 (Implementation), p.1153: \"we implement a new data generator that augments the training data during training. Our data generator takes as inputs the current model and original training set, augments the original data and then feeds the augmented data to the training process at each step.\" These passages show the use of metamorphic (coarse) image transformations applied in-memory during training but no explicit handling or discussion of serialization/quantization/clipping to ensure failures persist after saving to standard image formats."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Apply large semantic level mutations and perform floating point operations (zoom, contrast, shear), which may produce non-discrete pixel values. They don't perform clipping/rounding to ensure IO persistence.",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or claim that fault-inducing inputs found on one (surrogate) model transfer to other models. Sensei is designed as an in-training data-augmentation method that generates variants per model/seed during that model's training; the experiments measure per-model robust accuracy after applying Sensei to each model independently. The paper explicitly contrasts its approach with multi-model differential testing (DeepXplore) and states that Sensei does not require multiple models, indicating the design and evaluation do not target discovering or measuring transferable faults across models.",
                    "evidence": "p.1148, Abstract: \"We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets.\"; p.1150, Section 3.2 (Overview): \"Note that, Sensei is in-training data augmentation approach, i.e., data generation and augmentation happen on-the-fly during training.\"; p.1156, Section 6 (Related Work): \"DeepXplore [24] generates test inputs that lead to exhibit different behaviors by different models for the same task. Our approach does not require multiple models.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a data augmentation technique, SENSEI, to improve the robustness of a single Deep Neural Network (DNN) model during its training phase. The method uses a genetic algorithm to generate high-loss variants of training inputs, which are then immediately used to train that same model. The entire process of generating and utilizing these 'fault-inducing' inputs is self-contained within the training loop of one specific model. The experiments evaluate the robustness of multiple individual models after they have been trained with SENSEI, but they do not test whether the variants generated for one model can cause failures in other models performing the same task. The paper does not mention or evaluate the concept of attack transferability between different models.",
                    "evidence": "Page 5, Algorithm 2, 'Overall algorithm': The algorithm describes a training process for a single model 'M'. In each epoch, for each training input 'X[i]', variants are generated and evaluated using the current model 'M' (line 14: `f := fitness(M, children);`). The model 'M' is then retrained on the augmented data (line 19: `M := train(X, Y);`). This shows the generated inputs are specific to and consumed by the model being trained, not transferred to other models.\n\nPage 7, Section 4.3, 'Evaluation Metric': The paper's own evaluation metric is defined for a single model's performance: 'A DNN is robust around x if and only if M(x') = c for all x' \u2208 X'. This confirms that the evaluation is focused on the robustness of an individual model, not on cross-model failure patterns.\n\nPage 9, Table 2, 'The robust accuracy for Random, W-10 and SENSEI': This table presents the results for 15 different models. Each model is trained and evaluated independently. The methodology does not involve generating inputs on one model (e.g., GTSRB-1) and testing them on another (e.g., GTSRB-2)."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or claim that fault-inducing inputs found on one (surrogate) model transfer to other models. Sensei is designed as an in-training data-augmentation method that generates variants per model/seed during that model's training; the experiments measure per-model robust accuracy after applying Sensei to each model independently. The paper explicitly contrasts its approach with multi-model differential testing (DeepXplore) and states that Sensei does not require multiple models, indicating the design and evaluation do not target discovering or measuring transferable faults across models.",
                    "evidence": "p.1148, Abstract: \"We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets.\"; p.1150, Section 3.2 (Overview): \"Note that, Sensei is in-training data augmentation approach, i.e., data generation and augmentation happen on-the-fly during training.\"; p.1156, Section 6 (Related Work): \"DeepXplore [24] generates test inputs that lead to exhibit different behaviors by different models for the same task. Our approach does not require multiple models.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "No analysis of fault transferability in input level or mutation level",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility through bounded, rule-based constraints on allowed transformations and by design prevents cumulative drift (it always applies each mutation to the original seed). These measures ensure mutated inputs remain realistic by construction during the iterative search. However, the paper does not provide empirical perceptual or semantic plausibility assessments (no human study or task-specific perceptual metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity) for the final fault-inducing / augmented inputs, so it does not meet the 'High' requirement of providing empirical evidence that final inputs remain perceptually realistic.",
                    "evidence": "Section 3.3.2 (Generation of population), p.1150: \"once a new chromosome is generated through the mutation or crossover, it is validated to make sure that it is within the range of each transformation that we set globally (line 12). Furthermore, Sensei always applies the resulting transformation vector (chromosome) on the original image (as opposed to applying on an already transformed data) to prevent the resulting data from being unrealistic.\"; Section 4.2 (Generation of Realistic Variations), p.1153: \"To make sure the translated images are visually similar to natural ones, we restrict the space of allowed perturbations following [8] where it is applicable. The operations and corresponding restrictions ... rotation(x,d): rotate x by d degree within a range [-30, 30]. translation(x, d): ... within a range of [-10%, 10%] ... brightness(x,d): ... range of [-32, 32] ... contrast(x,d): ... range of [0.8, 1.2].\"; Section 4.3 (Evaluation Metric), p.1153: \"robust accuracy is the proportion of images in the testing dataset where the prediction of a DNN does not fluctuate with any small realistic perturbations...\" \u2014 the paper's evaluation uses robust accuracy as the reported metric and does not report any human evaluation or perceptual-similarity scores (e.g., LPIPS, SSIM) for the generated/augmented images."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility using rule-based constraints. It defines a set of image transformations (rotation, translation, shear, zoom, brightness, contrast) intended to simulate 'natural environmental variations' and restricts the parameters of these transformations to bounded ranges to ensure the resulting images are 'visually similar to natural ones'. Crucially, the method is designed to prevent the cumulative effect of mutations across iterations, which could lead to unrealistic inputs. It achieves this by always applying transformations to the original seed image, not to an already-mutated one. However, the paper does not provide any empirical validation that the final augmented inputs are perceptually realistic, such as through a human study or by using quantitative metrics like SSIM or LPIPS. The plausibility is asserted based on the design constraints rather than being empirically verified, which prevents a 'High' rating.",
                    "evidence": "Page 6, Section 3.3.2, 'Generation of population': 'Furthermore, SENSEI always applies the resulting transformation vector (chromosome) on the original image (as opposed to applying on an already transformed data) to prevent the resulting data from being unrealistic.' \nPage 7, Section 4.2, 'Generation of Realistic Variations': 'SENSEI focuses on improving the robustness of DNN models by augmenting training data with natural environmental variations. ... To make sure the translated images are visually similar to natural ones, we restrict the space of allowed perturbations... The operations and corresponding restrictions with respect to an image x are as follows: \u2022 rotation(x,d): rotate x by d degree within a range [-30, 30]. \u2022 translation(x, d): horizontally or vertically translate x by d pixels within a range of [-10%, 10%] of image size. ... These image operations preserve the content of original image.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility through bounded, rule-based constraints on allowed transformations and by design prevents cumulative drift (it always applies each mutation to the original seed). These measures ensure mutated inputs remain realistic by construction during the iterative search. However, the paper does not provide empirical perceptual or semantic plausibility assessments (no human study or task-specific perceptual metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity) for the final fault-inducing / augmented inputs, so it does not meet the 'High' requirement of providing empirical evidence that final inputs remain perceptually realistic.",
                    "evidence": "Section 3.3.2 (Generation of population), p.1150: \"once a new chromosome is generated through the mutation or crossover, it is validated to make sure that it is within the range of each transformation that we set globally (line 12). Furthermore, Sensei always applies the resulting transformation vector (chromosome) on the original image (as opposed to applying on an already transformed data) to prevent the resulting data from being unrealistic.\"; Section 4.2 (Generation of Realistic Variations), p.1153: \"To make sure the translated images are visually similar to natural ones, we restrict the space of allowed perturbations following [8] where it is applicable. The operations and corresponding restrictions ... rotation(x,d): rotate x by d degree within a range [-30, 30]. translation(x, d): ... within a range of [-10%, 10%] ... brightness(x,d): ... range of [-32, 32] ... contrast(x,d): ... range of [0.8, 1.2].\"; Section 4.3 (Evaluation Metric), p.1153: \"robust accuracy is the proportion of images in the testing dataset where the prediction of a DNN does not fluctuate with any small realistic perturbations...\" \u2014 the paper's evaluation uses robust accuracy as the reported metric and does not report any human evaluation or perceptual-similarity scores (e.g., LPIPS, SSIM) for the generated/augmented images."
                },
                "manual": {
                    "value": "Medium",
                    "why": "coonstrained transformation but no naturalness measure of the fuzzed inputs",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ],
            "oracle": [
                "Metamorphic",
                "Property-based"
            ]
        }
    },
    "You et al. - 2023 - Regression Fuzzing for Deep Learning Systems": {
        "key": "you2023regression",
        "name": "DRFuzz",
        "year": "2023",
        "citation_count": "27",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper explicitly defines regression faults as cases where the new model version makes wrong predictions while the prior version was correct, and the proposed DRFuzz is designed to discover such mispredictions (diversity/fidelity of inputs). The evaluation and examples focus on classification errors, bias/overfitting explanations, and counts of regression mispredictions across image datasets and regression scenarios (including adversarially-trained models). The work does not claim or demonstrate uncovering high-impact safety/security violations (e.g., jailbreaks, data leakage, unauthorized actions, toxic or hallucinated outputs) nor bypassing explicit safety/policy mechanisms. Therefore DRFuzz uncovers model robustness/regression errors only, which corresponds to the Low severity level in the metric.",
                    "evidence": "Page 82, Introduction: \"Detecting regression faults refers to finding test inputs that make the current version produce wrong predictions but make the prior version produce correct predictions.\"; Page 84, Section II. DEFINITION, \"Definition 2. Regression fault. ... x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\"; Page 87, Section IV-A2 (Regression Scenarios): \"Adversarial Training ... fine-tunes the model by incorporating adversarial inputs and then produces a new version of the model.\" (evaluations report numbers of regression mispredictions in these scenarios but do not report safety/security violations such as data leakage or unauthorized actions)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper defines a 'regression fault' as an input that was correctly predicted by a prior model version (M1) but is incorrectly predicted by the subsequent version (M2). This aligns directly with the metric's definition of 'Low' severity, as it constitutes a model robustness error, specifically a prediction inconsistency resulting from model evolution. The paper's entire methodology and evaluation framework are built around finding and counting these mispredictions. There is no mention or analysis of uncovering unsafe behaviors (like toxicity or bias) or bypassing explicit safety or security mechanisms.",
                    "evidence": "Page 2, Section II. DEFINITION, Definition 2. Regression fault: \"Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., c_M1[x] = y ^ c_M2[x] != y.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper explicitly defines regression faults as cases where the new model version makes wrong predictions while the prior version was correct, and the proposed DRFuzz is designed to discover such mispredictions (diversity/fidelity of inputs). The evaluation and examples focus on classification errors, bias/overfitting explanations, and counts of regression mispredictions across image datasets and regression scenarios (including adversarially-trained models). The work does not claim or demonstrate uncovering high-impact safety/security violations (e.g., jailbreaks, data leakage, unauthorized actions, toxic or hallucinated outputs) nor bypassing explicit safety/policy mechanisms. Therefore DRFuzz uncovers model robustness/regression errors only, which corresponds to the Low severity level in the metric.",
                    "evidence": "Page 82, Introduction: \"Detecting regression faults refers to finding test inputs that make the current version produce wrong predictions but make the prior version produce correct predictions.\"; Page 84, Section II. DEFINITION, \"Definition 2. Regression fault. ... x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\"; Page 87, Section IV-A2 (Regression Scenarios): \"Adversarial Training ... fine-tunes the model by incorporating adversarial inputs and then produces a new version of the model.\" (evaluations report numbers of regression mispredictions in these scenarios but do not report safety/security violations such as data leakage or unauthorized actions)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's goal and design focus on discovering regression faults defined as inputs where the prior version is correct and the new version is incorrect (i.e., amplifying prediction differences between versions). DRFuzz steers exploration toward increasing prediction-probability differences and improving diversity/fidelity of detected faults, but it does not aim to induce specific attacker-chosen outputs or explicitly target a predefined target label/phrase. The mutation-selection reward and oracle are based on generic prediction-difference and fidelity metrics rather than a user-specified target output, so the approach performs untargeted exploration of regression faults rather than targeted attack discovery.",
                    "evidence": "p.82, Abstract: \"we propose a novel regression fuzzing technique called DRFuzz, which facilitates generating inputs that trigger diverse regression faults and have high fidelity.\" \n\np.83, Section II. DEFINITION (Regression fault): \"x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\" \n\np.84, Section III-A (Diversity Measurement): \"DRFuzz adopts the tuple [ta0, (cM1[tai] \u2192 cM2[tai])] to approximately distinguish the fault triggering inputs and uses the tuple diversity to represent the diversity of the fault-triggering inputs.\" \n\np.85, Section III-C (Heuristic-based Regression Fuzzing Process): \"we expect that in each iteration, the new test input generated through mutation can produce a larger prediction difference (measured by the prediction probability difference in each class) between versions than that produced by the test input before mutation.\" \n\np.85, Section III-C (Mutation Rule Selection, Formula (2)): \"Reward = #DiffTriggerInputs / #TotalSelect \u00d7 #FidelInputs / #TotalSelect\" (shows mutation prioritization is driven by counts of inputs that cause prediction differences and fidelity, not by steering to a specific attacker-chosen output)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, DRFuzz, performs untargeted exploration to discover generic regression faults. A regression fault is defined as any input that was correctly predicted by a prior model version but is incorrectly predicted by the new version. The fuzzing strategy is explicitly designed to amplify the prediction difference between the two model versions, thereby increasing the chance of finding any disagreement that constitutes a regression fault. The method does not incorporate a mechanism to steer the search toward a specific, predefined target output (e.g., misclassifying a 'cat' as a 'dog'). Instead, it aims to find as many diverse regression faults as possible, which are a form of prediction inconsistency.",
                    "evidence": "Page 2, Section II, Definition 2: 'Regression fault. Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.' This definition confirms that any misclassification by the new model (cM2[x] \u2260 y) constitutes a fault, without specifying a target class for the misprediction. \n\nPage 4, Section III-C: 'With this expectation, we design a search strategy in DRFuzz to guide the generation of test inputs toward the direction of amplifying the prediction difference between versions. In this way, fault-triggering test inputs can be generated more efficiently.' The guiding heuristic is the amplification of prediction difference in general, not steering towards a specific attacker-chosen outcome."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's goal and design focus on discovering regression faults defined as inputs where the prior version is correct and the new version is incorrect (i.e., amplifying prediction differences between versions). DRFuzz steers exploration toward increasing prediction-probability differences and improving diversity/fidelity of detected faults, but it does not aim to induce specific attacker-chosen outputs or explicitly target a predefined target label/phrase. The mutation-selection reward and oracle are based on generic prediction-difference and fidelity metrics rather than a user-specified target output, so the approach performs untargeted exploration of regression faults rather than targeted attack discovery.",
                    "evidence": "p.82, Abstract: \"we propose a novel regression fuzzing technique called DRFuzz, which facilitates generating inputs that trigger diverse regression faults and have high fidelity.\" \n\np.83, Section II. DEFINITION (Regression fault): \"x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\" \n\np.84, Section III-A (Diversity Measurement): \"DRFuzz adopts the tuple [ta0, (cM1[tai] \u2192 cM2[tai])] to approximately distinguish the fault triggering inputs and uses the tuple diversity to represent the diversity of the fault-triggering inputs.\" \n\np.85, Section III-C (Heuristic-based Regression Fuzzing Process): \"we expect that in each iteration, the new test input generated through mutation can produce a larger prediction difference (measured by the prediction probability difference in each class) between versions than that produced by the test input before mutation.\" \n\np.85, Section III-C (Mutation Rule Selection, Formula (2)): \"Reward = #DiffTriggerInputs / #TotalSelect \u00d7 #FidelInputs / #TotalSelect\" (shows mutation prioritization is driven by counts of inputs that cause prediction differences and fidelity, not by steering to a specific attacker-chosen output)."
                },
                "manual": {
                    "value": "Low",
                    "why": "Intargeted exploration for generic regression misclassifications - any inputs that cause the new model to misclassify examples previously classified correctly",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper goes beyond merely reporting discovered regression failures: it analyzes statistical failure patterns (per-class and per-behavior distributions) and links them to underlying vulnerabilities (e.g., overfitting/data bias). It also analyzes model-internal signals (prediction confidences and saliency via Grad-CAM) to explain why different faulty behaviors occur, connecting failures to reliance on different input features. These elements constitute diagnostic analysis that explains why failures occur and relate them to model weaknesses.",
                    "evidence": "Page 89, Section IV-B1 (Regression Explication based on DRFuzz): \"From Figure 3(a), we can observe that Classes 7, 8, and 4 contain the most regression faults, which means that the regression process may induce overfitting in these classes. Figure 3(b) shows that inputs of class 1 are wrongly predicted as class 8, inputs of class 9 are wrongly predicted as class 4, and inputs of classes 1, 2, and 9 tend to be predicted as 7 due to the regression process.\" \n\nPage 90, Section V-B (Diversity Analysis): \"To assist the visual analysis, we use the Gradient-weighted Class Activation Map (heatmap) [61] to visualize the importance of each feature in the prediction process... The important features of Figure 5(a) and Figure 5(b) for faulty behavior (1 \u2192 8) lie in the center of the images. The important features of Figure 5(c) and Figure 5(d) for faulty behavior (1 \u2192 9) lie on the left of the images. The phenomena above show that different faulty behaviors may be caused by different features...\" \n\nPage 86, Section III-C (Tree-based Seed Pool Trimming): \"we estimate the fault-triggering potential of the seed by the second highest confidence in the prediction vector... If the second highest confidence class in the predication vectors of t1.1 and t2.2 on the regression model is equal to cM2[t3.1], it means that t1.1 and t2.2 are more likely to trigger the same faulty behavior as that triggered by t3.1.\" \n\nPage 84, Section III-A (Diversity Measurement): \"Based on the static initial seeds and the dynamic faulty behaviors for fault-triggering test inputs, DRFuzz adopts the tuple [ta0, (cM1[tai] \u2192 cM2[tai])] to approximately distinguish the fault triggering inputs and uses the tuple diversity to represent the diversity of the fault-triggering inputs.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper provides a detailed diagnostic analysis of the discovered regression faults, linking them to underlying model vulnerabilities. It conducts a statistical analysis of failure patterns, identifying classes with a high concentration of regression faults. Crucially, it explicitly interprets these patterns as potential indicators of \"overfitting\" and \"bias\" introduced during the model evolution process. Furthermore, it uses Gradient-weighted Class Activation Maps (heatmaps) to visualize and analyze the input features that contribute to specific failures, connecting the failures to different feature dependencies. This combination of statistical pattern analysis and causal explanation in terms of overfitting, bias, and feature reliance meets the criteria for a 'High' rating.",
                    "evidence": "Page 8, Section IV-B, \"Regression Explication based on DRFuzz\": \"We further focus on utilizing regression faults (#RF) to explain the bias and risks induced in the regression process. For clarity of illustration, we select and report the results on subjects of MNIST-LeNet5... From Figure 3(a), we can observe that Classes 7, 8, and 4 contain the most regression faults, which means that the regression process may induce overfitting in these classes... The prediction results and the faulty behaviors facilitate explaining the bias (such as overfitting) in each class to judge if the regression process contributes to data relevance.\"\nPage 10, Section V-B, \"Diversity Analysis\": \"To assist the visual analysis, we use the Gradient-weighted Class Activation Map (heatmap) [61] to visualize the importance of each feature in the prediction process... The phenomena above show that different faulty behaviors may be caused by different features...\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper goes beyond merely reporting discovered regression failures: it analyzes statistical failure patterns (per-class and per-behavior distributions) and links them to underlying vulnerabilities (e.g., overfitting/data bias). It also analyzes model-internal signals (prediction confidences and saliency via Grad-CAM) to explain why different faulty behaviors occur, connecting failures to reliance on different input features. These elements constitute diagnostic analysis that explains why failures occur and relate them to model weaknesses.",
                    "evidence": "Page 89, Section IV-B1 (Regression Explication based on DRFuzz): \"From Figure 3(a), we can observe that Classes 7, 8, and 4 contain the most regression faults, which means that the regression process may induce overfitting in these classes. Figure 3(b) shows that inputs of class 1 are wrongly predicted as class 8, inputs of class 9 are wrongly predicted as class 4, and inputs of classes 1, 2, and 9 tend to be predicted as 7 due to the regression process.\" \n\nPage 90, Section V-B (Diversity Analysis): \"To assist the visual analysis, we use the Gradient-weighted Class Activation Map (heatmap) [61] to visualize the importance of each feature in the prediction process... The important features of Figure 5(a) and Figure 5(b) for faulty behavior (1 \u2192 8) lie in the center of the images. The important features of Figure 5(c) and Figure 5(d) for faulty behavior (1 \u2192 9) lie on the left of the images. The phenomena above show that different faulty behaviors may be caused by different features...\" \n\nPage 86, Section III-C (Tree-based Seed Pool Trimming): \"we estimate the fault-triggering potential of the seed by the second highest confidence in the prediction vector... If the second highest confidence class in the predication vectors of t1.1 and t2.2 on the regression model is equal to cM2[t3.1], it means that t1.1 and t2.2 are more likely to trigger the same faulty behavior as that triggered by t3.1.\" \n\nPage 84, Section III-A (Diversity Measurement): \"Based on the static initial seeds and the dynamic faulty behaviors for fault-triggering test inputs, DRFuzz adopts the tuple [ta0, (cM1[tai] \u2192 cM2[tai])] to approximately distinguish the fault triggering inputs and uses the tuple diversity to represent the diversity of the fault-triggering inputs.\""
                },
                "manual": {
                    "value": "High",
                    "why": "Explains bias and overfitting of specific classes in regression process. Provides behavior-level explanations (which classes or behaviors regress, per-class fault heatmaps, and faulty-behavior tuples). Interprets regression faults as evidence of bias or overfitting (specific classes suffering more after retraining).",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly incorporates multiple mechanisms to enforce input plausibility throughout fuzzing iterations and provides empirical human-evaluation evidence that the final fault-inducing inputs remain perceptually/semantically realistic. Concretely, DRFuzz: (1) constrains mutations (pixel-level mutations limited to 0.5% of pixels and image-level mutations applied to whole images to avoid unrealistic partial edits); (2) uses a GAN discriminator (trained on the original training set) to compute a per-seed fidelity threshold and filter out low-fidelity mutated inputs during the fuzzing process; and (3) limits cumulative drift via seed-generation controls (e.g., max_times to bound number of times a seed can be mutated and an exponential-decay seed selection probability) and tree-based trimming. The authors also report a manual/human evaluation comparing DRFuzz (with GAN filtering) to a variant without the GAN: two authors labeled sampled inputs (Cohen\u2019s kappa = 0.67), finding 86% of DRFuzz inputs labeled high-fidelity vs 57.5% for the no-GAN variant. These elements together satisfy the metric requirement of enforcing plausibility through iterations and providing empirical (human) evidence for plausibility of final fault-inducing inputs.",
                    "evidence": "Page 84, Section III-B (GAN-based Fidelity Assurance): \"During the fuzzing process, we feed an initial seed into the Discriminator to get a fidelity score, which serves as the fidelity threshold for filtering out the low-fidelity inputs generated from this initial seed.\"; Page 86, Table I / Section III-C (Mutation Rules): \"Pixel-Level Mutation aims to change a small set of pixels in an image ... DRFuzz randomly selects a small set of pixels (i.e., 0.5% total pixels) for mutation.\" and \"Image-Level Mutation ... this category of mutation rules can simulate real-world scenarios more effectively when applying them to the whole image.\"; Page 88, Section IV-A5 (Implementations and Experiments): \"... we set ... max_times to be 10 ...\" (bounding number of times a seed can be mutated); Page 90, Section IV-B2 (Contribution of the main component of DRFuzz): \"Specifically, we sample 100 inputs generated by DRFuzz_NG and DRFuzz, respectively, and two authors manually evaluate their fidelity individually. The Cohen\u2019s Kappa coefficient between their evaluation is 0.67. They label 86% of inputs generated by DRFuzz as high-fidelity and only 57.5% of inputs generated by DRFuzz_NG as high-fidelity.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper explicitly designs a mechanism, \"GAN-based Fidelity Assurance\", to enforce input plausibility throughout the fuzzing process. This component uses a discriminator, trained to distinguish between real and synthetic data, to filter out mutated inputs that are not semantically similar to the original data distribution. This ensures that generated inputs remain plausible during the fuzzing iterations. Furthermore, the paper provides direct empirical evidence of the effectiveness of this mechanism through a human evaluation. In an ablation study, two authors manually assessed the fidelity of 100 inputs generated with and without the GAN filter. The study found that 86% of inputs from the proposed method (DRFuzz) were high-fidelity, compared to only 57.5% from the variant without the GAN filter (DRFuzz_NG). This combination of a dedicated plausibility enforcement mechanism and empirical validation via human study aligns perfectly with the 'High' rating.",
                    "evidence": "Page 9, Section IV-B2, \"Contribution of the main component of DRFuzz\": \"We further manually evaluate the fidelity of inputs obtained with and without the fidelity assurance mechanism. Specifically, we sample 100 inputs generated by DRFuzz_NG and DRFuzz, respectively, and two authors manually evaluate their fidelity individually. The Cohen\u2019s Kappa coefficient between their evaluation is 0.67. They label 86% of inputs generated by DRFuzz as high-fidelity and only 57.5% of inputs generated by DRFuzz_NG as high-fidelity. This indicates that the GAN-based Fidelity Assurance technique can filter out more than 20% of fault-triggering inputs with low fidelity.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly incorporates multiple mechanisms to enforce input plausibility throughout fuzzing iterations and provides empirical human-evaluation evidence that the final fault-inducing inputs remain perceptually/semantically realistic. Concretely, DRFuzz: (1) constrains mutations (pixel-level mutations limited to 0.5% of pixels and image-level mutations applied to whole images to avoid unrealistic partial edits); (2) uses a GAN discriminator (trained on the original training set) to compute a per-seed fidelity threshold and filter out low-fidelity mutated inputs during the fuzzing process; and (3) limits cumulative drift via seed-generation controls (e.g., max_times to bound number of times a seed can be mutated and an exponential-decay seed selection probability) and tree-based trimming. The authors also report a manual/human evaluation comparing DRFuzz (with GAN filtering) to a variant without the GAN: two authors labeled sampled inputs (Cohen\u2019s kappa = 0.67), finding 86% of DRFuzz inputs labeled high-fidelity vs 57.5% for the no-GAN variant. These elements together satisfy the metric requirement of enforcing plausibility through iterations and providing empirical (human) evidence for plausibility of final fault-inducing inputs.",
                    "evidence": "Page 84, Section III-B (GAN-based Fidelity Assurance): \"During the fuzzing process, we feed an initial seed into the Discriminator to get a fidelity score, which serves as the fidelity threshold for filtering out the low-fidelity inputs generated from this initial seed.\"; Page 86, Table I / Section III-C (Mutation Rules): \"Pixel-Level Mutation aims to change a small set of pixels in an image ... DRFuzz randomly selects a small set of pixels (i.e., 0.5% total pixels) for mutation.\" and \"Image-Level Mutation ... this category of mutation rules can simulate real-world scenarios more effectively when applying them to the whole image.\"; Page 88, Section IV-A5 (Implementations and Experiments): \"... we set ... max_times to be 10 ...\" (bounding number of times a seed can be mutated); Page 90, Section IV-B2 (Contribution of the main component of DRFuzz): \"Specifically, we sample 100 inputs generated by DRFuzz_NG and DRFuzz, respectively, and two authors manually evaluate their fidelity individually. The Cohen\u2019s Kappa coefficient between their evaluation is 0.67. They label 86% of inputs generated by DRFuzz as high-fidelity and only 57.5% of inputs generated by DRFuzz_NG as high-fidelity.\""
                },
                "manual": {
                    "value": "High",
                    "why": "Every batch of mutated inputs is passed through the GAN discriminator and only natural mutated inputs are kept for execution and further mutation.",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper describes in-memory, floating-point mutations (pixel-level and image-level) and a GAN-based fidelity filter, but it does not describe any handling of standard I/O serialization effects (e.g., quantization, rounding, clipping) to ensure perturbations survive disk storage or common image encodings. It even explicitly states that the quantization scenario (which would exercise serialization/precision effects) is not considered. Therefore failures found by DRFuzz may rely on fragile, in-memory numerical perturbations that would be lost or altered by standard I/O serialization, and the approach does not explicitly enforce reproducibility under such operations.",
                    "evidence": "Page 84, Section III (Overview): \"DRFuzz selects a seed input each time to conduct mutation, and the mutated inputs are passed into the GAN-based Fidelity Assurance model to identify inputs with high fidelity, which are then executed by both the original model and the regression model.\" \n\nPage 85, Section III-C (Mutation Rules): \"Pixel-Level Mutation aims to change a small set of pixels in an image to generate a new test input. ... DRFuzz randomly selects a small set of pixels (i.e., 0.5% total pixels) for mutation.\" \n\nPage 88, Section IV-A2 (Regression Scenarios): \"Please note that the quantization scenario targeted by DiffChaser is not a regression scenario since quantization is conducted once-for-all without the characteristics of continuous evolution for regression. Hence, we do not consider it in our study.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The method, DRFuzz, utilizes a set of 16 mutation rules categorized into 'Pixel-Level Mutation' (e.g., adding Gaussian noise, shuffling patches) and 'Image-Level Mutation' (e.g., scaling, rotating, brightness adjustment). The 'Image-Level' mutations are semantic-preserving metamorphic transformations. The metric definition explicitly states that applying such transformations without explicit enforcement of reproducibility under serialization results in a 'Medium' rating. The paper does not mention any mechanism to account for I/O effects such as quantization, rounding, or clipping that occur during standard file saving operations (e.g., to PNG). The 'Pixel-Level' mutations, which introduce small numerical changes, are particularly susceptible to being lost or altered during such I/O operations. Since the fuzzing process and oracle evaluation are performed on in-memory representations without considering serialization effects, the method does not qualify for a 'High' rating.",
                    "evidence": "Page 4, Section C, Mutation Rules: \"In DRFuzz, we adopt all the mutation rules for images designed by the existing work [11], [27], [28]... In total, we implemented 16 mutation rules in DRFuzz... According to the usage of each mutation rule, we classify them into two categories, i.e., Pixel-Level Mutation and Image-Level Mutation.\"\n\nPage 4, Table I, \"MUTATION RULES\": This table lists the specific mutation operators. The 'Pixel-Level Mutation' rules include 'Pixel Adding Gaussian Noise' and 'Pixel Adding Salt & Pepper Noise'. The 'Image-Level Mutation' rules include 'Image Rotating' and 'Image Brightness Adjustment', which are examples of semantic-preserving metamorphic transformations."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "DRFuzz applies a variety of semantic-preserving metamorphic image transformations (e.g., rotation, scaling, brightness/contrast adjustment) which introduce coarse input changes that tend to survive standard serialization, but it does not explicitly account for or constrain the fine-grained numerical perturbations (e.g. pixel-level Gaussian or salt-and-pepper noise) to ensure they survive quantization, rounding or clipping when images are saved or transmitted. The paper makes no mention of handling I/O effects or verifying that mutations persist through common image formats, so serialization effects are only partially considered via the use of metamorphic transformations, yielding a \u201cMedium\u201d rating under the metric.",
                    "evidence": "Page 4, Table I (\u201cMUTATION RULES\u201d): lists both pixel-level numerical mutations and image-level metamorphic transformations (e.g. rotation, brightness adjustment).\n\nPage 4, Section III-C: \u201cAccording to the usage of each mutation rule, we classify them into two categories, i.e., Pixel-Level Mutation and Image-Level Mutation.\u201d  There is no discussion of serialization, quantization, rounding, or clipping effects in the mutation or oracle evaluation steps."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Generates and evaluates mutated inputs entirely in memory using both pixel level and image level semantic mutations. They use GAN-based fidelity filtering to ensure naturalness but does not ensure if the mutation will survive I/O.",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper defines and evaluates regression-fault-triggering inputs strictly with respect to a prior model M1 and its regression model M2 (i.e., it detects inputs where M1 is correct and M2 is incorrect). All described generation, fidelity filtering, and evaluation steps execute mutated inputs on that M1\u2013M2 pair. The experiments use multiple dataset/model pairs but do not evaluate whether inputs found on one model transfer to other models performing the same task, nor does the design include explicit mechanisms to discover or measure transferable attacks across different model implementations. Therefore the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "Section II (Definition), Definition 2 \u2014 \"Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\" (Section II, Definition 2)\n\nSection III (Approach) \u2014 \"the mutated inputs are passed into the GAN-based Fidelity Assurance model ... The inputs that do not trigger regression faults will be maintained ... the mutated inputs ... are then executed by both the original model and the regression model. If the two models provide different prediction results, then a regression-fault-triggering input is identified.\" (Section III, overview / Approach)\n\nSection IV-A1 (Subjects / Experimental setup) \u2014 \"we used four pairs of datasets and DNN models as subjects, i.e., LeNet-5 on MNIST, VGG16 on CIFAR10, AlexNet on Fashion-MNIST (FM), and ResNet18 on SVHN\" (Section IV-A1 Subjects). \n\nThroughout the paper (Approach and Evaluation sections) all testing and analysis focus on M1 vs. M2 pairs; there is no experiment or statement reusing inputs crafted on one model to test different, unseen target models nor any mechanism intended to discover transferable attacks across model implementations."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's entire methodology and evaluation are focused on 'regression fuzzing', which it defines as finding faults between a prior version of a model (M1) and its subsequent version (M2). The goal is to find inputs that were correctly classified by M1 but are misclassified by M2 after an evolution event (e.g., fine-tuning, pruning). The experiments are designed around creating these M1/M2 pairs and evaluating the proposed fuzzer, DRFuzz, in its ability to find such regression-specific faults. The paper does not perform any experiments to check if the fault-inducing inputs found for a specific model (e.g., LeNet-5) also trigger failures in other, different model architectures (e.g., VGG16) performing the same task. Therefore, it does not demonstrate attack transferability across models.",
                    "evidence": "Page 2, Section II. DEFINITION, Definition 2. Regression fault: \"Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1 [x] = y \u2227 cM2 [x] != y.\" This definition explicitly frames the problem as a comparison between two versions of the same model, not between different models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper defines and evaluates regression-fault-triggering inputs strictly with respect to a prior model M1 and its regression model M2 (i.e., it detects inputs where M1 is correct and M2 is incorrect). All described generation, fidelity filtering, and evaluation steps execute mutated inputs on that M1\u2013M2 pair. The experiments use multiple dataset/model pairs but do not evaluate whether inputs found on one model transfer to other models performing the same task, nor does the design include explicit mechanisms to discover or measure transferable attacks across different model implementations. Therefore the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "Section II (Definition), Definition 2 \u2014 \"Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\" (Section II, Definition 2)\n\nSection III (Approach) \u2014 \"the mutated inputs are passed into the GAN-based Fidelity Assurance model ... The inputs that do not trigger regression faults will be maintained ... the mutated inputs ... are then executed by both the original model and the regression model. If the two models provide different prediction results, then a regression-fault-triggering input is identified.\" (Section III, overview / Approach)\n\nSection IV-A1 (Subjects / Experimental setup) \u2014 \"we used four pairs of datasets and DNN models as subjects, i.e., LeNet-5 on MNIST, VGG16 on CIFAR10, AlexNet on Fashion-MNIST (FM), and ResNet18 on SVHN\" (Section IV-A1 Subjects). \n\nThroughout the paper (Approach and Evaluation sections) all testing and analysis focus on M1 vs. M2 pairs; there is no experiment or statement reusing inputs crafted on one model to test different, unseen target models nor any mechanism intended to discover transferable attacks across model implementations."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Zhang et al. - 2019 - Life after Speech Recognition Fuzzing Semantic Misinterpretation for Voice Assistant Applications": {
        "key": "zhang2019life",
        "name": "LipFuzzer",
        "year": "2019",
        "citation_count": "79",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's LipFuzzer uncovers misinterpretations in the NLU Intent Classifier that lead to violations of user intent and security expectations (e.g., vApp squatting, installation/invocation hijacks, phishing, privacy leakage, and device-control denial-of-service). These are higher-impact than simple robustness errors. However, the work does not claim to target or bypass explicit safety- or security-aligned defenses (e.g., adversarially trained or safety-filtered models); instead it exploits template/intent-matching design flaws and developer-supplied templates in a black-box setting. Therefore the failures found are severe (unsafe, unauthorized behaviors and privacy/security consequences) but are not described as bypassing explicit defended/safety-aligned mechanisms, which matches the metric's \"Medium\" category.",
                    "evidence": "Abstract (p.1): \"...the semantic inconsistency caused by the improper semantic interpretation of an Intent Classifier can create the opportunity of breaching the integrity of vApp processing when attackers delicately leverage some common spoken errors.\"; II.D Threat Model (p.4): \"Attack consequences. We summarize a list of example consequences of interacting with malicious vApps as follows: Denial of Service: This would occur if a device-controlling vApp is hijacked. Privacy Leakage: vApps intimately connect users\u2019 daily life to a more exposed digital world... Phishing: Third-party vApps\u2019 back-end processing is fully controlled by the developers...\"; VI-C vApp Store Evaluation (p.12): \"As a result, a total of 715 (71.5%) Amazon Alexa vApps and 59 (29.5%) Google Assistant are verified to be vulnerable.\"; VI-D Case Study - Attacking 'True Bank' (p.13): \"As a result, a total number of 3 malicious vApps are invoked, which clearly demonstrate the practicability of the attack.\"; II.D Threat Model (p.4): \"We assume that an adversary does not need to access any internal implementations/configurations of vApp including ASR, NLU components.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's fuzzing tool, LipFuzzer, is designed to discover semantic misinterpretations in voice assistant platforms. These failures cause a user's voice command to be incorrectly mapped to an unintended or malicious third-party application (vApp). This is a direct violation of user intent. The paper explicitly details how these failures can lead to undesired and unsafe outcomes, including denial of service (e.g., a garage door command failing), privacy leaks, and phishing attacks. These consequences go beyond simple model robustness errors and constitute unsafe behavior. However, the method does not target or bypass explicit safety or security mechanisms in defended models. Instead, it exploits the inherent fuzzy matching logic and vulnerabilities within the standard Natural Language Understanding (NLU) and Intent Classifier components of the voice assistant architecture.",
                    "evidence": "Page 5, Section II-D. Threat Model: \"Attack consequences. We summarize a list of example consequences of interacting with malicous vApps as follows: Denial of Service: This would occur if a device-controlling vApp is hijacked. For example, when a user says \u201cAlexa, close the garage door\u201d to a mismatched malicious vApp instead of a legitimate one, the garage door may be left open. ... Phishing: Third-party vApps\u2019 back-end processing is fully controlled by the developers. ... Audio can thus be manipulated by an attacker at runtime, substituting a benign audio file with a \u201cmalicious\" audio file. For example, ... with \u201cSorry, a critical problem occurred in your Amazon Pay, please answer with your account username and password to know more.\u201d\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper's LipFuzzer uncovers misinterpretations in the NLU Intent Classifier that lead to violations of user intent and security expectations (e.g., vApp squatting, installation/invocation hijacks, phishing, privacy leakage, and device-control denial-of-service). These are higher-impact than simple robustness errors. However, the work does not claim to target or bypass explicit safety- or security-aligned defenses (e.g., adversarially trained or safety-filtered models); instead it exploits template/intent-matching design flaws and developer-supplied templates in a black-box setting. Therefore the failures found are severe (unsafe, unauthorized behaviors and privacy/security consequences) but are not described as bypassing explicit defended/safety-aligned mechanisms, which matches the metric's \"Medium\" category.",
                    "evidence": "Abstract (p.1): \"...the semantic inconsistency caused by the improper semantic interpretation of an Intent Classifier can create the opportunity of breaching the integrity of vApp processing when attackers delicately leverage some common spoken errors.\"; II.D Threat Model (p.4): \"Attack consequences. We summarize a list of example consequences of interacting with malicious vApps as follows: Denial of Service: This would occur if a device-controlling vApp is hijacked. Privacy Leakage: vApps intimately connect users\u2019 daily life to a more exposed digital world... Phishing: Third-party vApps\u2019 back-end processing is fully controlled by the developers...\"; VI-C vApp Store Evaluation (p.12): \"As a result, a total of 715 (71.5%) Amazon Alexa vApps and 59 (29.5%) Google Assistant are verified to be vulnerable.\"; VI-D Case Study - Attacking 'True Bank' (p.13): \"As a result, a total number of 3 malicious vApps are invoked, which clearly demonstrate the practicability of the attack.\"; II.D Threat Model (p.4): \"We assume that an adversary does not need to access any internal implementations/configurations of vApp including ASR, NLU components.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "It does not bypass permissions or authentication; instead, it tricks the VA into misinterpreting intent. The resulting misbehavior (e.g., invoking the wrong vApp or disclosing data to a malicious app). Expose semantically incorrect but functionally valid behaviors that breach user-intent and specification expectations\u2014posing integrity and trust risks without directly bypassing safety or security mechanisms.",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's LipFuzzer is explicitly designed to steer exploration toward discovering security-relevant classes of misinterpretation (e.g., vApp squatting, hijacking, privacy/phishing consequences) by generating and verifying LAPSUS variants of seed voice-command templates. However, the method does not aim to produce a specific, attacker-chosen model output (e.g., force the classifier to emit one predefined intent/phrase); instead it searches for inputs that cause unintended/vulnerable behaviors (any incorrect invocation or installation). Thus it targets broader classes of security-violating behaviors rather than specific predetermined outputs, matching the 'Medium' level.",
                    "evidence": "Intro / Page 2: \"Our tool generates potential voice commands that are likely to incur a semantic inconsistency such that a user reaches an unintended vApp/functionality (i.e. users think they use voice commands correctly but yield unwanted results).\" (Section I, Our Approach paragraph).; Template Fuzzing / Verification / Page 8: \"We verify if a LAPSUS is effective by testing the synthesis audio with Alexa. ... We define that a LAPSUS is verified to be effective when it is incorrectly interpreted by the VA system. For example, if the vApp is not installed after an installation-related LAPSUS is played to VA devices, then this LAPSUS is effective.\" (Section IV-C, Verification).; Threat & Goals / Page 2\u20133: The paper frames consequences as classes of security violations (Denial of Service, Privacy Leakage, Phishing) caused by misinterpretation and states evaluation goals such as: \"(i) We empirically verify that the problematic Intent Classifier can lead to speech misinterpretation related to LAPSUS.\" (Section VI, Evaluation goals)."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, LipFuzzer, is designed for targeted attack discovery. Its explicit goal is to identify voice commands that are misinterpreted by a Voice Assistant's Intent Classifier to invoke a specific, attacker-chosen, malicious vApp instead of the intended legitimate one. This is known as a 'vApp squatting attack'. The fuzzing process is not untargeted; it starts with seed commands for legitimate vApps and uses linguistic models to generate mutations that are likely to be confused with a predefined target (the malicious vApp's invocation phrase). The oracle for success is whether the mutated command successfully triggers the unintended, malicious vApp. This directly aligns with the 'High' rating's definition of steering exploration toward specific, predefined target outcomes (i.e., invoking a specific malicious application).",
                    "evidence": "Page 4, Section C. Motivating Example: \"To better understand how Intent Classifier relates to the speech misinterpretation, we show an example of vApp squatting attacks... In this example, a maliciously uploaded vApp aims to hijack the victim vApp when users try to install the victim vApp. The correct form of installation voice command should be 'Alexa, install The True Bank Skill'. An adversary intentionally crafted his/her vApp with a confusing name of 'The Truth Bank Skill'. As a result, as shown in Figure 3, a new leaf node of 'truth' is injected into the intent classification tree. Thus, if a user accidentally speaks 'Alexa, install The Truth Bank Skill', the malicious vApp will be installed and executed...\" This example clearly illustrates a targeted attack where the goal is to have a specific malicious vApp installed."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "LipFuzzer is designed to generate misinterpreted voice commands (LAPSUS) that cause unintended vApp behaviors (e.g., wrong invocation or installation) but it does not require the operator to specify a particular malicious target outcome in advance. Instead, it explores any misinterpretations of seed templates that lead to security-relevant failures. This aligns with \u2018Medium,\u2019 where the fuzzer discovers broader classes of security-violating behaviors rather than steering toward a single, predefined model output.",
                    "evidence": "Section I (Our Approach): \u201cOur goal is to systematically evaluate how Intent Classifier behaves when inputting different forms of voice commands \u2026 any voice commands that can lead to such inconsistency as LAPSUS.\u201d\nSection IV-C (Verification): \u201cWe define that a LAPSUS is verified to be effective when it is incorrectly interpreted by the VA system.\u201d\nSection VI-C (vApp Store-wide Results): LipFuzzer flags thousands of potentially vulnerable vApps without requiring a pre-specified malicious invocation name."
                },
                "manual": {
                    "value": "Low",
                    "why": "untargeted exploration to expose semantic or intent-classification inconsistencies, sometimes aligning with attacker intent (e.g., LAPSUS examples), though its discovery of attacker intent is incidental rather than goal-optimized.",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper goes beyond merely reporting failing inputs: it diagnoses the root cause of misinterpretation (blaming the NLU Intent Classifier rather than ASR), categorizes failure types (pronunciation, vocabulary, grammar), builds statistical linguistic (BN) models and trains their weights from a user study, and uses statistical analysis (cutoff selection, effective LAPSUS rates, sampling verification of vulnerable vApps) to connect discovered failures to underlying design vulnerabilities (template-based intent classification and fuzzy-matching behavior). These analyses explain why failures occur (e.g., long/complex installation names are more vulnerable, fuzzy matching behavior differences between platforms) and quantify their prevalence, satisfying the metric\u2019s requirement for diagnostic insight via statistical failure patterns and links to model/design vulnerabilities.",
                    "evidence": "Page 2, Contributions: \"(i) We analyze the problem of the semantic inconsistency of speech interpretation in vApp, and uncover that this problem is deeply-rooted in NLU\u2019s Intent classifier.\" (Intro / Contributions)\n\nPage 11, Section VI-A (Intent Classifier Evaluation): \"In conclusion, we empirically verified that the Intent Classifier contributes most in the problem of semantic misinterpretation.\" \n\nPage 7, Section IV-B (Weight Training / Linguistic Modeling): \"After we have the initial BN (i.e., a graph with all edge weights set as 1) ready, we further train the weight (i.e., the probabilities of successful transition states) through a user study with audio recording.\" \n\nPage 11, Section VI-B (LipFuzzer Evaluation, Cutoff selection): \"By applying this cutoff value, we cut off 49.9% of generated LAPSUS population with the effectiveness rate increased to 59.52%.\"\n\nPage 13, Section VI-D (Case Study / Security Implication): \"With the current template-based Intent Classifier design, using simple and unique voice commands is the most effective way of preventing vApp squatting attacks.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper provides a diagnostic analysis that explains why the observed failures (semantic misinterpretations, or 'LAPSUS') occur by linking them to a fundamental vulnerability in the system's design. First, the evaluation methodologically isolates the failing component, empirically demonstrating that the 'Intent Classifier' is the primary source of the problem, rather than the Automatic Speech Recognition (ASR) system. Second, through a case study and analysis, the paper identifies a clear failure pattern: vApps with simple, unique names are more robust, while those with longer, more complex names are more vulnerable to misinterpretation. Finally, it explicitly connects this pattern to an underlying architectural vulnerability: the 'current template-based Intent Classifier design'. This design aggregates command templates from numerous third-party developers, creating a shared namespace where attackers can intentionally register commands that are linguistically similar to legitimate ones, thus enabling 'vApp squatting' attacks. This explanation moves beyond correlating failures with input characteristics and identifies the root cause within the model's design and operational context.",
                    "evidence": "Page 13, Section D, Case Study, \"Security Implication: The aforementioned results show that simple and unique names are more difficult to be misspoken according to our linguistic models. With the current template-based Intent Classifier design, using simple and unique voice commands is the most effective way of preventing vApp squatting attacks. However, it is difficult to achieve given the increasingly growing vApp population.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper goes beyond merely reporting failing inputs: it diagnoses the root cause of misinterpretation (blaming the NLU Intent Classifier rather than ASR), categorizes failure types (pronunciation, vocabulary, grammar), builds statistical linguistic (BN) models and trains their weights from a user study, and uses statistical analysis (cutoff selection, effective LAPSUS rates, sampling verification of vulnerable vApps) to connect discovered failures to underlying design vulnerabilities (template-based intent classification and fuzzy-matching behavior). These analyses explain why failures occur (e.g., long/complex installation names are more vulnerable, fuzzy matching behavior differences between platforms) and quantify their prevalence, satisfying the metric\u2019s requirement for diagnostic insight via statistical failure patterns and links to model/design vulnerabilities.",
                    "evidence": "Page 2, Contributions: \"(i) We analyze the problem of the semantic inconsistency of speech interpretation in vApp, and uncover that this problem is deeply-rooted in NLU\u2019s Intent classifier.\" (Intro / Contributions)\n\nPage 11, Section VI-A (Intent Classifier Evaluation): \"In conclusion, we empirically verified that the Intent Classifier contributes most in the problem of semantic misinterpretation.\" \n\nPage 7, Section IV-B (Weight Training / Linguistic Modeling): \"After we have the initial BN (i.e., a graph with all edge weights set as 1) ready, we further train the weight (i.e., the probabilities of successful transition states) through a user study with audio recording.\" \n\nPage 11, Section VI-B (LipFuzzer Evaluation, Cutoff selection): \"By applying this cutoff value, we cut off 49.9% of generated LAPSUS population with the effectiveness rate increased to 59.52%.\"\n\nPage 13, Section VI-D (Case Study / Security Implication): \"With the current template-based Intent Classifier design, using simple and unique voice commands is the most effective way of preventing vApp squatting attacks.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Peforms statistical analysis of failure patterns and correlated the failures with linguistic ambiguity",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper enforces plausibility throughout its fuzzing design and provides empirical human-driven evidence that the fault-inducing inputs remain realistic. LipFuzzer mutates NLP-preprocessed templates using linguistically-grounded LAPSUS Models (Bayesian Networks) constructed from curated linguistic knowledge and further weight-trained with audio recordings from a user study. The fuzzer applies a probability cutoff to filter low-likelihood mutations and explicitly limits mutations per seed (\"we only get one mutation for each voice command\"), reducing cumulative drift. Finally, generated inputs are converted to speech (TTS) and verified on real VA devices; the authors report human-collected/resynthesized examples and measured rates (e.g., 77% of synthesized transcriptions still induced semantic inconsistency), demonstrating that many fault-triggering inputs are perceptually/semantically plausible.",
                    "evidence": "Page 2, (Introduction / Our Approach): \"we design LipFuzzer\u2019s basic template fuzzing by first mutating NLP pre-processed voice command templates. Then, we input mutated voice commands to VA devices by using machine speech synthesis to eliminate the human factor of producing ASR-related misinterpretation.\"; Page 7, (IV.B Weight Training): \"Weight Training. After we have the initial BN ... we further train the weight ... through a user study with audio recording.\"; Page 8, (IV.C Verification / Template Fuzzing): \"Verification. We verify if a LAPSUS is effective by testing the synthesis audio with Alexa. To do that, we first synthesize speech from generated LAPSUS voice commands.\"; Page 11, (VI-B Cutoffs for Different Query Strategies): \"Note that, during the fuzzing, we only get one mutation for each voice command.\"; Page 11, (VI-A Intent Classifier Evaluation): \"Our result shows that 77% (84/109) still incur the semantic inconsistency, which means LAPSUS still exists.\"; Page 11, (VI-B The Cutoff Value): \"By applying this cutoff value, we cut off 49.9% of generated LAPSUS population with the effectiveness rate increased to 59.52%.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper enforces input plausibility throughout the fuzzing process by design, and provides empirical evidence that the discovered fault-inducing inputs are realistic. The fuzzer, LipFuzzer, is explicitly a \"linguistic-model-guided\" tool that generates mutations based on models of plausible human speech errors, termed LAPSUS. These models are constructed from real-world linguistic knowledge (e.g., pronunciation, vocabulary, grammar rules) and are statistically trained using data from a human user study. This ensures that the generated voice commands are not random or unrealistic, but rather emulate common speech errors. The paper further provides empirical evidence of plausibility by conducting a user study with 150 MTurk workers to collect real-world voice command errors. The study confirms that the types of errors generated by LipFuzzer naturally occur, with 38.14% of collected user audio recordings classified as LAPSUS. Examples of these realistic errors are provided, directly demonstrating their plausibility.",
                    "evidence": "Page 8, Section IV-B, \"Weight Training\": \"After we have the initial BN (i.e., a graph with all edge weights set as 1) ready, we further train the weight (i.e., the probabilities of successful transition states) through a user study with audio recording. In this user study, we find sentences or short expressions which contains the states in the models. Then, we ask users in the study to repeat these sentences or expressions... Next, we calculate how many times these transitions are observed.\"\n\nPage 11, Section VI-A, \"Intent Classifier Evaluation\": \"The goal of this experiment is to confirm the existence of LAPSUS in the real world. From the collected audio records, we first remove unqualified audio samples... As a result, 61.86% (193/312) of them are processed with intended skills and functionality. This means that 38.14% of the recorded voice commands are LAPSUS. We showcase examples of these LAPSUS in Table III.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper enforces plausibility throughout its fuzzing design and provides empirical human-driven evidence that the fault-inducing inputs remain realistic. LipFuzzer mutates NLP-preprocessed templates using linguistically-grounded LAPSUS Models (Bayesian Networks) constructed from curated linguistic knowledge and further weight-trained with audio recordings from a user study. The fuzzer applies a probability cutoff to filter low-likelihood mutations and explicitly limits mutations per seed (\"we only get one mutation for each voice command\"), reducing cumulative drift. Finally, generated inputs are converted to speech (TTS) and verified on real VA devices; the authors report human-collected/resynthesized examples and measured rates (e.g., 77% of synthesized transcriptions still induced semantic inconsistency), demonstrating that many fault-triggering inputs are perceptually/semantically plausible.",
                    "evidence": "Page 2, (Introduction / Our Approach): \"we design LipFuzzer\u2019s basic template fuzzing by first mutating NLP pre-processed voice command templates. Then, we input mutated voice commands to VA devices by using machine speech synthesis to eliminate the human factor of producing ASR-related misinterpretation.\"; Page 7, (IV.B Weight Training): \"Weight Training. After we have the initial BN ... we further train the weight ... through a user study with audio recording.\"; Page 8, (IV.C Verification / Template Fuzzing): \"Verification. We verify if a LAPSUS is effective by testing the synthesis audio with Alexa. To do that, we first synthesize speech from generated LAPSUS voice commands.\"; Page 11, (VI-B Cutoffs for Different Query Strategies): \"Note that, during the fuzzing, we only get one mutation for each voice command.\"; Page 11, (VI-A Intent Classifier Evaluation): \"Our result shows that 77% (84/109) still incur the semantic inconsistency, which means LAPSUS still exists.\"; Page 11, (VI-B The Cutoff Value): \"By applying this cutoff value, we cut off 49.9% of generated LAPSUS population with the effectiveness rate increased to 59.52%.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "They perform human study to train the linguistic model that help selection potential fault triggering input. However, the synthesized voice commands are not measured for naturalness",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "LipFuzzer generates mutated inputs as serialized audio (via Text-to-Speech / phoneme-to-speech) and uses those synthesized audio files as the actual test inputs to VA devices, and the paper describes verifying failures by playing the synthesized audio and observing the VA response. Because inputs are generated directly in a serialized format (audio) and the verification/oracle uses those serialized artifacts, the fault-inducing perturbations are preserved across I/O (serialization) and the paper explicitly accounts for this in both test-generation and oracle evaluation. This matches the metric's criterion that approaches which generate inputs directly in serialized formats are reproducible under standard I/O operations.",
                    "evidence": "Page 2, 'Our Approach' (Section I): \"...we input mutated voice commands to VA devices by using machine speech synthesis to eliminate the human factor of producing ASR-related misinterpretation.\" \n\nPage 8, 'C. Template Fuzzing' (Verification): \"We verify if a LAPSUS is effective by testing the synthesis audio with Alexa. To do that, we first synthesize speech from generated LAPSUS voice commands. Then we monitor the response of the VA. ... We define that a LAPSUS is verified to be effective when it is incorrectly interpreted by the VA system.\" \n\nPage 9, 'V. IMPLEMENTATION / Speech Synthesis': \"...we use two types of speech synthesis methods to generate LAPSUS. ... We use each vApp platforms\u2019 native speech synthesis tools: Amazon Polly [6] and Google Cloud TTS [9]. ... The second speech synthesis method is for vocabulary and grammar levels. We direct input fuzzing results generated with LipFuzzer to the above mentioned services and perform a Text-to-Speech conversion.\" \n\nPage 11, 'VI.A Intent Classifier Evaluation' (Experiment Setup/verification): \"...we synthesize audios (so that ASR processing is guaranteed to be correct) to the Amazon Echo device to check if the semantic inconsistency still exists.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's proposed method, LipFuzzer, generates fault-inducing inputs by mutating voice commands at a high level of abstraction (text, phonemes) and then uses speech synthesis systems (e.g., Amazon Polly, Google Cloud TTS, ESpeak) to create the final audio test cases. This approach generates inputs directly in a serialized format (an audio file like WAV or MP3). The metric definition explicitly states: 'We also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., synthesized speech...) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.' The perturbations are embedded in the content of the speech itself, not as fragile numerical artifacts on a waveform, and thus are not susceptible to being lost or altered by standard I/O operations like saving and reloading the audio file. Therefore, the method ensures that the generated fault-inducing inputs are reproducible.",
                    "evidence": "Page 9, Section V. IMPLEMENTATION, A. LipFuzzer, subsection \"Speech Synthesis\": \"In order to work with real VA platforms, We generate voice command records to verify the effectiveness of fuzzing results. For machine-generated voice commands, we use two types of speech synthesis methods to generate LAPSUS. The first speech synthesis method (for Phoneme-level) is phoneme-to-speech that is used for phoneme based fuzzing results... The second speech synthesis method is for vocabulary and grammar levels. We direct input fuzzing results generated with LipFuzzer to the above mentioned services and perform a Text-to-Speech conversion.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "LipFuzzer generates mutated inputs as serialized audio (via Text-to-Speech / phoneme-to-speech) and uses those synthesized audio files as the actual test inputs to VA devices, and the paper describes verifying failures by playing the synthesized audio and observing the VA response. Because inputs are generated directly in a serialized format (audio) and the verification/oracle uses those serialized artifacts, the fault-inducing perturbations are preserved across I/O (serialization) and the paper explicitly accounts for this in both test-generation and oracle evaluation. This matches the metric's criterion that approaches which generate inputs directly in serialized formats are reproducible under standard I/O operations.",
                    "evidence": "Page 2, 'Our Approach' (Section I): \"...we input mutated voice commands to VA devices by using machine speech synthesis to eliminate the human factor of producing ASR-related misinterpretation.\" \n\nPage 8, 'C. Template Fuzzing' (Verification): \"We verify if a LAPSUS is effective by testing the synthesis audio with Alexa. To do that, we first synthesize speech from generated LAPSUS voice commands. Then we monitor the response of the VA. ... We define that a LAPSUS is verified to be effective when it is incorrectly interpreted by the VA system.\" \n\nPage 9, 'V. IMPLEMENTATION / Speech Synthesis': \"...we use two types of speech synthesis methods to generate LAPSUS. ... We use each vApp platforms\u2019 native speech synthesis tools: Amazon Polly [6] and Google Cloud TTS [9]. ... The second speech synthesis method is for vocabulary and grammar levels. We direct input fuzzing results generated with LipFuzzer to the above mentioned services and perform a Text-to-Speech conversion.\" \n\nPage 11, 'VI.A Intent Classifier Evaluation' (Experiment Setup/verification): \"...we synthesize audios (so that ASR processing is guaranteed to be correct) to the Amazon Echo device to check if the semantic inconsistency still exists.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or report experiments where fault-inducing inputs (LAPSUS) discovered on one model/target are reused to test other models to show cross-model transferability. Although LipFuzzer is applied to both Amazon Alexa and Google Assistant stores (i.e., the authors run the tool on multiple platforms), the stated evaluation goals and the described experiments focus on finding and verifying LAPSUS per platform. The paper does not present (a) an explicit experiment that generates faults on a surrogate model and then reuses those same inputs on different/unknown target models, nor (b) design elements explicitly aimed at discovering or measuring the transferability of discovered fault-inducing inputs across independently implemented models performing the same task.",
                    "evidence": "Section/Pages and quotes from the paper:\n\n1) Abstract (p.1): \"We also use LipFuzzer to evaluate both Amazon Alexa and Google Assistant vApp platforms.\" \u2014 supports that both platforms were evaluated but does not indicate reuse/transfer experiments.\n\n2) Evaluation goals (Section VI, p.11): \"More specifically, our evaluation has three goals: (i) We empirically verify that the problematic Intent Classifier can lead to speech misinterpretation related to LAPSUS. (ii) We show LipFuzzer's performance in terms of the LAPSUS Models' accuracy and effectiveness. (iii) We use LipFuzzer to reveal that problematic templates widely exist in both Amazon Alexa and Google Assistant platforms.\" \u2014 none of these goals mentions generating fault-inducing inputs on one model and testing them on other models to assess transferability.\n\n3) vApp store experiment setup (Section VI-C, p.12): \"we apply the same LAPSUS Models used in LipFuzzer evaluation.\" and \"we evaluate LipFuzzer by using templates crawled from the Amazon Alexa Store and Google Assistant Store.\" \u2014 indicates application across stores but does not describe experiments that generate LAPSUS on one platform and then reuse them to test the other platform as a transferability study.\n\n(These quotes are from the paper's Abstract, Section VI (Evaluation), and Section VI-C (vApp Store Evaluation).)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper applies its fuzzing methodology to two different major Voice Assistant platforms (Amazon Alexa and Google Assistant), which represent distinct models performing the same task of semantic interpretation. The design of the fuzzer, LipFuzzer, explicitly incorporates mechanisms to discover transferable failures. It achieves this by building platform-agnostic 'LAPSUS Models' based on general linguistic knowledge of common speech errors (e.g., pronunciation, vocabulary, grammar) rather than on the specific implementation of any single platform's Natural Language Understanding (NLU) component. By using these general models to generate fault-inducing inputs for applications on both platforms, the paper demonstrates that the same classes of vulnerabilities exist across different models, fulfilling both criteria for a 'High' rating.",
                    "evidence": "Page 12, Section VI-C, \"vApp Store Evaluation\": \"1) Experiment Setup: We evaluate LipFuzzer by using templates crawled from the Amazon Alexa Store and Google Assistant Store. For Amazon Alexa Store, we acquired a seed template dataset of 98,261 voice commands from 32,892 vApps. For Google Assistant Store, we gathered 2,328 vApps with 9,044 voice commands.\" This is further supported by Table IV on Page 13, which shows the fuzzing results for both platforms. The design for transferability is evidenced on Page 6, Section IV-A, \"Fuzzing Input & Output\": \"The input of linguistic modeling is linguistic knowledge data shown in 1 of Figure 4. We choose LAPSUS-related linguistic knowledge from multiple sources [12] [26] [1] [20] [24].\" This confirms the models are built from general, platform-agnostic linguistic sources."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "Although the authors apply LipFuzzer separately to both Amazon Alexa and Google Assistant vApp platforms, they do not generate fault-inducing inputs on one platform and then reuse those exact inputs to test the other. The evaluation treats each platform in isolation, and the paper does not include any explicit experiment or analysis showing that LAPSUS examples found on a surrogate (e.g., Alexa) transfer to an unseen target (e.g., Google Assistant). Thus, it does not demonstrate cross-model transferability of specific fault-inducing inputs.",
                    "evidence": "Section VI\u2013C (vApp Store Evaluation, pp. 12\u201313): the authors describe running LipFuzzer \u201cby using templates crawled from the Amazon Alexa Store\u201d and \u201ctemplates crawled from the Google Assistant Store\u201d but do not report reusing the same LAPSUS across both platforms.  Section VI (Evaluation Goals, p. 11): none of the three stated goals mention testing the transfer of inputs from one model to another."
                },
                "manual": {
                    "value": "Low",
                    "why": "evaluates two voice assistant systems independently but does not analyze whether failures generalize",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Data-driven"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Woodlief et al. - 2022 - Semantic image fuzzing of AI perception systems": {
        "key": "woodlief2022semantic",
        "name": "semSensFuzz",
        "year": "2022",
        "citation_count": "18",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's semantic-fuzzing approach (semSensFuzz / semImFuzz) uncovers misinterpretations and undesirable perception outputs (e.g., objects misclassified after semantic additions or occlusions) that violate safety expectations for autonomous systems. The authors report many significant and extreme semantic failures (misclassifications caused by added cars/people) that could be safety-relevant. However, the work targets standard perception models and semantic mutations (add car, add pedestrian, change color) and does not demonstrate bypassing explicit safety, policy, or security mechanisms (e.g., attacking defended or safety-aligned models, inducing data leakage or unauthorized actions). Therefore the failures exposed are beyond simple low-level robustness errors but do not meet the paper\u2019s or the metric\u2019s definition of high severity (bypassing explicit defenses), placing the method in the Medium category.",
                    "evidence": "Page 1958, Abstract: \"we found that it synthesized tests with novel and subjectively realistic image inputs, and that it discovered inputs that revealed significant inconsistencies between the specified and computed interpretations.\" (Abstract) \n\nPage 1966, Section 6.4.1 (RQ1 Results): \"1210 SUT inconsistencies resulting from 884 mutations and that each SUT had over 100 inconsistencies. Further, each of the SUTs exhibited at least one significant inconsistency and 3 of the 5 SUTs combined to exhibit a total of 20 extreme inconsistencies.\" (Section 6.4.1) \n\nPage 1966, Figure 12 caption / discussion: \"The rightmost column shows an extreme inconsistency; an added car occludes a truck, causing EfficientPS to misclassify the truck, labelling portions as 'do not care' and the rest as building.\" (Section 6.4.1, example of semantic misclassification) \n\nPage 1961, Section 4.1: \"we created an extensible pipeline in Python for fuzzing perception systems of autonomous vehicles with a single front-facing camera, semImFuzz, that implements three mutations: changing the color of cars, adding cars, and adding pedestrians.\" (Section 4.1) \n\nPage 1965, Section 6.1 (SUTs): \"we evaluate it on five highly competitive perception systems submitted to the Cityscapes benchmark...\" (Section 6.1) \u2014 showing the evaluation targets are standard benchmark SUTs rather than defended or safety-aligned models."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, semImFuzz, uncovers failures that violate safety expectations by producing unsafe outputs, which aligns with the 'Medium' severity definition. The fuzzer operates by applying semantic mutations, such as adding cars or pedestrians to an existing scene. The evaluation shows that these mutations can cause state-of-the-art perception systems to make critical misinterpretations, such as misclassifying a truck as a building after another car is added to the scene. These are not mere robustness errors (as in the 'Low' definition) but significant perception failures that could lead to unsafe behavior in an autonomous vehicle. The work does not meet the 'High' criteria because it does not test against models with explicit, bypassable safety or security mechanisms; instead, it tests standard perception models.",
                    "evidence": "Page 10, Section 6.4.1, \"For contextualizing the magnitude of the inconsistencies, Figure 12 shows three mutations produced by semImFuzz and their interpretations by the EfficientPS SUT before and after the mutation. ... The rightmost column shows an extreme inconsistency; an added car occludes a truck, causing EfficientPS to misclassify the truck, labelling portions as 'do not care' and the rest as building.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper's semantic-fuzzing approach (semSensFuzz / semImFuzz) uncovers misinterpretations and undesirable perception outputs (e.g., objects misclassified after semantic additions or occlusions) that violate safety expectations for autonomous systems. The authors report many significant and extreme semantic failures (misclassifications caused by added cars/people) that could be safety-relevant. However, the work targets standard perception models and semantic mutations (add car, add pedestrian, change color) and does not demonstrate bypassing explicit safety, policy, or security mechanisms (e.g., attacking defended or safety-aligned models, inducing data leakage or unauthorized actions). Therefore the failures exposed are beyond simple low-level robustness errors but do not meet the paper\u2019s or the metric\u2019s definition of high severity (bypassing explicit defenses), placing the method in the Medium category.",
                    "evidence": "Page 1958, Abstract: \"we found that it synthesized tests with novel and subjectively realistic image inputs, and that it discovered inputs that revealed significant inconsistencies between the specified and computed interpretations.\" (Abstract) \n\nPage 1966, Section 6.4.1 (RQ1 Results): \"1210 SUT inconsistencies resulting from 884 mutations and that each SUT had over 100 inconsistencies. Further, each of the SUTs exhibited at least one significant inconsistency and 3 of the 5 SUTs combined to exhibit a total of 20 extreme inconsistencies.\" (Section 6.4.1) \n\nPage 1966, Figure 12 caption / discussion: \"The rightmost column shows an extreme inconsistency; an added car occludes a truck, causing EfficientPS to misclassify the truck, labelling portions as 'do not care' and the rest as building.\" (Section 6.4.1, example of semantic misclassification) \n\nPage 1961, Section 4.1: \"we created an extensible pipeline in Python for fuzzing perception systems of autonomous vehicles with a single front-facing camera, semImFuzz, that implements three mutations: changing the color of cars, adding cars, and adding pedestrians.\" (Section 4.1) \n\nPage 1965, Section 6.1 (SUTs): \"we evaluate it on five highly competitive perception systems submitted to the Cityscapes benchmark...\" (Section 6.1) \u2014 showing the evaluation targets are standard benchmark SUTs rather than defended or safety-aligned models."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper describes semSensFuzz / semImFuzz as a semantic, but non-guided, mutation-based fuzzer that generates varied inputs (e.g., add car, add person, recolor) and measures drops in pixel-level IoU to reveal inconsistencies. It uses random test/resource selection and does not implement or claim any mechanism to steer generation toward attacker-chosen specific outputs or target labels/phrases. The authors explicitly describe the approach as non-guided and state that guiding mutations by SUT performance (which could enable targeted discovery) is future work. Therefore the method performs untargeted exploration that discovers generic failures (misclassifications/inconsistencies), not targeted attacks.",
                    "evidence": "Page 1959, Introduction: \"This paper takes the first step towards our long-term vision by proposing and evaluating an approach for non-guided test generation that can, for example, incorporate vehicles and people into images.\" (Section 1, para 3).\nPage 1966, Evaluation / RQ1 Results: \"Since semImFuzz uses random test and resource selection, we computed the number of duplicate tests generated as an indication of saturation.\" (Section 6.4.1, para 3) \u2014 showing random/untargeted selection.\nPage 1968, Conclusion and Future Work: \"Following standard software fuzzing, we will examine how to use SUT performance to guide mutation type and parameter selection to more quickly find inconsistencies.\" (Section 7) \u2014 indicating guided/targeted steering is not part of the current implementation but planned future work."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, semSensFuzz, performs untargeted exploration to find generic perception failures. It applies semantic mutations (e.g., adding a car) to an input image and simultaneously updates the ground-truth interpretation to reflect this change. The oracle then checks for inconsistencies by comparing the model's output on the mutated image against the new, mutated ground truth. A failure is defined as a significant drop in the overall pixel-wise classification accuracy, not as the successful generation of a specific, attacker-chosen misclassification (e.g., forcing the model to classify a newly added car as a 'bicycle'). The goal is to uncover any misinterpretation that results from the mutation, which constitutes a generic failure.",
                    "evidence": "Page 10, Section 6.4, \"Results\": \"More formally, for a given SUT Percep, test case (r, interp), and corresponding mutation (r', interp'), the drop is given by drop = eval(Percep(r), interp) \u2013 eval(Percep(r'), interp'). The larger the value, the more error the mutation induced in the SUT... We categorize drops between 1 and 5 p.p. as moderate inconsistencies, between 5 and 10 as significant, and those greater than 10 as extreme inconsistencies.\" This evidence shows that the metric for failure is a general drop in performance (an inconsistency), not the achievement of a specific, predefined output."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper describes semSensFuzz / semImFuzz as a semantic, but non-guided, mutation-based fuzzer that generates varied inputs (e.g., add car, add person, recolor) and measures drops in pixel-level IoU to reveal inconsistencies. It uses random test/resource selection and does not implement or claim any mechanism to steer generation toward attacker-chosen specific outputs or target labels/phrases. The authors explicitly describe the approach as non-guided and state that guiding mutations by SUT performance (which could enable targeted discovery) is future work. Therefore the method performs untargeted exploration that discovers generic failures (misclassifications/inconsistencies), not targeted attacks.",
                    "evidence": "Page 1959, Introduction: \"This paper takes the first step towards our long-term vision by proposing and evaluating an approach for non-guided test generation that can, for example, incorporate vehicles and people into images.\" (Section 1, para 3).\nPage 1966, Evaluation / RQ1 Results: \"Since semImFuzz uses random test and resource selection, we computed the number of duplicate tests generated as an indication of saturation.\" (Section 6.4.1, para 3) \u2014 showing random/untargeted selection.\nPage 1968, Conclusion and Future Work: \"Following standard software fuzzing, we will examine how to use SUT performance to guide mutation type and parameter selection to more quickly find inconsistencies.\" (Section 7) \u2014 indicating guided/targeted steering is not part of the current implementation but planned future work."
                },
                "manual": {
                    "value": "Low",
                    "why": "Discovers generic exception failures",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight via statistical and input-structure analyses of discovered failures (counts by SUT and mutation type, severity buckets, time-series of inconsistencies, and manual inspection/false-positive rates), and it links many failures to input-level causes (occlusion/overlap, lighting, perspective mismatches, mutation type). However, it does not analyze observable model internals (neuron activations, coverage, layer/feature attributions) nor does it systematically connect failures to deeper model vulnerabilities such as reliance on spurious features, explicit dataset bias, or overfitting. Thus it meets the criteria for Medium: statistical/failure-pattern and input-structure diagnostics but no internal-model diagnostics or causal linkage to underlying model vulnerabilities.",
                    "evidence": "Page 1966, Section 6.3 Metrics: \"we rely on the Cityscapes benchmark evaluation tool (eval) which scores each SUT\u2019s performance based on the percentage of correctly classified pixels... drop = eval(Percep(r), interp) - eval(Percep(r'), interp')\". Page 1966-1967, Section 6.4 RQ1 Results: \"Figure 11 shows the counts of inconsistencies found in each category per SUT... semImFuzz found 1210 SUT inconsistencies resulting from 884 mutations and that each SUT had over 100 inconsistencies.\" Page 1967, Figure 12 discussion: \"The leftmost column shows a moderate inconsistency; an added person occludes a bus, causing EfficientPS to then classify the bus as a train... The rightmost column shows an extreme inconsistency; an added car occludes a truck, causing EfficientPS to misclassify the truck...\". Page 1967, Section 6.4: \"To gain a better grasp on the rate of false positives we manually inspect all the generated test cases that led to 5+ p.p. drop, and sample 10% of the test cases that led to a drop between 1 and 5 p.p.\" and Table 2 (page 1967) reports false positive rates per SUT and severity buckets. Page 1968, Section 6.4.3 RQ3 Results: \"The add car mutation induced the most inconsistencies (877), followed by the add person mutation (280), and then the mutation to change the car color (53)... This suggests that editing large regions of the image, even in a conforming manner, is insufficient... high level semantic mutations such as adding a car are required to exercise these perception systems and find inconsistencies.\" Page 1964, Section 4.4.4 Discriminators: \"The discriminator learned to differentiate between the classes, but could not differentiate conforming versus nonconforming images.\" -- Nowhere in the design, implementation, or evaluation sections does the paper present analysis of internal model signals (e.g., neuron activations, layer-wise coverage, or feature-attribution analyses) or establish causal links from discovered failures to underlying vulnerabilities such as reliance on spurious features or overfitting."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight through a statistical analysis of failure patterns. It analyzes and reports on the distribution of discovered inconsistencies across different Systems Under Test (SUTs), observing that the highest-performing models on the original benchmark exhibited the most failures. Furthermore, it analyzes the correlation between failures and the input structure by breaking down inconsistencies by the type of mutation applied (e.g., adding a car, changing a car's color). This analysis reveals that some semantic mutations are significantly more likely to induce failures than others. This level of analysis aligns with the 'Medium' rating. The paper does not, however, analyze observable model internals (e.g., neuron coverage) or link the discovered failures to underlying model vulnerabilities such as overfitting or dataset bias, which would be required for a 'High' rating.",
                    "evidence": "Page 10, Section 6.4.1, 'RQ 1 Results: Finding Inconsistencies': 'The distribution of inconsistencies among the SUTs is unexpected. NVIDIA SemSeg [34] and EfficientPS [24], the two highest scoring on the Cityscapes benchmark, had the highest number of inconsistencies in all three categories... Further analysis of the specific SUTs is needed to understand the factors involved in this performance, but these data suggest that the highest performing SUTs may be more brittle under certain conditions.'\n\nPage 11, Section 6.4.3, 'RQ 3 Results: Mutation Types': 'The add car mutation induced the most inconsistencies (877), followed by the add person mutation (280), and then the mutation to change the car color (53)... This further supports our notion that high level semantic mutations such as adding a car are required to exercise these perception systems and find inconsistencies.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight via statistical and input-structure analyses of discovered failures (counts by SUT and mutation type, severity buckets, time-series of inconsistencies, and manual inspection/false-positive rates), and it links many failures to input-level causes (occlusion/overlap, lighting, perspective mismatches, mutation type). However, it does not analyze observable model internals (neuron activations, coverage, layer/feature attributions) nor does it systematically connect failures to deeper model vulnerabilities such as reliance on spurious features, explicit dataset bias, or overfitting. Thus it meets the criteria for Medium: statistical/failure-pattern and input-structure diagnostics but no internal-model diagnostics or causal linkage to underlying model vulnerabilities.",
                    "evidence": "Page 1966, Section 6.3 Metrics: \"we rely on the Cityscapes benchmark evaluation tool (eval) which scores each SUT\u2019s performance based on the percentage of correctly classified pixels... drop = eval(Percep(r), interp) - eval(Percep(r'), interp')\". Page 1966-1967, Section 6.4 RQ1 Results: \"Figure 11 shows the counts of inconsistencies found in each category per SUT... semImFuzz found 1210 SUT inconsistencies resulting from 884 mutations and that each SUT had over 100 inconsistencies.\" Page 1967, Figure 12 discussion: \"The leftmost column shows a moderate inconsistency; an added person occludes a bus, causing EfficientPS to then classify the bus as a train... The rightmost column shows an extreme inconsistency; an added car occludes a truck, causing EfficientPS to misclassify the truck...\". Page 1967, Section 6.4: \"To gain a better grasp on the rate of false positives we manually inspect all the generated test cases that led to 5+ p.p. drop, and sample 10% of the test cases that led to a drop between 1 and 5 p.p.\" and Table 2 (page 1967) reports false positive rates per SUT and severity buckets. Page 1968, Section 6.4.3 RQ3 Results: \"The add car mutation induced the most inconsistencies (877), followed by the add person mutation (280), and then the mutation to change the car color (53)... This suggests that editing large regions of the image, even in a conforming manner, is insufficient... high level semantic mutations such as adding a car are required to exercise these perception systems and find inconsistencies.\" Page 1964, Section 4.4.4 Discriminators: \"The discriminator learned to differentiate between the classes, but could not differentiate conforming versus nonconforming images.\" -- Nowhere in the design, implementation, or evaluation sections does the paper present analysis of internal model signals (e.g., neuron activations, layer-wise coverage, or feature-attribution analyses) or establish causal links from discovered failures to underlying vulnerabilities such as reliance on spurious features or overfitting."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Provides correlations between input changes and output errors",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly designs and enforces per-mutation plausibility constraints (resource-based parameterization, preconditions, perspective and lighting checks) and it performs an explicit human validation of final fault-inducing inputs. However, the implementation still produces a substantial rate of non-conforming (false-positive) mutants, the authors report limitations of automated conformity checks (failed discriminator), and they do not address cumulative drift across multiple iterative mutations. Thus the work enforces plausibility at the individual-mutation level and validates final cases empirically, but does not fully ensure plausibility throughout iterative fuzzing and exhibits imperfect conformity enforcement.",
                    "evidence": "Section 3.2 (p.1960): \"Second, we associate a set of preconditions Prec with each mutation \u03b4, specified in terms of the interp that defines whether \u03b4 is applicable to a given t. If Prec(interp) is not satisfied, then \u03b4 is not applicable to that test.\" (Section name: 3.2 Semantic Mutations with semSensFuzz) \n\nSection 4.3.1 (p.1963): \"To ensure that the lighting conditions are similar, if the entity does not have a median value within 5 units (~2%) of the median value of the base image target area, then it fails the conformity check.\" (Section name: 4.3 Adding an Entity / Implementation) \n\nSection 4.3.1 (p.1962\u20131963): description of multiple conformity checks (vanishing point quadrant matching, bounding-box-on-road check, occlusion filtering) used before generating a mutated image. (Section name: 4.3 Adding an Entity / Implementation) \n\nSection 4.4.4 (p.1964): \"We explored this approach, training a binary classifier based on a CNN ... The discriminator learned to differentiate between the classes, but could not differentiate conforming versus nonconforming images.\" (Section name: 4.4.4 Discriminators for Conformity Checking) \n\nSection 6.4.1 and Table 2 (p.1966\u20131967): human-validation of plausibility: \"we manually inspect all the generated test cases that led to 5+ p.p. drop, and sample 10% of the test cases that led to a drop between 1 and 5 p.p.. The process entailed each author examining each image and classifying them as either a true positive or false positive.\" Table 2 (p.1967) reports substantial false-positive rates (e.g., EfficientPS: 43% for [1,5) p.p., 47% for [5,10), 53% for [10,100]). (Section name: 6.4.1 RQ1 Results: Finding Inconsistencies) \n\nSection 4.3.2 and Figures 9 (p.1963): authors enumerate concrete failure modes leading to nonconforming images (perspective mismatch, overlapping entities, inconsistent lighting, missing shadows). (Section name: 4.3 Adding an Entity / Potential for False Positives) \n\nSection 3.2 (p.1960): \"All of the mechanisms in our approach make a conscious trade off between conformity and a smaller space of available mutations.\" (Section name: 3.2 Semantic Mutations with semSensFuzz)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's methodology is fundamentally centered on maintaining input plausibility, which it terms \"conformity.\" The fuzzing design incorporates numerous mechanisms to enforce this throughout the mutation process, such as using semantic preconditions, leveraging real-world data for mutations, and performing explicit checks for consistent perspective, physical feasibility (e.g., placing a car on a road), and lighting. Most importantly, the paper provides direct empirical evidence assessing the plausibility of the final fault-inducing inputs. The authors conducted a manual study where they inspected all high-impact and a sample of moderate-impact failure-inducing images to classify them as conforming (\"true positive\") or non-conforming (\"false positive\"). The results of this human evaluation, including false positive rates and visual examples, are reported. This combination of enforcing plausibility by design and empirically validating the realism of the final outputs fully meets the criteria for a \"High\" rating.",
                    "evidence": "Page 11, Section 6.4, \"...one potential issue is the presence of test cases that are nonconforming, which leads to false inconsistencies. As highlighted in Section 4, there are several factors that can result in a nonconforming test case, and determining conformity is subjective. Still, to gain a better grasp on the rate of false positives we manually inspect all the generated test cases that led to 5+ p.p. drop, and sample 10% of the test cases that led to a drop between 1 and 5 p.p.. The process entailed each author examining each image and classifying them as either a true positive or false positive. If any of the three authors deemed an image a false positive, it was conservatively recorded as such.\" This is further supported by Table 2, \"False Positive Rate for Inconsistencies Found,\" and Figure 14, which provides visual examples of both \"True Positive\" and \"False Positive\" (i.e., non-conforming) results."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "semSensFuzz builds in strong, per-mutation plausibility constraints (semantic preconditions, real data parameterization, perspective & lighting checks) and even conducts a human validation of final fault-inducing inputs, but it does not enforce a global, iterative plausibility guarantee across successive mutations (no cumulative-drift control) nor apply quantitative perceptual metrics (e.g. LPIPS/SSIM) on the full mutated distribution. Thus it validates individual steps and final cases empirically, but stops short of the end-to-end, iteration-aware enforcement required for a \u201cHigh\u201d rating.",
                    "evidence": "Section 3.2 (p. 1960): \u201cwe associate a set of preconditions Prec with each mutation \u03b4\u2026 If Prec(interp) is not satisfied, then \u03b4 is not applicable\u2026\u201d\n\nSection 4.3.1 (p. 1962\u20131963): Perspective quadrant matching, on-road bounding-box check, occlusion filtering, and median HSV brightness checks are all applied before each \u2018add entity\u2019 mutation.\n\nSection 4.4.4 (p. 1964): \u201cWe explored\u2026 a discriminator\u2026 but it could not differentiate conforming versus nonconforming images.\u201d\n\nSection 6.4.1 & Table 2 (p. 1966\u20131967): Authors manually inspect all high-impact and a sampled set of moderate-impact fault inputs, reporting substantial false-positive rates (e.g. 43%\u201375% nonconforming in some bins)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Applies design constratraints but no measure of naturalness of the fault-triggering fuzzed inputs",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper applies semantic, metamorphic image-level mutations (e.g., adding cars/pedestrians, recoloring) rather than fine-grained numerical perturbations, so the generated failures are less likely to be fragile numerical artifacts. However, the authors do not explicitly account for standard I/O/serialization effects (clipping, quantization, rounding) nor constrain mutations to guarantee that fault-inducing changes persist after typical image serialization. Thus the work corresponds to the metric's \"semantic-preserving metamorphic transformations without explicit enforcement of reproducibility under serialization\" case.",
                    "evidence": "Page 1960, Section 3.1 (Problem definition): \"At a conceptual level, our approach thus constitutes a form of metamorphic testing [32] where existing test cases are converted into new ones...\"; Page 1962, Section 4.1 (semImFuzz Architecture / Implementation): \"We implemented each mutation using Numpy [13] along with OpenCV for Python [4] and Pillow [6], two common image processing libraries for Python.\"; Page 1963, Section 4.3.1 (Adding an Entity - Implementation): \"When adding the entity, the mutation takes the isolated portion of the image and overlays it at the same pixel coordinates on the base image. Figure 8 demonstrates this process.\"; Page 1963, Section 4.3.2 (Potential for False Positives): \"...so semImFuzz does not attempt to add a shadow when adding an entity. This can lead to nonconforming images, as shown in Figure 9d.\" These excerpts show the approach uses semantic/metamorphic pixel-level edits (not fine numerical perturbations) but contain no statement that the design explicitly models or enforces persistence of perturbations through standard I/O operations such as quantization, clipping, or rounding."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, `semImFuzz`, generates test cases by applying semantic-preserving metamorphic transformations to existing images, such as adding a car or changing an object's color. According to the metric's definition, using such transformations without explicitly enforcing reproducibility under serialization qualifies for a 'Medium' rating. The paper's methodology focuses heavily on ensuring the 'conformity' of the mutated images\u2014that they are visually realistic and physically plausible\u2014through various checks related to perspective, lighting, and object placement. However, the paper does not mention or explicitly account for numerical alterations that can occur during standard I/O operations like saving an image to a file (which involves quantization, rounding, or clipping). The test generation and execution pipeline appears to be conducted entirely in memory, without a step to serialize and then deserialize the mutated input to confirm that the failure-inducing perturbation persists.",
                    "evidence": "Page 6, Section 4.2, Changing Object Color: \"The simplest mutation we implemented changes the color of a single entity in the test case...\".\nPage 6, Section 4.3, Adding an Entity: \"Adding entities to scenes is one of the advances of semImFuzz. The goal of this mutation is to add an entity to a scene in a way that may impact the perception system such as adding a vehicle or pedestrian.\"\nPage 7, Section 4.3.2, Potential for False Positives: This section discusses sources of invalid tests, but focuses on semantic and visual realism issues like perspective mismatches, object overlap, inconsistent lighting, and missing shadows, rather than numerical artifacts lost during I/O operations. For instance: \"Another consideration is consistent lighting... This can lead to nonconforming images, as shown in Figure 9d.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper applies semantic, metamorphic image-level mutations (e.g., adding cars/pedestrians, recoloring) rather than fine-grained numerical perturbations, so the generated failures are less likely to be fragile numerical artifacts. However, the authors do not explicitly account for standard I/O/serialization effects (clipping, quantization, rounding) nor constrain mutations to guarantee that fault-inducing changes persist after typical image serialization. Thus the work corresponds to the metric's \"semantic-preserving metamorphic transformations without explicit enforcement of reproducibility under serialization\" case.",
                    "evidence": "Page 1960, Section 3.1 (Problem definition): \"At a conceptual level, our approach thus constitutes a form of metamorphic testing [32] where existing test cases are converted into new ones...\"; Page 1962, Section 4.1 (semImFuzz Architecture / Implementation): \"We implemented each mutation using Numpy [13] along with OpenCV for Python [4] and Pillow [6], two common image processing libraries for Python.\"; Page 1963, Section 4.3.1 (Adding an Entity - Implementation): \"When adding the entity, the mutation takes the isolated portion of the image and overlays it at the same pixel coordinates on the base image. Figure 8 demonstrates this process.\"; Page 1963, Section 4.3.2 (Potential for False Positives): \"...so semImFuzz does not attempt to add a shadow when adding an entity. This can lead to nonconforming images, as shown in Figure 9d.\" These excerpts show the approach uses semantic/metamorphic pixel-level edits (not fine numerical perturbations) but contain no statement that the design explicitly models or enforces persistence of perturbations through standard I/O operations such as quantization, clipping, or rounding."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Perturbations occur at a semantic level that is plausibly I/O-stable, but the paper neither enforces nor verifies persistence after I/O transformations, and minor artifacts from the mutation implementation may cause non-persistent faults.",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper reuses fault-inducing inputs generated by semImFuzz to test multiple different perception models: the same set of 150,000 semantically mutated tests was executed against each of the five SUTs, and the authors report aggregated counts of inconsistencies across SUTs (e.g., \"1210 SUT inconsistencies resulting from 884 mutations\"), which demonstrates reuse of generated failures to evaluate other models. However, the paper does not present design elements or explicit mechanisms aimed at producing or measuring transferable adversarial attacks (e.g., surrogate-model training, cross-model transfer rate analyses, or targeted transferability objectives). The evaluation reports per-SUT inconsistency counts and notes which mutations trigger errors, but it does not characterize transfer rates or describe methods explicitly designed to discover transferable security failures across models. These facts match the metric definition for Medium: inputs are reused to test transferability but the fuzzing design is not explicitly aimed at discovering transferable attacks.",
                    "evidence": "Section 6.2 (Tests Generated), p. 1966: \"For evaluation, we ran each SUT on 150,000 tests generated by semImFuzz comprised of 50,000 'Add Car' mutations, 50,000 'Add Person' mutations, and 50,000 'Change Car Color' mutations.\" \n\nSection 6.4.1 (RQ1 Results: Finding Inconsistencies), p. 1966: \"We first remark that semImFuzz found 1210 SUT inconsistencies resulting from 884 mutations and that each SUT had over 100 inconsistencies.\" \n\n(Absence of transfer-focused design) The paper's evaluation and methodology sections (Sections 3, 4, 6) describe mutation design, conformity checks, and per-SUT inconsistency reporting but do not describe surrogate-model-based strategies, explicit cross-model transferability metrics, or procedures to maximize/measure transfer of failures between models. For example, Section 6 (Evaluation) frames results as per-SUT inconsistency counts and severity drops rather than transfer rates or cross-model vulnerability analyses."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's evaluation reuses the generated fault-inducing inputs across multiple models but the fuzzing design is not explicitly aimed at discovering transferable attacks. The methodology involves creating a single set of 150,000 mutated inputs and then executing this same set against five different perception models (SUTs) performing the same task. This experimental design inherently tests for transferability by observing how different models react to the same set of inputs. However, the design of the fuzzer, `semImFuzz`, focuses on generating semantically valid and conforming mutations based on real-world constraints (e.g., adding cars to roads, changing colors) rather than incorporating explicit mechanisms to discover vulnerabilities that are common across different model architectures. Therefore, it satisfies the condition of reusing inputs but lacks the explicit design goal for transferability required for a 'High' rating.",
                    "evidence": "Page 9, Section 6.2 Tests Generated, \"For evaluation, we ran each SUT on 150,000 tests generated by semImFuzz comprised of 50,000 \"Add Car\" mutations, 50,000 \"Add Person\" mutations, and 50,000 \u201cChange Car Color\u201d mutations.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper reuses fault-inducing inputs generated by semImFuzz to test multiple different perception models: the same set of 150,000 semantically mutated tests was executed against each of the five SUTs, and the authors report aggregated counts of inconsistencies across SUTs (e.g., \"1210 SUT inconsistencies resulting from 884 mutations\"), which demonstrates reuse of generated failures to evaluate other models. However, the paper does not present design elements or explicit mechanisms aimed at producing or measuring transferable adversarial attacks (e.g., surrogate-model training, cross-model transfer rate analyses, or targeted transferability objectives). The evaluation reports per-SUT inconsistency counts and notes which mutations trigger errors, but it does not characterize transfer rates or describe methods explicitly designed to discover transferable security failures across models. These facts match the metric definition for Medium: inputs are reused to test transferability but the fuzzing design is not explicitly aimed at discovering transferable attacks.",
                    "evidence": "Section 6.2 (Tests Generated), p. 1966: \"For evaluation, we ran each SUT on 150,000 tests generated by semImFuzz comprised of 50,000 'Add Car' mutations, 50,000 'Add Person' mutations, and 50,000 'Change Car Color' mutations.\" \n\nSection 6.4.1 (RQ1 Results: Finding Inconsistencies), p. 1966: \"We first remark that semImFuzz found 1210 SUT inconsistencies resulting from 884 mutations and that each SUT had over 100 inconsistencies.\" \n\n(Absence of transfer-focused design) The paper's evaluation and methodology sections (Sections 3, 4, 6) describe mutation design, conformity checks, and per-SUT inconsistency reporting but do not describe surrogate-model-based strategies, explicit cross-model transferability metrics, or procedures to maximize/measure transfer of failures between models. For example, Section 6 (Evaluation) frames results as per-SUT inconsistency counts and severity drops rather than transfer rates or cross-model vulnerability analyses."
                },
                "manual": {
                    "value": "Low",
                    "why": "Mutated inputs are evaluated independently across multiple inputs but no measure whether same failures discovered by the fauzzer transfer to other models",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Data-driven"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Lee et al. - 2020 - Effective white-box testing of deep neural networks with adaptive neuron-selection strategy": {
        "key": "lee2020effective",
        "name": "ADAPT",
        "year": "2020",
        "citation_count": "81",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's contribution and evaluation focus on increasing coverage and finding adversarial inputs (misclassifications) for standard pre-trained models (LeNet, VGG, ResNet). All reported failures are robustness errors (incorrectly classified labels / adversarial images). The authors do not evaluate or claim bypassing of any explicit safety, policy, or security mechanisms, nor do they test defended or safety-aligned models or demonstrate jailbreaks, data leakage, or unauthorized actions. Therefore the failures uncovered correspond to model robustness errors only, matching the metric's Low category.",
                    "evidence": "Page 165 (Abstract): \"Experiments with real-world network models and datasets show that Adapt is remarkably more effective than existing testing techniques in terms of coverage and adversarial inputs found.\" \n\nPage 172 (Section 4.3, 'Adversarial Inputs'): \"Table 3 reports the average number of mutations per image (Mutations), the average number of adversarial inputs found per image (Adv. Inputs), the total number of incorrectly classified labels (Labels), and the number of initial inputs (Seeds) from which adversarial inputs were found.\" \n\nPage 173 (Section 4.3): \"In particular, the adversarial images found for VGG-19 and ResNet-50 were visually indistinguishable from the original ones.\" \n\nPage 169 (Section 4.1 / Table 2): \"We used two datasets and four neural network models in Table 2... LeNet-4, LeNet-5, VGG-19 and ResNet-50\" (standard pre-trained models; no defended/safety-aligned models are listed). \n\nPage 174 (Conclusion): \"we demonstrated that Adapt is significantly more effective than existing white-box and grey-box techniques in increasing coverage and finding adversarial inputs.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, ADAPT, is designed and evaluated to find adversarial inputs that cause misclassifications in standard image recognition models. The failures are defined as \"model robustness errors,\" where small, semantically preserving mutations to an input image cause the model to produce an incorrect label. The evaluation metrics focus on the number of adversarial inputs found and the quantity of incorrectly classified labels. The paper does not test for or claim to uncover unsafe, biased, or toxic behaviors, nor does it attempt to bypass any explicit safety or security mechanisms in defended models. The uncovered failures are purely mispredictions, which aligns perfectly with the 'Low' severity definition.",
                    "evidence": "Page 9, Section 4.3, Adversarial Inputs: \"Table 3 reports the average number of mutations per image (Mutations), the average number of adversarial inputs found per image (Adv. Inputs), the total number of incorrectly classified labels (Labels), and the number of initial inputs (Seeds) from which adversarial inputs were found. An incorrectly classified label consists of an original image and a found label.\" This quote demonstrates that the failures are defined as incorrect classifications. Further evidence on the nature of the inputs is on page 7, Section 4.1: \"All the techniques, including ADAPT, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\", confirming the use of small, semantically preserving mutations."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's contribution and evaluation focus on increasing coverage and finding adversarial inputs (misclassifications) for standard pre-trained models (LeNet, VGG, ResNet). All reported failures are robustness errors (incorrectly classified labels / adversarial images). The authors do not evaluate or claim bypassing of any explicit safety, policy, or security mechanisms, nor do they test defended or safety-aligned models or demonstrate jailbreaks, data leakage, or unauthorized actions. Therefore the failures uncovered correspond to model robustness errors only, matching the metric's Low category.",
                    "evidence": "Page 165 (Abstract): \"Experiments with real-world network models and datasets show that Adapt is remarkably more effective than existing testing techniques in terms of coverage and adversarial inputs found.\" \n\nPage 172 (Section 4.3, 'Adversarial Inputs'): \"Table 3 reports the average number of mutations per image (Mutations), the average number of adversarial inputs found per image (Adv. Inputs), the total number of incorrectly classified labels (Labels), and the number of initial inputs (Seeds) from which adversarial inputs were found.\" \n\nPage 173 (Section 4.3): \"In particular, the adversarial images found for VGG-19 and ResNet-50 were visually indistinguishable from the original ones.\" \n\nPage 169 (Section 4.1 / Table 2): \"We used two datasets and four neural network models in Table 2... LeNet-4, LeNet-5, VGG-19 and ResNet-50\" (standard pre-trained models; no defended/safety-aligned models are listed). \n\nPage 174 (Conclusion): \"we demonstrated that Adapt is significantly more effective than existing white-box and grey-box techniques in increasing coverage and finding adversarial inputs.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's testing method (Adapt) is designed to maximize internal coverage (NC, TKNC) by selecting neurons and using their gradients to generate inputs; its exploration is coverage-driven and its oracle checks for newly covered identifiers and misclassification (incorrect labels). The authors evaluate success by coverage increase and by the number/variety of adversarial (misclassified) inputs found. Nowhere does the paper describe steering the fuzzer toward attacker-chosen specific outputs (e.g., forcing a particular target label or phrase) or toward broader semantic security goals (jailbreaks, toxicity). Thus the approach performs untargeted exploration that discovers generic faults (misclassifications/inconsistencies) rather than targeted attacks.",
                    "evidence": "Page 166, \u00a72.3 (White-Box Testing of Neural Networks): \"One of the main goals of neural network testing is to generate test cases (i.e. input vectors) that maximize a given coverage metric (Cov) in a limited testing budget... Find T = {I1, . . . , In } that maximizes \u03a3_{Ii \u2208 T} Cov(Run(DNN, Ii)).\" ; Page 166, Algorithm 1: mutation step uses gradients of selected neurons to increase their outputs but the objective is coverage (lines showing \"I' \u2190 I' + \u03bb \u00b7 \u2202(\u03a3_{n\u2208N} Neuron(n, I'))/\u2202I'\", and the check \"if Cov(O') \u2288 C \u2227 Constraint(I, I') then ...\"). ; Page 172, \u00a74.3 (Adversarial Inputs / Evaluation): \"Table 3 reports the average number of mutations per image (Mutations), the average number of adversarial inputs found per image (Adv. Inputs), the total number of incorrectly classified labels (Labels), and the number of initial inputs (Seeds) from which adversarial inputs were found.\" ; Abstract (page 165): \"Experiments with real-world network models and datasets show that Adapt is remarkably more effective than existing testing techniques in terms of coverage and adversarial inputs found.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper presents ADAPT, a white-box testing technique whose primary objective is to maximize neuron coverage metrics. The method generates new inputs by calculating gradients of internal neurons to increase their activation values, with the goal of exploring new internal states of the network. The discovery of adversarial inputs, which the paper evaluates as generic misclassifications, is a consequence of this coverage-guided exploration rather than the result of steering the search towards a predefined, specific output label or a broader class of security-violating behaviors. The entire process is designed for untargeted exploration to find any fault.",
                    "evidence": "Page 3, Section 2.3 White-Box Testing of Neural Networks: \"One of the main goals of neural network testing is to generate test cases (i.e. input vectors) that maximize a given coverage metric (Cov) in a limited testing budget.\" \n\nPage 5, Algorithm 2: The condition for accepting a new test input is based on whether it increases coverage: \"if Cov(O') \u2284 C \u2227 Constraint(I, I') then W \u2190 W \u222a {I'}; C \u2190 C \u222a Cov(O')\". This shows the exploration is guided by covering new neurons, not by reaching a target output. \n\nPage 9, Section 4.3 Adversarial Inputs: The evaluation measures the effectiveness in finding generic misclassifications, not targeted ones. \"Table 3 reports the average number of mutations per image (Mutations), the average number of adversarial inputs found per image (Adv. Inputs), the total number of incorrectly classified labels (Labels)...\" This confirms the adversarial discovery is untargeted."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's testing method (Adapt) is designed to maximize internal coverage (NC, TKNC) by selecting neurons and using their gradients to generate inputs; its exploration is coverage-driven and its oracle checks for newly covered identifiers and misclassification (incorrect labels). The authors evaluate success by coverage increase and by the number/variety of adversarial (misclassified) inputs found. Nowhere does the paper describe steering the fuzzer toward attacker-chosen specific outputs (e.g., forcing a particular target label or phrase) or toward broader semantic security goals (jailbreaks, toxicity). Thus the approach performs untargeted exploration that discovers generic faults (misclassifications/inconsistencies) rather than targeted attacks.",
                    "evidence": "Page 166, \u00a72.3 (White-Box Testing of Neural Networks): \"One of the main goals of neural network testing is to generate test cases (i.e. input vectors) that maximize a given coverage metric (Cov) in a limited testing budget... Find T = {I1, . . . , In } that maximizes \u03a3_{Ii \u2208 T} Cov(Run(DNN, Ii)).\" ; Page 166, Algorithm 1: mutation step uses gradients of selected neurons to increase their outputs but the objective is coverage (lines showing \"I' \u2190 I' + \u03bb \u00b7 \u2202(\u03a3_{n\u2208N} Neuron(n, I'))/\u2202I'\", and the check \"if Cov(O') \u2288 C \u2227 Constraint(I, I') then ...\"). ; Page 172, \u00a74.3 (Adversarial Inputs / Evaluation): \"Table 3 reports the average number of mutations per image (Mutations), the average number of adversarial inputs found per image (Adv. Inputs), the total number of incorrectly classified labels (Labels), and the number of initial inputs (Seeds) from which adversarial inputs were found.\" ; Abstract (page 165): \"Experiments with real-world network models and datasets show that Adapt is remarkably more effective than existing testing techniques in terms of coverage and adversarial inputs found.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight based on observable model internals and statistical failure patterns: it defines and uses 29 neuron features (including activation frequency, layer type and position), analyzes which features/neurons the adaptive strategy favors, and reports correlations between coverage and the variety of adversarial labels. These analyses explain which internal neuron characteristics are associated with effective failure discovery. However, the paper does not directly link discovered failures to higher-level underlying vulnerabilities such as reliance on non-robust or spurious features, dataset bias, or overfitting, so it does not meet the 'High' threshold.",
                    "evidence": "Page 167, Section 3.1 (Table 1 - Neuron features): \"We designed 29 atomic features that describe characteristics of the neurons... The variable features 18-29 describe the properties of neurons, where these features continuously change over the testing procedure.\"; Page 173, Section 4.4 (Learned Insights): \"Figure 4 shows the changes of the weights of the top-3 (red) and bottom-3 (blue) features... As testing goes by, the solid lines were being skewed toward one direction... the learning algorithm extracts the characteristics of strategies, which increase coverage, well.\"; Page 172, Section 4.3 (Correlation between Coverage and Adversarial Inputs): \"From Figure 1 and Table 3, we can notice that coverage and variety of labels are highly correlated, while coverage and the number of adversarial inputs are not correlated strongly.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by analyzing observable model internals and their correlation with the discovery of failures (adversarial inputs). Section 4.4, \"Learned Insights\", is dedicated to analyzing which neuron characteristics (e.g., layer position, connection types, weight values, activation history) are most important for creating effective neuron-selection strategies to find failures. This is a direct analysis of model internals to understand the failure discovery process. Additionally, Section 4.3 analyzes the statistical correlation between coverage metrics and the variety of adversarial inputs found. However, it does not extend this analysis to explain the failures in terms of underlying model vulnerabilities like spurious features or dataset bias, which would be required for a 'High' rating.",
                    "evidence": "Page 11, Section 4.4, Learned Insights: \"f4, which indicates the layers located in the back 25% of the network, is learned as the most important feature that neurons to select should have, and f2, which indicates the layers located relatively front of the network, is included in bottom-3 features in most cases (LeNet-4, LeNet-5, and ResNet-50). This trend indicates that neurons with higher expressive ability, which means that neurons are located in deeper convolutional layers [38], should be selected while testing the deep neural networks.\"\n\nPage 10, Section 4.3, Correlation between Coverage and Adversarial Inputs: \"From Figure 1 and Table 3, we can notice that coverage and variety of labels are highly correlated, while coverage and the number of adversarial inputs are not correlated strongly. In testing VGG-19 with NC, the order of coverage value of each technique exactly matches the order of the number of the labels found by each technique.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight based on observable model internals and statistical failure patterns: it defines and uses 29 neuron features (including activation frequency, layer type and position), analyzes which features/neurons the adaptive strategy favors, and reports correlations between coverage and the variety of adversarial labels. These analyses explain which internal neuron characteristics are associated with effective failure discovery. However, the paper does not directly link discovered failures to higher-level underlying vulnerabilities such as reliance on non-robust or spurious features, dataset bias, or overfitting, so it does not meet the 'High' threshold.",
                    "evidence": "Page 167, Section 3.1 (Table 1 - Neuron features): \"We designed 29 atomic features that describe characteristics of the neurons... The variable features 18-29 describe the properties of neurons, where these features continuously change over the testing procedure.\"; Page 173, Section 4.4 (Learned Insights): \"Figure 4 shows the changes of the weights of the top-3 (red) and bottom-3 (blue) features... As testing goes by, the solid lines were being skewed toward one direction... the learning algorithm extracts the characteristics of strategies, which increase coverage, well.\"; Page 172, Section 4.3 (Correlation between Coverage and Adversarial Inputs): \"From Figure 1 and Table 3, we can notice that coverage and variety of labels are highly correlated, while coverage and the number of adversarial inputs are not correlated strongly.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Correlates neuron features with faults",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility during fuzzing via an explicit distance constraint (Constraint(I,I')) and by tuning mutation parameters (e.g., \u03b71) to limit drift; the authors also report maintaining the L2 distance between original and mutated images (\u22480.05) during experiments and note that some adversarial images were \"visually indistinguishable\" from originals. However, the paper does not provide an explicit empirical assessment of the perceptual or semantic realism of the final fault-inducing inputs using human studies or task-specific quantitative measures (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity). Therefore it enforces plausibility at mutation time and validates it qualitatively, but lacks rigorous quantitative / human-evaluation evidence required for a High rating.",
                    "evidence": "Section 2.3 (Algorithm discussion), p.166: \"For example, Constraint(I, I') is true if L2 distance between I and I' is less than a certain threshold.\"; Section 3.2 (Hyperparameters), p.169: \"The second hyper-parameter \u03b71 is the number of times that the set of selected neurons is used for generating new inputs... We manually set the \u03b71 to 3 so that the distance is not too far.\"; Section 4.1 (Experimental setup), p.170: \"All the techniques, including Adapt, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\"; Section 4.3 (Adversarial Inputs), p.172: \"...the adversarial images found for VGG-19 and ResNet-50 were visually indistinguishable from the original ones.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces a plausibility constraint during input generation and provides a qualitative assessment of the final fault-inducing inputs. A bounded L2-distance constraint is maintained between the initial seed and any mutated input, which helps control the cumulative effect of perturbations. The authors also qualitatively assess the realism of the generated inputs, stating that for the large ImageNet models, the adversarial examples were 'visually indistinguishable from the original ones' and providing visual examples in Figure 3. However, this does not meet the 'High' standard because the plausibility assessment is not a formal human study or based on quantitative perceptual metrics (e.g., LPIPS, SSIM), and the L2-norm is a simple proxy for maintaining inputs within a truly plausible distribution.",
                    "evidence": "Page 7, Section 4.1, Experimental Setup: \"All the techniques, including ADAPT, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\"\n\nPage 10, Section 4.3, Correlation between Coverage and Adversarial Inputs: \"In particular, the adversarial images found for VGG-19 and ResNet-50 were visually indistinguishable from the original ones.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility during fuzzing via an explicit distance constraint (Constraint(I,I')) and by tuning mutation parameters (e.g., \u03b71) to limit drift; the authors also report maintaining the L2 distance between original and mutated images (\u22480.05) during experiments and note that some adversarial images were \"visually indistinguishable\" from originals. However, the paper does not provide an explicit empirical assessment of the perceptual or semantic realism of the final fault-inducing inputs using human studies or task-specific quantitative measures (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity). Therefore it enforces plausibility at mutation time and validates it qualitatively, but lacks rigorous quantitative / human-evaluation evidence required for a High rating.",
                    "evidence": "Section 2.3 (Algorithm discussion), p.166: \"For example, Constraint(I, I') is true if L2 distance between I and I' is less than a certain threshold.\"; Section 3.2 (Hyperparameters), p.169: \"The second hyper-parameter \u03b71 is the number of times that the set of selected neurons is used for generating new inputs... We manually set the \u03b71 to 3 so that the distance is not too far.\"; Section 4.1 (Experimental setup), p.170: \"All the techniques, including Adapt, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\"; Section 4.3 (Adversarial Inputs), p.172: \"...the adversarial images found for VGG-19 and ResNet-50 were visually indistinguishable from the original ones.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Norm-bounded",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's testing method mutates inputs by adding gradients directly to floating\u2011point input tensors and evaluates failures in memory (constraining L2 distance), but it does not mention any handling of serialization effects (clipping, quantization, or encoding) or generating inputs directly in serialized formats. Because the technique relies on fine-grained numeric gradient perturbations (I' = I' + \u03bb * gradient) and there is no explicit treatment to ensure those perturbations survive common I/O operations (e.g., 8-bit image encoding), fault\u2011inducing perturbations could be lost or altered when inputs are written to or read from disk. Thus the paper ignores I/O/serialization effects during mutation and oracle evaluation and scores Low on Failure Reproducibility.",
                    "evidence": "Page 166, Section 2.3 (Algorithm 1): \"I' \u2190 I' + \u03bb \u00b7 \u2202(\u03a3_{n\u2208N} Neuron(n, I'))/\u2202I'\" (shows mutations applied by adding gradients to the input tensor in memory).\nPage 168, Section 3.2 (Algorithm 2): \"I' \u2190 I' + \u03bb \u00b7 \u2202(\u03a3_{n\u2208N} Neuron(n, I'))/\u2202I'\" (the adaptive algorithm likewise generates inputs by in\u2011memory gradient additions).\nPage 170, Section 4.1 (Experimental setup): \"All the techniques, including Adapt, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\" (shows constraint on numeric perturbation magnitude but no mention of serialization/clipping/rounding handling).\nPage 169, Section 4 (Implementation): \"We implemented our technique in a tool, called Adapt, using Python 3.6.3, Tensorflow 1.14.0, and Keras 2.2.4 without any modification of the frameworks.\" (implementation context indicates in\u2011framework tensor operations; nowhere in the paper do the authors describe explicitly accounting for serialization / I/O quantization effects when generating or evaluating test inputs)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a white-box testing method, ADAPT, which generates new test inputs by adding the gradient of selected neurons to an existing input's numerical representation. This process directly manipulates the in-memory floating-point values of the input tensor. The paper specifies that a constraint is applied to the generated inputs, but this constraint is a numerical L2-distance to the original input, which does not account for serialization effects like quantization, rounding, or clipping. The methodology described in Algorithm 2 and the experimental setup does not include any step to save the generated adversarial inputs to a standard file format (e.g., PNG, JPEG) and then reload them to verify if the failure persists. The entire process of mutation and evaluation appears to be performed on in-memory tensors, ignoring potential I/O effects. Consequently, the fault-inducing perturbations are likely fragile numerical artifacts that could be lost or altered by standard serialization.",
                    "evidence": "Page 7, Section 4.1, \"Experimental Setup\": \"All the techniques, including ADAPT, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\" This shows the constraint is purely numerical and does not address I/O effects.\nPage 5, Algorithm 2, Lines 14-16: The algorithm generates a new input `I'` via gradient addition, immediately runs the DNN on it (`O' \u2190 Run(DNN, I')`), and then checks for new coverage (`if Cov(O') ...`). This workflow contains no intermediate step to account for serialization or I/O operations, indicating that evaluation is performed on the raw numerical output of the mutation."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's testing method mutates inputs by adding gradients directly to floating\u2011point input tensors and evaluates failures in memory (constraining L2 distance), but it does not mention any handling of serialization effects (clipping, quantization, or encoding) or generating inputs directly in serialized formats. Because the technique relies on fine-grained numeric gradient perturbations (I' = I' + \u03bb * gradient) and there is no explicit treatment to ensure those perturbations survive common I/O operations (e.g., 8-bit image encoding), fault\u2011inducing perturbations could be lost or altered when inputs are written to or read from disk. Thus the paper ignores I/O/serialization effects during mutation and oracle evaluation and scores Low on Failure Reproducibility.",
                    "evidence": "Page 166, Section 2.3 (Algorithm 1): \"I' \u2190 I' + \u03bb \u00b7 \u2202(\u03a3_{n\u2208N} Neuron(n, I'))/\u2202I'\" (shows mutations applied by adding gradients to the input tensor in memory).\nPage 168, Section 3.2 (Algorithm 2): \"I' \u2190 I' + \u03bb \u00b7 \u2202(\u03a3_{n\u2208N} Neuron(n, I'))/\u2202I'\" (the adaptive algorithm likewise generates inputs by in\u2011memory gradient additions).\nPage 170, Section 4.1 (Experimental setup): \"All the techniques, including Adapt, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\" (shows constraint on numeric perturbation magnitude but no mention of serialization/clipping/rounding handling).\nPage 169, Section 4 (Implementation): \"We implemented our technique in a tool, called Adapt, using Python 3.6.3, Tensorflow 1.14.0, and Keras 2.2.4 without any modification of the frameworks.\" (implementation context indicates in\u2011framework tensor operations; nowhere in the paper do the authors describe explicitly accounting for serialization / I/O quantization effects when generating or evaluating test inputs)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper presents Adapt as a white-box testing technique that operates on a single target DNN (Algorithm 2 takes a single DNN as input) and evaluates it per-model. The experiments report per-model coverage and adversarial inputs but do not test or report whether fault-inducing inputs produced on one model transfer to other models. The authors explicitly note that DeepXplore's differential (multi-DNN) testing was not used in its multi-model mode: \u201cwe used it with a single DNN and labeled data in our experiments.\u201d There is no experiment, analysis, or mechanism described to generate or evaluate transferable (cross-model) fault-inducing inputs or surrogate-model attacks. Therefore the paper does not demonstrate attack transferability across models.",
                    "evidence": "Section 3.2 (Algorithm 2): \"1: procedure Adapt(DNN, I,Cov)\" (Algorithm 2 signature shows Adapt operates on a single DNN) \u2014 (page 168, Section 3.2, Algorithm 2).\n\nSection 4.1 Experimental Setup: \"DeepXplore originally performs differential testing requiring multiple DNNs without labeled data but we used it with a single DNN and labeled data in our experiments.\" (shows the authors evaluated tools in single-model mode and did not perform multi-model/differential transfer experiments) \u2014 (page 170, Section 4.1 Experimental Setup).\n\nOverall evaluation description and results (Sections 4.2\u20134.3) report per-model coverage and adversarial inputs but contain no experiments or discussion on reusing inputs across models or on surrogate-model transferability \u2014 (pages 171\u2013173, Sections 4.2\u20134.3)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's methodology and experimental evaluation focus on generating fault-inducing inputs for a single, white-box model at a time. The proposed method, ADAPT, is designed to optimize its neuron-selection strategy based on the internal structure and feedback from the specific model under test. The experiments are conducted independently for each of the four models (LeNet-4, LeNet-5, VGG-19, ResNet-50). The results report the number of adversarial inputs found for each model individually. There is no procedure or result mentioned where fault-inducing inputs generated for one model (e.g., LeNet-4) are tested against another model performing the same task (e.g., LeNet-5) to assess transferability. The paper even notes that it adapted a baseline technique, DeepXplore, to work with a single DNN, moving away from its original multi-DNN differential testing approach.",
                    "evidence": "Page 7, Section 4.1, \"Experimental Setup\": \"DeepXplore originally performs differential testing requiring multiple DNNs without labeled data but we used it with a single DNN and labeled data in our experiments.\" This quote shows a deliberate choice to focus on single-model testing, not cross-model transferability. Additionally, Table 3 on page 9, \"Effectiveness for finding adversarial inputs,\" presents results for each model in separate, independent sections, with no indication that inputs generated for one model were ever tested on another."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper presents Adapt as a white-box testing technique that operates on a single target DNN (Algorithm 2 takes a single DNN as input) and evaluates it per-model. The experiments report per-model coverage and adversarial inputs but do not test or report whether fault-inducing inputs produced on one model transfer to other models. The authors explicitly note that DeepXplore's differential (multi-DNN) testing was not used in its multi-model mode: \u201cwe used it with a single DNN and labeled data in our experiments.\u201d There is no experiment, analysis, or mechanism described to generate or evaluate transferable (cross-model) fault-inducing inputs or surrogate-model attacks. Therefore the paper does not demonstrate attack transferability across models.",
                    "evidence": "Section 3.2 (Algorithm 2): \"1: procedure Adapt(DNN, I,Cov)\" (Algorithm 2 signature shows Adapt operates on a single DNN) \u2014 (page 168, Section 3.2, Algorithm 2).\n\nSection 4.1 Experimental Setup: \"DeepXplore originally performs differential testing requiring multiple DNNs without labeled data but we used it with a single DNN and labeled data in our experiments.\" (shows the authors evaluated tools in single-model mode and did not perform multi-model/differential transfer experiments) \u2014 (page 170, Section 4.1 Experimental Setup).\n\nOverall evaluation description and results (Sections 4.2\u20134.3) report per-model coverage and adversarial inputs but contain no experiments or discussion on reusing inputs across models or on surrogate-model transferability \u2014 (pages 171\u2013173, Sections 4.2\u20134.3)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Whitebox"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Ben Braiek and Khomh - 2019 - DeepEvolution A Search-Based Testing Approach for Deep Neural Networks": {
        "key": "braiek2019deepevolution",
        "name": "DeepEvolution",
        "year": "2019",
        "citation_count": "53",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "All failures the paper reports are robustness-style errors (misclassifications / 'erroneous behaviors') and divergences between original and quantized models (quantization defects). The authors evaluate DeepEvolution by generating semantically-preserving input mutations that increase neuron coverage and then report misclassified synthetic inputs and difference-inducing inputs for quantized models. The paper does not claim or show that DeepEvolution discovers unsafe/ toxic outputs, policy/safety bypasses, data leakage, jailbreaks, or other security-critical violations of defended or safety-aligned models; therefore the uncovered failures correspond to low-severity robustness errors under the provided metric.",
                    "evidence": "Page 456, Section V.C (RQ2) \u2014 Motivation: \"The objective is to assess the effectiveness of our approach in testing the robustness of the DNN; by finding misclassified synthetic inputs.\" \n\nPage 456, Section V.C (RQ2) \u2014 Findings: \"DeepEvolution can effectively generate test cases that trigger erroneous behaviors of the DNN.\" \n\nPage 457, Section V.D (RQ3) \u2014 Findings: \"DeepEvolution can effectively detect defects introduced during DNN quantization, outperforming the coverage-guided fuzzing tool TensorFuzz.\" \n\nPage 454, Abstract: \"we could successfully find several corner-case behaviors\" and \"DeepEvolution outperformed Tensorfuzz ... in detecting latent defects introduced during the quantization of the models.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, DeepEvolution, focuses on uncovering model robustness errors through metamorphic testing. The evaluation is centered on two types of failures: 1) misclassifications resulting from semantically-preserving input transformations (e.g., changing image brightness or rotation), and 2) prediction inconsistencies between a full-precision model and its quantized version. These failures perfectly align with the metric's definition of 'model robustness errors'. The paper does not mention or evaluate the detection of unsafe behaviors (such as toxicity or bias), nor does it test the bypassing of any explicit safety or security mechanisms. The entire scope of failure detection is confined to robustness and consistency checks.",
                    "evidence": "Page 457, Section V-C, RQ2: Detection of DNN Erroneous Behaviors\n- Motivation: 'The objective is to assess the effectiveness of our approach in testing the robustness of the DNN; by finding misclassified synthetic inputs.'\n- Findings: 'DeepEvolution can effectively generate test cases that trigger erroneous behaviors of the DNN. Table II presents erroneous behaviors detected by each metaheuristic algorithm.' The paper clarifies that these 'erroneous behaviors' are misclassifications.\n\nPage 457, Section V-D, RQ3: DNN Quantization Defects\n- Motivation: 'The goal is to assess the usefulness of Deep-Evolution in finding difference-inducing inputs that expose potential quantization defects.'\n- Findings: 'Table III presents the number of synthetic test data that were able to induce a difference between the DNN\u2019s outcomes (difference-inducing inputs); exposing quantization defects.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "All failures the paper reports are robustness-style errors (misclassifications / 'erroneous behaviors') and divergences between original and quantized models (quantization defects). The authors evaluate DeepEvolution by generating semantically-preserving input mutations that increase neuron coverage and then report misclassified synthetic inputs and difference-inducing inputs for quantized models. The paper does not claim or show that DeepEvolution discovers unsafe/ toxic outputs, policy/safety bypasses, data leakage, jailbreaks, or other security-critical violations of defended or safety-aligned models; therefore the uncovered failures correspond to low-severity robustness errors under the provided metric.",
                    "evidence": "Page 456, Section V.C (RQ2) \u2014 Motivation: \"The objective is to assess the effectiveness of our approach in testing the robustness of the DNN; by finding misclassified synthetic inputs.\" \n\nPage 456, Section V.C (RQ2) \u2014 Findings: \"DeepEvolution can effectively generate test cases that trigger erroneous behaviors of the DNN.\" \n\nPage 457, Section V.D (RQ3) \u2014 Findings: \"DeepEvolution can effectively detect defects introduced during DNN quantization, outperforming the coverage-guided fuzzing tool TensorFuzz.\" \n\nPage 454, Abstract: \"we could successfully find several corner-case behaviors\" and \"DeepEvolution outperformed Tensorfuzz ... in detecting latent defects introduced during the quantization of the models.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "DeepEvolution is driven by coverage-maximizing fitness and metamorphic transformations to increase input diversity and reveal misclassifications or model divergences (e.g., after quantization). The paper repeatedly describes goals such as increasing neuronal coverage, finding misclassified synthetic inputs, and exposing difference-inducing inputs between original and quantized models. There is no description of steering the search toward attacker-chosen specific outputs (e.g., a particular target label or phrase) or an oracle that enforces a predefined target output; instead the method performs untargeted exploration to reveal generic failures.",
                    "evidence": "Abstract (page 454): \"DeepEvolution aims to detect inconsistencies and potential defects in DNN models\"; \"Using a coverage-based fitness function to guide the exploration process; it aims to ensure a maximum diversity in the generated test cases.\" (Abstract, p.454)\n\nIII. DEEPEVOLUTION: TESTING WORKFLOW (page 455): \"From one generation of candidates to another, DeepEvolution performs follow-up tests with the resulting transformed inputs and stores the failed tests that exhibit erroneous DNNs behavior or induce a divergence between the original DNN and its quantized version.\" (Section III, p.455)\n\nIV.B DNN Coverage (page 455): \"We define the following fitness function: Fitness = \u03b1 \u00d7 NLNC + \u03b2 \u00d7 NGNC\" \u2014 the fitness is explicitly coverage-based (local and global neuron coverage), showing the search objective is coverage rather than a specific target output. (Section IV.B, p.455)\n\nV.C RQ2 (page 456): \"The objective is to assess the effectiveness of our approach in testing the robustness of the DNN; by finding misclassified synthetic inputs.\" and \"DeepEvolution can effectively generate test cases that trigger erroneous behaviors of the DNN.\" (Section V, RQ2, p.456)\n\nV.D RQ3 (page 456\u2013457): The evaluation compares the number of \"difference-inducing inputs\" that expose quantization defects vs. TensorFuzz, again showing the goal is to find generic divergences rather than attacker-specified outputs. (Section V.D, p.456-457)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, DeepEvolution, performs untargeted exploration to find generic failures. The system's exploration is driven by a fitness function designed to maximize neuron coverage, which encourages broad exploration of the model's behavior rather than steering it towards a specific, predefined outcome. The types of failures it aims to discover are explicitly stated as general misclassifications and inconsistencies (divergences) between a model and its quantized version. The evaluation focuses on counting the number of any erroneous behaviors (misclassifications) and quantization defects found, without any objective of reaching a specific target label or behavior. This aligns with the definition of untargeted exploration for discovering generic failures.",
                    "evidence": "Page 456, Section V.B: 'We define the following fitness function: Fitness = a \u00d7 NLNC + \u03b2 \u00d7 NGNC'. This fitness function is based on maximizing local and global neuron coverage, an untargeted exploration strategy.\n\nPage 457, Section V.C (RQ2 Motivation): 'The objective is to assess the effectiveness of our approach in testing the robustness of the DNN; by finding misclassified synthetic inputs.' This identifies the goal as finding any misclassification, which is a generic failure.\n\nPage 457, Section V.D (RQ3 Motivation): 'The goal is to assess the usefulness of Deep-Evolution in finding difference-inducing inputs that expose potential quantization defects.' This refers to discovering any inconsistency between model versions, another form of generic, untargeted fault discovery."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DeepEvolution is driven by coverage-maximizing fitness and metamorphic transformations to increase input diversity and reveal misclassifications or model divergences (e.g., after quantization). The paper repeatedly describes goals such as increasing neuronal coverage, finding misclassified synthetic inputs, and exposing difference-inducing inputs between original and quantized models. There is no description of steering the search toward attacker-chosen specific outputs (e.g., a particular target label or phrase) or an oracle that enforces a predefined target output; instead the method performs untargeted exploration to reveal generic failures.",
                    "evidence": "Abstract (page 454): \"DeepEvolution aims to detect inconsistencies and potential defects in DNN models\"; \"Using a coverage-based fitness function to guide the exploration process; it aims to ensure a maximum diversity in the generated test cases.\" (Abstract, p.454)\n\nIII. DEEPEVOLUTION: TESTING WORKFLOW (page 455): \"From one generation of candidates to another, DeepEvolution performs follow-up tests with the resulting transformed inputs and stores the failed tests that exhibit erroneous DNNs behavior or induce a divergence between the original DNN and its quantized version.\" (Section III, p.455)\n\nIV.B DNN Coverage (page 455): \"We define the following fitness function: Fitness = \u03b1 \u00d7 NLNC + \u03b2 \u00d7 NGNC\" \u2014 the fitness is explicitly coverage-based (local and global neuron coverage), showing the search objective is coverage rather than a specific target output. (Section IV.B, p.455)\n\nV.C RQ2 (page 456): \"The objective is to assess the effectiveness of our approach in testing the robustness of the DNN; by finding misclassified synthetic inputs.\" and \"DeepEvolution can effectively generate test cases that trigger erroneous behaviors of the DNN.\" (Section V, RQ2, p.456)\n\nV.D RQ3 (page 456\u2013457): The evaluation compares the number of \"difference-inducing inputs\" that expose quantization defects vs. TensorFuzz, again showing the goal is to find generic divergences rather than attacker-specified outputs. (Section V.D, p.456-457)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight based on observable model internals: it defines and uses neuron-coverage measures (local and global) and a coverage-based fitness function, reports how generated inputs increase these coverage measures (Table I), and links increases in coverage to the discovery of erroneous behaviors (Section V, RQ1\u2013RQ2). However, it does not perform deeper diagnostic analyses that explain why failures occur in terms of underlying model vulnerabilities (e.g., reliance on spurious/non-robust features, dataset bias, or overfitting), nor does it analyze neuron characteristics beyond coverage (activation frequency, specific layer/position contributions) or present class-level/statistical failure pattern analyses that would support a \u201cHigh\u201d rating.",
                    "evidence": "Section IV.B (page 455) defines the internals-based signals and fitness: \"We adapt the Neuron Coverage (NC) metric proposed by Pei et al. [9] to capture two levels of coverage (i.e., local and global) for each test input... Local neurons coverage (NLNC)... Global neurons coverage (NGNC)... We define the following fitness function: Fitness = \u03b1 \u00d7 NLNC + \u03b2 \u00d7 NGNC (1)\". Section V.B (page 456) reports coverage gains: \"DeepEvolution significantly boosts the neuronal coverage. Table I shows the final neuronal coverage ratio achieved by each implemented swarm-based metaheuristic.\" Section V.C (pages 456-457) links coverage to failures: \"DeepEvolution can effectively generate test cases that trigger erroneous behaviors of the DNN... This result suggests that DeepEvolution is capable of obtaining adversarial inputs for each original input and that the local coverage level integrated in the fitness function plays an important role in assessing how much the DNN\u2019s state of the transformed input is different from the state that resulted from the original input.\" (Section titles: IV.B \"DNN Coverage\", V.B \"RQ1: DNN Coverage Increase\", V.C \"RQ2: Detection of DNN Erroneous Behaviors\")"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight into the discovered failures by correlating their occurrence with an observable model internal: neuronal coverage. The entire approach is premised on the idea that increasing neuronal coverage will lead to the discovery of more erroneous behaviors. The evaluation section explicitly analyzes this connection, stating that generating inputs geared towards improving coverage triggers more internal states of the DNN, which leads to a higher chance of detecting defects. This constitutes an analysis of failure patterns through their correlation with a model internal metric, which fits the 'Medium' criteria. The paper does not, however, extend this analysis to explain the failures in terms of underlying model vulnerabilities like non-robust features or dataset bias, which would be required for a 'High' rating.",
                    "evidence": "Page 457, Section V-C, 'RQ2: Detection of DNN Erroneous Behaviors': '...it indicates that generating synthetic test inputs towards improving the neuronal coverage could trigger more states of a DNN, incurring higher chances of defect detection, which is consistent with the practical purpose of testing criteria used in traditional software testing. [...] This result suggests that DeepEvolution is capable of obtaining adversarial inputs for each original input and that the local coverage level integrated in the fitness function plays an important role in assessing how much the DNN\u2019s state of the transformed input is different from the state that resulted from the original input.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight based on observable model internals: it defines and uses neuron-coverage measures (local and global) and a coverage-based fitness function, reports how generated inputs increase these coverage measures (Table I), and links increases in coverage to the discovery of erroneous behaviors (Section V, RQ1\u2013RQ2). However, it does not perform deeper diagnostic analyses that explain why failures occur in terms of underlying model vulnerabilities (e.g., reliance on spurious/non-robust features, dataset bias, or overfitting), nor does it analyze neuron characteristics beyond coverage (activation frequency, specific layer/position contributions) or present class-level/statistical failure pattern analyses that would support a \u201cHigh\u201d rating.",
                    "evidence": "Section IV.B (page 455) defines the internals-based signals and fitness: \"We adapt the Neuron Coverage (NC) metric proposed by Pei et al. [9] to capture two levels of coverage (i.e., local and global) for each test input... Local neurons coverage (NLNC)... Global neurons coverage (NGNC)... We define the following fitness function: Fitness = \u03b1 \u00d7 NLNC + \u03b2 \u00d7 NGNC (1)\". Section V.B (page 456) reports coverage gains: \"DeepEvolution significantly boosts the neuronal coverage. Table I shows the final neuronal coverage ratio achieved by each implemented swarm-based metaheuristic.\" Section V.C (pages 456-457) links coverage to failures: \"DeepEvolution can effectively generate test cases that trigger erroneous behaviors of the DNN... This result suggests that DeepEvolution is capable of obtaining adversarial inputs for each original input and that the local coverage level integrated in the fitness function plays an important role in assessing how much the DNN\u2019s state of the transformed input is different from the state that resulted from the original input.\" (Section titles: IV.B \"DNN Coverage\", V.B \"RQ1: DNN Coverage Increase\", V.C \"RQ2: Detection of DNN Erroneous Behaviors\")"
                },
                "manual": {
                    "value": "Low",
                    "why": "Uses neuron coveage as heuristic guide, does not show correlation between coverage and neurons",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper enforces plausibility constraints both in the mutation design and in candidate selection: transformation parameters are manually tuned to valid domain intervals (defining a constrained sub-space of exploration), the compound transformation limits affine changes (only one affine transform) to avoid unrealistic images, and mutated inputs with low perceptual similarity are rejected using SSIM. The authors also report an empirical plausibility check: they sampled generated images (95% confidence, 5% error) and verified them manually. These points show plausibility is enforced throughout the search and that final generated inputs were assessed quantitatively (SSIM) and via manual inspection.",
                    "evidence": "Section IV.A (Metamorphic Transformation), p.455: \"To infer the valid domain interval of each transformation\u2019s parameters, we manually tune them to set up the appropriate range of values, i.e., high and low boundaries, that preserves the input semantics before and after each transformation... To verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8]... Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.\"; Section IV.C (Swarm-based Metaheuristics), p.456: \"To ensure semantically preserving transformations, we use the valid domain intervals of transformations that we have already tuned manually to create the high and low boundaries vectors, defining the sub-space of exploration.\"; Section VI (Threats to Validity), p.457: \"The manual tuning of metamorphic transformations\u2019 domain and the threshold of SSIM could affect the validity of our results. To mitigate this threat, we selected a sample of our generated images using a confidence level of 95% and an error margin of 5%, and verified them manually. We found them to be correct.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, DeepEvolution, explicitly incorporates mechanisms to enforce input plausibility throughout the test generation process and provides empirical evidence that the resulting inputs are realistic. Plausibility is enforced in two ways: 1) by manually tuning the parameter ranges for metamorphic transformations to ensure they are 'semantically-preserving', and 2) by using a quantitative perceptual quality metric, the Structural Similarity Index (SSIM), to filter and reject any generated inputs that fall below a tuned similarity threshold. This use of SSIM directly addresses the metric's criteria for using task-specific quantitative measures. Furthermore, the paper provides empirical evidence of the final inputs' plausibility by stating that the authors manually verified a statistically significant sample of the generated images and confirmed their correctness.",
                    "evidence": "Page 456, Section IV.A, 'To verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8] which assesses the similarity between two images based on the visual impact of three characteristics: luminance, contrast, and structure... Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.'\n\nPage 458, Section VI. Threats to Validity, 'The manual tuning of metamorphic transformations\u2019 domain and the threshold of SSIM could affect the validity of our results. To mitigate this threat, we selected a sample of our generated images using a confidence level of 95% and an error margin of 5%, and verified them manually. We found them to be correct.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper enforces plausibility constraints both in the mutation design and in candidate selection: transformation parameters are manually tuned to valid domain intervals (defining a constrained sub-space of exploration), the compound transformation limits affine changes (only one affine transform) to avoid unrealistic images, and mutated inputs with low perceptual similarity are rejected using SSIM. The authors also report an empirical plausibility check: they sampled generated images (95% confidence, 5% error) and verified them manually. These points show plausibility is enforced throughout the search and that final generated inputs were assessed quantitatively (SSIM) and via manual inspection.",
                    "evidence": "Section IV.A (Metamorphic Transformation), p.455: \"To infer the valid domain interval of each transformation\u2019s parameters, we manually tune them to set up the appropriate range of values, i.e., high and low boundaries, that preserves the input semantics before and after each transformation... To verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8]... Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.\"; Section IV.C (Swarm-based Metaheuristics), p.456: \"To ensure semantically preserving transformations, we use the valid domain intervals of transformations that we have already tuned manually to create the high and low boundaries vectors, defining the sub-space of exploration.\"; Section VI (Threats to Validity), p.457: \"The manual tuning of metamorphic transformations\u2019 domain and the threshold of SSIM could affect the validity of our results. To mitigate this threat, we selected a sample of our generated images using a confidence level of 95% and an error margin of 5%, and verified them manually. We found them to be correct.\""
                },
                "manual": {
                    "value": "High",
                    "why": "Enforces naturalness across iterations by computing SSIM after each mutation and rejecting any visually distorted image.",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper applies metamorphic transformations to in-memory floating-point image tensors and enforces semantic equivalence via manually tuned parameter domains and an SSIM threshold, but it does not account for standard I/O serialization effects (e.g., 8-bit quantization, rounding, clipping, or image encoding) when generating or validating test inputs. The only quantization discussed in the paper concerns model weight quantization (32-bit to 16-bit) used for a differential oracle, not input serialization. Because the method neither constrains mutations to survive common serialization nor evaluates reproducibility after saving/encoding, fault-inducing perturbations could be lost after standard I/O operations; this matches the metric definition for \"Low.\"",
                    "evidence": "Page 455, IV.A Metamorphic Transformation: \"To infer the valid domain interval of each transformation\u2019s parameters, we manually tune them to set up the appropriate range of values, i.e., high and low boundaries, that preserves the input semantics before and after each transformation, with respect to the data distribution.\" \n\nPage 455, IV.A Metamorphic Transformation: \"To verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8] ... Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.\" \n\nPage 457, V.D RQ3: \"TensorFuzz [5] performs a coverage-guided fuzzing process to generate mutated inputs that are able to expose disagreements between a DNN trained on MNIST (that is 32-bit floating point precision) and its quantized versions where all weights are truncated to 16-bit floating points.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's proposed method, DeepEvolution, relies on applying a set of semantic-preserving metamorphic transformations to generate new test inputs. These transformations include pixel-value changes (contrast, brightness, blur) and affine transformations (rotation, scaling). The metric definition explicitly states that a method that 'applies semantic-preserving metamorphic transformations without explicit enforcement of reproducibility under serialization' should be rated as 'Medium'. While the paper ensures that the transformed inputs are semantically similar to the originals using the Structural Similarity Index (SSIM), it does not describe any mechanism to explicitly account for I/O effects such as rounding or clipping that occur when saving an in-memory floating-point representation of an image to a standard integer-based file format. The transformations are defined over continuous parameter ranges, and there is no discussion of whether the precise numerical perturbations that might trigger a failure would persist after serialization.",
                    "evidence": "Page 455, Section IV-A, 'Metamorphic Transformation': 'First, we gather a list of parametric image-based transformations that can be organised in two groups: 1) Pixel-value transformations: change image contrast, image brightness, image blur, image sharpness and random perturbations within a limited interval. 2) Affine transformations: image translation, image scaling, image shearing, and image rotation.' and 'To verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8] which assesses the similarity between two images based on the visual impact of three characteristics: luminance, contrast, and structure.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "DeepEvolution relies exclusively on in-memory, semantic-preserving metamorphic transformations (pixel-value and affine changes) and filters by SSIM to ensure semantic equivalence, but never addresses or constrains the numerical perturbations with respect to standard I/O serialization (e.g. 8-bit image quantization, rounding, clipping or encoding). According to the metric, methods that apply metamorphic transformations without explicit handling of serialization effects should be rated \u2018Medium\u2019.",
                    "evidence": "Section IV.A (p. 455): \u2018\u2018First, we gather a list of parametric image-based transformations\u2026 pixel-value transformations\u2026 affine transformations\u2026 To verify that generated inputs remain semantically equivalent\u2026 we compute a Structural Similarity Index (SSIM)\u2026\u2019\u2019  Nowhere does the paper mention applying or evaluating inputs after standard image serialization (rounding, quantization, clipping or encoding)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Semantic-level pixel transformations (contrast, blur, rotation, etc.) but no clipping/rounding to ensure IO persistence",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly reuses fault-inducing inputs generated by DeepEvolution to detect divergences between an original model and its quantized version (i.e., it reuses generated inputs to test another model variant), but the fuzzing design is not presented as being explicitly aimed at discovering transferable attacks across independently implemented/trained models. The experiments focus on per-dataset single-model instantiations (LeNet for MNIST, CifarNet for CIFAR-10) and compare original vs. quantized versions; they do not evaluate reuse of generated faults across different model implementations trained for the same task nor claim mechanisms specifically for crafting transferable substitute-model attacks.",
                    "evidence": "Section III (page 455): \"From one generation of candidates to another, DeepEvolution performs follow-up tests with the resulting transformed inputs and stores the failed tests that exhibit erroneous DNNs behavior or induce a divergence between the original DNN and its quantized version.\" \n\nSection V.D (page 456-457) Findings: \"DeepEvolution can effectively detect defects introduced during DNN quantization, outperforming the coverage-guided fuzzing tool TensorFuzz.\" (Table III reports number of \"difference-inducing inputs\" found for quantization defects.)\n\nExperiment setup (page 456): \"For each dataset (i.e., MNIST and CIFAR-10), we took, respectively, the official open-source implementation of Tensorflow models, LeNet [3] and CifarNet [4] to allow the reproducibility of our results and comparisons with our approach.\" \n\nRelated work contrast (page 457, Section VII): Paper cites DeepXplore's use of \"multiple DNNs\u2019 implementations\" for differential testing but does not describe performing similar cross-model reuse in DeepEvolution."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's methodology and evaluation focus on generating test cases to find defects within a single Deep Neural Network (DNN) model or between a model and its own quantized version. The experiments are conducted on two distinct models, LeNet and CifarNet, but they are used for different datasets (MNIST and CIFAR-10, respectively) and are treated as separate, independent evaluation subjects. The paper does not describe any experiment or methodology where fault-inducing inputs generated for one model are subsequently used to test a different model performing the same task. Therefore, the concept of attack transferability is not assessed.",
                    "evidence": "Page 456, Section V, EMPIRICAL EVALUATION: The research questions guiding the evaluation are: \"RQ1: How much can DeepEvolution increase the coverage of generated test cases? RQ2: Can DeepEvolution detect diverse erroneous behaviors in DNN models? RQ3: Can DeepEvolution detect divergences induced by DNN quantization?\". All three questions are focused on the behavior of a single model under test. \n\nPage 456, Section V.A, Experiment Setup: \"DNNs. For each dataset (i.e., MNIST and CIFAR-10), we took, respectively, the official open-source implementation of Tensorflow models, LeNet [3] and CifarNet [4] to allow the reproducibility of our results and comparisons with our approach.\" This shows that the experiments test one specific model per dataset, rather than testing multiple models on a single dataset to check for transferability.\n\nPage 457, Section V.D, RQ3: DNN Quantization Defects: The evaluation for quantization defects involves comparing a model to its own modified version, not to a different model. \"TensorFuzz [5] performs a coverage-guided fuzzing process to generate mutated inputs that are able to expose disagreements between a DNN trained on MNIST (that is 32-bit floating point precision) and its quantized versions...\" This further illustrates that the testing scope is limited to a single model architecture at a time."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DeepEvolution does not demonstrate attack transferability across independently trained model implementations on the same task. The paper only reuses generated fault\u2010inducing inputs to compare a model with its own quantized variant, not to test different models (e.g., using LeNet inputs on another MNIST model or CifarNet inputs on another CIFAR-10 model). There is no experimental evaluation or design mechanism aimed at assessing whether faults transfer across distinct model architectures trained for the same task.",
                    "evidence": "Section III (page 455) describes follow-up tests only between the original DNN and its quantized version. Section V.D (pages 456\u2013457) evaluates quantization defects by comparing each model to its own truncated-precision variant; no experiments reuse inputs across different model implementations performing the same task."
                },
                "manual": {
                    "value": "Low",
                    "why": "Use quantized model for differntial testing based oracle, not for checking fauilure transferability",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic",
                "Differential"
            ]
        }
    },
    "Ma et al. - 2021 - HDTest Differential Fuzz Testing of Brain-Inspired Hyperdimensional Computing": {
        "key": "ma2021hdtest",
        "name": "HDTest",
        "year": "2021",
        "citation_count": "39",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "All failures the paper reports are classic model robustness errors (adversarially-induced mispredictions on MNIST digits). The method generates adversarial inputs that cause incorrect class predictions and evaluates distance/iteration metrics; it does not demonstrate uncovering unsafe, biased, toxic, data\u2011leaking, or security\u2011critical behaviors, nor does it bypass explicit safety, policy, or defense mechanisms in safety\u2011aligned or defended models. The paper frames results as misclassification/adversarial examples and uses retraining to improve robustness, which is consistent with discovering low\u2011severity robustness faults rather than high/medium severity safety or security violations.",
                    "evidence": "Abstract (p.391, Abstract): \"Experimental results on MNIST dataset show that HDTest can generate thousands of adversarial inputs with invisible perturbations that can successfully fool HDC models. On average, HDTest can generate 400 adversarial images within one minute running on commodity computer.\"; Section IV - HDTest Framework (Alg. 1) (p.393, Algorithm 1): comment \"* Indicating successful mis-predcited image generated. */\" (lines 8-11) showing the detection criterion is label disagreement (misprediction); Section V.A - Experimental Setup (p.394, Experimental Setup): \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"; Section V.D - Case Study on Defense Against Adversarial Attacks (p.395, Case Study): \"We run HDTest to generate 1000 adversarial images... after retraining, the rate of successful attack rate drops more than 20%.\" (shows focus on robustness improvement rather than bypassing safety or extracting sensitive information)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, HDTest, is designed and evaluated exclusively for its ability to uncover model robustness errors. Specifically, it focuses on generating adversarial inputs through minor perturbations that cause the Hyperdimensional Computing (HDC) model to produce incorrect classifications (mispredictions). The goal, as stated and demonstrated, is to \"fool HDC models\" and expose these misclassifications. The experiments are conducted on the MNIST handwritten digit dataset, where a failure is defined as predicting the wrong digit. The paper does not explore or uncover any unsafe behaviors (e.g., toxicity, bias) or attempt to bypass any explicit safety, security, or policy mechanisms. The entire scope is confined to prediction inconsistencies under input mutations.",
                    "evidence": "Page 1, Abstract: \"Using handwritten digit classification as an example, we show that HDTest can generate thousands of adversarial inputs with negligible perturbations that can successfully fool HDC models.\"\n\nPage 1, Introduction, Figure 1 Caption: \"An example of adversarial image of HDC by mutating some pixels in the image:(a) the original image as '8'; (b) the pixels mutated; (c) the mutated image wrongly predicted as '3'.\"\n\nPage 4, Section IV, Figure 3: The diagram illustrating the HDTest framework contains a box that explicitly states the \"Objective: Generate mis-predicted images\"."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "All failures the paper reports are classic model robustness errors (adversarially-induced mispredictions on MNIST digits). The method generates adversarial inputs that cause incorrect class predictions and evaluates distance/iteration metrics; it does not demonstrate uncovering unsafe, biased, toxic, data\u2011leaking, or security\u2011critical behaviors, nor does it bypass explicit safety, policy, or defense mechanisms in safety\u2011aligned or defended models. The paper frames results as misclassification/adversarial examples and uses retraining to improve robustness, which is consistent with discovering low\u2011severity robustness faults rather than high/medium severity safety or security violations.",
                    "evidence": "Abstract (p.391, Abstract): \"Experimental results on MNIST dataset show that HDTest can generate thousands of adversarial inputs with invisible perturbations that can successfully fool HDC models. On average, HDTest can generate 400 adversarial images within one minute running on commodity computer.\"; Section IV - HDTest Framework (Alg. 1) (p.393, Algorithm 1): comment \"* Indicating successful mis-predcited image generated. */\" (lines 8-11) showing the detection criterion is label disagreement (misprediction); Section V.A - Experimental Setup (p.394, Experimental Setup): \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"; Section V.D - Case Study on Defense Against Adversarial Attacks (p.395, Case Study): \"We run HDTest to generate 1000 adversarial images... after retraining, the rate of successful attack rate drops more than 20%.\" (shows focus on robustness improvement rather than bypassing safety or extracting sensitive information)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's HDTest design seeks to cause any change in the model's predicted label relative to the original input (i.e., produce a mis-prediction), rather than steering the fuzzer toward a specific attacker-chosen target output or class. HDTest obtains the original predicted label as a reference and judges success when a mutated seed's prediction differs from that reference. Its fitness metric reduces similarity to the original class vector (to encourage misclassification), and the framework explicitly states it works without knowing the true label. These design choices indicate untargeted exploration that discovers generic misclassifications rather than targeted attacks.",
                    "evidence": "IV. HDTest Framework (Alg. 1 & description), p.393: \"HDTest takes the original input image t without necessarily knowing the label of it.\"; Algorithm 1 (lines quoted in paper): \"y = HDC(t)\" and \"if y' in y_seeds != y then /* Indicating successful mis-predcited image generated. */\"; Fig.3 (HDTest overview), p.393: diagram label \"Objective: Generate mis-predicted images\"; IV. HDTest Framework (fitness definition), p.393: \"fitness = 1 - Cosim(AM[y], HDC(seed)), where AM[y] is the reference vector in the associative memory and HDC(seed) is the query HV of the seed.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper describes HDTest as a tool based on differential fuzz testing. The method's objective is to generate inputs that cause a different prediction compared to the original, unmutated input. The success condition for finding an adversarial input is explicitly defined as a discrepancy between the prediction for the original input and the prediction for the mutated input. The fuzzer does not attempt to steer the model towards a specific, attacker-chosen output label. Instead, it performs an untargeted search for any misclassification, which corresponds to the definition of untargeted exploration for generic failures.",
                    "evidence": "Page 3, Section IV, HDTest Framework: \"HDTest takes the original input image t without necessarily knowing the label of it. HDTest then applies mutation algorithms on the original input t to generate new input t'. Both the generated input and the original input are then sent to the HDC classifier for prediction. We then check if the two predicted labels are different, and if yes, this indicates a successful generation of an adversarial input.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's HDTest design seeks to cause any change in the model's predicted label relative to the original input (i.e., produce a mis-prediction), rather than steering the fuzzer toward a specific attacker-chosen target output or class. HDTest obtains the original predicted label as a reference and judges success when a mutated seed's prediction differs from that reference. Its fitness metric reduces similarity to the original class vector (to encourage misclassification), and the framework explicitly states it works without knowing the true label. These design choices indicate untargeted exploration that discovers generic misclassifications rather than targeted attacks.",
                    "evidence": "IV. HDTest Framework (Alg. 1 & description), p.393: \"HDTest takes the original input image t without necessarily knowing the label of it.\"; Algorithm 1 (lines quoted in paper): \"y = HDC(t)\" and \"if y' in y_seeds != y then /* Indicating successful mis-predcited image generated. */\"; Fig.3 (HDTest overview), p.393: diagram label \"Objective: Generate mis-predicted images\"; IV. HDTest Framework (fitness definition), p.393: \"fitness = 1 - Cosim(AM[y], HDC(seed)), where AM[y] is the reference vector in the associative memory and HDC(seed) is the query HV of the seed.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "Finds generic misclassifications",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight based on observable model internals (hypervector similarity/distance) and statistical failure patterns (per-class L1/L2 distances and average fuzzing iterations). HDTest explicitly uses cosine-similarity between query and class hypervectors to guide fuzzing and reports per-class statistics and sample-level \"vulnerable cases.\" However, the paper does not offer deeper causal analysis that links failures to underlying model vulnerabilities such as reliance on non-robust or spurious features, dataset bias, or overfitting, so it does not meet the \u2018High\u2019 level.",
                    "evidence": "Section IV (HDTest Framework), p.393: \"The fitness of seeds are defined as: fitness = 1 - Cosim(AM[y], HDC(seed)), where AM[y] is the reference vector in the associative memory and HDC(seed) is the query HV of the seed, encoded by the HDC model.\"; Section V.B (Mutation Strategies Analysis), p.394: \"Such vulnerable cases bring potential security loopholes since they can result in incorrect behaviors of HDC models by only minor and even negligible perturbations. Therefore, such images should be emphasized when defending attacks to the HDC systems, and HDTest is able to pinpoint and highlight them.\"; Section V.C (Per-class Analysis), p.395: \"We then perform a per-class analysis on L1/L2 distance and fuzzing iterations as shown in Fig. 7. We can observe that for some classes such as '1', the average fuzzing iteration is drastically higher ... For other classes such as '9', the average fuzzing iteration is lower.\"; Section V.C (Per-class Analysis), p.395: \"Moreover, we do not observe apparent relation between fuzzing iteration and distance metrics.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by conducting a statistical analysis of failure patterns. Specifically, in the 'Per-class Analysis' section, the authors examine the difficulty of generating adversarial examples (failures) for each digit class in the MNIST dataset. They observe and attempt to explain correlations between the input data's characteristics (visual similarity between digits) and the fuzzing effort required to induce a failure. For example, they note that the digit '1' is harder to generate adversarial examples for, attributing this to its visual dissimilarity from other digits. Conversely, they find it easier for the digit '9' and hypothesize this is due to its visual similarity to '8' and '3'. This analysis of class-level error concentration and its correlation with input structure qualifies for a 'Medium' rating. The analysis does not, however, extend to explaining why these failures occur by linking them to underlying model vulnerabilities like non-robust features or overfitting, which would be required for a 'High' rating.",
                    "evidence": "Page 5, Section V. EXPERIMENTAL RESULTS, C. Per-class Analysis: 'We then perform a per-class analysis on L1/L2 distance and fuzzing iterations as shown in Fig. 7. We can observe that for some classes such as \u201c1\u201d, the average fuzzing iteration is drastically higher, which means that it is relatively difficult to generate adversarial examples. For other classes such as \u201c9\u201d, the average fuzzing iteration is lower. This is reasonable because all the other digits except for \u201c7\u201d are visually dissimilar from \u201c1\u201d while \u201c9\u201d has quite a few similarities such as \u201c8\u201d and \u201c3\u201d.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight based on observable model internals (hypervector similarity/distance) and statistical failure patterns (per-class L1/L2 distances and average fuzzing iterations). HDTest explicitly uses cosine-similarity between query and class hypervectors to guide fuzzing and reports per-class statistics and sample-level \"vulnerable cases.\" However, the paper does not offer deeper causal analysis that links failures to underlying model vulnerabilities such as reliance on non-robust or spurious features, dataset bias, or overfitting, so it does not meet the \u2018High\u2019 level.",
                    "evidence": "Section IV (HDTest Framework), p.393: \"The fitness of seeds are defined as: fitness = 1 - Cosim(AM[y], HDC(seed)), where AM[y] is the reference vector in the associative memory and HDC(seed) is the query HV of the seed, encoded by the HDC model.\"; Section V.B (Mutation Strategies Analysis), p.394: \"Such vulnerable cases bring potential security loopholes since they can result in incorrect behaviors of HDC models by only minor and even negligible perturbations. Therefore, such images should be emphasized when defending attacks to the HDC systems, and HDTest is able to pinpoint and highlight them.\"; Section V.C (Per-class Analysis), p.395: \"We then perform a per-class analysis on L1/L2 distance and fuzzing iterations as shown in Fig. 7. We can observe that for some classes such as '1', the average fuzzing iteration is drastically higher ... For other classes such as '9', the average fuzzing iteration is lower.\"; Section V.C (Per-class Analysis), p.395: \"Moreover, we do not observe apparent relation between fuzzing iteration and distance metrics.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Correlates difficulty of per-class failure generation with input characterstics",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints during fuzzing (they set an explicit distance threshold to keep perturbations \u201cinvisible\u201d and discard candidates beyond it) and reports quantitative distance metrics (normalized L1/L2) and sample images for the final adversarial inputs. However, the paper does not perform stronger perceptual/semantic evaluations (no human study, no task-specific perceptual metrics such as LPIPS/SSIM) nor does it explicitly analyze cumulative drift across multiple mutation iterations beyond the single distance threshold. This matches the \"Medium\" description: plausibility is enforced/validated (bounded constraints and final-distance reporting) but cumulative-effects and perceptual validation are not thoroughly addressed.",
                    "evidence": "Section IV. HDTest Framework (page ~393-394): \"To ensure the added perturbations are within an 'invisible' range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.\" \n\nSection V.A Experimental Setup (page ~394): \"We use different metrics to evaluate HDTest: normalized L1 distance and normalized L2 distance between the mutated image and original image, the average fuzzing iterations and the execution time to successfully generate 1000 adversarial images.\" \n\nSection V.B Mutation Strategies Analysis (page ~394-395): \"We provide sample original images, mutated pixels, and generated images of HDTest in Fig. 4\u20136, under gauss, rand, and shift respectively.\" \n\n(Absence of stronger perceptual/semantic evaluation): Nowhere in the paper do the authors report human evaluation or task-specific perceptual metrics (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity) for the final fault-inducing inputs; their evaluation is limited to L1/L2 distances, iteration counts, runtime, and example figures."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces a form of input plausibility by using a bounded constraint on the perturbations. It explicitly sets an L2 distance threshold and discards any generated images that exceed this limit, with the stated goal of keeping perturbations within an \"invisible\" range. This method constitutes a rule-based constraint on mutations. Furthermore, the paper validates the plausibility of the final generated adversarial inputs by measuring and reporting the normalized L1 and L2 distances from the original inputs, stating that smaller distances are preferred. However, it does not meet the criteria for a \"High\" rating because it does not provide more rigorous empirical evidence of perceptual realism, such as through human studies or by using dedicated perceptual similarity metrics like SSIM or LPIPS. The use of L1/L2 distance serves as a basic validation, fitting the \"Medium\" definition.",
                    "evidence": "Page 4, Section IV, Paragraph 4: 'To ensure the added perturbations are within an \"invisible\" range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.'\n\nPage 4, Section V.A, Paragraph 1: 'We use different metrics to evaluate HDTest: normalized L1 distance and normalized L2 distance between the mutated image and original image, the average fuzzing iterations and the execution time to successfully generate 1000 adversarial images. A smaller distance between the generated image and the original image is preferred by HDTest because it means less perturbation added to the image.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints during fuzzing (they set an explicit distance threshold to keep perturbations \u201cinvisible\u201d and discard candidates beyond it) and reports quantitative distance metrics (normalized L1/L2) and sample images for the final adversarial inputs. However, the paper does not perform stronger perceptual/semantic evaluations (no human study, no task-specific perceptual metrics such as LPIPS/SSIM) nor does it explicitly analyze cumulative drift across multiple mutation iterations beyond the single distance threshold. This matches the \"Medium\" description: plausibility is enforced/validated (bounded constraints and final-distance reporting) but cumulative-effects and perceptual validation are not thoroughly addressed.",
                    "evidence": "Section IV. HDTest Framework (page ~393-394): \"To ensure the added perturbations are within an 'invisible' range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.\" \n\nSection V.A Experimental Setup (page ~394): \"We use different metrics to evaluate HDTest: normalized L1 distance and normalized L2 distance between the mutated image and original image, the average fuzzing iterations and the execution time to successfully generate 1000 adversarial images.\" \n\nSection V.B Mutation Strategies Analysis (page ~394-395): \"We provide sample original images, mutated pixels, and generated images of HDTest in Fig. 4\u20136, under gauss, rand, and shift respectively.\" \n\n(Absence of stronger perceptual/semantic evaluation): Nowhere in the paper do the authors report human evaluation or task-specific perceptual metrics (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity) for the final fault-inducing inputs; their evaluation is limited to L1/L2 distances, iteration counts, runtime, and example figures."
                },
                "manual": {
                    "value": "Medium",
                    "why": "norm-bounded",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's fuzzing/mutation design applies fine-grained numerical perturbations (e.g., Gaussian and random noise) and evaluates adversariality entirely by sending in-memory mutated inputs to the HDC classifier, but it does not account for standard I/O serialization effects (quantization, rounding, clipping) nor constrain mutations to guarantee they survive serialization. The authors explicitly describe adding gaussian/random noise and using an L2-distance threshold to keep perturbations 'invisible', and they evaluate success by comparing predictions on the original and mutated inputs in memory. Nowhere do they describe converting/saving mutated inputs to common serialized formats (e.g., 8-bit images) or handling clipping/rounding so that failures remain reproducible after standard I/O operations. Because I/O/serialization effects are ignored, fault-inducing perturbations could be lost or altered when inputs are serialized, so the paper meets the 'Low' level of the metric.",
                    "evidence": "Section III.A Encoding (page 392): \"For the MNIST dataset we use in this paper, since the image size is 28\u00d728 and the pixel range is 0 to 255 in greyscale, we flat a single image into an array with 784 elements with values ranging from 0 to 255.\" \n\nSection IV HDTest Framework / Mutation algorithm (page 393): Algorithm 1 and surrounding text describe in-memory comparison: \"Both the generated input and the original input are then sent to the HDC classifier for prediction. We then check if the two predicted labels are different, and if yes, this indicates a successful generation of an adversarial input.\" (Alg.1 lines and paragraph) \n\nSection IV Mutation strategies / Table I (page 393): lists fine-grained numerical mutations such as \"gauss apply gaussian noise over the entire image\" and \"rand apply random noise over the entire image.\" \n\nSection IV (page 394): \"To ensure the added perturbations are within an 'invisible' range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.\" \n\n(There is no discussion in the paper of serialization, saving mutated inputs to standard image/audio formats, quantization, rounding, or explicit handling of clipping/rounding effects in either test-case generation or oracle evaluation.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's fuzzing method, HDTest, applies numerical mutation strategies like adding Gaussian and random noise to input images. The methodology does not describe any mechanism to account for standard input/output (I/O) serialization effects such as quantization (rounding floating-point pixel values to integers) or clipping (ensuring pixel values remain within the valid [0, 255] range for 8-bit images). The fuzzing loop feeds the mutated inputs, which likely exist as in-memory floating-point arrays after mutation, directly to the model for evaluation. This allows for the possibility that discovered failures are caused by small numerical perturbations that would be lost or altered if the adversarial image were saved to a standard file format (e.g., PNG) and then reloaded. Consequently, the reproducibility of the generated failures is not guaranteed.",
                    "evidence": "Page 4, Table I lists mutation strategies including `gauss` and `rand` which are described as 'apply gaussian noise over the entire image' and 'apply random noise over the entire image'. The paper's algorithm and experimental setup sections do not mention any steps for handling quantization or clipping. Algorithm 1 on page 3 shows that mutated `seeds` are generated (Line 6) and then directly fed to the model `y_seeds = HDC(seeds)` (Line 7) without any intermediate serialization step to ensure the perturbations would persist."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's fuzzing/mutation design applies fine-grained numerical perturbations (e.g., Gaussian and random noise) and evaluates adversariality entirely by sending in-memory mutated inputs to the HDC classifier, but it does not account for standard I/O serialization effects (quantization, rounding, clipping) nor constrain mutations to guarantee they survive serialization. The authors explicitly describe adding gaussian/random noise and using an L2-distance threshold to keep perturbations 'invisible', and they evaluate success by comparing predictions on the original and mutated inputs in memory. Nowhere do they describe converting/saving mutated inputs to common serialized formats (e.g., 8-bit images) or handling clipping/rounding so that failures remain reproducible after standard I/O operations. Because I/O/serialization effects are ignored, fault-inducing perturbations could be lost or altered when inputs are serialized, so the paper meets the 'Low' level of the metric.",
                    "evidence": "Section III.A Encoding (page 392): \"For the MNIST dataset we use in this paper, since the image size is 28\u00d728 and the pixel range is 0 to 255 in greyscale, we flat a single image into an array with 784 elements with values ranging from 0 to 255.\" \n\nSection IV HDTest Framework / Mutation algorithm (page 393): Algorithm 1 and surrounding text describe in-memory comparison: \"Both the generated input and the original input are then sent to the HDC classifier for prediction. We then check if the two predicted labels are different, and if yes, this indicates a successful generation of an adversarial input.\" (Alg.1 lines and paragraph) \n\nSection IV Mutation strategies / Table I (page 393): lists fine-grained numerical mutations such as \"gauss apply gaussian noise over the entire image\" and \"rand apply random noise over the entire image.\" \n\nSection IV (page 394): \"To ensure the added perturbations are within an 'invisible' range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.\" \n\n(There is no discussion in the paper of serialization, saving mutated inputs to standard image/audio formats, quantization, rounding, or explicit handling of clipping/rounding effects in either test-case generation or oracle evaluation.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "In memory floating point mutation, no safeguards for IO preservation",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether adversarial / fault-inducing inputs generated on one HDC model transfer to other models performing the same task. All experiments generate and use adversarial inputs against a single HDC classifier on MNIST (including using generated inputs to retrain that same classifier). The paper does not report reusing seeds/adversarial inputs on other independently-trained HDC models, nor does it include explicit cross-model transferability experiments or mechanisms designed to test transfer across different model implementations.",
                    "evidence": "V.A Experimental Setup (page 394): \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"  V.D Case Study on Defense Against Adversarial Attacks (page 395): \"First, for a specific HDC model, we run HDTest to generate 1000 adversarial images. ... We feed the first subset of adversarial images to the HDC model with correct labels to retrain the HDC model ... Next, we use the second subset of adversarial images to attack the HDC model. Our experimental results show that, after retraining, the rate of successful attack rate drops more than 20%.\"  V.E Discussion (page 395): \"While we only consider one dataset in this paper due to the fact that HDC is still exploring its use in image classification domain, HDTest can be naturally extended to other HDC model structures because it considers a general greybox assumption with only HV distance information.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper introduces HDTest, a differential fuzz testing method specifically designed for Hyperdimensional Computing (HDC) models. The evaluation of HDTest is conducted on a single HDC model architecture trained for the MNIST classification task. The experiments focus on generating adversarial inputs that cause mispredictions in this specific model and then using these generated inputs to retrain and improve the robustness of that same model. The paper does not perform any experiments to test whether the fault-inducing inputs generated for this one model also cause failures in other, independently trained HDC models or in models based on different architectures (e.g., DNNs) performing the same task. Therefore, the transferability of the generated attacks is not demonstrated.",
                    "evidence": "Page 4, Section V-A, Experimental Setup: \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"\n\nPage 6, Section V-D, Case Study on Defense Against Adversarial Attacks: \"First, for a specific HDC model, we run HDTest to generate 1000 adversarial images... We feed the first subset of adversarial images to the HDC model with correct labels to retrain the HDC model, i.e., updating the reference HVs. Next, we use the second subset of adversarial images to attack the HDC model.\" This section describes a process of retraining and then re-attacking the same model, not testing a different one."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether adversarial / fault-inducing inputs generated on one HDC model transfer to other models performing the same task. All experiments generate and use adversarial inputs against a single HDC classifier on MNIST (including using generated inputs to retrain that same classifier). The paper does not report reusing seeds/adversarial inputs on other independently-trained HDC models, nor does it include explicit cross-model transferability experiments or mechanisms designed to test transfer across different model implementations.",
                    "evidence": "V.A Experimental Setup (page 394): \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"  V.D Case Study on Defense Against Adversarial Attacks (page 395): \"First, for a specific HDC model, we run HDTest to generate 1000 adversarial images. ... We feed the first subset of adversarial images to the HDC model with correct labels to retrain the HDC model ... Next, we use the second subset of adversarial images to attack the HDC model. Our experimental results show that, after retraining, the rate of successful attack rate drops more than 20%.\"  V.E Discussion (page 395): \"While we only consider one dataset in this paper due to the fact that HDC is still exploring its use in image classification domain, HDTest can be naturally extended to other HDC model structures because it considers a general greybox assumption with only HV distance information.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Du et al. - 2019 - DeepStellar model-based quantitative analysis of stateful deep learning systems": {
        "key": "du2019deepstellar",
        "name": "DeepStellar",
        "year": "2019",
        "citation_count": "188",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's DeepStellar framework is explicitly used to find and detect adversarial inputs that cause RNNs to produce incorrect or targeted outputs (e.g., targeted adversarial audio transcriptions and misclassifications). Those failures violate user intent and can produce undesired or unsafe outputs, which matches the \"Medium\" severity class. The paper does not claim to bypass explicit safety/policy or security mechanisms nor evaluate defended or safety-aligned models (no results on circumventing defenses, jailbreaks, data-exfiltration, or models with explicit safety mechanisms), so it does not meet the \"High\" category.",
                    "evidence": "Page 477, Abstract: \"we design two algorithms powered by the quantitative measures for adversarial sample detection and coverage-guided test generation.\" \n\nPage 481, Section 5.1 (Adversarial Sample Detection): \"Adversarial sample detection aims to check whether a given input is an adversarial sample at runtime. We propose to use the trace similarity metrics to measure the behavioral differences between two inputs. Based on this idea, we develop a new approach to detect adversarial samples for RNNs.\" \n\nPage 483, Section 6.1 (Data Preparation): \"we generate 1,100 (100 seeds\u00d7 11 targets) adversarial audios, which took about 12 days in total ...\" (shows they generate targeted adversarial ASR examples). \n\nPage 484, Section 6.4 (Results): \"The results confirm that the trace similarity-based method is effective for adversarial sample detection under carefully selected abstraction configurations, with more than 89% prediction accuracy.\" \n\nPage 482, Section 6.1 (Models and Dataset / Table 1): lists standard pre-trained models used (e.g., \"we selected two versions of Mozilla pre-trained DeepSpeech\"), with no mention of testing or bypassing defended / safety-aligned models."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, DeepStellar, focuses on generating and detecting 'adversarial samples' in Recurrent Neural Network (RNN) based systems. These failures are defined as inputs that are slightly perturbed from benign samples to cause incorrect predictions. The evaluation is conducted on image classification (MNIST) and automated speech recognition (DeepSpeech) models. The uncovered failures are misclassifications or incorrect transcriptions, which fall squarely under the definition of 'model robustness errors, such as mispredictions or prediction inconsistencies under semantically preserving input mutations'. The paper does not describe generating or uncovering any unsafe, biased, or toxic behaviors, nor does it test against models with explicit safety or security mechanisms that could be bypassed.",
                    "evidence": "Page 7, Section 5.2 Coverage-Guided Testing of RNNs: 'In this section, we propose a Coverage-Guided Testing (CGT) technique which aims to generate adversarial samples incorrectly recognized by an RNN.' and later in the description of Algorithm 2: 'A mutant is an adversarial sample if it is predicted incorrectly by the network.' This explicitly defines a failure as a misprediction, which is a robustness error."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DeepStellar\u2019s fuzzing uncovers adversarial inputs that cause RNNs to misclassify or mistranscribe (robustness errors) but does not target or bypass any explicit safety, policy, or security mechanisms, nor does it induce hallucinated, biased, toxic, or otherwise unsafe outputs. The failures are classic model\u2010robustness mispredictions under small input perturbations, fitting the \u201cLow\u201d severity category.",
                    "evidence": "Page 479\u2013480, \u00a73.3 and \u00a75.2: \u201ca mutant is an adversarial sample if it is predicted incorrectly by the network.\u201d\nPage 483, \u00a76.1 (Data Preparation): \u201cwe generate \u2026 adversarial audios \u2026\u201d and then measure word\u2010error\u2010rate, showing the focus is on transcription inaccuracies.\nNo sections of the paper describe testing against defended or safety\u2010aligned models or bypassing explicit security/safety policies."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "DeepStellar's testing component (Coverage-Guided Testing, CGT) is designed to generate inputs that produce incorrect predictions (i.e., discover misclassifications) or increase coverage, rather than to drive the model toward attacker-chosen, specific outputs. The CGT oracle treats a mutant as an adversarial sample when it is \"predicted incorrectly by the network\" and retains mutants that increase coverage; its selection logic checks for incorrect inference, not for reaching a predefined target label or phrase. Although the authors used externally-produced targeted adversarial examples (e.g., targeted audio commands) for evaluation, that targeted generation was performed by existing attack tools for the experiments, not by the DeepStellar fuzzing/generation strategy itself. Therefore the approach performs untargeted exploration (discovering generic faults/misclassifications) rather than steering exploration toward attacker-chosen outputs.",
                    "evidence": "Section 5.2 (Coverage-Guided Testing): \"we propose a Coverage-Guided Testing (CGT) technique which aims to generate adversarial samples incorrectly recognized by an RNN.\" (paper, Section 5.2, p. 481). Algorithm 2 (Coverage guided testing of RNN): the algorithm marks a mutant as a failure when it is mispredicted: \"if Failed(a\u2019, result) then F \u2190 F {a\u2019}\" (paper, Section 5.2, Algorithm 2, p. 481). Section 6.1 (Experiment Settings) describes targeted adversarial examples used for evaluation but indicates they were generated separately: \"To be specific, we select the 11 commands [13] as the targets and generate 1,100 (100 seeds \u00d7 11 targets) adversarial audios, which took about 12 days in total on 4 GPUs\" (paper, Section 6.1, p. 483). These quotes show (1) CGT seeks incorrect predictions/coverage increases (untargeted), and (2) targeted examples in the study were produced by external attack generation for evaluation rather than by DeepStellar steering tests toward attacker-chosen outputs."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method for generating adversarial samples, called Coverage-Guided Testing (CGT), performs untargeted exploration to discover generic failures. The explicit goal is to find inputs that are \"incorrectly recognized by an RNN,\" without specifying a particular target output. The testing oracle, which determines if a generated sample is a failure, is based on metamorphic testing. This means it identifies failures when meaning-preserving mutations cause the model's prediction to change, which is a form of generic inconsistency or misclassification. The exploration process, as detailed in Algorithm 2, is guided by increasing state and transition coverage metrics, not by steering towards a predefined, attacker-chosen outcome. While the paper mentions using external tools to generate targeted adversarial samples, this is done solely to create a dataset for evaluating their separate adversarial sample *detection* algorithm, not as a feature of their own test *generation* method.",
                    "evidence": "Page 7, Section 5.2, 'Coverage-Guided Testing of RNNs': The paper describes its generation technique's goal as untargeted: 'we propose a Coverage-Guided Testing (CGT) technique which aims to generate adversarial samples incorrectly recognized by an RNN.' The failure condition is defined as a generic error: 'A mutant is an adversarial sample if it is predicted incorrectly by the network.' The method for identifying these failures relies on a metamorphic oracle rather than a specific target: 'For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human\u2019s perspective during testing.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DeepStellar's testing component (Coverage-Guided Testing, CGT) is designed to generate inputs that produce incorrect predictions (i.e., discover misclassifications) or increase coverage, rather than to drive the model toward attacker-chosen, specific outputs. The CGT oracle treats a mutant as an adversarial sample when it is \"predicted incorrectly by the network\" and retains mutants that increase coverage; its selection logic checks for incorrect inference, not for reaching a predefined target label or phrase. Although the authors used externally-produced targeted adversarial examples (e.g., targeted audio commands) for evaluation, that targeted generation was performed by existing attack tools for the experiments, not by the DeepStellar fuzzing/generation strategy itself. Therefore the approach performs untargeted exploration (discovering generic faults/misclassifications) rather than steering exploration toward attacker-chosen outputs.",
                    "evidence": "Section 5.2 (Coverage-Guided Testing): \"we propose a Coverage-Guided Testing (CGT) technique which aims to generate adversarial samples incorrectly recognized by an RNN.\" (paper, Section 5.2, p. 481). Algorithm 2 (Coverage guided testing of RNN): the algorithm marks a mutant as a failure when it is mispredicted: \"if Failed(a\u2019, result) then F \u2190 F {a\u2019}\" (paper, Section 5.2, Algorithm 2, p. 481). Section 6.1 (Experiment Settings) describes targeted adversarial examples used for evaluation but indicates they were generated separately: \"To be specific, we select the 11 commands [13] as the targets and generate 1,100 (100 seeds \u00d7 11 targets) adversarial audios, which took about 12 days in total on 4 GPUs\" (paper, Section 6.1, p. 483). These quotes show (1) CGT seeks incorrect predictions/coverage increases (untargeted), and (2) targeted examples in the study were produced by external attack generation for evaluation rather than by DeepStellar steering tests toward attacker-chosen outputs."
                },
                "manual": {
                    "value": "Low",
                    "why": "Broad exploration to uncover functional errors",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight into failures by analyzing observable model internals and statistical failure patterns. It (1) constructs an abstract state-transition (DTMC) model of RNN internals, (2) defines trace-similarity metrics and multiple state/transition coverage criteria over that model, and (3) performs statistical analyses that correlate these internal signals with prediction differences and adversarial behavior (e.g., significant Spearman / Mann-Whitney results, coverage increases for adversarial samples, and observations that rarely-visited states/transitions are exercised by adversarial inputs). However, the paper stops short of linking discovered failures to deeper causes such as reliance on specific non-robust/spurious features, dataset bias, or overfitting; therefore it does not meet the \u201cHigh\u201d bar but clearly exceeds \u201cLow.\u201d",
                    "evidence": "Page 479, Section 3.3 (Trace Similarity Metrics): \"The trace similarity metrics range over [0, 1], where 0 indicates disjoint sets (i.e., traces induced by x and y are totally different), while 1 indicating equal sets (i.e., the traces are similar).\"  \n\nPage 480, Section 4 (Coverage Criteria for RNN): \"The goal of the RNN coverage criteria is to measure the sufficiency of test data in exercising the trained as well as the unseen behaviors.\"  \n\nPage 484, Section 6.2 (RQ1: Trace Similarity): \"We perform a statistical analysis on the correlation between the trace similarity and the prediction difference over the slightly perturbed samples and their original benign samples.... All reported correlations are statistically significant (with p << 0.01). Negative association of Spearman correlation indicates that the larger the similarity metrics, the less different the predicted transcripts would be.\"  \n\nPage 484, Section 6.3 (RQ2: Coverage Criteria): \"We observed that finer-grained abstract models tend to have larger coverage increase ratio.... WSCov\u2019/WTCov\u2019 is larger, with a competitive performance as BSCov and BTCov, indicating rarely visited states and transitions tend to be covered by adversarial/perturbed samples.\"  \n\nPage 477 (intro/overview), Abstract/Intro summary: \"(2) the coverage criteria are useful in revealing erroneous behaviors (generating three times more adversarial samples than random testing and hundreds times more than the unrolling approach).\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by analyzing the relationship between discovered failures (adversarial samples) and observable model internals. It proposes an abstract state transition model for RNNs and defines several coverage criteria based on it. The evaluation explicitly analyzes the sensitivity of these coverage criteria to adversarial samples, demonstrating a correlation between increased coverage and the presence of erroneous behaviors. The paper also identifies a statistical failure pattern, noting that adversarial samples tend to cover rarely visited states and transitions within the abstract model. This constitutes an analysis of model internals and statistical failure patterns. However, the analysis does not extend to explaining the underlying root causes of model vulnerability, such as dataset bias or overfitting, which would be required for a 'High' rating.",
                    "evidence": "Page 9, Section 6.3 RQ2. Coverage Criteria, Results: \"Table 4 reports the coverage results using different coverage criteria on different dataset... The coverage increase ratio indicates the sensitivity of the coverage criteria to adversarial samples. We observed that finer-grained abstract models tend to have larger coverage increase ratio... In contrast, the increase ratio of WSCov'/WTCov' is larger, with a competitive performance as BSCov and BTCov, indicating rarely visited states and transitions tend to be covered by adversarial/perturbed samples.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight into failures by analyzing observable model internals and statistical failure patterns. It (1) constructs an abstract state-transition (DTMC) model of RNN internals, (2) defines trace-similarity metrics and multiple state/transition coverage criteria over that model, and (3) performs statistical analyses that correlate these internal signals with prediction differences and adversarial behavior (e.g., significant Spearman / Mann-Whitney results, coverage increases for adversarial samples, and observations that rarely-visited states/transitions are exercised by adversarial inputs). However, the paper stops short of linking discovered failures to deeper causes such as reliance on specific non-robust/spurious features, dataset bias, or overfitting; therefore it does not meet the \u201cHigh\u201d bar but clearly exceeds \u201cLow.\u201d",
                    "evidence": "Page 479, Section 3.3 (Trace Similarity Metrics): \"The trace similarity metrics range over [0, 1], where 0 indicates disjoint sets (i.e., traces induced by x and y are totally different), while 1 indicating equal sets (i.e., the traces are similar).\"  \n\nPage 480, Section 4 (Coverage Criteria for RNN): \"The goal of the RNN coverage criteria is to measure the sufficiency of test data in exercising the trained as well as the unseen behaviors.\"  \n\nPage 484, Section 6.2 (RQ1: Trace Similarity): \"We perform a statistical analysis on the correlation between the trace similarity and the prediction difference over the slightly perturbed samples and their original benign samples.... All reported correlations are statistically significant (with p << 0.01). Negative association of Spearman correlation indicates that the larger the similarity metrics, the less different the predicted transcripts would be.\"  \n\nPage 484, Section 6.3 (RQ2: Coverage Criteria): \"We observed that finer-grained abstract models tend to have larger coverage increase ratio.... WSCov\u2019/WTCov\u2019 is larger, with a competitive performance as BSCov and BTCov, indicating rarely visited states and transitions tend to be covered by adversarial/perturbed samples.\"  \n\nPage 477 (intro/overview), Abstract/Intro summary: \"(2) the coverage criteria are useful in revealing erroneous behaviors (generating three times more adversarial samples than random testing and hundreds times more than the unrolling approach).\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Shows correlation between coverage criteria and faults",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility at the level of individual mutation steps: it uses domain-specific, metamorphic transformations with conservative parameters (e.g., small speed/volume/noise changes for audio; small brightness/contrast/translation/rotation for images) and states that these perturbations are imperceptible. However, DeepStellar does not describe mechanisms that prevent or measure cumulative drift across multiple fuzzing/mutation iterations, nor does it provide empirical human evaluation or task-specific perceptual metrics (e.g., SSIM/LPIPS for images, PESQ/STOI for audio, or Perplexity for language) to validate that final fault-inducing inputs remain perceptually/semantically realistic. The coverage-guided testing algorithm (Algorithm 2) accepts mutants based on coverage increase alone, with no stated plausibility filter applied to the retained seeds; the paper therefore enforces plausibility in individual mutation choices but overlooks cumulative-effects and lacks quantitative or user-study evidence for final-input plausibility, matching the rubric for \u201cMedium.\u201d",
                    "evidence": "Section 5.2 Coverage-Guided Testing (p. 481): \"Mutation operators are often specific to the application domains. For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human\u2019s perspective during testing.\" (Algorithm 2 also shows mutants are retained when they increase coverage: \"Randomly pick transformation t with a random parameter p; A = mutate(t, p, a); ... else if CoverageIncrease(cov, Q) then Q <- Q \u222a a' .\")\n\nSection 6.1 Data Preparation (p. 482\u2013483): \"For each ASR model, we generate 10,000 perturbed audios from original benign ones with existing audio data augmentation techniques [5] (i.e., speed and volume adjustment, low/high-frequency filtering, noise blending).... For each MNIST model, we also generate 10,000 benign perturbed samples with existing image transformation techniques [50] (i.e., image contrast, brightness, translation, scaling, shearing, rotation and add white noise). ... For both audio and image case, we set conservative parameters for transformation so that the perturbation on original samples is slight and imperceptible. Note that all the adversarial samples are also with minimal perturbations and not perceptible by human.\"\n\nEvaluation sections (e.g., RQ3 & RQ4, pp. 484\u2013486) report detection AUROC, coverage numbers and counts of adversarial samples, but the paper does not report any human studies or task-specific perceptual/semantic quality metrics (e.g., LPIPS/SSIM for images, PESQ/STOI for audio, or Perplexity for language) to empirically validate final fault-inducing inputs' realism."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's coverage-guided testing (CGT) method is designed with input plausibility in mind. It explicitly states the use of a \"metamorphic mutation strategy... that would keep the prediction meaning from the human's perspective during testing.\" The transformations used for generating perturbed inputs are standard, bounded operations (e.g., image contrast, brightness, rotation), which enforce plausibility within individual mutation steps. The paper also claims that the perturbations are \"slight and imperceptible\" and that adversarial samples are \"not perceptible by human.\" \n\nHowever, the evaluation does not meet the criteria for 'High'. Firstly, the paper does not discuss or provide a mechanism to control for the cumulative effect of mutations over many iterations, which could cause inputs to drift from the plausible distribution. Secondly, the claim of imperceptibility is not substantiated with empirical evidence, such as a human study or quantitative, task-specific metrics (e.g., LPIPS, SSIM, PESQ). This approach aligns with the 'Medium' definition, as it enforces plausibility in individual mutation steps via bounded constraints but does not address cumulative effects or provide rigorous empirical validation of the final outputs' realism.",
                    "evidence": "Page 7, Section 5.2: \"For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human's perspective during testing.\"\n\nPage 8, Section 6.1, Data Preparation: \"For each MNIST model, we also generate 10,000 benign perturbed samples with existing image transformation techniques [50] (i.e., image contrast, brightness, translation, scaling, shearing, rotation and add white noise).\"\n\nPage 8, Section 6.1, Data Preparation: \"For both audio and image case, we set conservative parameters for transformation so that the perturbation on original samples is slight and imperceptible. Note that all the adversarial samples are also with minimal perturbations and not perceptible by human.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility at the level of individual mutation steps: it uses domain-specific, metamorphic transformations with conservative parameters (e.g., small speed/volume/noise changes for audio; small brightness/contrast/translation/rotation for images) and states that these perturbations are imperceptible. However, DeepStellar does not describe mechanisms that prevent or measure cumulative drift across multiple fuzzing/mutation iterations, nor does it provide empirical human evaluation or task-specific perceptual metrics (e.g., SSIM/LPIPS for images, PESQ/STOI for audio, or Perplexity for language) to validate that final fault-inducing inputs remain perceptually/semantically realistic. The coverage-guided testing algorithm (Algorithm 2) accepts mutants based on coverage increase alone, with no stated plausibility filter applied to the retained seeds; the paper therefore enforces plausibility in individual mutation choices but overlooks cumulative-effects and lacks quantitative or user-study evidence for final-input plausibility, matching the rubric for \u201cMedium.\u201d",
                    "evidence": "Section 5.2 Coverage-Guided Testing (p. 481): \"Mutation operators are often specific to the application domains. For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human\u2019s perspective during testing.\" (Algorithm 2 also shows mutants are retained when they increase coverage: \"Randomly pick transformation t with a random parameter p; A = mutate(t, p, a); ... else if CoverageIncrease(cov, Q) then Q <- Q \u222a a' .\")\n\nSection 6.1 Data Preparation (p. 482\u2013483): \"For each ASR model, we generate 10,000 perturbed audios from original benign ones with existing audio data augmentation techniques [5] (i.e., speed and volume adjustment, low/high-frequency filtering, noise blending).... For each MNIST model, we also generate 10,000 benign perturbed samples with existing image transformation techniques [50] (i.e., image contrast, brightness, translation, scaling, shearing, rotation and add white noise). ... For both audio and image case, we set conservative parameters for transformation so that the perturbation on original samples is slight and imperceptible. Note that all the adversarial samples are also with minimal perturbations and not perceptible by human.\"\n\nEvaluation sections (e.g., RQ3 & RQ4, pp. 484\u2013486) report detection AUROC, coverage numbers and counts of adversarial samples, but the paper does not report any human studies or task-specific perceptual/semantic quality metrics (e.g., LPIPS/SSIM for images, PESQ/STOI for audio, or Perplexity for language) to empirically validate final fault-inducing inputs' realism."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Design contraints on perturbations but no naturalness measure of final fault-triggering inputs",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper uses semantic-preserving, metamorphic transformations and synthesized inputs (e.g., audio augmentation and TTS-generated reference audios) as its primary mutation and test-generation mechanisms, which provide partial robustness to I/O effects. However, the paper does not explicitly account for serialization effects (quantization, clipping, rounding, or encoding) during mutation or oracle evaluation, nor does it state constraints to ensure that fault-inducing numerical perturbations survive typical serialization. Therefore, it neither enforces end-to-end reproducibility under standard I/O nor completely ignores metamorphic/serialized-input approaches; this matches the metric's \u201cMedium\u201d level (metamorphic transformations without explicit enforcement of reproducibility under serialization).",
                    "evidence": "Section 5.2 (Coverage-Guided Testing of RNNs), p. 482: \"For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human\u2019s perspective during testing.\"; Section 5.1 (Adversarial Sample Detection for RNNs), p. 481: \"For ASR, given an audio input a that is transcribed to texts t by the RNN, we generate the reference audios using off-the-shelf text-to-speech engines (e.g., Google cloud text-to-speech [4]) that generate the audio t with correct and clear pronouncing.\"; Section 6.1 (Data Preparation), p. 483: \"For each ASR model, we generate 10,000 perturbed audios from original benign ones with existing audio data augmentation techniques [5] (i.e., speed and volume adjustment, low/high-frequency filtering, noise blending).\"  (No text in the paper discusses explicit handling of quantization, clipping, rounding, or other serialization effects when producing or evaluating mutated inputs.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's coverage-guided testing (CGT) approach relies on metamorphic transformations to generate new test inputs for both image and audio domains. The paper explicitly lists the transformations used: 'image contrast, brightness, translation, scaling, shearing, rotation and add white noise' for images, and 'speed and volume adjustment, low/high-frequency filtering, noise blending' for audio. The metric definition states that applying 'semantic-preserving metamorphic transformations without explicit enforcement of reproducibility under serialization' corresponds to a 'Medium' value. The paper's methodology for test generation (Algorithm 2) and its evaluation do not describe any process of saving and reloading the mutated inputs to account for I/O effects like quantization, clipping, or rounding. The mutations are applied and evaluated on in-memory data representations, without explicit confirmation that the failures would persist after serialization.",
                    "evidence": "Page 7, Section 5.2: \"For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human's perspective during testing.\"\n\nPage 8, Section 6.1, 'Data Preparation': \"For each MNIST model, we also generate 10,000 benign perturbed samples with existing image transformation techniques [50] (i.e., image contrast, brightness, translation, scaling, shearing, rotation and add white noise).\"\n\nPage 8, Section 6.1, 'Data Preparation': \"For each ASR model, we generate 10,000 perturbed audios from original benign ones with existing audio data augmentation techniques [5] (i.e., speed and volume adjustment, low/high-frequency filtering, noise blending).\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper uses semantic-preserving, metamorphic transformations and synthesized inputs (e.g., audio augmentation and TTS-generated reference audios) as its primary mutation and test-generation mechanisms, which provide partial robustness to I/O effects. However, the paper does not explicitly account for serialization effects (quantization, clipping, rounding, or encoding) during mutation or oracle evaluation, nor does it state constraints to ensure that fault-inducing numerical perturbations survive typical serialization. Therefore, it neither enforces end-to-end reproducibility under standard I/O nor completely ignores metamorphic/serialized-input approaches; this matches the metric's \u201cMedium\u201d level (metamorphic transformations without explicit enforcement of reproducibility under serialization).",
                    "evidence": "Section 5.2 (Coverage-Guided Testing of RNNs), p. 482: \"For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human\u2019s perspective during testing.\"; Section 5.1 (Adversarial Sample Detection for RNNs), p. 481: \"For ASR, given an audio input a that is transcribed to texts t by the RNN, we generate the reference audios using off-the-shelf text-to-speech engines (e.g., Google cloud text-to-speech [4]) that generate the audio t with correct and clear pronouncing.\"; Section 6.1 (Data Preparation), p. 483: \"For each ASR model, we generate 10,000 perturbed audios from original benign ones with existing audio data augmentation techniques [5] (i.e., speed and volume adjustment, low/high-frequency filtering, noise blending).\"  (No text in the paper discusses explicit handling of quantization, clipping, rounding, or other serialization effects when producing or evaluating mutated inputs.)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Applies deephunter metamorphic mutations that are plausibly I/O-stable but does not apply clipping and rounding to ensure",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate whether fault-inducing inputs transfer across different models. All algorithms and experiments are formulated and executed per-target-model (the abstract model M and the RNN D are paired), adversarial examples were generated for particular models only, and the authors explicitly note they could only generate adversarial samples for one DeepSpeech version due to tool compatibility. There are no experiments reusing inputs crafted on one model to test other models performing the same task, nor design elements aimed at discovering transferable attacks.",
                    "evidence": "Section 5.1 (page 481): \"input :D: RNN-based DL system, M: Abstract model of D\" and \"We first collect a set of benign samples B and a set of adversarial samples A.\" (Algorithm 1) \u2014 shows detection is trained per given RNN and its abstract model. Section 5.2 (page 482): \"input :I: Initial seeds, D: RNN-based DL system, M: Abstract model of D\" (Algorithm 2) \u2014 testing is guided against a single RNN instance D with its model M. Section 6.1 Data Preparation (page 483): \"Finally, we only successfully generate adversarial samples for DeepSpeech-0.1.1 because there exists a compatibility issue between DeepSpeech-0.3.0 and the adversarial attack tools [8] we used.\" \u2014 indicates adversarial samples were not produced for multiple models and no cross-model reuse/transfer experiments were performed. Section 6.4 (page 485): \"...we trained a classifier for each model to detect adversarial samples.\" \u2014 confirms evaluation is per-model, not testing transferability across models."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes methods for generating and detecting adversarial samples for Recurrent Neural Networks (RNNs). The core methodology involves creating a model-specific abstraction (a Discrete-Time Markov Chain or DTMC) based on the internal state transitions of a single, trained RNN. The evaluation is conducted on multiple models performing the same tasks (e.g., MNIST-LSTM and MNIST-GRU for image classification). However, the adversarial samples are always generated for a specific model and then evaluated on that same model. The paper does not perform any experiments where fault-inducing inputs generated for one model are reused to test for failures on another model. The methodology is inherently designed to analyze a single model at a time and lacks any mechanism to support the discovery of transferable failures.",
                    "evidence": "Page 7, Algorithm 2, \"Coverage guided testing of RNN\": The algorithm for generating adversarial samples takes a single \"RNN-based DL system, M: Abstract model of D\" as input. All subsequent steps within the testing loop, including prediction, coverage analysis, and failure checking, are performed with respect to this single model `D` and its abstract model `M`. This demonstrates the single-model focus of the generation technique.\n\nPage 10, Section 6.4, \"RQ3. Adversarial Sample Detection\": In the setup for evaluating the detection mechanism, the paper states: \"With the constructed dataset, we trained a classifier for each model to detect adversarial samples.\" This confirms that the detection process is model-specific and does not involve testing the transferability of adversarial examples between different models.\n\nPage 10, Section 6.5, \"RQ4. Coverage-guided Testing\": The evaluation of the coverage-guided testing is presented separately for MNIST-LSTM and MNIST-GRU in Table 6. The accompanying text describes the testing process for each model independently, without any mention of cross-testing the generated adversarial samples between the two models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate whether fault-inducing inputs transfer across different models. All algorithms and experiments are formulated and executed per-target-model (the abstract model M and the RNN D are paired), adversarial examples were generated for particular models only, and the authors explicitly note they could only generate adversarial samples for one DeepSpeech version due to tool compatibility. There are no experiments reusing inputs crafted on one model to test other models performing the same task, nor design elements aimed at discovering transferable attacks.",
                    "evidence": "Section 5.1 (page 481): \"input :D: RNN-based DL system, M: Abstract model of D\" and \"We first collect a set of benign samples B and a set of adversarial samples A.\" (Algorithm 1) \u2014 shows detection is trained per given RNN and its abstract model. Section 5.2 (page 482): \"input :I: Initial seeds, D: RNN-based DL system, M: Abstract model of D\" (Algorithm 2) \u2014 testing is guided against a single RNN instance D with its model M. Section 6.1 Data Preparation (page 483): \"Finally, we only successfully generate adversarial samples for DeepSpeech-0.1.1 because there exists a compatibility issue between DeepSpeech-0.3.0 and the adversarial attack tools [8] we used.\" \u2014 indicates adversarial samples were not produced for multiple models and no cross-model reuse/transfer experiments were performed. Section 6.4 (page 485): \"...we trained a classifier for each model to detect adversarial samples.\" \u2014 confirms evaluation is per-model, not testing transferability across models."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic",
                "Property-based"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ]
        }
    },
    "Huang et al. - 2022 - Coverage-Guided Testing for Recurrent Neural Networks": {
        "key": "huang2021coverage",
        "name": "TESTRNN",
        "year": "2021",
        "citation_count": "66",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's TESTRNN method explicitly uncovers undesired and security-relevant behaviors (adversarial mispredictions and backdoor-triggered misclassifications) \u2014 i.e., failures that violate user intent or safety expectations. However, the paper does not demonstrate bypassing explicit safety/policy/security mechanisms in defended or safety-aligned models (for example, adversarially-trained or otherwise defended models), nor does it claim to induce jailbreaks, data leakage, or unauthorized actions in defended systems. Thus the failures uncovered are higher than mere low-level robustness errors but do not meet the paper-evidence threshold for the highest severity class that requires bypassing explicit defenses.",
                    "evidence": "Page 1191 (Abstract / Introduction): \"In this article, we study the RNN defects, focusing on adversarial samples [1] and backdoor samples [10].\"\n\nPage 1192 (Contributions): \"TESTRNN can not only find adversarial behaviors for the robustness of RNNs (see Section VII-C1) but also identify backdoor inputs for the security of RNNs (see Section VII-C2).\"\n\nPage 1200 (Section VII-A / Experimental Setup): \"We train two handwritten digits recognition models, one of which is benign classifier and the other one is the malicious classifier subject to the backdoor attack in [17].\"\n\nPage 1201 (Section VII-C2, sensitivity / conclusion): \"Answer to RQ4: The TESTRNN metrics can identify the difference between the backdoor input and the normal input (to malicious models).\"\n\nPage 1201 (Section VII-C2, limitation noted): \"We remark that the aforementioned experiment only confirms that test metrics are sensitive to backdoor samples when testing an attacked model.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper focuses on uncovering model robustness errors, specifically adversarial samples and backdoor samples, which lead to mispredictions. The test oracle, which defines failure, is explicitly based on finding prediction inconsistencies under small, semantically preserving input mutations. The experimental evaluation demonstrates the method's ability to find inputs that are slightly perturbed from a seed input but result in a different classification. While the paper also investigates the detection of backdoor inputs, this is framed as another type of defect leading to mispredictions, and the method is not evaluated against models with explicit safety or security mechanisms that could be bypassed. The types of failures uncovered are limited to misclassifications and do not extend to unsafe behaviors like generating toxic or biased content.",
                    "evidence": "Page 8, Section VI-C, \"Test Set Evaluation\": \"A test case x' is said to not pass the oracle if x' is within the norm-ball of some known sample x, i.e., ||x \u2212 x\u2032||2 \u2264 r_oracle, and if x\u2032 has a different classification from x, i.e., \u03c6(x) \u2260 \u03c6(x\u2032). Take the definition in [42], a test case does not pass the oracle is an adversarial sample.\" This quote explicitly defines a failure as a robustness error (a misprediction within a small perturbation radius), which aligns with the 'Low' severity rating."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The failures uncovered by TESTRNN are limited to mispredictions\u2014both adversarial examples (small\u2010perturbation misclassifications) and backdoor\u2010triggered misclassifications\u2014without demonstrating the bypass of any explicit safety, policy, or security mechanisms in defended or safety\u2010aligned models. There is no evidence of inducing unsafe or unauthorized behaviors (such as jailbreaks, data leakage, or toxic/hallucinatory outputs), only prediction inconsistencies under input mutations or backdoor triggers. This aligns with the \u201cLow\u201d severity class, which covers robustness errors without exposing unsafe behavior or bypassing defenses.",
                    "evidence": "Section VI-C (Test Set Evaluation): \u201cA test case x\u2032 is said to not pass the oracle if x\u2032 is within the norm-ball of some known sample x \u2026 and if x\u2032 has a different classification from x \u2026 a test case does not pass the oracle is an adversarial sample.\u201d\n\nSection VII-C1 and VII-C2: All reported failures are misclassifications (adversarial examples and backdoor inputs) on undefended models; no experiments on adversarially trained or safety-aligned models are reported."
                },
                "manual": {
                    "value": "Low",
                    "why": "Even though it can detect backdoor triggers, this is purely diagnostic. The fuzzer exposes low-level functional and robustness failures (misclassifications, metamorphic inconsistencies, and backdoor-induced activation anomalies) without breaching behavioral or safety boundaries.",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The TESTRNN method steers exploration toward satisfying internal coverage conditions (via a genetic \"targeted mutation\" that minimises coverage loss), and its evaluation shows it finds broader security-relevant behaviours (adversarial samples and backdoor-triggered inputs). However, TESTRNN does not aim to induce specific attacker-chosen outputs/labels/phrases: its fitness functions target coverage conditions (internal patterns), and its oracle treats any label change within the norm-ball as an adversarial failure rather than checking for a predefined target output. Therefore the approach supports discovery of broader classes of security-violating behaviours rather than targeted attacks to a specific output.",
                    "evidence": "Page 1192, Contributions: \"We implement the proposed coverage metrics into a prototype tool TESTRNN ... which includes two algorithms \u2014 a random mutation and a genetic algorithm based targeted mutation \u2014 for test case generation. In particular, targeted mutation uses the coverage knowledge to guide the test case generation.\" (Section: Contributions)\n\nPage 1197-1198, Section VI-B (Targeted Mutation / Fitness): \"For the three classes of test conditions (BC, SC, TC) with respect to some s \u2208 S and a \u2208 A, we define the following fitness function as the distance to their respective targets, e.g., J_BC(x) = \u03b1_max \u2212 Nm(\u03be_{s,a,x,t}), J_SC(x) = \u03b1_SC \u2212 Nm(\u0394\u03be_{s,x,t}), J_TC(x) = sum_{j=1}^w dist(Nz(\u02c6\u03be_{s,x,j}), u_j) ... Intuitively, the fitness function (also called coverage loss) J(x) estimates the distance to the satisfaction of an unfulfilled test condition. J(x) \u2264 0 means that the test condition is covered. By generating test cases with the objective of gradually minimizing the loss, the targeted mutation is essentially a greedy search algorithm.\" (Section: Targeted Mutation / Fitness)\n\nPage 1198, Section VI-C1 (Test Oracle): \"A test case x' is said to not pass the oracle if x' is within the norm-ball of some known sample x, i.e., ||x - x'||_2 \u2264 r_oracle, and if x' has a different classification from x, i.e., \u03d5(x) \u2260 \u03d5(x'). ... a test case does not pass the oracle is an adversarial sample.\" (Section: Test Oracle)\n\nPage 1201, Section VII-C2 (Detecting Backdoor Input): \"We investigate the possibility of applying coverage-guided testing to the detection of backdoor input in neural networks. ... The experimental results describe that backdoor input activate same internal behavior with clean input for a benign classifier. In contrast to this, the backdoor input to malicious classifier will induce different internal activation, which can be seen from the apparent increase of coverage in T+B.\" (Section: Detecting Backdoor Input)\n\nPage 1192 and Section I / VII (Abstract and Evaluation summary): multiple places state TESTRNN \"can not only find adversarial behaviours for the robustness of RNNs ... but also identify backdoor inputs for the security of RNNs\" (Sections: Abstract / Contributions / VII-C), indicating the tool discovers broader classes of security-violating behaviours rather than targeting specific outputs."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper describes a coverage-guided testing approach where the primary objective is to maximize structural coverage metrics (Boundary, Stepwise, and Temporal Coverage) to find a diverse set of defects. The exploration is untargeted, aiming to discover any generic failure. This is evident in the design of the test oracle, which flags a defect if a perturbed input's classification is simply different from the original input's classification, without steering towards a predefined target label. The 'Targeted Mutation' component described in the paper is targeted towards satisfying uncovered internal test conditions (i.e., coverage goals), not towards achieving a specific, attacker-chosen output label. The paper explicitly states that its goal is diversification rather than identifying a particular type of defect.",
                    "evidence": "Page 7, Algorithm 1 & Section VI.C.1 Test Oracle: The test oracle defines a defect when `\u03c6(x) \u2260 \u03c6(x')`, which means the prediction for the mutated input `x'` is different from the original prediction for `x`. This is the definition of a generic misclassification, not a targeted attack. The algorithm's condition is: `if ||orig(x) - x'||2 \u2264 r_oracle and \u03c6(x) \u2260 \u03c6(x') then | Tadv \u2190 Tadv \u222a {x'}`."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "Although TESTRNN employs a \u2018targeted mutation\u2019 procedure, its fitness functions are geared toward satisfying internal coverage conditions (boundary, stepwise, temporal) rather than steering the model to produce any specific, attacker-chosen output. The oracle simply flags any change in the model\u2019s label within the norm\u2010ball as a failure (generic misclassification or backdoor activation), and no part of the fuzzer\u2019s design optimizes for inducing a predefined target label or phrase. Therefore, TESTRNN performs untargeted exploration of faults rather than targeted attack discovery.",
                    "evidence": "\u2013 Section VI.B (Targeted Mutation / Fitness): the fitness J(x) measures distance to internal coverage goals (e.g., J_BC(x)=\u03b1_max\u2212Nm(\u03be), J_SC(x)=\u03b1_SC\u2212Nm(\u0394\u03be), J_TC(x)=\u2211dist(Nz(\u0125\u03be),u)), not distance toward any particular output label.\n\u2013 Section VI.C.1 (Test Oracle): \u201cx' is said to not pass the oracle if ||x\u2212x'||\u2082\u2264r_oracle and \u03d5(x)\u2260\u03d5(x')\u201d; the failure condition is merely a label change, not matching a predefined attacker\u2010chosen label.\n\u2013 Throughout the paper, TESTRNN\u2019s objective is framed as maximizing coverage and uncovering diverse adversarial/backdoor behaviors, without any mechanism for specifying or directing attacks toward specific target outputs."
                },
                "manual": {
                    "value": "Low",
                    "why": "broadly explore the model's internal behavior and flag any unexpected classification (adversarial or backdoor) as a defect.",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by analyzing model internals (LSTM gates, cell states, hidden outputs) and temporal behaviors using the proposed coverage metrics (BC, SC, TC), and uses statistical analyses of failures (coverage increases, adversarial rates, uniqueness, diversity measures, and sensitivity to backdoor inputs) to characterize when and how failures occur. However, the paper stops short of providing deeper causal explanations that directly link discovered failures to broader underlying vulnerabilities (e.g., explicit attribution to spurious features, dataset bias, or overfitting) \u2014 instead it shows correlations and internal-activation patterns and uses those to detect and characterize failures.",
                    "evidence": "Page 1191, Abstract: \"TESTRNN enables meaningful information to be collected and exhibited for users to understand the testing results, which is an important step toward interpretable neural network testing.\" \n\nPage 1194, Section IV (LSTM Test Coverage Metrics): \"These metrics take into account both the values of structural information \u03be_{s,a}^t for s \u2208 S and a \u2208 A ... and their stepwise and bounded-length temporal relations.\" \n\nPage 1199, Section VII-C1 (Searching for Adversarial Samples): \"Fig. 5 reveals that normal perturbed samples can only satisfy part of test conditions, whereas the rest are more sensitive to the adversarial samples... coverage of RNN test metrics can be further increased in consideration of adversarial samples.\" \n\nPage 1201, Section VII-C1/Table V (Robustness comparison): \"As shown in Table V, adversarial samples rate and number of unique adversarial samples in the generated test suite are two important indicators for the robustness evaluation.\" \n\nPage 1201, Section VII-C2 (Detecting Backdoor Input in RNNs): \"The experimental results describe that backdoor input activate same internal behavior with clean input for a benign classifier. In contrast to this, the backdoor input to malicious classifier will induce different internal activation, which can be seen from the apparent increase of coverage in T+B.\" \n\nPage 1203, Section VII-G (Exhibition of Internal Working Mechanism): \"The working mechanism of LSTM networks can be understood via the test cases generated from TESTRNN. ... Coverage times denote the number of times a test condition is satisfied by running the test suite. Intuitively, coverage times represent the level of difficulty of asserting an input feature.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight into discovered failures by analyzing them through two primary lenses described in the metric. First, it correlates failures (both adversarial samples and backdoor attacks) with observable model internals, demonstrating that these failures correspond to an increase in the proposed coverage metrics, which capture new internal model behaviors. Second, it conducts a statistical analysis of failure patterns, specifically examining the diversity and distribution of the generated adversarial samples in the input space and comparing them to those generated by gradient-based attack methods. This analysis characterizes the nature of the discovered failures but does not extend to linking them to underlying root causes like dataset bias or spurious features, which would be required for a 'High' rating.",
                    "evidence": "Page 10, Section VII-C.1, Searching for Adversarial Samples (RQ3): \"Fig. 5 reveals that normal perturbed samples can only satisfy part of test conditions, whereas the rest are more sensitive to the adversarial samples. In all the plots, coverage of RNN test metrics can be further increased in consideration of adversarial samples. A more obvious example is, the TC coverage of IMDB model tend to saturate in the left side when only normal perturbed samples are utilized. In the right side, the coverage curve becomes steep, indicating the discovery of test cases capturing new internal behaviors.\"\n\nPage 12, Section VII-E, Comparison With Attack-Based Defect Detection (RQ6): \"We can see from the resulting diversity measurement and visualization that compared to attack methods, our testing method exercises different behaviors of RNN and generates a diverse set of test cases, intensively covering the input region around the seed input. This ability will be helpful in exposing more types of defects of the RNN (not merely in the gradient direction).\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by analyzing model internals (LSTM gates, cell states, hidden outputs) and temporal behaviors using the proposed coverage metrics (BC, SC, TC), and uses statistical analyses of failures (coverage increases, adversarial rates, uniqueness, diversity measures, and sensitivity to backdoor inputs) to characterize when and how failures occur. However, the paper stops short of providing deeper causal explanations that directly link discovered failures to broader underlying vulnerabilities (e.g., explicit attribution to spurious features, dataset bias, or overfitting) \u2014 instead it shows correlations and internal-activation patterns and uses those to detect and characterize failures.",
                    "evidence": "Page 1191, Abstract: \"TESTRNN enables meaningful information to be collected and exhibited for users to understand the testing results, which is an important step toward interpretable neural network testing.\" \n\nPage 1194, Section IV (LSTM Test Coverage Metrics): \"These metrics take into account both the values of structural information \u03be_{s,a}^t for s \u2208 S and a \u2208 A ... and their stepwise and bounded-length temporal relations.\" \n\nPage 1199, Section VII-C1 (Searching for Adversarial Samples): \"Fig. 5 reveals that normal perturbed samples can only satisfy part of test conditions, whereas the rest are more sensitive to the adversarial samples... coverage of RNN test metrics can be further increased in consideration of adversarial samples.\" \n\nPage 1201, Section VII-C1/Table V (Robustness comparison): \"As shown in Table V, adversarial samples rate and number of unique adversarial samples in the generated test suite are two important indicators for the robustness evaluation.\" \n\nPage 1201, Section VII-C2 (Detecting Backdoor Input in RNNs): \"The experimental results describe that backdoor input activate same internal behavior with clean input for a benign classifier. In contrast to this, the backdoor input to malicious classifier will induce different internal activation, which can be seen from the apparent increase of coverage in T+B.\" \n\nPage 1203, Section VII-G (Exhibition of Internal Working Mechanism): \"The working mechanism of LSTM networks can be understood via the test cases generated from TESTRNN. ... Coverage times denote the number of times a test condition is satisfied by running the test suite. Intuitively, coverage times represent the level of difficulty of asserting an input feature.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Linnks faulty behaviors to internal LSTM mechanisms. Correlates coverage with faults. identify where and how the failures arise internally. The analysis is correlational, not causal. ",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility in individual mutation steps using domain-specific, semantics-preserving mutation policies (e.g., EDA-based text mutations, SMILES enumeration for molecules) and by defining an oracle via norm-balls (r_oracle) intended to capture human-indistinguishability for continuous inputs. These design choices validate that many generated inputs remain realistic. However, the paper does not present any explicit mechanism to prevent cumulative drift across multiple mutation iterations (i.e., it does not enforce a global bound or filtering of plausibility across generations), nor does it report empirical human evaluation or task-specific perceptual/fluency metrics (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) to quantify the perceptual or semantic realism of the final fault-inducing inputs. Therefore the work enforces plausibility at the mutation-step / domain-rule level and validates plausibility conceptually, but overlooks cumulative effects and lacks quantitative or human-subjective evidence for final test cases.",
                    "evidence": "\u2022 Abstract (page 1191): \u201c...owing to its working with finer temporal semantics and the consideration of the naturalness of input perturbation.\u201d\n\n\u2022 Section VI-B, Mutation Policies \u2014 Random Mutation (page 1197): \u201cWhen the LSTM input has continuous values (e.g., image input), Gaussian noises with fixed mean and variance are added to the input. Meanwhile, for discrete value input (e.g., IMDB movie reviews for sentiment analysis), a set of problem-specific mutation functions M are defined.\u201d\n\n\u2022 Section VII-A, Experimental Setup \u2014 Input Mutation (page 1199): \u201cTo avoid this, we take a set M of mutants from the EDA toolkit [48], which was originally designed to augment the training data for improvement on text classification tasks. This ensures the mutated text paragraphs are always valid.\u201d\n\n\u2022 Section VI-C1, Test Oracle (page 1198): \u201cWe define a set of norm-balls, each of which is centered around a data sample with known label. The radius r_oracle of norm-balls intuitively means that a human cannot differentiate between inputs within a norm ball.\u201d\n\n\u2022 Section VII-A4, Oracle Setting (page 1199 / Table II discussion): \u201cNote, we let r_oracle = None for the Lipophilicity model, suggesting that no constraint is imposed on the norm ball.\u201d\n\n\u2022 Algorithm 1 (page 1197): shows orig mapping initialized for seeds and used only in the oracle check (detection): \u201corig[x] <- x for all x \u2208 T0\u201d and later \u201cif ||orig[x] - x'||2 \u2264 r_oracle, and \u03c6(orig[x]) \u2260 \u03c6(x') then Tadv \u2190 Tadv \u222a {x'}\u201d, indicating the paper checks distance to original seeds to label adversarial cases but does not describe rejecting or constraining candidates across successive mutation iterations to prevent cumulative drift.\n\n\u2022 Introduction / Motivation (page 1192): the authors assert their goal that generated cases be realistic: \u201c...our diversified test cases are 'closer' to their seeds (points on the RNN\u2019s data manifold), compared to other state-of-the-art tools, implying higher chance to be seen in the real-life operation, thus preserving the naturalness.\u201d\n\n\u2022 Absence of empirical perceptual/fluency evaluation: The paper does not report any human studies or quantitative perceptual/semantic measures (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) for assessing the realism or naturalness of final fault-inducing inputs (no such metrics or human-evaluation results appear in the Experimental Evaluation sections or tables)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility through bounded perturbations and domain-specific, rule-based mutation strategies designed to preserve semantic validity. For text-based inputs (IMDB dataset), it explicitly uses the EDA toolkit, which performs operations like synonym replacement to ensure the mutated text remains valid and meaningful. For molecular data (Lipophilicity dataset), mutations are constrained to different string representations of the same molecular structure. For continuous inputs like images (MNIST), it relies on bounded Gaussian noise and an L2-norm oracle, asserting that inputs within the norm-ball are indistinguishable to humans. The authors also provide a qualitative validation by comparing a 'natural' and comprehensible text sample generated by their tool against an incomprehensible one from a gradient-based attack (Figure 6). This approach aligns with the 'Medium' definition because it enforces plausibility through bounded/rule-based constraints and provides some validation of the final inputs. However, it does not provide a rigorous empirical evaluation of plausibility (e.g., via human studies or quantitative metrics like SSIM or Perplexity) and does not explicitly address the cumulative effects of mutations across iterations, especially for the continuous image and video domains.",
                    "evidence": "Page 9, Section VII-A, '3) Input Mutation': 'The input to IMDB model is a sequence of words, on which a random change may lead to an unrecognizable (and invalid) text paragraph. To avoid this, we take a set M of mutants from the EDA toolkit [48], which was originally designed to augment the training data for improvement on text classification tasks. This ensures the mutated text paragraphs are always valid.'\n\nPage 12, Section VII-E, 'E. Comparison With Attack-Based Defect Detection (RQ6)': 'It is easy to see that the adversarial review returned by the gradient attack is hard to comprehend, whereas the one from TESTRNN is much easier.' This is supported by the visual comparison in Figure 6.\n\nPage 7, Section VI-C, '1) Test Oracle': 'The radius roracle of norm-balls intuitively means that a human cannot differentiate between inputs within a norm ball. In this article, Euclidean distance, i.e., L2-norm || \u00b7 ||2 is used.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility in individual mutation steps using domain-specific, semantics-preserving mutation policies (e.g., EDA-based text mutations, SMILES enumeration for molecules) and by defining an oracle via norm-balls (r_oracle) intended to capture human-indistinguishability for continuous inputs. These design choices validate that many generated inputs remain realistic. However, the paper does not present any explicit mechanism to prevent cumulative drift across multiple mutation iterations (i.e., it does not enforce a global bound or filtering of plausibility across generations), nor does it report empirical human evaluation or task-specific perceptual/fluency metrics (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) to quantify the perceptual or semantic realism of the final fault-inducing inputs. Therefore the work enforces plausibility at the mutation-step / domain-rule level and validates plausibility conceptually, but overlooks cumulative effects and lacks quantitative or human-subjective evidence for final test cases.",
                    "evidence": "\u2022 Abstract (page 1191): \u201c...owing to its working with finer temporal semantics and the consideration of the naturalness of input perturbation.\u201d\n\n\u2022 Section VI-B, Mutation Policies \u2014 Random Mutation (page 1197): \u201cWhen the LSTM input has continuous values (e.g., image input), Gaussian noises with fixed mean and variance are added to the input. Meanwhile, for discrete value input (e.g., IMDB movie reviews for sentiment analysis), a set of problem-specific mutation functions M are defined.\u201d\n\n\u2022 Section VII-A, Experimental Setup \u2014 Input Mutation (page 1199): \u201cTo avoid this, we take a set M of mutants from the EDA toolkit [48], which was originally designed to augment the training data for improvement on text classification tasks. This ensures the mutated text paragraphs are always valid.\u201d\n\n\u2022 Section VI-C1, Test Oracle (page 1198): \u201cWe define a set of norm-balls, each of which is centered around a data sample with known label. The radius r_oracle of norm-balls intuitively means that a human cannot differentiate between inputs within a norm ball.\u201d\n\n\u2022 Section VII-A4, Oracle Setting (page 1199 / Table II discussion): \u201cNote, we let r_oracle = None for the Lipophilicity model, suggesting that no constraint is imposed on the norm ball.\u201d\n\n\u2022 Algorithm 1 (page 1197): shows orig mapping initialized for seeds and used only in the oracle check (detection): \u201corig[x] <- x for all x \u2208 T0\u201d and later \u201cif ||orig[x] - x'||2 \u2264 r_oracle, and \u03c6(orig[x]) \u2260 \u03c6(x') then Tadv \u2190 Tadv \u222a {x'}\u201d, indicating the paper checks distance to original seeds to label adversarial cases but does not describe rejecting or constraining candidates across successive mutation iterations to prevent cumulative drift.\n\n\u2022 Introduction / Motivation (page 1192): the authors assert their goal that generated cases be realistic: \u201c...our diversified test cases are 'closer' to their seeds (points on the RNN\u2019s data manifold), compared to other state-of-the-art tools, implying higher chance to be seen in the real-life operation, thus preserving the naturalness.\u201d\n\n\u2022 Absence of empirical perceptual/fluency evaluation: The paper does not report any human studies or quantitative perceptual/semantic measures (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) for assessing the realism or naturalness of final fault-inducing inputs (no such metrics or human-evaluation results appear in the Experimental Evaluation sections or tables)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Constraint bounded mutation but not evidence if naturalness measure of the generated inputs",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper partially considers serialization/I/O effects but does not fully or explicitly enforce reproducibility under standard serialization (e.g., quantization/encoding). The authors take steps that help make mutations persistently valid for discrete/serialized inputs (text via EDA mutants; SMILES enumeration) and they apply rounding/clipping for image pixel ranges, which mitigate some I/O loss. However, they do not describe explicit handling of standard I/O serialization effects such as 8-bit quantization, encoder rounding/clipping from common formats (PNG/JPEG/WAV/MP3), nor do they state that they generate inputs directly in serialized file formats or verify that failures persist after typical save/load operations. These facts match the definition of Medium: partial consideration (clipping/semantic-preserving metamorphoses) but no explicit enforcement of reproducibility under serialization.",
                    "evidence": "VI.B.1 Random Mutation (page 1197): \"When the LSTM input has continuous values (e.g., image input), Gaussian noises with fixed mean and variance are added to the input.\"  VII.A.3 Input Mutation (page 1199): \"For MNIST model, we add Gaussian noise to input image and round off the decimals around 0 and 1 to make the pixel value stay within the value range.\"  VII.A.3 Input Mutation (page 1199): \"The input to IMDB model is a sequence of words... To avoid this, we take a set M of mutants from the EDA toolkit... This ensures the mutated text paragraphs are always valid.\"  VII.A.4 Oracle Setting (page 1199): \"For continuous inputs, such as images and videos, we calculate the Euclidean distance as the measurement of perturbation.\"  (No section or quote in the paper describes explicit handling or verification of standard I/O serialization effects such as 8-bit quantization or file-format encoding/decoding to ensure failures persist after save/load.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's methodology partially addresses failure reproducibility by ensuring mutations are valid in their respective data domains, but it does not explicitly enforce or verify that failures persist after a full serialization/deserialization cycle. For continuous image data (MNIST), the mutation process includes a step to \"round off the decimals around 0 and 1 to make the pixel value stay within the value range,\" which partially accounts for I/O effects like clipping and quantization. For discrete data like text (IMDB) and chemical representations (Lipophilicity), the paper employs specialized toolkits (EDA and RDKit) that generate valid outputs, which can be considered a form of semantic-preserving transformation. However, the test oracle evaluates the mutated input directly from its in-memory representation without a described serialization step to confirm the failure's persistence. This combination of applying transformations that consider data validity but lacking explicit I/O verification in the oracle aligns with the 'Medium' rating.",
                    "evidence": "Page 9, Section VII-A, \"Input Mutation\": \"For MNIST model, we add Gaussian noise to input image and round off the decimals around 0 and 1 to make the pixel value stay within the value range.\"\nPage 9, Section VII-A, \"Input Mutation\": \"To avoid this, we take a set M of mutants from the EDA toolkit [48]... This ensures the mutated text paragraphs are always valid.\"\nPage 9, Section VII-A, \"Input Mutation\": \"For Lipophilicity model, we take a set M of mutants that change the SMILES string without affecting the molecular structure it represents... The enumerated SMILES strings are the test cases.\"\nPage 8, Section VII-C, \"Test Oracle\": The oracle's definition relies on comparing the in-memory mutated input `x'` to the original input `x` without mentioning a serialization step: \"A test case x' is said to not pass the oracle if x' is within the norm-ball of some known sample x, i.e., ||x - x'||2 \u2264 roracle, and if x' has a different classification from x, i.e., p(x) \u2260 \u03c6(x').\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper partially considers serialization/I/O effects but does not fully or explicitly enforce reproducibility under standard serialization (e.g., quantization/encoding). The authors take steps that help make mutations persistently valid for discrete/serialized inputs (text via EDA mutants; SMILES enumeration) and they apply rounding/clipping for image pixel ranges, which mitigate some I/O loss. However, they do not describe explicit handling of standard I/O serialization effects such as 8-bit quantization, encoder rounding/clipping from common formats (PNG/JPEG/WAV/MP3), nor do they state that they generate inputs directly in serialized file formats or verify that failures persist after typical save/load operations. These facts match the definition of Medium: partial consideration (clipping/semantic-preserving metamorphoses) but no explicit enforcement of reproducibility under serialization.",
                    "evidence": "VI.B.1 Random Mutation (page 1197): \"When the LSTM input has continuous values (e.g., image input), Gaussian noises with fixed mean and variance are added to the input.\"  VII.A.3 Input Mutation (page 1199): \"For MNIST model, we add Gaussian noise to input image and round off the decimals around 0 and 1 to make the pixel value stay within the value range.\"  VII.A.3 Input Mutation (page 1199): \"The input to IMDB model is a sequence of words... To avoid this, we take a set M of mutants from the EDA toolkit... This ensures the mutated text paragraphs are always valid.\"  VII.A.4 Oracle Setting (page 1199): \"For continuous inputs, such as images and videos, we calculate the Euclidean distance as the measurement of perturbation.\"  (No section or quote in the paper describes explicit handling or verification of standard I/O serialization effects such as 8-bit quantization or file-format encoding/decoding to ensure failures persist after save/load.)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Partially handle IO reproducibility by clipping image within 0-1 range (normalized) but did not consider that floating point precision may be lost.",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. All test-case generation and evaluation are performed per-target model (seed inputs are mutated into tests for that same model), and the experiments measure adversarial/backdoor discovery rates per model rather than reusing fault-inducing inputs generated on a surrogate to test other models. The paper also does not describe any fuzzing design or mechanisms aimed at producing inputs intended to transfer across different model implementations performing the same task.",
                    "evidence": "Page 1197, Section VI (Algorithm 1 / Test generation): \"The test suite T is initialized with T0, a corpus of seed inputs (Line 1). New test cases are generated by mutating seed inputs. It keeps the traceability of test cases via a mapping orig that maps each test case generated back to its seed origin.\" \n\nPage 1201, Section VII-C (Detecting RNN Defects): \"As shown in Table V, adversarial samples rate and number of unique adversarial samples in the generated test suite are two important indicators for the robustness evaluation. ... For a set of trained models, we pursue the model, the test suite of which contains less amount of adversarial samples and unique adversarial samples.\" \n\nPage 1202\u20131203, Section VII-F (Comparison With State-of-the-Art Testing Methods): \"We start from 100 seeds drawn from training set and generate 100 000 test cases by DeepStellar and TESTRNN, respectively. The test suites are evaluated for the coverage rate and number of adversarial samples.\" \n\n(These quoted passages show the paper's testing/generation and evaluation are carried out per model/seed and report per-model adversarial/backdoor findings; the paper contains no experimental description or results where fault-inducing inputs crafted on one (surrogate) model are reused to test other models, nor any described mechanisms explicitly aimed at discovering transferable attacks.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a coverage-guided testing method called TESTRNN, which is a white-box approach that generates test cases to maximize structural coverage for a specific RNN model under test. The methodology is designed to operate on a single model at a time. The experimental evaluation, including the comparison of model robustness in Table V, applies the test generation process independently to different models to compare their individual robustness characteristics (e.g., number of adversarial samples found). The paper does not contain any experiments where fault-inducing inputs generated for one model are used to test a different model to assess transferability. Furthermore, the test generation algorithm is guided by internal coverage metrics of the specific model being tested and does not incorporate any mechanisms designed to promote the discovery of transferable attacks.",
                    "evidence": "Page 11, Section VII-C, Table V and accompanying text: The paper evaluates the robustness of two different models per task (e.g., Model 1 and Model 2 for MNIST). However, the methodology described involves running the test generation process separately for each model to compare their individual vulnerability. The text explains the goal as comparing models: \"For a set of trained models, we pursue the model, the test suite of which contains less amount of adversarial samples and unique adversarial samples. For example, we pick up model 2 for Lipophilicity prediction, since the values of two indicators are way smaller than that of model 1.\" This indicates a comparison of the models' robustness via separate testing runs, not a test of attack transferability between them."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. All test-case generation and evaluation are performed per-target model (seed inputs are mutated into tests for that same model), and the experiments measure adversarial/backdoor discovery rates per model rather than reusing fault-inducing inputs generated on a surrogate to test other models. The paper also does not describe any fuzzing design or mechanisms aimed at producing inputs intended to transfer across different model implementations performing the same task.",
                    "evidence": "Page 1197, Section VI (Algorithm 1 / Test generation): \"The test suite T is initialized with T0, a corpus of seed inputs (Line 1). New test cases are generated by mutating seed inputs. It keeps the traceability of test cases via a mapping orig that maps each test case generated back to its seed origin.\" \n\nPage 1201, Section VII-C (Detecting RNN Defects): \"As shown in Table V, adversarial samples rate and number of unique adversarial samples in the generated test suite are two important indicators for the robustness evaluation. ... For a set of trained models, we pursue the model, the test suite of which contains less amount of adversarial samples and unique adversarial samples.\" \n\nPage 1202\u20131203, Section VII-F (Comparison With State-of-the-Art Testing Methods): \"We start from 100 seeds drawn from training set and generate 100 000 test cases by DeepStellar and TESTRNN, respectively. The test suites are evaluated for the coverage rate and number of adversarial samples.\" \n\n(These quoted passages show the paper's testing/generation and evaluation are carried out per model/seed and report per-model adversarial/backdoor findings; the paper contains no experimental description or results where fault-inducing inputs crafted on one (surrogate) model are reused to test other models, nor any described mechanisms explicitly aimed at discovering transferable attacks.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Oracle-guided"
            ]
        }
    },
    "Li et al. - 2021 - Testing DNN-based Autonomous Driving Systems under Critical Environmental Conditions": {
        "key": "li2021testing",
        "name": "TACTIC",
        "year": "2021",
        "citation_count": "51",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper's TACTIC method uncovers behaviours that violate safety expectations of autonomous driving systems (large steering-angle divergences under realistic environmental conditions), i.e., unsafe/undesired outputs that could lead to accidents. However, the paper does not claim or demonstrate bypassing explicit safety, policy, or security mechanisms (it targets standard pre-trained driving models and measures steering divergences), nor does it target defended/safety-aligned models or security-critical violations like data leakage or unauthorized actions. Thus the uncovered failures are safety-impactful but not shown to bypass explicit defenses, matching the 'Medium' severity definition.",
                    "evidence": "Page 1, Introduction: \"erroneous or unexpected behaviours that can lead to catastrophic accidents\".\nPage 4, Section 3.2.2 Metamorphic Error Analysis: \"Then the MR is defined as |\u03b8o \u2212 \u03b8t| < \u03f5, where the \u03f5 is a user-defined error bound.\" and \"If the new driving scenes synthesised with this style vector have larger steering angle divergences compared with the original driving scenes, then the style vector is considered to be able to detect more erroneous behaviours.\"\nPage 5, Section 4.2 Results: \"Compared with Rc and DeepRoad, TACTIC detects many more erroneous behaviours in all environmental types for both four error bounds.\" (see Table 1 and Table 2 reporting large numbers of steering-angle errors at 10\u00b0, 20\u00b0, 30\u00b0, 40\u00b0 thresholds).\n(Section names and quotes taken directly from the paper: Introduction; 3.2.2 Metamorphic Error Analysis; 4.2 Comparison with Baselines on Effectiveness; Tables 1\u20132.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, TACTIC, is designed to uncover model robustness errors by identifying inconsistencies in the model's predictions. An 'erroneous behaviour' is explicitly defined using a metamorphic relation where the steering angle predicted for a scene with synthesized environmental conditions is compared to the steering angle for the original scene. A failure is registered if the difference between these two angles exceeds a predefined error bound (\u03b5). This approach directly corresponds to finding 'prediction inconsistencies under semantically preserving input mutations', which is the definition for the 'Low' severity level. The paper does not evaluate whether these steering angle divergences lead to concretely unsafe outcomes (like collisions) nor does it test against models with explicit safety or security mechanisms that could be bypassed.",
                    "evidence": "Page 5, Section 3.2.2. METAMORPHIC ERROR ANALYSIS.: \"Formally, let x_o be an original driving scene and x_t be a newly generated driving scene synthesised based on x_o in a target environmental type. The \u03b8_o and \u03b8_t represent the steering angles for x_o and x_t, respectively. Then the MR is defined as |\u03b8_o - \u03b8_t| < \u03b5, where the \u03b5 is a user-defined error bound.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The failures uncovered by TACTIC are solely model robustness errors\u2014steering\u2010angle prediction inconsistencies under semantically preserving input mutations (realistic environmental variations). The paper measures these errors via a metamorphic relation on steering angles and does not evaluate or demonstrate bypassing any explicit safety, policy, or security mechanisms in defended or safety\u2010aligned models, nor does it induce higher\u2010level unsafe behaviors (e.g., collisions, unauthorized actions, data leakage). Accordingly, under the metric definition, these are low\u2010severity robustness failures.",
                    "evidence": "Section 3.2.2 (Metamorphic Error Analysis): \u2018\u2018Formally, let x_o be an original driving scene and x_t be a newly generated driving scene \u2026 Then the MR is defined as |\u03b8_o - \u03b8_t| < \u03b5 \u2026 If the new driving scenes \u2026 have larger steering\u2010angle divergences \u2026 the style vector is considered to be able to detect more erroneous behaviours.\u2019\u2019\n\nNo sections in the paper describe testing against defended or safety\u2010aligned models, or any bypass of explicit safety/policy/security mechanisms."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The TACTIC method searches the style (environmental-condition) space to maximise detection of erroneous behaviours defined broadly (coverage increase and steering-angle divergence), not to steer the model to a specific, attacker-chosen output. Its fitness objective combines (1) coverage-based diversity and (2) mean steering-angle divergence (Fd), and the oracle is a metamorphic relation that flags large steering-angle divergences. Thus TACTIC intentionally discovers broader classes of unsafe/erroneous behaviours (e.g., large deviations in steering under critical environmental conditions) rather than producing a predefined target output (a specific steering value or explicit attacker-chosen command).",
                    "evidence": "Section 3 (The TACTIC Approach), p.3: \"Then TACTIC sets a search objective which is to maximise the number and diversity of erroneous behaviours and employs a search-based method, e.g., (1+1) Evolution Strategy (ES)... to identify the critical environmental conditions.\"; Section 3.2.2 (Metamorphic Error Analysis), p.4: \"the MR used in TACTIC is defined as that the steering angles should be consistent among the driving scenes transformed from the same ones by applying different style vectors... the MR is further relaxed to a steering angle divergence within an error bound.\"; Section 3.2.3 (Fitness Function Definition), p.4: \"The ability to detect diverse erroneous behaviours ... Fc(s) ... The ability to detect more erroneous behaviours ... Fd(s) = 1/|Io| sum_{x in Io} |f(x) - f(g(x,s))| ... F(s) = wc * Fc(s) + wd * norm(Fd(s)).\" These quotes show TACTIC aims to find broadly unsafe behaviours (coverage increases and steering divergences) rather than to induce a specific attacker-chosen output."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, TACTIC, performs untargeted exploration to find generic failures. The goal is to identify environmental conditions under which the autonomous driving system is 'more prone to errors'. An error is defined via a metamorphic relation, which checks for inconsistency in predictions. Specifically, it measures if the steering angle for a synthesized image diverges significantly from the steering angle for the original image. The search objective is to maximize this divergence and neuron coverage, not to steer the model towards a specific, predefined steering angle. This aligns with the definition of untargeted exploration for discovering generic failures like prediction inconsistencies.",
                    "evidence": "Page 5, Section 3.2.2, METAMORPHIC ERROR ANALYSIS: 'In particular, the MR used in TACTIC is defined as that the steering angles should be consistent among the driving scenes transformed from the same ones by applying different style vectors, i.e., retain behaviour consistency. ... Formally, let x_o be an original driving scene and x_t be a newly generated driving scene synthesised based on x_o in a target environmental type. The \u03b8_o and \u03b8_t represent the steering angles for x_o and x_t, respectively. Then the MR is defined as |\u03b8_o - \u03b8_t| < \u03f5, where the \u03f5 is a user-defined error bound.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "TACTIC does not steer its exploration toward any specific, attacker-chosen output. Instead, it performs an untargeted search over environmental condition styles to maximize generic fault metrics (neuron coverage increase and steering-angle divergence). Its oracle is a metamorphic relation that flags any significant deviation in steering angle rather than a match to a predefined target steering value, so it discovers generic misbehaviors rather than targeted attacks.",
                    "evidence": "Section 3 (The TACTIC Approach), p.3: \u201cThen TACTIC sets a search objective which is to maximise the number and diversity of erroneous behaviours\u2026.\u201d; Section 3.2.2 (Metamorphic Error Analysis), p.4: \u201cthe MR used in TACTIC is defined as that the steering angles should be consistent\u2026 the MR is further relaxed to a steering angle divergence within an error bound.\u201d; Section 3.2.3 (Fitness Function Definition), p.4: \u201cF(s) = w_c\u00b7F_c(s) + w_d\u00b7norm(F_d(s))\u201d, where F_d measures mean |\u03b8_o \u2013 \u03b8_t|, with no attacker-specified target output."
                },
                "manual": {
                    "value": "Low",
                    "why": "Finds inconsistent steering under environmental variations",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by measuring and using observable model-internal signals (neuron-level coverage metrics KMNC and NBC) as part of the search fitness and by reporting statistical patterns of failures (coverage achieved and numbers of errors across environmental types and error bounds). However, it does not explain why failures occur in terms of underlying model vulnerabilities (e.g., reliance on non-robust or spurious features, dataset bias, or overfitting) nor does it analyze specific neuron characteristics or layers to link failures to root causes. Therefore the work meets the criterion for Medium but not High.",
                    "evidence": "Page 4, Section 3.2.1 \"TESTING COVERAGE CRITERIA\": \"In this work, we employ DNN coverage criteria to measure the ability of a style vector to detect diverse erroneous behaviours... we carefully select two fine-grained coverage metrics on the sub-neuron level (Ma et al., 2018) for TACTIC to support, i.e., k-multisection Neuron Coverage (KMNC) and Neuron Boundary Coverage (NBC).\"; Page 5, Section 3.2.3 \"FITNESS FUNCTION DEFINITION\": \"F(s) = wc * Fc(s) + wd * norm(Fd(s)), where s is a style vector... Fc(s) = |Rs|/|Rt| ... Fd(s) = 1/|Io| sum_{x in Io} |f(x) - f(g(x,s))|.\"; Page 6, Section 4.2 \"Comparison with Baselines on Effectiveness\": \"Table 1 and Table 2 summarise the results of comparing TACTICNBC with Rc and DeepRoad, respectively... In terms of the achieved coverage, compared with Rc, TACTIC achieves higher coverage in all environmental types... In terms of the detected erroneous behaviours, compared with Rc and DeepRoad, TACTIC detects many more erroneous behaviours in all environmental types for both four error bounds.\" (Table 1 entries example) \"NBC 35.92% 3.18% ... NUMBER OF ERRORS 10\u00b0 18675.1 2971.2 ...\"; Page 8, Conclusion: \"Large-scale experiments demonstrate that TACTIC can effectively identify critical environmental conditions and synthesise realistic testing driving scenes. Compared to the state-of-the-art approaches, TACTIC can reveal more diverse and more erroneous behaviours for the popular DNN-based ADSs, and meanwhile, reach a satisfactory testing coverage.\" (no accompanying analysis linking failures to underlying model vulnerabilities)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by analyzing the correlation between observable model internals (DNN coverage) and the number of discovered failures. The core methodology uses DNN coverage metrics (KMNC and NBC) as part of its fitness function to guide the search for critical environmental conditions that cause erroneous behaviors. The experimental evaluation consistently presents the achieved coverage alongside the number of detected errors, demonstrating that their approach, which increases coverage, also reveals more diverse and numerous failures. This constitutes an analysis of model internals and their connection to failures. However, the paper does not extend this analysis to explain the root causes of these failures in terms of underlying model vulnerabilities such as reliance on spurious features, dataset bias, or overfitting.",
                    "evidence": "Page 6, Section 3.2.3. FITNESS FUNCTION DEFINITION: 'The ability to detect diverse erroneous behaviours of a style vector is measured by the increase in the testing coverage measured by KMNC or NBC. Let Rt be the set of uncovered KMNC/NBC regions so far during the searchbased testing process and Rs be the set of regions that will be newly covered by the synthesised driving scenes using style vector s. The ability of s to detect diverse erroneous behaviours is calculated as: Fc(s) = |Rs|/|Rt|'. \n\nPage 7, Section 4.2. Comparison with Baselines on Effectiveness, Results: 'In terms of the achieved coverage, compared with Rc, TACTIC achieves higher coverage in all environmental types, demonstrating that TACTIC can detect more diverse erroneous behaviours than Rc. ... In terms of the detected erroneous behaviours, compared with Rc and DeepRoad, TACTIC detects many more erroneous behaviours in all environmental types for both four error bounds.' This is further supported by Tables 1 and 2 which list coverage metrics and the number of errors side-by-side for comparison."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by measuring and using observable model-internal signals (neuron-level coverage metrics KMNC and NBC) as part of the search fitness and by reporting statistical patterns of failures (coverage achieved and numbers of errors across environmental types and error bounds). However, it does not explain why failures occur in terms of underlying model vulnerabilities (e.g., reliance on non-robust or spurious features, dataset bias, or overfitting) nor does it analyze specific neuron characteristics or layers to link failures to root causes. Therefore the work meets the criterion for Medium but not High.",
                    "evidence": "Page 4, Section 3.2.1 \"TESTING COVERAGE CRITERIA\": \"In this work, we employ DNN coverage criteria to measure the ability of a style vector to detect diverse erroneous behaviours... we carefully select two fine-grained coverage metrics on the sub-neuron level (Ma et al., 2018) for TACTIC to support, i.e., k-multisection Neuron Coverage (KMNC) and Neuron Boundary Coverage (NBC).\"; Page 5, Section 3.2.3 \"FITNESS FUNCTION DEFINITION\": \"F(s) = wc * Fc(s) + wd * norm(Fd(s)), where s is a style vector... Fc(s) = |Rs|/|Rt| ... Fd(s) = 1/|Io| sum_{x in Io} |f(x) - f(g(x,s))|.\"; Page 6, Section 4.2 \"Comparison with Baselines on Effectiveness\": \"Table 1 and Table 2 summarise the results of comparing TACTICNBC with Rc and DeepRoad, respectively... In terms of the achieved coverage, compared with Rc, TACTIC achieves higher coverage in all environmental types... In terms of the detected erroneous behaviours, compared with Rc and DeepRoad, TACTIC detects many more erroneous behaviours in all environmental types for both four error bounds.\" (Table 1 entries example) \"NBC 35.92% 3.18% ... NUMBER OF ERRORS 10\u00b0 18675.1 2971.2 ...\"; Page 8, Conclusion: \"Large-scale experiments demonstrate that TACTIC can effectively identify critical environmental conditions and synthesise realistic testing driving scenes. Compared to the state-of-the-art approaches, TACTIC can reveal more diverse and more erroneous behaviours for the popular DNN-based ADSs, and meanwhile, reach a satisfactory testing coverage.\" (no accompanying analysis linking failures to underlying model vulnerabilities)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Relies on coverage-guided mutation and assumes based on prior work, that higher coverage correlates with fault discovery",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper enforces input plausibility by performing all mutations/search inside a learned style space produced by a MUNIT image-to-image translation model: style vectors sampled or evolved from that space are decoded by the generator to produce test images, so mutated inputs remain drawn from a distribution of real environmental conditions captured by data. The authors also provide empirical human-evaluation evidence that the final synthesized, fault-inducing driving scenes are perceptually realistic (a user study with 34 participants showing TACTIC images are judged realistic and correctly classified to their environmental types). Together, these show (1) plausibility is enforced throughout the fuzzing/search process by constraining search to the MUNIT style space and (2) final fault-inducing inputs were validated as realistic via a user study.",
                    "evidence": "Page 1, Introduction: \"Because of the richness of styles, the synthesised scenes are realistic enough (as shown by Figure 1(d)) to test whether the DNN-based ADS can perform correctly under the environmental conditions corresponding to the obtained styles.\" \n\nPage 3, Section 3.1 (Environmental Conditions): \"To encode the environmental condition space of an environmental type, we leverage MUNIT ... One insight of MUNIT is that the representation of images can be decomposed into a content space that is domain-invariant and a style space that captures domain-specific characteristics ... we can use the style space of an environmental type in a well-trained MUNIT model to represent the environmental condition space of the type.\" \n\nPage 4, Section 3.3 (Search of Critical Environmental Conditions): \"The (1+1) ES is initialised with an initial individual, which is a style vector randomly sampled from the style space in the MUNIT model. ... a new individual s' is firstly generated based on the current individual s. Then, the fitness values of s and s' is evaluated ... the individual with higher fitness value is kept.\" \n\nPage 6, Section 4.3 (Comparison with Baselines on Image Quality / User Study): \"we conduct a user study to evaluate the realism of the testing driving scenes synthesised by TACTIC ... We received answers from a total of 34 participants ... The results demonstrate that the testing driving scenes synthesised by TACTIC are more realistic than the ones synthesised by affine image transformation (e.g., DeepTest) or high-fidelity simulation (e.g., PreScan) and can better reflect real environments.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's proposed method, TACTIC, is explicitly designed to generate plausible and realistic test inputs. It uses a GAN-based image-to-image translation model (MUNIT) trained on real-world images of various environmental conditions, which inherently constrains the generated outputs to a plausible distribution. The paper goes further and provides direct empirical evidence of this plausibility through a dedicated user study. This study compares the realism of scenes generated by TACTIC against real-world images and other baseline methods. The results show that a significant portion of TACTIC's generated images were considered as realistic as real-world images by human participants. This combination of using a plausibility-aware generation mechanism and empirically validating the realism of the final fault-inducing inputs with a human study directly meets the criteria for a 'High' rating.",
                    "evidence": "Page 8, Section 4.3, 'Comparison with Baselines on Image Quality': 'We conduct a user study to evaluate the realism of the testing driving scenes synthesised by TACTIC... Specifically, we design an online questionnaire consisting of two questions: (1) \u201cWhich driving scene is more realistic?\u201d, and (2) \u201cWhich environmental type does the driving scene belong to?\u201d.'\nPage 8, Section 4.3, Results: 'The results demonstrate that the testing driving scenes synthesised by TACTIC are more realistic than the ones synthesised by affine image transformation (e.g., DeepTest) or high-fidelity simulation (e.g., PreScan) and can better reflect real environments.'\nPage 9, Figure 5 and accompanying text: The results of the user study are presented, showing that for 44% of image pairs (27% 'both realistic' + 16% 'synthesized scene' + 1% 'both unrealistic' is likely a typo in my reading, the text says 44% for TACTIC and DeepRoad where synthesized are not distinguishable from real ones), TACTIC's synthesized scenes were considered as realistic or more realistic than real-world scenes, a stark contrast to DeepTest (6%) and PreScan (14%)."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper enforces input plausibility by performing all mutations/search inside a learned style space produced by a MUNIT image-to-image translation model: style vectors sampled or evolved from that space are decoded by the generator to produce test images, so mutated inputs remain drawn from a distribution of real environmental conditions captured by data. The authors also provide empirical human-evaluation evidence that the final synthesized, fault-inducing driving scenes are perceptually realistic (a user study with 34 participants showing TACTIC images are judged realistic and correctly classified to their environmental types). Together, these show (1) plausibility is enforced throughout the fuzzing/search process by constraining search to the MUNIT style space and (2) final fault-inducing inputs were validated as realistic via a user study.",
                    "evidence": "Page 1, Introduction: \"Because of the richness of styles, the synthesised scenes are realistic enough (as shown by Figure 1(d)) to test whether the DNN-based ADS can perform correctly under the environmental conditions corresponding to the obtained styles.\" \n\nPage 3, Section 3.1 (Environmental Conditions): \"To encode the environmental condition space of an environmental type, we leverage MUNIT ... One insight of MUNIT is that the representation of images can be decomposed into a content space that is domain-invariant and a style space that captures domain-specific characteristics ... we can use the style space of an environmental type in a well-trained MUNIT model to represent the environmental condition space of the type.\" \n\nPage 4, Section 3.3 (Search of Critical Environmental Conditions): \"The (1+1) ES is initialised with an initial individual, which is a style vector randomly sampled from the style space in the MUNIT model. ... a new individual s' is firstly generated based on the current individual s. Then, the fitness values of s and s' is evaluated ... the individual with higher fitness value is kept.\" \n\nPage 6, Section 4.3 (Comparison with Baselines on Image Quality / User Study): \"we conduct a user study to evaluate the realism of the testing driving scenes synthesised by TACTIC ... We received answers from a total of 34 participants ... The results demonstrate that the testing driving scenes synthesised by TACTIC are more realistic than the ones synthesised by affine image transformation (e.g., DeepTest) or high-fidelity simulation (e.g., PreScan) and can better reflect real environments.\""
                },
                "manual": {
                    "value": "High",
                    "why": "Test images are generated through a GAN trained to mimic real driving scenes, so naturalness is preserved throughout fuzzing iterations. Provides human evaluation.",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper generates test inputs by applying learned, semantic image-to-image (MUNIT) transformations to produce full synthesized driving images and uses a metamorphic relation (steering-angle consistency) as the oracle. These are coarse, perceptible, semantic-preserving transformations that are less likely to rely on fragile, in-memory floating-point perturbations \u2014 which corresponds to the metric's notion that metamorphic transformations provide only partial enforcement of reproducibility. However, the paper does not explicitly reason about or constrain serialization effects (clipping, quantization, file encoding) nor does it state that it enforces that fault-inducing perturbations survive standard I/O; there is no discussion of handling rounding/quantization/clipping during generation or oracle evaluation. Because TACTIC relies on image-level (GAN) synthesis and metamorphic checking (partial robustness) but does not explicitly account for standard I/O serialization effects, the appropriate rating is Medium.",
                    "evidence": "Section 3.1 (Environmental Conditions), paper: \"To encode the environmental condition space of an environmental type, we leverage MUNIT, a multimodal unsupervised image-to-image translation framework based on Generative Adversarial Networks (GAN).\" and \"Then, to produce x2, M recombines the content c1 with a style vector s2 sampled from the style space S2 of X2 by the decoder G2 of X2, i.e., x2 = G2(c1, s2).\" (Section 3.1) ; Section 3.2.2 (Metamorphic Error Analysis), paper: \"Following the existing work (Tian et al., 2018), we also adopt Metamorphic Relation (MR) (Chen et al., 2020) as the test oracle ... the MR is defined as |\u03b8o \u2212 \u03b8t| < \u03b5, where the \u03b5 is a user-defined error bound.\" (Section 3.2.2) ; Section 4.2 (Setup/Experiments), paper: \"4 critical environmental conditions are generated ... which are separately applied on the Udacity testing dataset to synthesise testing driving scenes (i.e., 4 \u00d7 5614 testing driving scenes are totally generated).\" (Section 4.2) ; \u2014 Nowhere in the method or evaluation (Sections 3 and 4) does the paper discuss handling of standard I/O serialization effects (e.g., clipping, quantization, rounding, image file encoding) or constraining perturbations so they explicitly survive serialization during test generation or oracle evaluation."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's proposed method, TACTIC, uses a GAN-based image-to-image translation model (MUNIT) to apply semantic-preserving metamorphic transformations, specifically altering the environmental conditions (e.g., weather, time of day) of driving scenes. According to the metric definition, using such transformations without explicit enforcement of reproducibility under serialization (e.g., quantization, clipping) is categorized as 'Medium'. The paper does not describe any steps to ensure that the pixel-level changes introduced by the GAN would persist after standard I/O operations, such as saving the image to an 8-bit format like PNG. The focus is on the realism and bug-finding effectiveness of the generated scenes, but there is no mention of constraining the mutations or verifying the oracle's findings post-serialization to account for potential loss of fault-inducing perturbations.",
                    "evidence": "Page 3, Section 3.1, Environmental Conditions: \"To encode the environmental condition space of an environmental type, we leverage MUNIT, a multimodal unsupervised image-to-image translation framework based on Generative Adversarial Networks (GAN).\" This confirms the use of a generative model for transformation. \n\nPage 5, Figure 4: This figure illustrates the semantic nature of the transformations, showing an original scene transformed into 'Rain' and 'Snow in Daytime' versions. \n\nThe paper's methodology sections (3. The TACTIC Approach) and evaluation sections (4. Experimental Evaluation) lack any discussion of handling serialization effects. The metamorphic oracle described in Section 3.2.2 operates on the generated scene `xt` directly, without any mention of an intermediate serialization step to confirm the failure's persistence."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper generates test inputs by applying learned, semantic image-to-image (MUNIT) transformations to produce full synthesized driving images and uses a metamorphic relation (steering-angle consistency) as the oracle. These are coarse, perceptible, semantic-preserving transformations that are less likely to rely on fragile, in-memory floating-point perturbations \u2014 which corresponds to the metric's notion that metamorphic transformations provide only partial enforcement of reproducibility. However, the paper does not explicitly reason about or constrain serialization effects (clipping, quantization, file encoding) nor does it state that it enforces that fault-inducing perturbations survive standard I/O; there is no discussion of handling rounding/quantization/clipping during generation or oracle evaluation. Because TACTIC relies on image-level (GAN) synthesis and metamorphic checking (partial robustness) but does not explicitly account for standard I/O serialization effects, the appropriate rating is Medium.",
                    "evidence": "Section 3.1 (Environmental Conditions), paper: \"To encode the environmental condition space of an environmental type, we leverage MUNIT, a multimodal unsupervised image-to-image translation framework based on Generative Adversarial Networks (GAN).\" and \"Then, to produce x2, M recombines the content c1 with a style vector s2 sampled from the style space S2 of X2 by the decoder G2 of X2, i.e., x2 = G2(c1, s2).\" (Section 3.1) ; Section 3.2.2 (Metamorphic Error Analysis), paper: \"Following the existing work (Tian et al., 2018), we also adopt Metamorphic Relation (MR) (Chen et al., 2020) as the test oracle ... the MR is defined as |\u03b8o \u2212 \u03b8t| < \u03b5, where the \u03b5 is a user-defined error bound.\" (Section 3.2.2) ; Section 4.2 (Setup/Experiments), paper: \"4 critical environmental conditions are generated ... which are separately applied on the Udacity testing dataset to synthesise testing driving scenes (i.e., 4 \u00d7 5614 testing driving scenes are totally generated).\" (Section 4.2) ; \u2014 Nowhere in the method or evaluation (Sections 3 and 4) does the paper discuss handling of standard I/O serialization effects (e.g., clipping, quantization, rounding, image file encoding) or constraining perturbations so they explicitly survive serialization during test generation or oracle evaluation."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Apply metamorphic transformation using GAN model but does not consider whether GAN transformed images are still IO reproducible (no mention of clipping/rounding)",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate transferability of fault-inducing inputs across models. TACTIC is defined and executed per subject model (the search takes the subject DNN as input and returns critical conditions for that model), and all experiments generate and evaluate critical styles separately for each ADS. There is no experiment that crafts fault-inducing inputs on one surrogate model and reuses them to test other models, nor does the design include explicit mechanisms aimed at finding transferable attacks across different model implementations.",
                    "evidence": "Page 3, Section 3.3 \"Search of Critical Environmental Conditions\": \"TACTIC receives as inputs the subject DNN-based ADS N, the MUNIT model M trained for the given environmental type, and a set Io of the original driving scenes. The output of TACTIC is a set S of the critical environmental conditions of the given environmental type.\"; Page 4, Section 4.2 \"Comparison with Baselines on Effectiveness - Setup\": \"We execute TACTIC with two coverage-guiding strategies: KMNC (denoted as TACTICKMNC) and NBC (denoted as TACTICNBC), respectively, on each of the three subject DNN-based ADSs, under the five environmental types.\"; Page 4, Section 4.2 \"Setup\" (describing baselines): \"For Rc, when testing the three subject systems, we randomly sample 4 style vectors (equal to the number of critical style vectors generated by TACTIC) for each of the environmental types and separately apply the 4 random style vectors on the Udacity testing dataset to synthesise testing driving scenes...\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a method, TACTIC, to identify critical environmental conditions for a specific, or 'subject', Deep Neural Network (DNN) based Autonomous Driving System (ADS). The search for these conditions is guided by model-specific metrics, such as neuron coverage and steering angle divergence, which are calculated on the subject model. The experimental evaluation involves three different ADS models (Dave-orig, Dave-dropout, Chauffeur). However, the TACTIC framework is executed independently for each model to find its specific critical conditions. The paper does not contain any experiments or discussion on whether the fault-inducing environmental conditions discovered for one model also cause erroneous behavior in the other models. Therefore, it does not demonstrate the transferability of the discovered failures.",
                    "evidence": "Page 6, Section 3.3, \"Search of Critical Environmental Conditions\": \"TACTIC receives as inputs the subject DNN-based ADS N, the MUNIT model M trained for the given environmental type, and a set Io of the original driving scenes.\" This statement clarifies that the method is designed to operate on a single 'subject' model 'N' at a time. \n\nPage 7, Section 4.2, \"Comparison with Baselines on Effectiveness\", paragraph \"Setup\": \"We execute TACTIC with two coverage-guiding strategies: KMNC (denoted as TACTIC KMNC ) and NBC (denoted as TACTIC NBC ), respectively, on each of the three subject DNN-based ADSs, under the five environmental types.\" This shows that the experiments were run separately for each of the three models, rather than generating inputs on one and testing them on others. The results are then presented independently for each model, confirming the lack of a transferability analysis."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate transferability of fault-inducing inputs across models. TACTIC is defined and executed per subject model (the search takes the subject DNN as input and returns critical conditions for that model), and all experiments generate and evaluate critical styles separately for each ADS. There is no experiment that crafts fault-inducing inputs on one surrogate model and reuses them to test other models, nor does the design include explicit mechanisms aimed at finding transferable attacks across different model implementations.",
                    "evidence": "Page 3, Section 3.3 \"Search of Critical Environmental Conditions\": \"TACTIC receives as inputs the subject DNN-based ADS N, the MUNIT model M trained for the given environmental type, and a set Io of the original driving scenes. The output of TACTIC is a set S of the critical environmental conditions of the given environmental type.\"; Page 4, Section 4.2 \"Comparison with Baselines on Effectiveness - Setup\": \"We execute TACTIC with two coverage-guiding strategies: KMNC (denoted as TACTICKMNC) and NBC (denoted as TACTICNBC), respectively, on each of the three subject DNN-based ADSs, under the five environmental types.\"; Page 4, Section 4.2 \"Setup\" (describing baselines): \"For Rc, when testing the three subject systems, we randomly sample 4 style vectors (equal to the number of critical style vectors generated by TACTIC) for each of the environmental types and separately apply the 4 random style vectors on the Udacity testing dataset to synthesise testing driving scenes...\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Oracle-guided"
            ]
        }
    },
    "Wang et al. - 2023 - DistXplore Distribution-Guided Testing for Evaluating and Enhancing Deep Learning Systems": {
        "key": "wang2023distxplore",
        "name": "DistXplore",
        "year": "2023",
        "citation_count": "13",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's method (DistXplore) is explicitly framed as a distribution-guided testing technique for finding misclassified inputs and improving model robustness. All objectives, metrics, and evaluations focus on generating \u2018\u2018erroneous inputs\u2019\u2019 (i.e., mispredictions / misclassifications), making them hard to detect by adversarial-example detectors and using them to enrich training data. The paper evaluates the ability to bypass detection by defenses (Dissector, A2D) and to produce hard-to-detect misclassification errors, but it does not claim to uncover or induce high-impact safety or security failures such as jailbreaks, data leakage, unauthorized actions, or toxic/hallucinatory outputs in safety-aligned models. Therefore according to the metric definitions, DistXplore uncovers only model robustness errors (mispredictions) and should be rated Low.",
                    "evidence": "Page 69, Introduction: \"DL testing aims to generate test cases that evaluate the robustness of DL systems, i.e., discover the data that is valid but cannot be predicted correctly (e.g., the dataset d and e in Fig. 1), and enhance the robustness...\" \n\nPage 71, Section 3.1.1 (Model Evaluation): \"To evaluate the model\u2019s robustness, we aim to generate the erroneous inputs that are hard to be detected by existing defense techniques.\" \n\nPage 72, Section 4 (Setup), RQ1: \"RQ1: How effective is DistXplore in detecting errors that can bypass the defense methods?\" and footnote: \"The error in the paper refers to the erroneous inputs that are missclassi\ufb01ed.\" \n\nPage 75, Section 4.2.1 (Results): \"The overall results show that DistXplore (column DistX) can generate more strong errors that are difficult to be detected by defense techniques compared with baselines.\" \n\n(These quotations show the paper focuses on misclassification/robustness errors and evasion of adversarial detectors, not on bypassing safety/policy mechanisms or producing unsafe/ high-impact behaviors.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, DistXplore, focuses exclusively on generating inputs that are misclassified by the model, which directly corresponds to the definition of 'model robustness errors' in the metric. A footnote explicitly defines an 'error' as an 'erroneous input that is missclassified'. The entire evaluation framework is built around discovering and quantifying these mispredictions. While the paper claims these errors can bypass 'defense methods' (Dissector and A2D), these methods are adversarial example detectors designed to identify perturbed inputs, not explicit safety or security mechanisms that prevent unsafe behavior like toxicity, bias, or jailbreaks. The paper does not present any evidence of generating unsafe or toxic content, nor does it attempt to bypass safety-aligned policies. Therefore, the failures uncovered are limited to robustness errors.",
                    "evidence": "Page 6, Section 4, Footnote 1: 'The error in the paper refers to the erroneous inputs that are missclassified.'\n\nPage 5, Section 3.1.1 Model Evaluation: 'To evaluate the model\u2019s robustness, we aim to generate the erroneous inputs that are hard to be detected by existing defense techniques... Thus, DistXplore aims to generate test cases (in a class) that are statistically similar to the training data in other classes.'\n\nPage 7, Section 4.1.3 Defense Methods: This section describes the defense methods used for evaluation (Dissector and A2D). Both are designed to 'detect adversarial examples' or 'adversarial samples', not to enforce safety policies or prevent unsafe content generation."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's method (DistXplore) is explicitly framed as a distribution-guided testing technique for finding misclassified inputs and improving model robustness. All objectives, metrics, and evaluations focus on generating \u2018\u2018erroneous inputs\u2019\u2019 (i.e., mispredictions / misclassifications), making them hard to detect by adversarial-example detectors and using them to enrich training data. The paper evaluates the ability to bypass detection by defenses (Dissector, A2D) and to produce hard-to-detect misclassification errors, but it does not claim to uncover or induce high-impact safety or security failures such as jailbreaks, data leakage, unauthorized actions, or toxic/hallucinatory outputs in safety-aligned models. Therefore according to the metric definitions, DistXplore uncovers only model robustness errors (mispredictions) and should be rated Low.",
                    "evidence": "Page 69, Introduction: \"DL testing aims to generate test cases that evaluate the robustness of DL systems, i.e., discover the data that is valid but cannot be predicted correctly (e.g., the dataset d and e in Fig. 1), and enhance the robustness...\" \n\nPage 71, Section 3.1.1 (Model Evaluation): \"To evaluate the model\u2019s robustness, we aim to generate the erroneous inputs that are hard to be detected by existing defense techniques.\" \n\nPage 72, Section 4 (Setup), RQ1: \"RQ1: How effective is DistXplore in detecting errors that can bypass the defense methods?\" and footnote: \"The error in the paper refers to the erroneous inputs that are missclassi\ufb01ed.\" \n\nPage 75, Section 4.2.1 (Results): \"The overall results show that DistXplore (column DistX) can generate more strong errors that are difficult to be detected by defense techniques compared with baselines.\" \n\n(These quotations show the paper focuses on misclassification/robustness errors and evasion of adversarial detectors, not on bypassing safety/policy mechanisms or producing unsafe/ high-impact behaviors.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly guides test generation toward predefined target classes (chosen by the tester) by minimizing the distribution difference (MMD) between a source-class test suite and the training data of a specified target class. DistXplore's objective, algorithm inputs, and experimental setup all use a target class c' as a steering signal, and the authors show that decreasing the MMD to a chosen target class increases the proportion of errors classified as that target (Error_target Rate). Thus the fuzzer actively steers exploration to induce attacker-chosen outputs (target labels) rather than performing untargeted fault discovery.",
                    "evidence": "Page 71, Section 3.1.1 (Model Evaluation): \"Thus, DistXplore aims to generate test cases (in a class) that are statistically similar to the training data in other classes.\"; Page 71, Section 3.1.1 (definition): \"given a DNN f and a test suite S belonging to a source class c, we define its distribution difference with respect to the training data (T_{c'}) in another target class c' as: DF(S, c') = MMD(f(S), f(T_{c'}))\"; Page 72, Section 3.2 / Algorithm 1 (inputs): Algorithm 1 Input lists \"f : the target DNN, S : a seed test suite from class c, c' : the target class, phi : target distribution difference\" showing c' is an explicit algorithm input; Page 73, Section 4.1.4 (Experiment setup): \"For each class c, we run DistXplore multiple times (i.e., 9) by setting different target classes c' with Algo. 1. Finally, for each model, we run DistXplore 90 times (i.e., 10 source classes \u00d7 9 target classes).\"; Page 75, Section 4.2.1 (results / analysis): \"We collect the best chromosome S after each iteration and calculate: ... 4) Error_target Rate: the proportion of errors (in S) predicted as the target class ... The results show that, during the optimization, the distribution of S is getting closer to the training distribution of the target class (see MMD_target) ... Meanwhile, Error Rate and Error_target Rate are increasing, indicating that more errors are generated and gradually become statistically indistinguishable between the original class c and target class c'.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's proposed method, DistXplore, is explicitly designed for targeted attack discovery. The core mechanism involves selecting a source class and a predefined target class. The fuzzer's objective is to generate test cases from the source class that are statistically similar to the training data of the chosen target class. This is achieved by minimizing the Maximum Mean Discrepancy (MMD) between the generated test suite and the training data of the target class. This design directly steers the exploration toward inducing the model to misclassify an input into a specific, attacker-chosen output class, which aligns perfectly with the 'High' rating for this metric.",
                    "evidence": "Page 5, Section 3.1.1, Model Evaluation: \"Thus, DistXplore aims to generate test cases (in a class) that are statistically similar to the training data in other classes. Formally, given a DNN f and a test suite Sc belonging to a source class c, we define its distribution difference with respect to the training data (Tc') in another target class c' as: DFf(Sc, c') = MMD(fi(Sc), fi(Tc')) where fi refers to the output of the layer l and c' \u2260 c.\"\n\nPage 6, Section 3.2, Distribution-Guided Test Generation: \"The objective function can be defined as DFf(Sc, c') \u2248 v... where Sc is the test suite belonging to c, c' is a target class... For the goal of model evaluation, v is set as 0, i.e., to generate Sc that is statistically indistinguishable from the training data in c'.\"\n\nPage 6, Algorithm 1: The algorithm's inputs are listed as: \"Input: f: the target DNN, Sc: a seed test suite from class c, c': the target class, v: target distribution difference\"."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly guides test generation toward predefined target classes (chosen by the tester) by minimizing the distribution difference (MMD) between a source-class test suite and the training data of a specified target class. DistXplore's objective, algorithm inputs, and experimental setup all use a target class c' as a steering signal, and the authors show that decreasing the MMD to a chosen target class increases the proportion of errors classified as that target (Error_target Rate). Thus the fuzzer actively steers exploration to induce attacker-chosen outputs (target labels) rather than performing untargeted fault discovery.",
                    "evidence": "Page 71, Section 3.1.1 (Model Evaluation): \"Thus, DistXplore aims to generate test cases (in a class) that are statistically similar to the training data in other classes.\"; Page 71, Section 3.1.1 (definition): \"given a DNN f and a test suite S belonging to a source class c, we define its distribution difference with respect to the training data (T_{c'}) in another target class c' as: DF(S, c') = MMD(f(S), f(T_{c'}))\"; Page 72, Section 3.2 / Algorithm 1 (inputs): Algorithm 1 Input lists \"f : the target DNN, S : a seed test suite from class c, c' : the target class, phi : target distribution difference\" showing c' is an explicit algorithm input; Page 73, Section 4.1.4 (Experiment setup): \"For each class c, we run DistXplore multiple times (i.e., 9) by setting different target classes c' with Algo. 1. Finally, for each model, we run DistXplore 90 times (i.e., 10 source classes \u00d7 9 target classes).\"; Page 75, Section 4.2.1 (results / analysis): \"We collect the best chromosome S after each iteration and calculate: ... 4) Error_target Rate: the proportion of errors (in S) predicted as the target class ... The results show that, during the optimization, the distribution of S is getting closer to the training distribution of the target class (see MMD_target) ... Meanwhile, Error Rate and Error_target Rate are increasing, indicating that more errors are generated and gradually become statistically indistinguishable between the original class c and target class c'.\""
                },
                "manual": {
                    "value": "High",
                    "why": "Guides test generation to minimize distribution differences between source class inputs and target class.",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight into why failures occur by analyzing observable model internals (logits-layer activations) via distribution-distance (MMD) and by reporting statistical failure patterns (e.g., Error Rate, Error_target Rate, detection rates versus MMD). DistXplore measures MMD on the model's logits, tracks how MMD_target decreases while error rates and detection-avoidance increase, and uses class-level distribution comparisons and distribution-diversity analyses (RQ3/RQ4) to connect discovered failures to distribution shift vulnerabilities. However, the paper does not provide deeper causal analysis tying failures to specific spurious/non-robust features, neuron-level characteristics (beyond using the logits layer), or detailed per-neuron activation analyses, so it does not meet the bar for \"High.\"",
                    "evidence": "Page 71, Section 3.1.1 (Model Evaluation): \"Formally, given a DNN F and a test suite S belonging to a source class c, we define its distribution difference with respect to the training data (D_{T_{c'}}) in another target class c' as: DF(S, c') = MMD(f_c(S), f_{c'}(T_{c'})) ... The distribution difference is measured on a specific layer of the DNN. In this paper, we select the logits layer, i.e., the layer before the softmax layer, which is frequently used in previous works ... Intuitively, the smaller the value DF(S, c'), the more difficult it is for the model F to distinguish S and T_{c'}. Hence, it is more likely to generate undetectable errors by minimizing their distribution difference.\" (Section 3.1.1, p.71)\n\nPage 75, Section 4.2.1 (RQ1 results / optimization behavior): \"The results show that, during the optimization, the distribution of S is getting closer to the training distribution of the target class (see MMD_target) and getting farther away from the source class (see MMD_source). Meanwhile, Error Rate and Error_target Rate are increasing, indicating that more errors are generated and gradually become statistically indistinguishable between the original class c and target class c'. The effect of indistinguishability can be further confirmed by the detection results (i.e., Dissector and A2D): errors become indistinguishable and difficult to detect while the MMD_target decreases.\" (Section 4.2.1, p.75)\n\nPage 70, Section 2.1.4 (MMD definition / internals used): \"To measure the distribution difference, we adopt the metric Maximum Mean Discrepancy ... The empirical estimation of MMD between the two distributions in RKHS, denoted as MMD(X,Y), can be calculated as: ...\" (Section 2.1.4, p.70)\n\nPage 76, Section 4.2.4 (RQ4 / statistical analysis of distribution diversity): \"Compared to the results DistXplore, we found that the accuracy drops if only considering the distribution difference diversity (DistXplore^{df}) or target diversity (DistXplore^{tc}), which indicates the usefulness of both kinds of diversity in improving the robustness.\" (Section 4.2.4, p.76)\n\nAdditional supporting material: the paper repeatedly links failures to distribution shift and training-vs-real-world distribution differences (e.g., Introduction and Sec.2.1.3), and evaluates detection rates (Table 1) and statistical metrics (Error Rate / Error_target Rate) across iterations to explain why failures are hard to detect (pp.69-76)."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper provides a diagnostic analysis that explains why failures occur by linking them to an underlying model vulnerability. The central thesis is that models fail when presented with inputs from one class that are distributionally very similar to another class. The proposed method, DistXplore, is designed to generate such inputs by minimizing the Maximum Mean Discrepancy (MMD) between a test suite of a source class and the training data of a target class. The evaluation in Figure 5 and the surrounding text explicitly demonstrates that as this distributional difference decreases, the model's error rate increases. This analysis directly connects the discovered failures to the model's vulnerability of having a non-robust decision boundary, which is a consequence of the 'distribution shift' between the training data and more varied, potentially ambiguous real-world data. This provides a clear, data-driven explanation for the cause of the failures generated by the tool.",
                    "evidence": "Page 5, Section 3.1.1, Model Evaluation: \"Conversely, if the data distributions between two classes are very similar, the model may struggle to make accurate predictions. Thus, DistXplore aims to generate test cases (in a class) that are statistically similar to the training data in other classes. ... Intuitively, the smaller the value DFf(Sc, c'), the more difficult it is for the model f to distinguish Sc and Tc'. Hence, it is more likely to generate undetectable errors by minimizing their distribution difference.\"\n\nPage 9, Section 4.2.1, Figure 5 and accompanying text: \"The results show that, during the optimization, the distribution of S is getting closer to the training distribution of the target class (see MMD_target) and getting farther away from the source class (see MMD_source). Meanwhile, Error Rate and Error_target Rate are increasing, indicating that more errors are generated and gradually become statistically indistinguishable between the original class c and target class c'.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "While DistXplore goes beyond merely reporting failures by providing a statistical analysis of how distributional closeness (measured via MMD on the logits layer) correlates with rising error rates and lower detectability, it stops short of a deeper causal diagnosis. The paper shows that as the distributions of two classes overlap, the model\u2019s decision boundary fails\u2014linking failures to distribution shift\u2014but does not dissect specific non-robust or spurious features, dataset bias sources, or per-neuron activations beyond the logits. Thus it provides solid statistical diagnostics but not the finer-grained, feature-level vulnerability analysis required for a \u201cHigh\u201d rating.",
                    "evidence": "\u2022 Section 3.1.1 (p. 71): Defines DF(S,c\u2032)=MMD(f_c(S),f_{c\u2032}(T_{c\u2032})) on the logits layer and motivates minimizing it to generate undetectable errors.  \n\u2022 Section 4.2.1 (p. 75): Shows that as MMD_target decreases, both Error Rate and Error_target Rate rise and defenses (Dissector, A2D) fail more often\u2014i.e., statistical failure patterns linked to distribution shift.  \n\u2022 Throughout (e.g., Sec 2.1.3, Sec 4.2.4): The paper ties failures to distribution shift vulnerabilities but does not analyze specific spurious features or neuron-level activations beyond the logits layer."
                },
                "manual": {
                    "value": "Medium",
                    "why": "links the occurrence and detectability of errors to the statistical closeness (measured via MMD) between the distribution of generated test inputs and the distribution of another class at the logits layer.",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper primarily applies semantic-preserving metamorphic image transformations (translation, rotation, brightness, etc.) as mutation operators and constrains those transformations for validity, but it does not explicitly account for serialization effects (quantization, rounding, clipping) nor does it state that inputs are generated in serialized formats. According to the metric, metamorphic transformations provide only partial enforcement of failure reproducibility when serialization effects are not explicitly handled; therefore DistXplore matches the 'Medium' level.",
                    "evidence": "Page 71, Section 3.2 \"Distribution-Guided Test Generation\": \"In this paper, we mainly focus on image classification tasks. DistXplore can be easily extended to other domains. We select the diverse image transformations (e.g., translation, rotation, brightness) used in DeepTest [54] and DeepHunter [63]. For each selected gene, the mutation randomly selects a transformation function to mutate it. To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation...\" \n\nPage 71, Algorithm 1 / 3.2: \"A uniform crossover is performed between the selected two chromosomes in the input level, i.e., genes in a chromosome are inputs of the model. Each gene in the chromosome can be selected to mutate with a selection probability...\" \n\nPage 72, Section 4.1.1 \"Implementation\": \"We have implemented DistXplore in Python 3.6 based on DL framework Keras (ver.2.3.1) with TensorFlow (ver.1.15.2).\" \n\n(Throughout Sections 3 and 4 the paper describes mutation as in-memory transformations on image tensors and constraining transformations for validity, but it contains no discussion of serialization, quantization, rounding, clipping, saving to common storage formats (PNG/JPEG), or generating inputs directly in serialized formats.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, DistXplore, uses semantic-preserving metamorphic transformations (e.g., translation, rotation, brightness) as its mutation operators for generating test cases. According to the metric definition, using such transformations provides partial enforcement of failure reproducibility. However, the paper does not describe any explicit mechanism to account for serialization effects like quantization or rounding. The constraints mentioned (L0 and L\u221e norms) are used to guarantee the general validity of the generated inputs by limiting the perturbation magnitude, not to ensure that the fault-inducing perturbations survive standard I/O operations. This lack of explicit handling of serialization effects prevents a 'High' rating, placing the method in the 'Medium' category.",
                    "evidence": "Page 6, Section 3.2, Distribution-Guided Test Generation: \"In this paper, we mainly focus on image classification tasks. DistXplore can be easily extended to other domains. We select the diverse image transformations (e.g., translation, rotation, brightness) used in DeepTest [54] and DeepHunter [63]. ... To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation with both L0 and L\u221e.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper primarily applies semantic-preserving metamorphic image transformations (translation, rotation, brightness, etc.) as mutation operators and constrains those transformations for validity, but it does not explicitly account for serialization effects (quantization, rounding, clipping) nor does it state that inputs are generated in serialized formats. According to the metric, metamorphic transformations provide only partial enforcement of failure reproducibility when serialization effects are not explicitly handled; therefore DistXplore matches the 'Medium' level.",
                    "evidence": "Page 71, Section 3.2 \"Distribution-Guided Test Generation\": \"In this paper, we mainly focus on image classification tasks. DistXplore can be easily extended to other domains. We select the diverse image transformations (e.g., translation, rotation, brightness) used in DeepTest [54] and DeepHunter [63]. For each selected gene, the mutation randomly selects a transformation function to mutate it. To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation...\" \n\nPage 71, Algorithm 1 / 3.2: \"A uniform crossover is performed between the selected two chromosomes in the input level, i.e., genes in a chromosome are inputs of the model. Each gene in the chromosome can be selected to mutate with a selection probability...\" \n\nPage 72, Section 4.1.1 \"Implementation\": \"We have implemented DistXplore in Python 3.6 based on DL framework Keras (ver.2.3.1) with TensorFlow (ver.1.15.2).\" \n\n(Throughout Sections 3 and 4 the paper describes mutation as in-memory transformations on image tensors and constraining transformations for validity, but it contains no discussion of serialization, quantization, rounding, clipping, saving to common storage formats (PNG/JPEG), or generating inputs directly in serialized formats.)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "metamorphic transformations can yield decimal-valued pixels internally which may change when re-quantized to integers.",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability across models. DistXplore formulates and optimizes test suites using distributions measured on a target model's internal layer outputs (the logits) and the evaluation and robustness enhancement steps retrain or test the same model on which test suites were generated. The experiments run the method separately on multiple models/datasets, but the paper does not reuse fault-inducing inputs crafted on one (surrogate) model to test other models, nor does it describe any mechanism intended to discover or promote attacks that transfer across different model implementations.",
                    "evidence": "\u2022 Section 3.1.1 (page 71, \"Model Evaluation\"): \"The distribution difference is measured on a specific layer of the DNN. In this paper, we select the logits layer, i.e., the layer before the softmax layer...\" (shows method depends on internal layer outputs of the target model).\n\n\u2022 Section 4.1.4 (page 73, \"Configuration of DistXplore\"): \"For each class, we run DistXplore multiple times ... Finally, for each model, we run DistXplore 90 times (i.e., 10 source classes \u00d7 9 target classes).\" (shows runs are per-model; no cross-model reuse is described).\n\n\u2022 Section 4.2.3 (page 75, \"RQ3: Robustness Enhancement\"): \"For each tool, we fine-tune the original model 20 epochs ... by adding the new data generated from each tool, and evaluate the empirical robustness of the new model on the validation dataset.\" (shows generated tests are used to retrain/evaluate the same model rather than testing other models).\n\n\u2022 Section 3.1.2 (page 71, \"Model Enhancement\"): description of target selection and distribution diversity refers to selecting other classes as targets (inter-class guidance), not other models or surrogate-model attacks: \"We select the classes in the same task as targets because the classification is based on their relationships...\".\n\nThere is no text in the paper that (a) states that fault-inducing inputs generated on one model were reused to test other models, (b) measures cross-model transfer of discovered faults, or (c) proposes mechanisms specifically designed to produce inputs intended to transfer across different model implementations."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's methodology and evaluation focus exclusively on generating fault-inducing inputs for a single model under test and then evaluating the properties of those inputs (e.g., their ability to bypass defenses) on that same model. The concept of attack transferability\u2014testing if inputs that cause faults on one model also cause faults on another model performing the same task\u2014is never discussed or evaluated. The experimental setup, described in Section 4, details procedures for generating errors, evaluating their strength against defenses, and using them for retraining. All these steps are performed within the context of the specific model for which the errors were generated. There is no mention of reusing these inputs to test other models.",
                    "evidence": "Page 7, Section 4.1.4, Experiment Setup, RQ Setup: \"To demonstrate the capability of DistXplore in generating strong errors for model evaluation (RQ1), we collect the test suite in the last iteration for every pair (c, c') (i.e., the best chromosome returns from Algo 1). For each model, we collect a total number of 90 chromosomes over 90 pairs, which are used to evaluate the strength of these errors. The strength of errors is measured by the success rate of bypassing defenses.\"\n\nThis quote illustrates that the evaluation is conducted on a per-model basis, where errors are generated for a specific model and their strength is evaluated against defenses on that same model, without assessing transferability to other models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability across models. DistXplore formulates and optimizes test suites using distributions measured on a target model's internal layer outputs (the logits) and the evaluation and robustness enhancement steps retrain or test the same model on which test suites were generated. The experiments run the method separately on multiple models/datasets, but the paper does not reuse fault-inducing inputs crafted on one (surrogate) model to test other models, nor does it describe any mechanism intended to discover or promote attacks that transfer across different model implementations.",
                    "evidence": "\u2022 Section 3.1.1 (page 71, \"Model Evaluation\"): \"The distribution difference is measured on a specific layer of the DNN. In this paper, we select the logits layer, i.e., the layer before the softmax layer...\" (shows method depends on internal layer outputs of the target model).\n\n\u2022 Section 4.1.4 (page 73, \"Configuration of DistXplore\"): \"For each class, we run DistXplore multiple times ... Finally, for each model, we run DistXplore 90 times (i.e., 10 source classes \u00d7 9 target classes).\" (shows runs are per-model; no cross-model reuse is described).\n\n\u2022 Section 4.2.3 (page 75, \"RQ3: Robustness Enhancement\"): \"For each tool, we fine-tune the original model 20 epochs ... by adding the new data generated from each tool, and evaluate the empirical robustness of the new model on the validation dataset.\" (shows generated tests are used to retrain/evaluate the same model rather than testing other models).\n\n\u2022 Section 3.1.2 (page 71, \"Model Enhancement\"): description of target selection and distribution diversity refers to selecting other classes as targets (inter-class guidance), not other models or surrogate-model attacks: \"We select the classes in the same task as targets because the classification is based on their relationships...\".\n\nThere is no text in the paper that (a) states that fault-inducing inputs generated on one model were reused to test other models, (b) measures cross-model transfer of discovered faults, or (c) proposes mechanisms specifically designed to produce inputs intended to transfer across different model implementations."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility via bounded, rule-based mutation constraints (they adopt conservative mutation bounds and use Lp-norm validity constraints) and additionally reports an explicit human validation of final fault-inducing inputs (manual inspection with high \u2018validity ratio\u2019). However, the paper does not report quantitative perceptual metrics (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) nor an explicit analysis of cumulative drift across many fuzzing iterations (it relies on per-mutation constraints and conservative configuration but does not measure or bound accumulated drift over iterations). This matches the 'Medium' level: plausibility of final inputs is validated, and individual mutation steps are constrained, but cumulative-effects across iterations are not explicitly analyzed or controlled.",
                    "evidence": "Page 70, Section 2.1.2 (Data Validity): \"In practice, the Lp norm [40] is usually used to guarantee the validity of the generated data by the existing DL testing and adversarial attack techniques. Specifically, given a valid input x, the new test case x' generated by adding some perturbations on x is considered as valid if ||x' - x|| < \u03b5, where \u03b5 is a safe radius.\"; Page 72, Section 3.2 (Distribution-Guided Test Generation): \"To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation with both 0 and \u221e.\"; Page 76, Section 4.2.2 (RQ2: Efficiency of DistXplore): \"In order to evaluate the validity of the generated inputs, we conducted a manual investigation by randomly selecting 10,000 erroneous inputs from the testing outputs of each model and calculating the average validity ratio. The validity ratios were found to be 98.5%+, 96.5%+, 98.7%+, and 95.3%+ for MNIST, Fashion\u2011MNIST, CIFAR\u201110, and SVHN datasets, respectively.\"; (No mention in the paper of perceptual/semantic quantitative metrics such as LPIPS, SSIM, PESQ, STOI or Perplexity, nor of an explicit assessment of cumulative mutation drift across iterations.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper satisfies the criteria for 'Medium' by both enforcing plausibility in individual mutation steps and validating the plausibility of the final generated inputs. Plausibility is enforced by adopting a 'conservative strategy' that constrains image transformations using both L0 and L\u221e norms, which aligns with the 'bounded or rule-based constraints' mentioned in the metric definition. However, the paper does not discuss or address the potential for cumulative drift over multiple iterations. Crucially, the authors explicitly validate the plausibility of the final fault-inducing inputs through a large-scale manual investigation (a form of human study), reporting high validity ratios across multiple datasets. This direct assessment of the final outputs clearly meets the 'Medium' criteria. The method does not qualify for 'High' because it does not demonstrate a mechanism to ensure inputs remain within a plausible distribution *throughout* the entire fuzzing process to prevent cumulative drift.",
                    "evidence": "Page 6, Section 3.2, \"Distribution-Guided Test Generation\": \"To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation with both L0 and L\u221e.\"\n\nPage 10, Section 4.2.2, \"RQ2: Efficiency of DistXplore\": \"In order to evaluate the validity of the generated inputs, we conducted a manual investigation by randomly selecting 10,000 erroneous inputs from the testing outputs of each model and calculating the average validity ratio. The validity ratios were found to be 98.5%+, 96.5%+, 98.7%+, and 95.3%+ for MNIST, Fashion-MNIST, CIFAR-10, and SVHN datasets, respectively. The results demonstrate that DistXplore is capable of generating valid inputs with high proportions.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility via bounded, rule-based mutation constraints (they adopt conservative mutation bounds and use Lp-norm validity constraints) and additionally reports an explicit human validation of final fault-inducing inputs (manual inspection with high \u2018validity ratio\u2019). However, the paper does not report quantitative perceptual metrics (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) nor an explicit analysis of cumulative drift across many fuzzing iterations (it relies on per-mutation constraints and conservative configuration but does not measure or bound accumulated drift over iterations). This matches the 'Medium' level: plausibility of final inputs is validated, and individual mutation steps are constrained, but cumulative-effects across iterations are not explicitly analyzed or controlled.",
                    "evidence": "Page 70, Section 2.1.2 (Data Validity): \"In practice, the Lp norm [40] is usually used to guarantee the validity of the generated data by the existing DL testing and adversarial attack techniques. Specifically, given a valid input x, the new test case x' generated by adding some perturbations on x is considered as valid if ||x' - x|| < \u03b5, where \u03b5 is a safe radius.\"; Page 72, Section 3.2 (Distribution-Guided Test Generation): \"To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation with both 0 and \u221e.\"; Page 76, Section 4.2.2 (RQ2: Efficiency of DistXplore): \"In order to evaluate the validity of the generated inputs, we conducted a manual investigation by randomly selecting 10,000 erroneous inputs from the testing outputs of each model and calculating the average validity ratio. The validity ratios were found to be 98.5%+, 96.5%+, 98.7%+, and 95.3%+ for MNIST, Fashion\u2011MNIST, CIFAR\u201110, and SVHN datasets, respectively.\"; (No mention in the paper of perceptual/semantic quantitative metrics such as LPIPS, SSIM, PESQ, STOI or Perplexity, nor of an explicit assessment of cumulative mutation drift across iterations.)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Does not explicitly enforce naturalness across fuzzing iterations. Apply norm constraints in each mutation step but cumulative effect is ignored. Checks visual validity on the final outputs via human evaluation.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ]
        }
    },
    "Liu et al. - 2022 - QATest A Uniform Fuzzing Framework for Question Answering Systems": {
        "key": "liu2022qatest",
        "name": "QATest",
        "year": "2022",
        "citation_count": "21",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "All evidence in the paper shows QATest uncovers robustness errors (mispredictions and inconsistent answers under semantically-preserving transformations) such as wrong spans, irrelevant answers, incorrect <NoAnswer> decisions, and failures to perform reasoning. The authors define a bug as M(q') \u2260 M(q) for transformed (semantically consistent) inputs and illustrate detected bug types in a case study (e.g., \u201c<NoAnswer> questions are answered with irrelevant content\u201d, \u201cAnswers that are unrelated to the question\u201d, etc.). The paper contains no indication that QATest targets or successfully bypasses explicit safety/policy/security mechanisms, tests safety-aligned or adversarially-defended models, or uncovers high-severity violations like jailbreaks, data leakage, or unauthorized actions. Therefore the failures are best categorized as low-severity (model robustness errors) under the given metric.",
                    "evidence": "Page 1, Abstract: \"The experiment results show that the tests generated by QATest detect hundreds of erroneous behaviors of QA systems efficiently.\"; Page 4, Section 3.2 Test Oracle (Definition 3.1): \"A mutant q' \u2208 T(q) is regarded as a bug case of M if M(q') \u2260 M(q).\"; Page 9, Section 5.4 Answer to RQ4: Case Study: \"we show five types of representative erroneous behaviors... (1) <NoAnswer> questions are answered with irrelevant content... (3) Answers that are unrelated to the question... (5) Answers without logical reasoning.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes QATest, a fuzzing framework based on metamorphic testing. The core principle is to apply semantically preserving transformations to input questions and check for prediction inconsistencies. A failure is defined as the model producing a different answer for the transformed question compared to the original, i.e., `M(q') \u2260 M(q)`. This directly aligns with the definition of uncovering model robustness errors. The paper's case study of detected failures (RQ4) categorizes them as various types of incorrect answers, such as answering an answerable question with `<NoAnswer>`, providing answers unrelated to the question, or extracting the wrong keywords. These are all examples of mispredictions or inconsistencies. The paper does not test against models with explicit safety mechanisms, nor does it aim to produce or evaluate unsafe, toxic, or biased outputs.",
                    "evidence": "Page 4, Section 3.2.1, Test Oracle: \"Metamorphic Relations (MRs) refer to the relationship expected to be followed between input and output when the target program is executed multiple times. ... We define a series of semantically consistent transformations T for questions q e Q. The generated question q' can be obtained by applying the transformation t\u2208T to the original question q, and q' has the same intent and meaning as q. Therefore, the QA system should output the same answer for the q and q'.\"\n\nPage 10, Section 5.4, Answer to RQ4: Case Study: \"In this RQ, we investigate and classify the erroneous behaviors of the QA system detected by QATest. In Table 6, we show five types of representative erroneous behaviors. ... (2) Answerable questions are answered with <NoAnswer>. ... the question has a corresponding answer in the context, but the system can't find a matching answer and outputs <NoAnswer>. (3) Answers that are unrelated to the question. Another type of error is that the QA system's answers are not related to the question. In example 3, the question is asking about the year, but the output of the QA system is a person's name.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "All evidence in the paper shows QATest uncovers robustness errors (mispredictions and inconsistent answers under semantically-preserving transformations) such as wrong spans, irrelevant answers, incorrect <NoAnswer> decisions, and failures to perform reasoning. The authors define a bug as M(q') \u2260 M(q) for transformed (semantically consistent) inputs and illustrate detected bug types in a case study (e.g., \u201c<NoAnswer> questions are answered with irrelevant content\u201d, \u201cAnswers that are unrelated to the question\u201d, etc.). The paper contains no indication that QATest targets or successfully bypasses explicit safety/policy/security mechanisms, tests safety-aligned or adversarially-defended models, or uncovers high-severity violations like jailbreaks, data leakage, or unauthorized actions. Therefore the failures are best categorized as low-severity (model robustness errors) under the given metric.",
                    "evidence": "Page 1, Abstract: \"The experiment results show that the tests generated by QATest detect hundreds of erroneous behaviors of QA systems efficiently.\"; Page 4, Section 3.2 Test Oracle (Definition 3.1): \"A mutant q' \u2208 T(q) is regarded as a bug case of M if M(q') \u2260 M(q).\"; Page 9, Section 5.4 Answer to RQ4: Case Study: \"we show five types of representative erroneous behaviors... (1) <NoAnswer> questions are answered with irrelevant content... (3) Answers that are unrelated to the question... (5) Answers without logical reasoning.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "QATest is a metamorphic, coverage-driven fuzzer that detects inconsistencies and generic erroneous behaviors (e.g., differing answers, <NoAnswer> vs answer, unrelated answers). Its oracle and bug definition are based on output inconsistency between a seed question and its semantically-equivalent transform (M(q') != M(q)). The guidance components (N-Gram coverage, perplexity priority, seed selection) steer generation toward diverse and authentic inputs, not toward attacker-chosen outputs or broader security classes (e.g., toxicity or jailbreaks). The paper contains no mechanism for specifying or steering the fuzzer to induce a particular target output or a security-class objective. Therefore the method performs untargeted exploration revealing generic failures rather than targeted attack discovery.",
                    "evidence": "Page 1, Abstract: \"It provides the first uniform solution to generate tests with oracle information automatically for various QA systems...\"; Page 4, Section 3.2.1 Test Oracle / Definition 3.1: \"Therefore, the QA system should output the same answer for the q and q'.\" and \"A mutant q' \u2208 T(q) is regarded as a bug case of M if M(q') \u2260 M(q).\"; Page 5, Section 3.3 Guidance Criteria Design: \"we propose two testing criteria as guidance in the test generation process... N-Gram coverage and perplexity priority...\" (criteria are about input coverage and authenticity rather than targeting outputs); Page 9, Section 5.4 Case Study: listing bug types such as \"(1) <NoAnswer> questions are answered with irrelevant content\" and \"(2) Answerable questions are answered with <NoAnswer>.\" (examples show generic erroneous behaviors rather than attacker-chosen target outputs)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The QATest framework uses metamorphic testing as its core principle for discovering faults. The test oracle is defined by the expectation that a semantically equivalent question (generated via transformation) should yield the same answer as the original question. A bug is detected if the transformed question produces a different answer, i.e., `M(q') \u2260 M(q)`. This approach is inherently untargeted, as it seeks to find any inconsistent prediction rather than steering the system towards a specific, attacker-chosen output or a predefined class of security-violating behavior. The guidance criteria, N-Gram coverage and perplexity priority, are designed to increase the diversity and quality of input test cases, not to guide the search towards a particular target output.",
                    "evidence": "Page 5, Section 3.2.1, Definition 3.1: 'Given a QA system M, a semantic transformation set T, and a seed question set Q, a mutant question corresponding to q \u2208 Q with transformation t \u2208 T can be represented as q' = t(q). A mutant q' \u2208 T(q) is regarded as a bug case of M if M(q') \u2260 M(q).'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "QATest is a metamorphic, coverage-driven fuzzer that detects inconsistencies and generic erroneous behaviors (e.g., differing answers, <NoAnswer> vs answer, unrelated answers). Its oracle and bug definition are based on output inconsistency between a seed question and its semantically-equivalent transform (M(q') != M(q)). The guidance components (N-Gram coverage, perplexity priority, seed selection) steer generation toward diverse and authentic inputs, not toward attacker-chosen outputs or broader security classes (e.g., toxicity or jailbreaks). The paper contains no mechanism for specifying or steering the fuzzer to induce a particular target output or a security-class objective. Therefore the method performs untargeted exploration revealing generic failures rather than targeted attack discovery.",
                    "evidence": "Page 1, Abstract: \"It provides the first uniform solution to generate tests with oracle information automatically for various QA systems...\"; Page 4, Section 3.2.1 Test Oracle / Definition 3.1: \"Therefore, the QA system should output the same answer for the q and q'.\" and \"A mutant q' \u2208 T(q) is regarded as a bug case of M if M(q') \u2260 M(q).\"; Page 5, Section 3.3 Guidance Criteria Design: \"we propose two testing criteria as guidance in the test generation process... N-Gram coverage and perplexity priority...\" (criteria are about input coverage and authenticity rather than targeting outputs); Page 9, Section 5.4 Case Study: listing bug types such as \"(1) <NoAnswer> questions are answered with irrelevant content\" and \"(2) Answerable questions are answered with <NoAnswer>.\" (examples show generic erroneous behaviors rather than attacker-chosen target outputs)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper goes beyond merely reporting discovered failures: it provides statistical analyses of failure patterns (counts, failure rates, diversity metrics across datasets and systems, and iteration effects) and presents a case study that classifies common error types found (five representative bug types). However, it does not analyze observable model internals (neuron activations, layer coverage, or similar signals) nor does it empirically link failures to underlying model vulnerabilities such as reliance on spurious features, dataset bias, or overfitting. These characteristics match the metric's \"Medium\" level (statistical failure-pattern analysis and diagnostic insight) rather than \"High\" (which requires linking failures to underlying model vulnerabilities via internals or stronger evidence).",
                    "evidence": "p.8, Section 5.1 (Metamorphic Transformations): \"For all QA systems, applying the three types of MR alone can generate more failed tests than the original test set, which indicates that each type of MR has the ability to reveal erroneous behaviors and discover more potential flaws in QA systems than the original test set.\" \n\np.8, Section 5.2 (Answer to RQ2: Guidance Criteria): \"the test sets generated using the two testing criteria as guidance can generate more failed tests and achieve higher bug detection rates than those generated without guidance.\" (Table 4 shows counts and proportions of failed tests under different guidance strategies.)\n\np.9, Section 5.3 (Answer to RQ3: Test Diversity): \"we count the number of different original seed data corresponding to the failed tests generated by each method... To reflect the diversity of bug categories, we calculate the ratio of the number of seeds that can trigger bugs to the total number, which is expressed as diversity.\" (Table 5 reports these diversity metrics across systems/datasets.)\n\np.10, Section 5.4 (Answer to RQ4: Case Study): \"we show five types of representative erroneous behaviors... (1) <NoAnswer> questions are answered with irrelevant content... (2) Answerable questions are answered with <NoAnswer>... (3) Answers that are unrelated to the question... (4) Answers with wrong keywords in the context... (5) Answers without logical reasoning.\" (Table 6 gives concrete examples for these types.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight by performing a statistical analysis of failure patterns. In the case study (RQ4), the authors investigate and classify the erroneous behaviors detected by their framework, QATest, into five distinct types. This classification is based on analyzing the input questions and the incorrect outputs from the Question Answering (QA) system. The bug types identified include '<NoAnswer> questions are answered with irrelevant content', 'Answerable questions are answered with <NoAnswer>', and 'Answers without logical reasoning'. This categorization represents a statistical analysis of failure patterns, which aligns with the 'Medium' criteria. The analysis does not, however, extend to linking these failures to underlying model vulnerabilities (like overfitting or dataset bias) or analyzing observable model internals (like neuron activations), which would be required for a 'High' rating.",
                    "evidence": "Page 10, Section 5.4, Answer to RQ4: Case Study: \"In this RQ, we investigate and classify the erroneous behaviors of the QA system detected by QATest. In Table 6, we show five types of representative erroneous behaviors. ... Specifically, we summarize the bug types below. (1) <NoAnswer> questions are answered with irrelevant content. ... (2) Answerable questions are answered with <NoAnswer>. ... (3) Answers that are unrelated to the question. ... (4) Answers with wrong keywords in the context. ... (5) Answers without logical reasoning.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper goes beyond merely reporting discovered failures: it provides statistical analyses of failure patterns (counts, failure rates, diversity metrics across datasets and systems, and iteration effects) and presents a case study that classifies common error types found (five representative bug types). However, it does not analyze observable model internals (neuron activations, layer coverage, or similar signals) nor does it empirically link failures to underlying model vulnerabilities such as reliance on spurious features, dataset bias, or overfitting. These characteristics match the metric's \"Medium\" level (statistical failure-pattern analysis and diagnostic insight) rather than \"High\" (which requires linking failures to underlying model vulnerabilities via internals or stronger evidence).",
                    "evidence": "p.8, Section 5.1 (Metamorphic Transformations): \"For all QA systems, applying the three types of MR alone can generate more failed tests than the original test set, which indicates that each type of MR has the ability to reveal erroneous behaviors and discover more potential flaws in QA systems than the original test set.\" \n\np.8, Section 5.2 (Answer to RQ2: Guidance Criteria): \"the test sets generated using the two testing criteria as guidance can generate more failed tests and achieve higher bug detection rates than those generated without guidance.\" (Table 4 shows counts and proportions of failed tests under different guidance strategies.)\n\np.9, Section 5.3 (Answer to RQ3: Test Diversity): \"we count the number of different original seed data corresponding to the failed tests generated by each method... To reflect the diversity of bug categories, we calculate the ratio of the number of seeds that can trigger bugs to the total number, which is expressed as diversity.\" (Table 5 reports these diversity metrics across systems/datasets.)\n\np.10, Section 5.4 (Answer to RQ4: Case Study): \"we show five types of representative erroneous behaviors... (1) <NoAnswer> questions are answered with irrelevant content... (2) Answerable questions are answered with <NoAnswer>... (3) Answers that are unrelated to the question... (4) Answers with wrong keywords in the context... (5) Answers without logical reasoning.\" (Table 6 gives concrete examples for these types.)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility at individual mutation steps and uses automatic, task-relevant measures to prefer authentic candidates (ROUGE-1 quality check and a POS-based ``perplexity priority'' used to select/retain seeds). It also implements realistic transformation operators (back-translation, BERT insertion, WordNet synonyms, entity-alias substitution, OCR/keyboard typo simulation). However, the work does not provide an explicit human evaluation or dedicated empirical assessment of the final fault-inducing inputs\u2019 semantic or perceptual realism, and it does not present a formal mechanism that strictly bounds cumulative drift across many iterative mutations (rather, it relies on selection heuristics such as perplexity priority and ROUGE threshold). These properties match the metric\u2019s \u201cMedium\u201d level: per-step plausibility checks and quantitative plausibility heuristics are present, but cumulative-iteration effects are not strictly enforced nor validated by human study or external fluency/quality evaluations of final failing inputs.",
                    "evidence": "Section 3.1 / Algorithm 1 (p.3): \"quality_score \u2190 Rouge1(s, s'); if quality_score > \u03c4 then // acceptable ... D.append(s')\" \u2014 shows ROUGE-1 is used as a per-generation quality filter.\n\nSection 3.2.5 Quality Assessment (p.3): \"To ensure the quality of generated questions, we employ the ROUGE-1 metric [35] to evaluate the questions generated by metamorphic transformations.\" (ROUGE used to discard low-quality generations).\n\nSection 3.3.2 Perplexity Priority (p.4): \"To ensure that the generated test questions have good authenticity and rationality, we consider applying perplexity priority as guidance for seed data retention... per(q) = p(seq(q))^{-1/n}. In each iteration, QATest selects the generated question with the lowest perplexity score in a generated batch.\" \u2014 shows a quantitative authenticity/prioritization metric used during fuzzing.\n\nSection 4.3 Sentence Transformations (p.6): \"...the back translation operator also employs the pre-trained language model... For the entity alias substitution operator, we apply ... TagMe... For each transformation, if the generated question is exactly the same as the original question, it is considered a failed transformation...\" \u2014 shows use of realistic NLP methods and sensible discard rules for trivial/failed transformations.\n\nSection 6.3 Threats to Validity (p.9): \"...it is impossible to guarantee the reliability of the generated question without manual inspection. To ensure the quality of the generated questions, most of the transformations are implemented with pre-trained language models ... Besides, we apply the existing ROUGE metric to evaluate the quality of the generated data and discard the low-quality generated questions directly.\" \u2014 acknowledges no manual/human evaluation and that quality control is via automatic metrics rather than a human study or external perceptual/fluency evaluation of final failing inputs."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper enforces input plausibility throughout the fuzzing process using two distinct, explicit mechanisms. First, after each mutation, a quality assessment is performed using the ROUGE-1 metric to measure the similarity between the original and transformed question. Generated questions that fall below a certain similarity threshold are discarded, ensuring that individual mutation steps do not drastically alter the input's semantic meaning. Second, to address the cumulative effect of mutations across iterations, the framework uses a 'perplexity priority' criterion to guide the selection of new seeds. This mechanism prioritizes generated questions that are more linguistically fluent and authentic (i.e., have lower perplexity) to be added back into the seed pool for further fuzzing. The combination of ROUGE-based filtering and perplexity-guided seed selection ensures that the generated inputs remain within a plausible distribution throughout the process. These quantitative, task-specific measures (ROUGE for similarity, perplexity for fluency) serve as the empirical evidence that the framework maintains realistic inputs, as required by the metric definition.",
                    "evidence": "1. Page 5, Section 3.2.5, Quality Assessment: \"To ensure the quality of generated questions, we employ the ROUGE-1 metric [35] to evaluate the questions generated by metamorphic transformations.\" \n2. Page 4, Algorithm 1: The algorithm shows that generated questions are only kept if their ROUGE score is above a threshold: \"if quality_score > \u03c4 then // acceptable D.append(s');\"\n3. Page 6, Section 3.3.2, Perplexity Priority: \"To ensure that the generated test questions have good authenticity and rationality, we consider applying perplexity priority as guidance for seed data retention.\"\n4. Page 7, Definition 3.5 (Perplexity priority): The framework selects the question with the lowest perplexity to feed back into the seed set: \"s** = arg min per(q) q\u2208G\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "QATest incorporates per\u2010step plausibility checks\u2014filtering each mutated question by a ROUGE\u20101 similarity threshold and prioritizing seeds with low perplexity to encourage fluent, realistic inputs\u2014but it does not formally bound cumulative drift across multiple iterations nor does it perform an explicit human or task\u2010specific quantitative evaluation of the final fault\u2010inducing inputs. Thus, while individual mutation steps are plausibility\u2010constrained, the framework relies on heuristics rather than a strict mechanism or empirical validation to ensure that deeply mutated inputs remain within the realistic question distribution, which aligns with the \u201cMedium\u201d level.",
                    "evidence": "Section 3.2.5 (Quality Assessment): uses ROUGE\u20101 to discard low\u2010quality transformations on each step\nSection 3.3.2 (Perplexity Priority): guides seed retention by selecting the lowest\u2010perplexity (most fluent) questions\nSection 6.3 (Threats to Validity): acknowledges absence of manual inspection or human study and reliance on automatic metrics rather than external plausibility validation of final failing inputs"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Checks quality drift between successive mutations but overlooks drift across many iterations",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper evaluates QATest by applying its metamorphic transformations and guidance criteria separately to multiple QA systems (UnifiedQA, ALBERT, DrQA, MARL), but it does not present any experiments in which fault-inducing inputs crafted on one model are reused to test other models, nor does it describe design elements intended to discover or prioritize transferable attacks across models. The testing guidance (N-Gram coverage and perplexity priority) and seed selection are aimed at diversity and authenticity of generated questions, not at identifying inputs that transfer across different model implementations. Consequently, the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "1) Abstract (page 1, Abstract): \"To evaluate the performance of QATest, we experiment with it on four QA systems that are designed for different tasks. The experiment results show that the tests generated by QATest detect hundreds of erroneous behaviors of QA systems efficiently.\" \u2014 indicates per-system evaluation but does not claim cross-model reuse. \n\n2) Section 4.1 (page 7, \"Research Questions\"): The four RQs listed are RQ1 (Metamorphic Transformations), RQ2 (Testing Criteria), RQ3 (Test Diversity), and RQ4 (Case Study). None ask about or evaluate transferability across models: \"RQ1 (Metamorphic Transformations): How effective are different transformation methods for generating tests for various types of QA systems?... RQ4 (Case Study): Analysis of the bug types detected by QATest.\" \n\n3) Section 4.2.2 (page 8, \"QA systems\"): \"To evaluate the performance of QATest, we select four state-of-art QA systems as the testing subjects in our experiments. We briefly introduce them as follows: UnifiedQA ..., ALBERT ..., DrQA ..., MARL ...\" \u2014 shows multiple target systems but no description of crafting faults on one and reusing them on others. \n\n4) Section 3.3 (page 4, \"Guidance Criteria Design\") and Section 3.3.1/3.3.2 (N-Gram Coverage / Perplexity Priority): These criteria are defined to improve test diversity and authenticity (e.g., \"N-Gram coverage... select the particular question that can increase the richness of the seed set\" and \"perplexity priority... to ensure that the generated test questions have good authenticity\"), with no mention of mechanisms to identify or prioritize inputs likely to transfer across models."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper does not investigate whether the fault-inducing inputs generated for one model transfer to other models. The evaluation is conducted on four distinct Question Answering (QA) systems, but each is treated as an independent target. The experiments generate test cases from a seed dataset and apply them to a corresponding QA system to find failures. There is no mention of reusing the failure-inducing questions found on one model (e.g., ALBERT) to test another model (e.g., UnifiedQA). The research questions and experimental results focus on the effectiveness, efficiency, and diversity of the fuzzing framework for individual systems. The design of the fuzzer, including its guidance criteria (N-Gram coverage and perplexity priority), is aimed at improving bug detection on a single target rather than discovering shared vulnerabilities or transferable attacks across models.",
                    "evidence": "Page 8, Section 5.1, \"Answer to RQ1: Metamorphic Transformations\": \"To evaluate the effectiveness of each type of MR applied by QATest, we select 500 test cases from each data set and apply the MRs proposed in Section 3.2 to generate test data sets. Then, we record the number of failed tests in the generated test set on the corresponding QA systems, which is shown in Table 3.\" This statement describes an experimental process where each QA system is tested independently. The results presented in Tables 3, 4, and 5 consistently show separate evaluations for each system, with no cross-testing of generated inputs."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper evaluates QATest by applying its metamorphic transformations and guidance criteria separately to multiple QA systems (UnifiedQA, ALBERT, DrQA, MARL), but it does not present any experiments in which fault-inducing inputs crafted on one model are reused to test other models, nor does it describe design elements intended to discover or prioritize transferable attacks across models. The testing guidance (N-Gram coverage and perplexity priority) and seed selection are aimed at diversity and authenticity of generated questions, not at identifying inputs that transfer across different model implementations. Consequently, the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "1) Abstract (page 1, Abstract): \"To evaluate the performance of QATest, we experiment with it on four QA systems that are designed for different tasks. The experiment results show that the tests generated by QATest detect hundreds of erroneous behaviors of QA systems efficiently.\" \u2014 indicates per-system evaluation but does not claim cross-model reuse. \n\n2) Section 4.1 (page 7, \"Research Questions\"): The four RQs listed are RQ1 (Metamorphic Transformations), RQ2 (Testing Criteria), RQ3 (Test Diversity), and RQ4 (Case Study). None ask about or evaluate transferability across models: \"RQ1 (Metamorphic Transformations): How effective are different transformation methods for generating tests for various types of QA systems?... RQ4 (Case Study): Analysis of the bug types detected by QATest.\" \n\n3) Section 4.2.2 (page 8, \"QA systems\"): \"To evaluate the performance of QATest, we select four state-of-art QA systems as the testing subjects in our experiments. We briefly introduce them as follows: UnifiedQA ..., ALBERT ..., DrQA ..., MARL ...\" \u2014 shows multiple target systems but no description of crafting faults on one and reusing them on others. \n\n4) Section 3.3 (page 4, \"Guidance Criteria Design\") and Section 3.3.1/3.3.2 (N-Gram Coverage / Perplexity Priority): These criteria are defined to improve test diversity and authenticity (e.g., \"N-Gram coverage... select the particular question that can increase the richness of the seed set\" and \"perplexity priority... to ensure that the generated test questions have good authenticity\"), with no mention of mechanisms to identify or prioritize inputs likely to transfer across models."
                },
                "manual": {
                    "value": "Low",
                    "why": "No cross model failure pattern analysis",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "QATest generates test inputs via semantic, metamorphic text transformations (MR1\u2013MR3) and uses text-based quality checks (ROUGE, perplexity) but does not explicitly account for standard I/O serialization effects (e.g., quantization, clipping, or encoding-induced loss) or enforce that failures persist across serialization. The method therefore relies on semantic-preserving metamorphic transformations (which the metric treats as providing only partial enforcement of reproducibility) without explicit handling of serialization effects, matching the metric's \"Medium\" category.",
                    "evidence": "Section 3.2 Test Generation with MRs (page 3\u20134): \"we implement three textual semantically consistent transformations, i.e., grammatical component-based, sentence structure-based, and adversarial perturbation-based transformations.\" \n\nSection 3.2.1 Test Oracle (page 4): \"Metamorphic Relations (MRs) ... We define a series of semantically consistent transformations T for questions q \u2208 Q. The generated question q' can be obtained by applying the transformation t \u2208 T to the original question q, and q' has the same intent and meaning as q. Therefore, the QA system should output the same answer for the q and q'.\" \n\nSection 3.2.5 Quality Assessment (page 5): \"To ensure the quality of generated questions, we employ the ROUGE-1 metric [35] to evaluate the questions generated by metamorphic transformations.\" \n\nSection 4.3 Sentence Transformations (page 7): \"Most of the transformations are implemented with a tool named nlpaug... We employ the WordNet library... BERT... TagMe...\" \n\n(Throughout the paper there is no mention of explicit handling of I/O serialization effects such as quantization, clipping, or re-encoding to verify that failures persist after standard serialization.)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The method proposed in the paper, QATest, generates test cases by applying metamorphic transformations directly to natural language text to create new questions. These transformations include operations like synonym replacement, reverse translation, introducing typos (keyboard mistakes), and simulating OCR errors. The resulting test inputs are new text strings. The metric definition explicitly states: \"We also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., synthesized speech, rendered images, or generated text) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.\" The perturbations introduced by QATest are at the character or word level (e.g., changing 'number' to 'numbfr'), which are discrete and not susceptible to the numerical fragility issues like rounding, clipping, or quantization that the metric is designed to guard against. By generating inputs directly in their final text format, the framework inherently ensures that any fault-inducing perturbations are stable and reproducible under standard I/O operations.",
                    "evidence": "Page 4, Section 3.2, \"Test Generation with MRs\": \"To simulate the possible question QA systems may receive in real usage scenarios, we implement three textual semantically consistent transformations, i.e., grammatical component-based, sentence structure-based, and adversarial perturbation-based transformations.\"\n\nPage 6, Table 1, \"Example questions with metamorphic transformations.\": This table provides concrete examples of the text-based mutations, such as 'Original question: What is a string over a Greek number when considering a computational problem?' being transformed into 'Keyboard mistake: What is a string over a Greek numbfr when considering a computational problem?'. This demonstrates the generation of discrete, reproducible text inputs."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "QATest generates mutated questions directly as text strings\u2014the final serialized format consumed by QA systems\u2014using semantic and adversarial perturbations at the character and word level. Because these inputs are already in their serialized form (plain text), they are inherently stable under standard I/O operations (no floating\u2010point quantization or clipping applies). This aligns with the metric\u2019s definition that approaches producing inputs directly in serialized formats (e.g., generated text) fully ensure failure reproducibility.",
                    "evidence": "Metric definition: \u201cWe also consider fuzzing approaches that generate inputs directly in serialized formats (e.g., \u2026 generated text) as reproducible under this metric, since such inputs are inherently stable with respect to standard I/O operations.\u201d\n\nPaper, Section 3.2 (page 4): \u201cwe implement three textual semantically consistent transformations \u2026 grammatical component\u2010based, sentence structure\u2010based, and adversarial perturbation\u2010based transformations.\u201d\n\nTable 1 (page 6): Examples of text\u2010level mutations (e.g., keyboard mistakes, synonym substitution) producing final question strings without floating\u2010point artifacts."
                },
                "manual": {
                    "value": "High",
                    "why": "Text level mutation ",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Data-driven"
            ]
        }
    },
    "Wang et al. - 2022 - BET black-box efficient testing for convolutional neural networks": {
        "key": "wang2022bet",
        "name": "BET",
        "year": "2022",
        "citation_count": "26",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper describes BET as a black-box testing method that finds \"error-inducing inputs\" which cause mispredictions or inconsistencies (e.g., between a model and its quantized versions). All experiments and metrics (Err-Num, Label-Num, SR, Inact-Rate) focus on misclassification and disagreement detection and on improving model accuracy via fine-tuning. The paper does not describe discovering or exploiting safety/policy/security mechanisms (e.g., jailbreaks, data leakage, unauthorized actions) nor testing defended or safety-aligned models to bypass explicit safeguards. Therefore the failures uncovered are robustness errors (mispredictions/inconsistencies) rather than high- or medium-severity safety/security violations.",
                    "evidence": "Page 164, Abstract: \"It is important to test convolutional neural networks (CNNs) to identify defects (e.g. error-inducing inputs) before deploying them in security-sensitive scenarios.\" \n\nPage 164-165, Introduction: \"They aim at identifying defects of CNN models such as finding adversarial inputs which induce errors. These error-inducing inputs can be used to fine-tune the model to improve its classification accuracy.\" \n\nPage 166, Section 2.2 (Related Work): \"There are two common scenarios for CNN testing, i.e., differential testing (DT) [31, 46] and single model testing (ST) [12, 19]. DT finds error-inducing inputs which trigger behavior inconsistencies among multiple models... ST only focuses on one target model... an error-inducing input x_err is found if C(x_err) \u2260 l.\" \n\nPage 171, Section 4.2 (Evaluation of Black-Box Differential Testing): \"we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\" \n\nPage 172, Tables and Evaluation (4.3 & 4.4): metrics and results (Err-Num, Label-Num, SR, Inact-Rate) all measure misclassifications, label diversity, and success in finding error-inducing inputs; Section 4.4 shows such inputs are used to fine-tune models to \"improve target models\u2019 accuracy by 2 \u223c 3%.\" (no discussion of bypassing defenses or inducing unsafe/hallucinatory/toxic outputs)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, BET, focuses exclusively on identifying 'error-inducing inputs' which are defined as inputs that cause misclassifications or prediction inconsistencies. The entire methodology and evaluation are centered on finding inputs that change a model's prediction label (Single Model Testing) or cause disagreements between two models with similar functionality, such as a model and its quantized version (Differential Testing). The experiments are conducted on standard image classification tasks without any mention of safety, bias, toxicity, or other unsafe behaviors. The goal is to find robustness errors (mispredictions) efficiently, which can then be used to improve the model's classification accuracy. There is no evidence that the method is designed to or capable of uncovering unsafe behaviors or bypassing explicit security mechanisms.",
                    "evidence": "Page 4, Section 2.2, 'Related Work': 'There are two common scenarios for CNN testing, i.e., differential testing (DT) [31, 46] and single model testing (ST) [12, 19]. DT finds error-inducing inputs which trigger behavior inconsistencies among multiple models... For example, given two CNN models C1, C2 with similar functionality, an error-inducing input xerr is found if C1(xerr) \u2260 C2(xerr). ST only focuses on one target model C to find error-inducing inputs and requires labels for test data. For example, given a CNN model under test C, an original input x and the corresponding label l, an error-inducing input xerr is found if C(xerr) \u2260 l.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper describes BET as a black-box testing method that finds \"error-inducing inputs\" which cause mispredictions or inconsistencies (e.g., between a model and its quantized versions). All experiments and metrics (Err-Num, Label-Num, SR, Inact-Rate) focus on misclassification and disagreement detection and on improving model accuracy via fine-tuning. The paper does not describe discovering or exploiting safety/policy/security mechanisms (e.g., jailbreaks, data leakage, unauthorized actions) nor testing defended or safety-aligned models to bypass explicit safeguards. Therefore the failures uncovered are robustness errors (mispredictions/inconsistencies) rather than high- or medium-severity safety/security violations.",
                    "evidence": "Page 164, Abstract: \"It is important to test convolutional neural networks (CNNs) to identify defects (e.g. error-inducing inputs) before deploying them in security-sensitive scenarios.\" \n\nPage 164-165, Introduction: \"They aim at identifying defects of CNN models such as finding adversarial inputs which induce errors. These error-inducing inputs can be used to fine-tune the model to improve its classification accuracy.\" \n\nPage 166, Section 2.2 (Related Work): \"There are two common scenarios for CNN testing, i.e., differential testing (DT) [31, 46] and single model testing (ST) [12, 19]. DT finds error-inducing inputs which trigger behavior inconsistencies among multiple models... ST only focuses on one target model... an error-inducing input x_err is found if C(x_err) \u2260 l.\" \n\nPage 171, Section 4.2 (Evaluation of Black-Box Differential Testing): \"we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\" \n\nPage 172, Tables and Evaluation (4.3 & 4.4): metrics and results (Err-Num, Label-Num, SR, Inact-Rate) all measure misclassifications, label diversity, and success in finding error-inducing inputs; Section 4.4 shows such inputs are used to fine-tune models to \"improve target models\u2019 accuracy by 2 \u223c 3%.\" (no discussion of bypassing defenses or inducing unsafe/hallucinatory/toxic outputs)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "High",
                    "why": "The BET method explicitly implements a tunable objective function that optimizes for particular class labels and can be driven to enter the decision boundary of a specified label. The paper describes both a DT objective (which includes an explicit per-label term C(x)[li] and per-label query budgets) and an ST objective SOF(x)=C(x)[li], allows dynamic / user-specified prioritization of labels (including ad-hoc risk-driven priorities), and states that when the objective targets a specific label the testing process is led to generate error-inducing inputs with that corresponding label. These design choices steer exploration toward predefined target outcomes (specific labels), meeting the metric's definition of supporting targeted attack discovery.",
                    "evidence": "Page 167, Section 3.3 Tunable Objective Function: \"If the objective function targets a specific label, the testing process will be led to enter the decision boundary of this specific label to explore corner cases, i.e., generate error-inducing inputs with the corresponding label.\"\n\nPage 167, Section 3.3 (Tunable objective function for ST): \"SOF(x) = C(x)[li]\" (where li is the selected label to optimize).\n\nPage 167, Section 3.3 (label ordering / prioritization): \"We prioritize labels that are prone to be vulnerable... We also allow users to prioritize labels as they want to support ad-hoc testing cases. For example... it is more risky if an ordinary employee is misclassified as a manager... users may prioritize finding error-inducing inputs that would be misclassified as managers.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, BET, is explicitly designed to support targeted attack discovery by steering exploration towards specific, predefined class labels. This is achieved through a 'tunable objective function' that aims to maximize the output confidence for a target label `li`. The paper clearly states that this function can be used to target a specific label to explore its decision boundary and that users are allowed to prioritize labels for ad-hoc testing. This mechanism directly aligns with the definition of steering exploration toward a specific, predefined target outcome.",
                    "evidence": "Page 5, Section 3.3, Tunable Objective Function: 'If the objective function targets a specific label, the testing process will be led to enter the decision boundary of this specific label to explore corner cases, i.e., generate error-inducing inputs with the corresponding label.'\n\nPage 6, Section 3.3, Tunable objective function for ST: 'The tunable objective function for ST is given in Equation 6. SOF(x) = C(x) [li]... In Equation 6, we only optimize C(x) [li]...'\n\nPage 6, Section 3.3: 'We also allow users to prioritize labels as they want to support ad-hoc testing cases.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The BET method explicitly implements a tunable objective function that optimizes for particular class labels and can be driven to enter the decision boundary of a specified label. The paper describes both a DT objective (which includes an explicit per-label term C(x)[li] and per-label query budgets) and an ST objective SOF(x)=C(x)[li], allows dynamic / user-specified prioritization of labels (including ad-hoc risk-driven priorities), and states that when the objective targets a specific label the testing process is led to generate error-inducing inputs with that corresponding label. These design choices steer exploration toward predefined target outcomes (specific labels), meeting the metric's definition of supporting targeted attack discovery.",
                    "evidence": "Page 167, Section 3.3 Tunable Objective Function: \"If the objective function targets a specific label, the testing process will be led to enter the decision boundary of this specific label to explore corner cases, i.e., generate error-inducing inputs with the corresponding label.\"\n\nPage 167, Section 3.3 (Tunable objective function for ST): \"SOF(x) = C(x)[li]\" (where li is the selected label to optimize).\n\nPage 167, Section 3.3 (label ordering / prioritization): \"We prioritize labels that are prone to be vulnerable... We also allow users to prioritize labels as they want to support ad-hoc testing cases. For example... it is more risky if an ordinary employee is misclassified as a manager... users may prioritize finding error-inducing inputs that would be misclassified as managers.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight beyond merely reporting failures: it gives a model-informed explanation (Section 3.1) of why the proposed perturbations produce failures by relating them to convolutional kernel structure (the split into contiguous sign-based zones and the resulting amplified output change), and it analyzes statistical failure patterns in evaluation (Err-Num, Label-Num, SR, Inact-Rate). However, it does not present empirical analyses of internal neuron signals (no measured neuron coverage, activation frequency, or layer-by-layer activation statistics) nor does it connect discovered failures to broader sources such as dataset bias, spurious features, or overfitting. These facts match the metric\u2019s Medium level: diagnostic insight via analysis of observable model internals/statistical failure patterns is present, but deeper internal/neuron-level or dataset-bias/spurious-feature attribution is absent.",
                    "evidence": "p.166, Section 3.1 (Key Insight): \"A convolutional kernel can be split into multiple continuous zones where the corresponding weights have the same sign (+ or -). ... CNNs are prone to be affected by continuous perturbations.\"; p.166, Section 3.1 (analysis / equations): \"Since all elements in Ui have the same sign, we can induce that Ui \u00d7 \u03b4con_i is positive and is larger than |Ui \u00d7 \u03b4dis_i|, which makes Dif_dis < Dif_con.\" (explains why continuous perturbations are more likely to change outputs and induce failures); p.170, Section 4.1 (Metrics): \"Err-Num. ... Label-Num. ... SR.\" (shows statistical analysis of failure patterns used in evaluation); p.172, Section 4.3 (Inact-Rate discussion): \"We use the inactivation rate (Inact-Rate) to reflect this problem and compare the inactivation rate for all methods.\"; p.165, Introduction / background: \"Without any inner knowledge of target models, black-box testing methods cannot measure the coverage but can only find error-inducing inputs with diversified labels.\" (indicating the paper does not measure neuron coverage or per-neuron activations)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's evaluation focuses exclusively on the efficiency and effectiveness of its method (BET) in discovering error-inducing inputs. The performance is measured using quantitative metrics such as the number of errors found (Err-Num), the diversity of misclassified labels (Label-Num), and the success rate (SR) of finding at least one error. The paper does not provide any diagnostic analysis of the discovered failures. There is no investigation into observable model internals (as it is a black-box method) or any statistical analysis of failure patterns to understand why the models fail. The paper does not attempt to link the discovered failures to underlying model vulnerabilities like dataset bias, overfitting, or spurious features. In fact, the 'Discussion' section explicitly identifies the development of new metrics to \"understand the inner status of the target models\" by analyzing the discovered failures as future work, confirming the absence of such diagnostics in the current study.",
                    "evidence": "Page 12, Section 5, \"Novel metrics in black-box testing\": \"White-box model testing methods focus on fully exploring various internal states of target models [19]. ... But this metric is not suitable for black-box testing methods since testers cannot try to improve the neuron coverage without any inner knowledge of the neurons. Thus, we need new metrics for black-box testing methods to understand the inner status of the target models during the test. We think this challenge may be solved through learning or knowledge extraction from the error-inducing inputs acquired during the test. Thus, we list our second future work, i.e., specifying a new metric to reflect the inner states of target models for black-box testing.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "Although the paper presents a theoretical rationale for why CNNs are vulnerable to continuous perturbations (Section 3.1), it does not perform any empirical diagnostic analysis of the actual failures it discovers. The evaluation reports only aggregate performance metrics (Err-Num, Label-Num, SR, Inact-Rate) but never examines model internals (e.g. neuron activations, layer\u2010wise behavior) or statistics of the failures (e.g. class\u2010wise error concentration, correlations with input structure) to explain why the model fails in particular cases. The authors even cite as future work the need to develop new metrics to understand internal states in black\u2010box testing, confirming that no such diagnostics are present in this study.",
                    "evidence": "\u2013 Sections 4.1\u20134.3 report only Err-Num, Label-Num, SR and Inact-Rate, with no analysis of failure patterns or internals.\n\u2013 Section 5 (\u201cNovel metrics in black\u2010box testing\u201d) states: \u201cwe need new metrics for black\u2010box testing methods to understand the inner status of the target models\u2026 we list as future work\u2026 specifying a new metric to reflect the inner states of target models.\u201d"
                },
                "manual": {
                    "value": "Low",
                    "why": "Only reports failures without any insight on root-cause",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces bounded, input-centric perturbations throughout its mutator design and explicitly uses an L_infty constraint (L\u221e = 13/255) for all methods, and the mutator logic ensures each pixel is toggled only between original \u00b1 \u03b5 (so cumulative drift beyond the bound is prevented). These design choices constrain mutations to visually small changes and maintain per-pixel plausibility. However, the paper does not provide any empirical assessment of the perceptual or semantic realism of the final fault-inducing inputs (no human study or task-specific perceptual/fluency metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity are reported). According to the metric definition, this corresponds to Medium: plausibility is enforced in mutation design (bounded/rule-based), but the paper omits explicit plausibility validation of final inputs.",
                    "evidence": "Section 4.1 Hyperparameters (page 171): \"We use the widely used L\u221e metric [4] to restrict perturbations for error-inducing inputs on all compared methods... i.e., L\u221e = 13/255 on all datasets (all compared methods use the same setting).\"; Section 3.5 Mutator, \"Perturbation values\" (page 169): \"...we limit the perturbations to each byte to two choices {+\u03b5, \u2212\u03b5}... Given an original input Eo, a current best test case Eb... if Eb[i] = Eo[i], we randomly add perturbation value of +\u03b5 or \u2212\u03b5 to this byte. If Eb[i] = Eo[i] \u2212 \u03b5, we change this byte to Eo[i] + \u03b5. Otherwise, Eb[i] = Eo[i] + \u03b5 must hold and we change this byte to Eo[i] \u2212 \u03b5.\" (shows per-pixel changes remain within \u00b1\u03b5 of original); Section 4.3 Result of Inact-Rate (page 172): \"The Inact-Rate of BET is 0.0%. The perturbations are directly added on pixel values as integers, BET does not suffer the inactivation issue.\" (confirms direct integer pixel edits and bounded changes); Section 4.1 Metrics (page 171): lists evaluation metrics as Err-Num, Label-Num, SR, (and later Inact-Rate) but contains no perceptual or human-evaluation measures (no LPIPS/SSIM/PESQ/STOI/Perplexity or human study reported)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility to a limited extent by using a bounded perturbation constraint. Specifically, it uses the L\u221e norm to restrict the magnitude of modifications applied to the original input images. This is a common technique to ensure the perturbed image does not deviate excessively from the original, thereby acting as a proxy for plausibility. However, the paper does not provide any empirical validation to confirm that the resulting error-inducing inputs are perceptually or semantically realistic. There is no mention of human studies or quantitative, task-specific metrics like SSIM or LPIPS to assess the visual quality of the generated inputs. The evaluation focuses exclusively on the number and diversity of errors found. Therefore, the method meets the criteria for \"Medium\" as it enforces plausibility through a bounded constraint but does not empirically validate the plausibility of the final outputs.",
                    "evidence": "Page 8, Section 4.1 Experimental Setup, subsection Hyperparameters: \"We use the widely used L\u221e metric [4] to restrict perturbations for error-inducing inputs on all compared methods. To set values of L\u221e, we first run all compared methods under different L\u221e for multiple times, and then we choose values that make them perform relatively well, i.e., L\u221e = 13/255 on all datasets (all compared methods use the same setting).\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces bounded, input-centric perturbations throughout its mutator design and explicitly uses an L_infty constraint (L\u221e = 13/255) for all methods, and the mutator logic ensures each pixel is toggled only between original \u00b1 \u03b5 (so cumulative drift beyond the bound is prevented). These design choices constrain mutations to visually small changes and maintain per-pixel plausibility. However, the paper does not provide any empirical assessment of the perceptual or semantic realism of the final fault-inducing inputs (no human study or task-specific perceptual/fluency metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity are reported). According to the metric definition, this corresponds to Medium: plausibility is enforced in mutation design (bounded/rule-based), but the paper omits explicit plausibility validation of final inputs.",
                    "evidence": "Section 4.1 Hyperparameters (page 171): \"We use the widely used L\u221e metric [4] to restrict perturbations for error-inducing inputs on all compared methods... i.e., L\u221e = 13/255 on all datasets (all compared methods use the same setting).\"; Section 3.5 Mutator, \"Perturbation values\" (page 169): \"...we limit the perturbations to each byte to two choices {+\u03b5, \u2212\u03b5}... Given an original input Eo, a current best test case Eb... if Eb[i] = Eo[i], we randomly add perturbation value of +\u03b5 or \u2212\u03b5 to this byte. If Eb[i] = Eo[i] \u2212 \u03b5, we change this byte to Eo[i] + \u03b5. Otherwise, Eb[i] = Eo[i] + \u03b5 must hold and we change this byte to Eo[i] \u2212 \u03b5.\" (shows per-pixel changes remain within \u00b1\u03b5 of original); Section 4.3 Result of Inact-Rate (page 172): \"The Inact-Rate of BET is 0.0%. The perturbations are directly added on pixel values as integers, BET does not suffer the inactivation issue.\" (confirms direct integer pixel edits and bounded changes); Section 4.1 Metrics (page 171): lists evaluation metrics as Err-Num, Label-Num, SR, (and later Inact-Rate) but contains no perceptual or human-evaluation measures (no LPIPS/SSIM/PESQ/STOI/Perplexity or human study reported)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "norm-bounded",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly avoids the continuous\u2192discrete inactivation problem by generating perturbations directly on pixel/byte values (integers) and reports an Inact-Rate of 0.0%, which shows they consider discretization/clipping at the byte/pixel level during mutation and oracle evaluation. However, the paper does not explicitly discuss standard I/O serialization effects (e.g., image encoding/decoding, lossy formats, rounding introduced by file formats like JPEG/PNG or audio encoders) or any explicit handling/verification that failures remain after saving/loading inputs via common storage formats. This is a partial (but not full) treatment of serialization: they enforce reproducibility with respect to integer pixel representation and avoid rounding-inversion issues from gradient-based continuous perturbations, but they do not demonstrate explicit handling of file-format quantization/encoding effects. Under the metric definition, this corresponds to a Medium rating.",
                    "evidence": "Section 3.1 / p.166: \"Note that BET clearly departs from the white-box testing methods that first rely on gradient-based calculation and then transfer calculated decimals back to integers which may lead to ineffective samples (see inactivation issue in Section 4.3).\" Section 3.5 Mutator / p.169: \"Note each byte has 256 kinds of perturbations. Thus, for input with N pixels, the search space of perturbations is 256^N which is too large to explore efficiently. Instead, we limit the perturbations to each byte to two choices {+\u03b5, \u2212\u03b5}, where \u03b5 is a hyperparameter specific to application scenarios.\" Section 4.3 Results (Inact-Rate) / p.172: \"The Inact-Rate of BET is 0.0%. The perturbations are directly added on pixel values as integers, BET does not suffer the inactivation issue.\" Section 4.2 (Differential Testing setup) / p.170: discusses quantized models (8-bit/16-bit) via TensorFlow-Lite (model quantization), but there is no corresponding discussion in the methodology or evaluation about saving inputs to disk or explicitly modelling standard I/O encoding/decoding (PNG/JPEG/etc.) effects that might alter pixel values after serialization."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, BET, is explicitly designed to avoid the loss of fault-inducing perturbations during standard I/O operations by operating directly on integer pixel values. The authors identify this problem as the \"inactivation issue,\" where perturbations calculated in a continuous domain (floating-point) are lost when converted back to discrete integer values for storage or use. BET's mutation strategy involves adding integer values directly to the bytes of the input image, ensuring that the perturbations are inherently robust to rounding and quantization. The paper provides empirical evidence for this by measuring an \"Inact-Rate\" of 0.0% for BET, demonstrating that all discovered failures are reproducible after discretization, which directly addresses the core requirement of this metric.",
                    "evidence": "Page 10, Section 4.3, \"Result of Inact-Rate\": \"The Inact-Rate of BET is 0.0%. The perturbations are directly added on pixel values as integers, BET does not suffer the inactivation issue. However, baseline methods, especially the ones that use gradient-based methods suffer from this problem.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "While BET explicitly avoids the continuous\u2192discrete inactivation issue by generating and applying integer perturbations directly to pixel/byte values\u2014achieving an Inact-Rate of 0.0% and thus ensuring reproducibility with respect to in-memory discretization\u2014it does not address or evaluate any effects introduced by standard I/O serialization (e.g., image encoding/decoding, lossy formats, PNG/JPEG quantization). There is no discussion of writing inputs to disk or validating that failures persist after common file\u2010format operations. Hence, the paper provides only a partial treatment of serialization effects, corresponding to a Medium rating under the Failure Reproducibility metric.",
                    "evidence": "Section 4.3 (Inact-Rate 0.0%) shows integer\u2010level perturbations avoid inactivation when mapping floats to ints, but throughout Sections 3\u20134 there is no consideration of saving/loading via common storage formats (PNG/JPEG) or handling lossy encoding effects."
                },
                "manual": {
                    "value": "High",
                    "why": "Applies integer perturbations. Shows failure stability empricaclly. Shows ADAPT, DLFUZZ, DeepExplore failures are unstable",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly targets differential testing (DT) and includes a tunable objective function designed to maximize disagreements between a target model and other models (Equation 5). The authors construct surrogate/alternative implementations (quantized 8-bit and 16-bit variants) and generate fault-inducing inputs that cause disagreements between the original model and these other models; they report DT results (Tables 1\u20134) demonstrating many such cross-model disagreement inputs. Thus BET both (1) reuses/queries inputs across multiple models and (2) incorporates an explicit mechanism (the DT objective term summing differences with other models) to discover transferable failures across model implementations.",
                    "evidence": "Page 166, Section 2.2 (Related Work / definition of DT): \"There are two common scenarios for CNN testing, i.e., differential testing (DT) [31, 46] and single model testing (ST) [12, 19]. DT finds error-inducing inputs which trigger behavior inconsistencies among multiple models that have similar functionality ... For example, given two CNN models C1, C2 with similar functionality, an error-inducing input x_err is found if C1(x_err) \u2260 C2(x_err).\" \n\nPage 167, Section 3.3 (Tunable objective function for DT): \"Tunable objective function for DT. For DT, we hope to identify inputs that can maximize the prediction difference between the target model with other models.... The tunable objective function for DT is defined in Equation 5. DOF(x) = C(x)[l_i] + \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] |, ... The second term \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] | aims to find disagreements between the target model C with a batch of other models C\u0302 = {C\u0302_1, C\u0302_2, ...}, as such disagreements indicate error-inducing inputs are found.\" (Equation 5 and surrounding text)\n\nPage 170\u2013171, Section 4.2 (Evaluation: Constructing differential models / Results): \"Constructing differential models. ... We apply these two options to all models and we get corresponding 8-bits and 16-bits quantized models as shown in Table 1. Then, we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\" \n\nPage 171, Section 4.2 (Results): \"Results. Table 2, Table 3 and Table 4 show the results of DT on all datasets and models, respectively. Overall, under the same experimental conditions, BET significantly outperforms DiffChaser on all comparison metrics.\" (The presence of these DT-specific evaluation tables shows BET-generated inputs are evaluated across model variants to demonstrate cross-model disagreements.)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, BET, is explicitly designed and evaluated for a differential testing (DT) scenario, which aligns with the concept of attack transferability. The fuzzing design incorporates an explicit mechanism\u2014a tunable objective function for DT\u2014specifically to discover transferable failures by maximizing the prediction differences between a target model and other related models. The evaluation section is dedicated to demonstrating this capability, where generated inputs are systematically tested against pairs of models (an original and its quantized version) to find disagreements, which are a form of transferable failure.",
                    "evidence": "Page 6, Section 3.3, \"Tunable objective function for DT\": \"For DT, we hope to identify inputs that can maximize the prediction difference between the target model with other models... The tunable objective function for DT is defined in Equation 5... The second term \u03a3|C(x)[lo] \u2013 \u0108i(x)[lo]| aims to find disagreements between the target model C with a batch of other models \u0108 = {C1, C2, ...\u0108n}, as such disagreements indicate error-inducing inputs are found.\"\n\nPage 8, Section 4.2, \"Evaluation of Black-Box Differential Testing\": \"Constructing differential models. To get a convincing DT result, we directly use Tensorflow-Lite... to construct quantized models for DT... Then, we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly targets differential testing (DT) and includes a tunable objective function designed to maximize disagreements between a target model and other models (Equation 5). The authors construct surrogate/alternative implementations (quantized 8-bit and 16-bit variants) and generate fault-inducing inputs that cause disagreements between the original model and these other models; they report DT results (Tables 1\u20134) demonstrating many such cross-model disagreement inputs. Thus BET both (1) reuses/queries inputs across multiple models and (2) incorporates an explicit mechanism (the DT objective term summing differences with other models) to discover transferable failures across model implementations.",
                    "evidence": "Page 166, Section 2.2 (Related Work / definition of DT): \"There are two common scenarios for CNN testing, i.e., differential testing (DT) [31, 46] and single model testing (ST) [12, 19]. DT finds error-inducing inputs which trigger behavior inconsistencies among multiple models that have similar functionality ... For example, given two CNN models C1, C2 with similar functionality, an error-inducing input x_err is found if C1(x_err) \u2260 C2(x_err).\" \n\nPage 167, Section 3.3 (Tunable objective function for DT): \"Tunable objective function for DT. For DT, we hope to identify inputs that can maximize the prediction difference between the target model with other models.... The tunable objective function for DT is defined in Equation 5. DOF(x) = C(x)[l_i] + \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] |, ... The second term \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] | aims to find disagreements between the target model C with a batch of other models C\u0302 = {C\u0302_1, C\u0302_2, ...}, as such disagreements indicate error-inducing inputs are found.\" (Equation 5 and surrounding text)\n\nPage 170\u2013171, Section 4.2 (Evaluation: Constructing differential models / Results): \"Constructing differential models. ... We apply these two options to all models and we get corresponding 8-bits and 16-bits quantized models as shown in Table 1. Then, we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\" \n\nPage 171, Section 4.2 (Results): \"Results. Table 2, Table 3 and Table 4 show the results of DT on all datasets and models, respectively. Overall, under the same experimental conditions, BET significantly outperforms DiffChaser on all comparison metrics.\" (The presence of these DT-specific evaluation tables shows BET-generated inputs are evaluated across model variants to demonstrate cross-model disagreements.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "Uses differential testing based oracle to detect failure, not to check transferability from one to another.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential",
                "Property-based"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Zhang et al. - 2020 - Towards characterizing adversarial defects of deep learning software from the lens of uncertainty": {
        "key": "zhang2020towards",
        "name": "KuK",
        "year": "2020",
        "citation_count": "101",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "High",
                    "why": "The paper's KuK fuzzer intentionally generates \"uncommon\" adversarial inputs that explicitly bypass existing adversarial defense mechanisms. The authors evaluate multiple defenses and report that these uncommon inputs reduce defense effectiveness substantially (e.g., large drops in detection/mitigation success, and concrete examples where defenses fail on the uncommon data). Because the method finds inputs that defeat explicit safety/security safeguards (adversarial defenses), it uncovers high-impact failures under the metric definition.",
                    "evidence": "Page 740, Introduction / Contributions: \"Our further evaluation reveals that the uncommon data generated by our method is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%.\" \n\nPage 740, Introduction / Contributions: \"For example, on the model NIN, the uncommon data achieve 97.5% success rate on bypassing the mutation-based defense, while the common data only make it 5.5%.\"\n\nPage 748, Section 5.2 (RQ4) / Answer to RQ4: \"Answer to RQ4: The uncommon data inputs are not well defended by existing defense techniques while the common data are relatively easier to be defended. In particular, the binary classifier and mutation-based detection approaches are less useful in defending the uncommon data inputs (e.g., with only 5% and 10% on NIN and ResNet-20, respectively).\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper proposes a method, implemented as a tool named KuK, to generate \"uncommon\" input data (adversarial examples) based on uncertainty metrics. A key part of the paper's evaluation is to test whether this generated uncommon data can bypass explicit, state-of-the-art defense mechanisms designed to protect models against adversarial attacks. The results demonstrate that the generated inputs successfully evade a suite of defense techniques\u2014including mutation-based detection, defensive distillation, feature squeezing, and binary classifiers\u2014with significantly higher success rates compared to common adversarial examples. This directly aligns with the 'High' severity rating, which requires uncovering failures by bypassing explicit safety or security mechanisms in defended models.",
                    "evidence": "Page 10, Section 5.2, \"RQ4: Evaluation on Defense Techniques\": \"To demonstrate the usefulness of the generated uncommon data in Table 5, this experiment intends to study whether the data can bypass the existing defense techniques. Setting. Since different defense techniques are proposed on different subject datasets, we selected popular techniques based on the datasets. For MNIST and CIFAR10 dataset, we selected the following defense techniques: binary activation classifier [13], mutation-based adversarial attack detection [53], defensive distillation [40], label smoothing [17], and feature squeezing [57]. For ImageNet, we selected the mutation-based adversarial attack detection [53], input transformations [15] and pixel deflection [43].\" \n\nPage 11, \"Answer to RQ4\": \"The uncommon data inputs are not well defended by existing defense techniques while the common data are relatively easier to be defended. In particular, the binary classifier and mutation-based detection approaches are less useful in defending the uncommon data inputs (e.g., with only 5% and 10% on NIN and ResNet-20, respectively).\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper's KuK fuzzer intentionally generates \"uncommon\" adversarial inputs that explicitly bypass existing adversarial defense mechanisms. The authors evaluate multiple defenses and report that these uncommon inputs reduce defense effectiveness substantially (e.g., large drops in detection/mitigation success, and concrete examples where defenses fail on the uncommon data). Because the method finds inputs that defeat explicit safety/security safeguards (adversarial defenses), it uncovers high-impact failures under the metric definition.",
                    "evidence": "Page 740, Introduction / Contributions: \"Our further evaluation reveals that the uncommon data generated by our method is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%.\" \n\nPage 740, Introduction / Contributions: \"For example, on the model NIN, the uncommon data achieve 97.5% success rate on bypassing the mutation-based defense, while the common data only make it 5.5%.\"\n\nPage 748, Section 5.2 (RQ4) / Answer to RQ4: \"Answer to RQ4: The uncommon data inputs are not well defended by existing defense techniques while the common data are relatively easier to be defended. In particular, the binary classifier and mutation-based detection approaches are less useful in defending the uncommon data inputs (e.g., with only 5% and 10% on NIN and ResNet-20, respectively).\""
                },
                "manual": {
                    "value": "High",
                    "why": "Demostrates its ability to bypass adversarail defense mechanisms",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's fuzzer (KuK) steers test generation by optimizing uncertainty-based objectives (PCS and VRO) to produce uncommon inputs and to cause misclassification (AEs) or produce benign examples with particular uncertainty patterns. Its objectives and fitness functions are defined in terms of prediction confidence and Bayesian uncertainty (PCS, VRO) and in some cases an AE/BE indicator, but nowhere does KuK attempt to force a particular attacker-chosen output label or specific target prediction. Thus the exploration is untargeted with respect to attacker-selected outputs and instead discovers generic failures (adversarial examples / misclassifications and unusual uncertainty-pattern inputs).",
                    "evidence": "Page 740, Introduction: \"we propose a genetic algorithm (GA) based automated test generation technique that iteratively generates uncommon input samples guided by uncertainty metrics. We implement the proposed technique as a tool named KuK (to Know the UnKnown)...\" (Sec. 1).; Page 744, Sec. 3.3 (Characterizing Data Behavior): \"we adopt a two-dimensional metric, i.e., (PCS, VRO), to characterize the BEs and AEs on a specific DNN model. Based on this, the data is classified into four patterns ... LH, HH, LL and HL.\"; Page 746, Sec. 4 (Objective and Fitness Calculation): \"To generate AEs which belong to the HL type, the objective is set as: PCS(x, M) > p \u2227 VRO(x, M) < v \u2227 x is an AE, where p and v are the configurable parameters...\" (Eq. 3) \u2014 objective specified by uncertainty metrics and AE status, not by a target label; Page 746, Sec. 4 (also Eq. 4): \"To generate BEs that belong to LH type, the objective is set as: PCS(x, M) < p \u2227 VRO(x, M) > v and x is a BE\" \u2014 again objectives are uncertainty-pattern based rather than steering toward attacker-chosen outputs."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed method, KuK, is designed for untargeted exploration to find generic failures. The goal is to generate adversarial examples (AEs) that exhibit specific, \"uncommon\" uncertainty patterns, rather than inducing predefined, attacker-chosen outputs. The paper defines an AE as any input that causes a misclassification, without specifying what the incorrect prediction should be. The method's genetic algorithm uses fitness functions that guide the search towards inputs that are misclassified and also have certain uncertainty characteristics (e.g., high prediction confidence and low variation ratio). There is no mechanism described to steer the mutations towards a specific target label. Therefore, the method performs untargeted exploration to find generic misclassifications.",
                    "evidence": "Page 9, Section 4: The objective for generating a specific type of adversarial example (HL AEs) is defined as 'PCS(x, M) > p \u2227 VRO(x, M) < v \u2227 x is an AE'. The associated fitness function includes a term 'where AE(x) is 1 if x is an AE. Otherwise, it is 0.' An AE is earlier defined on Page 4, Section 2.1 as an input `x'` causing 'a different prediction decision of a DNN M (i.e., LM(x') \u2260 LM(x))'. This demonstrates the goal is to achieve any misclassification that fits the uncertainty profile, not a specific target output."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's fuzzer (KuK) steers test generation by optimizing uncertainty-based objectives (PCS and VRO) to produce uncommon inputs and to cause misclassification (AEs) or produce benign examples with particular uncertainty patterns. Its objectives and fitness functions are defined in terms of prediction confidence and Bayesian uncertainty (PCS, VRO) and in some cases an AE/BE indicator, but nowhere does KuK attempt to force a particular attacker-chosen output label or specific target prediction. Thus the exploration is untargeted with respect to attacker-selected outputs and instead discovers generic failures (adversarial examples / misclassifications and unusual uncertainty-pattern inputs).",
                    "evidence": "Page 740, Introduction: \"we propose a genetic algorithm (GA) based automated test generation technique that iteratively generates uncommon input samples guided by uncertainty metrics. We implement the proposed technique as a tool named KuK (to Know the UnKnown)...\" (Sec. 1).; Page 744, Sec. 3.3 (Characterizing Data Behavior): \"we adopt a two-dimensional metric, i.e., (PCS, VRO), to characterize the BEs and AEs on a specific DNN model. Based on this, the data is classified into four patterns ... LH, HH, LL and HL.\"; Page 746, Sec. 4 (Objective and Fitness Calculation): \"To generate AEs which belong to the HL type, the objective is set as: PCS(x, M) > p \u2227 VRO(x, M) < v \u2227 x is an AE, where p and v are the configurable parameters...\" (Eq. 3) \u2014 objective specified by uncertainty metrics and AE status, not by a target label; Page 746, Sec. 4 (also Eq. 4): \"To generate BEs that belong to LH type, the objective is set as: PCS(x, M) < p \u2227 VRO(x, M) > v and x is a BE\" \u2014 again objectives are uncertainty-pattern based rather than steering toward attacker-chosen outputs."
                },
                "manual": {
                    "value": "Low",
                    "why": "aims to maximize model uncertainty and reveal generic misclassifications",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides substantive diagnostic analysis of failures using observable model-internal uncertainty signals and statistical failure patterns (single-shot PCS and Bayesian multi-shot metrics such as VRO/VR/PE/MI). It systematically characterizes adversarial and benign examples in a PCS\u2013VRO space, identifies common vs. uncommon uncertainty patterns, and quantifies how those patterns affect defense success. However, it does not connect discovered failures to deeper underlying vulnerabilities such as reliance on spurious/non-robust features, dataset bias, or overfitting (no neuron-level, layer/position, or coverage-based causal analysis), so it does not meet the \"High\" criterion. This aligns with the metric\u2019s \"Medium\" definition: providing diagnostic insight via analysis of observable internals and statistical failure patterns but not explaining root causes like spurious features or dataset bias.",
                    "evidence": "Abstract (p.740): \"We first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data.\"; Section 3.2 RQ1 (p.743): \"The objective of RQ1 is to study the relationship between uncertainty metrics and adversarial examples. In particular, we analyze the effectiveness of uncertainty metrics in distinguishing AEs and BEs.\"; Section 3.3 Characterizing Data Behavior (p.744): \"Therefore, we adopt a two-dimensional metric, i.e., (PCS, VRO), to characterize the BEs and AEs on a specific DNN model. Based on this, the data is classified into four patterns (see Fig. 2 (a)): low PCS and high VRO (LH), high PCS and high VRO (HH), low PCS and low VRO (LL) and high PCS and low VRO (HL).\"; Section 3.4 Answer to RQ2 (p.746): \"Answer to RQ2: BEs and AEs usually belong to the HL type and LH type, respectively. Compared with state-of-the-art adversarial attacks, testing tools to some extent generate different AEs.\"; Section 5.2 (p.748): \"The overall result shows that the existing defense techniques perform poorly on the uncommon data we generated.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides significant diagnostic insight through an extensive statistical analysis of failure patterns. It introduces and empirically validates a method for characterizing adversarial examples (failures) based on their uncertainty profiles, specifically using Prediction Confidence Score (PCS) and Variation Ratio (VRO). The research identifies that adversarial examples generated by standard methods typically exhibit a common pattern of low PCS and high VRO. This characterization represents a deep statistical analysis of failure patterns, which directly aligns with the criteria for a 'Medium' rating. The paper uses this analysis to guide the generation of 'uncommon' failures with different uncertainty patterns. However, the analysis does not extend to linking these observed failure patterns to underlying model vulnerabilities such as reliance on non-robust features, dataset bias, or overfitting. The focus remains on the characteristics of the model's output behavior (uncertainty) rather than the root causes within the model's learned representation, thus not meeting the criteria for 'High'.",
                    "evidence": "Page 7, Section 3.3, Characterizing Data Behavior: \"Therefore, we adopt a two-dimensional metric, i.e., (PCS, VRO), to characterize the BEs and AEs on a specific DNN model. Based on this, the data is classified into four patterns (see Fig. 2 (a)): low PCS and high VRO (LH), high PCS and high VRO (HH), low PCS and low VRO (LL) and high PCS and low VRO (HL). The categorization provides a way to understand and analyze the behaviors of AEs and BEs.\"\nPage 8, Answer to RQ2: \"Answer to RQ2: BEs and AEs usually belong to the HL type and LH type, respectively. Compared with state-of-the-art adversarial attacks, testing tools to some extent generate different AEs.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides substantive diagnostic analysis of failures using observable model-internal uncertainty signals and statistical failure patterns (single-shot PCS and Bayesian multi-shot metrics such as VRO/VR/PE/MI). It systematically characterizes adversarial and benign examples in a PCS\u2013VRO space, identifies common vs. uncommon uncertainty patterns, and quantifies how those patterns affect defense success. However, it does not connect discovered failures to deeper underlying vulnerabilities such as reliance on spurious/non-robust features, dataset bias, or overfitting (no neuron-level, layer/position, or coverage-based causal analysis), so it does not meet the \"High\" criterion. This aligns with the metric\u2019s \"Medium\" definition: providing diagnostic insight via analysis of observable internals and statistical failure patterns but not explaining root causes like spurious features or dataset bias.",
                    "evidence": "Abstract (p.740): \"We first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data.\"; Section 3.2 RQ1 (p.743): \"The objective of RQ1 is to study the relationship between uncertainty metrics and adversarial examples. In particular, we analyze the effectiveness of uncertainty metrics in distinguishing AEs and BEs.\"; Section 3.3 Characterizing Data Behavior (p.744): \"Therefore, we adopt a two-dimensional metric, i.e., (PCS, VRO), to characterize the BEs and AEs on a specific DNN model. Based on this, the data is classified into four patterns (see Fig. 2 (a)): low PCS and high VRO (LH), high PCS and high VRO (HH), low PCS and low VRO (LL) and high PCS and low VRO (HL).\"; Section 3.4 Answer to RQ2 (p.746): \"Answer to RQ2: BEs and AEs usually belong to the HL type and LH type, respectively. Compared with state-of-the-art adversarial attacks, testing tools to some extent generate different AEs.\"; Section 5.2 (p.748): \"The overall result shows that the existing defense techniques perform poorly on the uncommon data we generated.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Shows correlatin",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints during generation (bounded pixel perturbations and avoiding certain transformations) but does not provide any empirical evaluation (human study or perceptual/semantic similarity metrics) of the final fault-inducing inputs. Concretely, KuK\u2019s GA uses L_infty bounds and avoids affine/crossover operations that would create invalid images, which constrains individual mutation steps and initialization relative to the original seed. However, the authors do not report task-specific perceptual/semantic measures (e.g., SSIM/LPIPS) or any human evaluation to demonstrate that the final uncommon AEs/BEs remain perceptually or semantically realistic. This matches the metric\u2019s \u201cMedium\u201d level: plausibility is enforced by bounded/rule-based constraints but final-input plausibility is not validated empirically.",
                    "evidence": "Section 4 (Uncommon Data Generation) and Section 5.1 (RQ3 setting):\n\n1) Constraint on mutations / avoiding invalid transforms (Section 4, Population Initialization): \"In order to generate high-quality images (i.e., recognizable by human), we abandon the affine transformation (e.g., translation and rotation [55]) as the crossover may generate invalid images. We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\" (Section 4, p. 746)\n\n2) Explicit L\u221e mutation radius in evaluation settings (Section 5.1): \"For the mutation process, the radius of L\u221e is set as 0.3.\" (Section 5.1, p. 746)\n\n3) Lack of explicit perceptual / human plausibility assessment: the paper\u2019s evaluations (RQ3/RQ4) report counts, PCS/VRO statistics and defense success rates but contain no reported human study or task-specific perceptual/semantic metrics (e.g., SSIM, LPIPS, PESQ, STOI, or Perplexity) for final fault-inducing inputs. For example, the experimental descriptions and results focus on generated counts and PCS/VRO distributions (Tables 4\u20136) and defense bypass rates (Section 5.2), but no perceptual-quality or human-evaluation results are presented (Sections 3\u20135, pp. 743\u2013748)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility using a bounded constraint but does not empirically validate the plausibility of the final generated inputs. The method uses an L\u221e norm to constrain the allowable changes between the original seed image and its generated counterparts, with the explicit goal of generating \"high-quality images (i.e., recognizable by human)\". This use of a bounded perturbation is a mechanism to control plausibility. However, the paper provides no empirical assessment to confirm that this goal was achieved. There are no human studies, quantitative perceptual metrics (e.g., SSIM, LPIPS), or even qualitative examples of the final fault-inducing images to demonstrate their realism or semantic validity. This fulfills the criteria for a \"Medium\" rating, as it includes a plausibility enforcement mechanism but lacks the empirical validation of the final outputs required for a \"High\" rating.",
                    "evidence": "Page 8, Section 4, \"Population Initialization\": \"In order to generate high-quality images (i.e., recognizable by human), we abandon the affine transformation (e.g., translation and rotation [55]) as the crossover may generate invalid images. We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\"\nPage 9, Section 5.1, \"Setting\": \"For the mutation process, the radius of L\u221e is set as 0.3.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints during generation (bounded pixel perturbations and avoiding certain transformations) but does not provide any empirical evaluation (human study or perceptual/semantic similarity metrics) of the final fault-inducing inputs. Concretely, KuK\u2019s GA uses L_infty bounds and avoids affine/crossover operations that would create invalid images, which constrains individual mutation steps and initialization relative to the original seed. However, the authors do not report task-specific perceptual/semantic measures (e.g., SSIM/LPIPS) or any human evaluation to demonstrate that the final uncommon AEs/BEs remain perceptually or semantically realistic. This matches the metric\u2019s \u201cMedium\u201d level: plausibility is enforced by bounded/rule-based constraints but final-input plausibility is not validated empirically.",
                    "evidence": "Section 4 (Uncommon Data Generation) and Section 5.1 (RQ3 setting):\n\n1) Constraint on mutations / avoiding invalid transforms (Section 4, Population Initialization): \"In order to generate high-quality images (i.e., recognizable by human), we abandon the affine transformation (e.g., translation and rotation [55]) as the crossover may generate invalid images. We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\" (Section 4, p. 746)\n\n2) Explicit L\u221e mutation radius in evaluation settings (Section 5.1): \"For the mutation process, the radius of L\u221e is set as 0.3.\" (Section 5.1, p. 746)\n\n3) Lack of explicit perceptual / human plausibility assessment: the paper\u2019s evaluations (RQ3/RQ4) report counts, PCS/VRO statistics and defense success rates but contain no reported human study or task-specific perceptual/semantic metrics (e.g., SSIM, LPIPS, PESQ, STOI, or Perplexity) for final fault-inducing inputs. For example, the experimental descriptions and results focus on generated counts and PCS/VRO distributions (Tables 4\u20136) and defense bypass rates (Section 5.2), but no perceptual-quality or human-evaluation results are presented (Sections 3\u20135, pp. 743\u2013748)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "norm-bounded",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's fuzzing / test-generation approach operates by applying floating-point pixel-level mutations (noise, pixel crossover, L\u221e constraints) directly to in-memory image tensors and does not mention or enforce any constraints to preserve perturbations under standard I/O serialization (e.g., 8-bit quantization, clipping, rounding), nor does it generate inputs directly in serialized formats. Because the method neither accounts for serialization effects during test-case generation nor during oracle/evaluation, fault-inducing perturbations introduced in memory can be lost or altered when images are saved or encoded, so the paper ignores I/O effects and therefore rates Low on Failure Reproducibility.",
                    "evidence": "Page 746, Section 4 (Uncommon Data Generation) \u2014 \"Population Initialization. Given an input image, we first generate a set of images by randomly adding noise to it... We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\" \n\nPage 746, Section 4 (Uncommon Data Generation) \u2014 \"For the crossover, we adopt the tournament selection strategy ... The two selected images are used to perform the crossover by randomly exchanging the corresponding pixels. After the crossover, each image is randomly mutated by adding white noise, to increase the diversity of the population.\" \n\nPage 746, Section 5 (Evaluation, Implementation) \u2014 \"We implemented the proposed test generation tool, KuK, in Python based on Keras [5] (2.2.4) with TensorFlow [1] (1.12.0) as backend.\" \n\n(These passages show mutations are applied to image tensors via floating-point noise and pixel operations and evaluated in-memory using standard DL frameworks; the paper contains no discussion of serialization, quantization, clipping, saving to common image formats, or steps to ensure perturbations persist after typical I/O operations.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a genetic algorithm-based method, KuK, to generate test inputs by applying mutations. The primary mutation operators involve adding numerical noise to images. Specifically, the paper mentions \"randomly adding noise to it\" for population initialization and mutating images by \"adding white noise\" during the genetic algorithm's evolution phase. These operations manipulate the in-memory numerical representation of the inputs. The methodology does not describe any steps to account for serialization effects like quantization, rounding, or clipping that would occur when saving these perturbed images to standard file formats (e.g., 8-bit PNG). Consequently, the small, fault-inducing numerical perturbations are likely fragile and may not be reproducible after standard I/O operations, which aligns with the definition for a 'Low' rating.",
                    "evidence": "Page 8, Section 4, \"UNCOMMON DATA GENERATION\": \"Population Initialization. Given an input image, we first generate a set of images by randomly adding noise to it. In order to generate high-quality images (i.e., recognizable by human), we abandon the affine transformation (e.g., translation and rotation [55]) as the crossover may generate invalid images. We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\"\n\nPage 9, Section 4, \"UNCOMMON DATA GENERATION\": \"Crossover and Mutation. ... After the crossover, each image is randomly mutated by adding white noise, to increase the diversity of the population.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper's fuzzing / test-generation approach operates by applying floating-point pixel-level mutations (noise, pixel crossover, L\u221e constraints) directly to in-memory image tensors and does not mention or enforce any constraints to preserve perturbations under standard I/O serialization (e.g., 8-bit quantization, clipping, rounding), nor does it generate inputs directly in serialized formats. Because the method neither accounts for serialization effects during test-case generation nor during oracle/evaluation, fault-inducing perturbations introduced in memory can be lost or altered when images are saved or encoded, so the paper ignores I/O effects and therefore rates Low on Failure Reproducibility.",
                    "evidence": "Page 746, Section 4 (Uncommon Data Generation) \u2014 \"Population Initialization. Given an input image, we first generate a set of images by randomly adding noise to it... We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\" \n\nPage 746, Section 4 (Uncommon Data Generation) \u2014 \"For the crossover, we adopt the tournament selection strategy ... The two selected images are used to perform the crossover by randomly exchanging the corresponding pixels. After the crossover, each image is randomly mutated by adding white noise, to increase the diversity of the population.\" \n\nPage 746, Section 5 (Evaluation, Implementation) \u2014 \"We implemented the proposed test generation tool, KuK, in Python based on Keras [5] (2.2.4) with TensorFlow [1] (1.12.0) as backend.\" \n\n(These passages show mutations are applied to image tensors via floating-point noise and pixel operations and evaluated in-memory using standard DL frameworks; the paper contains no discussion of serialization, quantization, clipping, saving to common image formats, or steps to ensure perturbations persist after typical I/O operations.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "Applies small floating point perturbations without clipping/rounding",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or measure whether fault-inducing inputs crafted on one model transfer to other models performing the same task. All experiments generate and evaluate adversarial / uncommon inputs per target model (attacks and KuK generation are applied to each model separately) and the evaluation of \"transfer\" in the paper is limited to applying inputs crafted on an original model to a defended/retrained version of that same model (a defense transfer scenario), not to distinct other model implementations or architectures. There is no reported experiment that reuses inputs generated on one surrogate model to test other (unseen) models or that explicitly measures cross-model transferability across different model implementations (e.g., LeNet-5 \u2192 NIN or ResNet \u2192 MobileNet).",
                    "evidence": "Page 740, Introduction (research questions): \"RQ4: To what extent are the uncommon samples defended by existing adversarial defense techniques compared with the common ones?\" \u2014 shows focus on defenses, not cross-model transfer.\n\nPage 743, Section 3.1.3 Data Preparation: \"For each dataset, we prepared the following three sets of data: one set of benign examples, one set of AEs generated from the attack methods, and one set of AEs generated by testing tools... For each input in BenignData, we generated four types of AEs with the four attack methods, resulting in a total of 36,000 AEs.\" \u2014 indicates attacks/AEs are generated per model/dataset but does not describe applying them to different model implementations.\n\nPage 748, Section 5.2 Results: \"The reason is that a new model is retrained with these defense techniques, while the attacks are generated regarding to the original one, making it a more challenging transfer attack scenario.\" \u2014 the only 'transfer' discussion refers to transfer to a retrained/defended version of the same model, not to other distinct models or architectures.\n\n(Throughout the Methods and Evaluation sections (e.g., Tables 2, 4, 5 and 6) adversarial/uncommon inputs are reported per model; no experiment or section describes testing inputs crafted on one model against other model implementations to measure cross-model transferability.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper evaluates whether the 'uncommon data' generated for an original model can bypass defense techniques that involve retraining the model (e.g., defensive distillation and label smoothing). The authors explicitly describe this as a 'transfer attack scenario', thus testing if the generated inputs (fault-inducing inputs) transfer to a different but related model. However, the proposed generation method, KuK, is not explicitly designed to discover transferable attacks. Its genetic algorithm is guided by fitness functions based on the uncertainty metrics of a single target model, without incorporating mechanisms to generalize the attack across multiple models.",
                    "evidence": "Page 11, Section 5.2 RQ4: Evaluation on Defense Techniques, 'For other defense techniques, the reduction in success rate appears smaller than that of binary classifier and mutation-based detection. For example, the success rates drop to 78.3% and 76.2% for defensive distillation and label smoothing on NIN. The reason is that a new model is retrained with these defense techniques, while the attacks are generated regarding to the original one, making it a more challenging transfer attack scenario. For example, defensive distillation retrains a more robust model by reducing the gradients. In this case, some of the data, which are uncommon for original model, become common data w.r.t. the retrained model, because of some weight variation. However, it still can be seen from the results that the uncommon data reveal stronger transferability.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not include any experiments or analysis of whether fault-inducing inputs generated on one model transfer to other, independently trained model implementations performing the same task. All adversarial and uncommon inputs are crafted and evaluated per target model (including defense-retrained variants of that same model), but no inputs generated on one architecture are tested on a different architecture or distinct model instance without access. Therefore it does not demonstrate cross-model transferability.",
                    "evidence": "Page 743 (\u00a73.1.3 Data Preparation): adversarial examples are generated \u201cfor each input in BenignData\u2026 on each model,\u201d with no mention of re-using those inputs on different models. \nPage 748 (\u00a75.2 RQ4): discussion of \u201ctransfer attack scenario\u201d refers only to attacks against a retrained/defended version of the same model, not to separate model implementations (e.g., LeNet-5\u2192NIN or ResNet\u2192MobileNet). \nThroughout (e.g. Tables 2, 4\u20136) all results report attacks and defenses per model, with no cross-model evaluation of transferability."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Tests whether the same inputs still cause failures on the defended model (RQ4) but lacks explicit mechansim to discover transferable attacks",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Pang et al. - 2022 - MDPFuzz testing models solving Markov decision processes": {
        "key": "pang2022mdpfuzz",
        "name": "MDPFuzz",
        "year": "2022",
        "citation_count": "39",
        "assessments": {
            "Failure Severity": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper demonstrates that MDPFuzz uncovers high-impact, safety-related failures (explicitly defined as crashes such as vehicle or airplane collisions) across multiple real-world MDP scenarios (autonomous driving, aircraft collision avoidance, multi-agent navigation, etc.). These are violations of user safety expectations (unsafe/undesired outputs). However, the paper does not claim to bypass explicit safety- or security-focused defenses, nor does it evaluate or report bypassing defended or safety-aligned models (e.g., adversarially trained or policy-gated systems) or revealing security-critical behaviors like data leakage or jailbreaks. Thus the appropriate classification is Medium: it uncovers unsafe, high-impact outputs but does not demonstrate bypassing explicit safety/security mechanisms.",
                    "evidence": "Page 378, Abstract: \"MDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states.\"; Page 378, Abstract: \"We evaluate five state-of-the-art models for solving MDPs, including supervised DNN, RL, IL, and multi-agent RL... During a 12-hour run, we find over 80 crash-triggering state sequences on each model.\"; Page 385, Section 6 (Testing Oracles (Crash Definition)): \"For DNN-based ACAS Xu, we check collisions of the DNN-controlled airplane with other airplanes.\"; Page 381, Section 4 (Definition 2): \"We regard crashes incurred from a valid and solvable initial state as bugs of a model pi solving MDP.\"; Page 385, Section 6 (MDP Initial State Sampling, Mutation, and Validation): \"We guarantee that there exist optimal actions to avoid the crash and we do not use an initial state that is not solvable.\"; Page 382, Section 5 (Assumptions): \"In blackbox settings, the internal of the agent model (our testing target) is not revealed to MDPFuzz... MDPFuzz has no access to the blackbox model internals.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper introduces MDPFuzz, a testing framework designed to find failures where models solving Markov decision processes (MDPs) enter \"abnormal and dangerous states.\" The methodology explicitly moves beyond finding simple robustness errors like prediction inconsistencies, which would fall under the 'Low' severity category. Instead, it defines its testing oracles based on concrete, unsafe behaviors that violate safety expectations for the given domain. For example, in autonomous driving, a failure is a vehicle collision; for aircraft collision avoidance, it is a collision between planes; for a walking agent, it is falling over. These are clear examples of unsafe behaviors that violate user intent and safety expectations. However, the paper does not describe the tested models as having explicit, separate safety or security mechanisms that are being bypassed (a requirement for 'High' severity). It finds failures in the primary, intended function of state-of-the-art models in safety-critical scenarios, thus fitting the 'Medium' category.",
                    "evidence": "Page 5, Section 4, \"Inconsistencies vs. Crashes\": \"Real-world models for solving MDPs can quickly recover from 'inconsistent' predictions. This demands the creation of oracles over concrete and severe abnormal states, such as collisions in airplane control or autonomous driving.\"\n\nPage 9, Section 6, \"Testing Oracles (Crash Definition)\": \"For RL and IL models in CARLA, we examine whether the model-controlled vehicle collides with other vehicles or buildings. For DNN-based ACAS Xu, we check collisions of the DNN-controlled airplane with other airplanes. For Coop Navi, we check collisions between MARL-controlled agents. For BipedalWalker, we check whether the RL-controlled walking agent falls (i.e., its head touches the ground).\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper demonstrates that MDPFuzz uncovers high-impact, safety-related failures (explicitly defined as crashes such as vehicle or airplane collisions) across multiple real-world MDP scenarios (autonomous driving, aircraft collision avoidance, multi-agent navigation, etc.). These are violations of user safety expectations (unsafe/undesired outputs). However, the paper does not claim to bypass explicit safety- or security-focused defenses, nor does it evaluate or report bypassing defended or safety-aligned models (e.g., adversarially trained or policy-gated systems) or revealing security-critical behaviors like data leakage or jailbreaks. Thus the appropriate classification is Medium: it uncovers unsafe, high-impact outputs but does not demonstrate bypassing explicit safety/security mechanisms.",
                    "evidence": "Page 378, Abstract: \"MDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states.\"; Page 378, Abstract: \"We evaluate five state-of-the-art models for solving MDPs, including supervised DNN, RL, IL, and multi-agent RL... During a 12-hour run, we find over 80 crash-triggering state sequences on each model.\"; Page 385, Section 6 (Testing Oracles (Crash Definition)): \"For DNN-based ACAS Xu, we check collisions of the DNN-controlled airplane with other airplanes.\"; Page 381, Section 4 (Definition 2): \"We regard crashes incurred from a valid and solvable initial state as bugs of a model pi solving MDP.\"; Page 385, Section 6 (MDP Initial State Sampling, Mutation, and Validation): \"We guarantee that there exist optimal actions to avoid the crash and we do not use an initial state that is not solvable.\"; Page 382, Section 5 (Assumptions): \"In blackbox settings, the internal of the agent model (our testing target) is not revealed to MDPFuzz... MDPFuzz has no access to the blackbox model internals.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Expose  safety critical violations that lead to crashes/collisions or unsafe actions in autonomous or multi-agent environments.",
                    "evidence": ""
                }
            },
            "Targeted Attack Discovery": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "MDPFuzz explicitly steers exploration toward detecting abnormal or dangerous behaviors (crashes) across MDP scenarios (e.g., collisions, falls) using oracles, cumulative-reward reduction, and sequence-freshness guidance. Its objective is to find broader classes of security-violating behaviors (crashes/abnormal states) rather than to induce a specific attacker-chosen model output (a particular label, phrase, or exact action). The paper repeatedly frames its goal as discovering crash-triggering state sequences (Definition 2), prioritizing seeds that lower cumulative reward, and checking whether the model enters unsafe states (the testing oracle), which matches the \u201cMedium\u201d category: discovery of broader unsafe behaviors rather than targeted-output attacks.",
                    "evidence": "Page 378, Abstract: \"MDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states.\"; Page 381, Section 4 (Inconsistencies vs. Crashes): \"Real-world models for solving MDPs can quickly recover from 'inconsistent' predictions. This demands the creation of oracles over concrete and severe abnormal states, such as collisions in airplane control or autonomous driving.\"; Page 381, Definition 2 (Crash-triggering state sequences): \"We regard crashes incurred from a valid and solvable initial state as bugs of a model \u03c0 solving MDP.\"; Page 384, Section 5.3 (Feedback from Freshness & Reward): \"MDPFuzz prioritizes mutated initial states that can reduce the cumulative rewards.\"; Page 385, Section 6 (Testing Oracles (Crash Definition)): \"For RL and IL models in CARLA, we examine whether the model-controlled vehicle collides with other vehicles or buildings. For DNN-based ACAS Xu, we check collisions of the DNN-controlled airplane with other airplanes.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, MDPFuzz, is designed to discover broad classes of security-violating or unsafe behaviors, which aligns with the 'Medium' rating. The testing oracle is explicitly defined to detect 'abnormal and dangerous states' such as collisions or falls, rather than any generic failure (Low) or a specific, pre-determined output state (High). The entire fuzzing process is guided towards finding inputs that lead to these general categories of undesirable outcomes.",
                    "evidence": "Page 9, Section 6, \"Testing Oracles (Crash Definition)\": \"For RL and IL models in CARLA, we examine whether the model-controlled vehicle collides with other vehicles or buildings. For DNN-based ACAS Xu, we check collisions of the DNN-controlled airplane with other airplanes. For Coop Navi, we check collisions between MARL-controlled agents. For BipedalWalker, we check whether the RL-controlled walking agent falls (i.e., its head touches the ground). As expected, the crash or abnormal state is defined case by case, and orthogonal to MDPFuzz. Users can configure MDPFuzz with other undesired behaviors as long as such abnormal states can induce reasonably low rewards.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "MDPFuzz explicitly steers exploration toward detecting abnormal or dangerous behaviors (crashes) across MDP scenarios (e.g., collisions, falls) using oracles, cumulative-reward reduction, and sequence-freshness guidance. Its objective is to find broader classes of security-violating behaviors (crashes/abnormal states) rather than to induce a specific attacker-chosen model output (a particular label, phrase, or exact action). The paper repeatedly frames its goal as discovering crash-triggering state sequences (Definition 2), prioritizing seeds that lower cumulative reward, and checking whether the model enters unsafe states (the testing oracle), which matches the \u201cMedium\u201d category: discovery of broader unsafe behaviors rather than targeted-output attacks.",
                    "evidence": "Page 378, Abstract: \"MDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states.\"; Page 381, Section 4 (Inconsistencies vs. Crashes): \"Real-world models for solving MDPs can quickly recover from 'inconsistent' predictions. This demands the creation of oracles over concrete and severe abnormal states, such as collisions in airplane control or autonomous driving.\"; Page 381, Definition 2 (Crash-triggering state sequences): \"We regard crashes incurred from a valid and solvable initial state as bugs of a model \u03c0 solving MDP.\"; Page 384, Section 5.3 (Feedback from Freshness & Reward): \"MDPFuzz prioritizes mutated initial states that can reduce the cumulative rewards.\"; Page 385, Section 6 (Testing Oracles (Crash Definition)): \"For RL and IL models in CARLA, we examine whether the model-controlled vehicle collides with other vehicles or buildings. For DNN-based ACAS Xu, we check collisions of the DNN-controlled airplane with other airplanes.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Targets a class of faults (collission or dangerous states)",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight into why failures occur by analyzing observable model internals and statistical failure patterns, but it does not fully tie failures to higher-level root causes such as dataset bias, spurious features, or overfitting. Concretely, the authors (1) analyze neuron activation patterns for crash-triggering vs. normal sequences (t-SNE visualizations and clustering), (2) quantify sensitivity of seeds via a local sensitivity measure (|r - r\u0394| / ||\u0394S||2) to prioritize inputs, and (3) build and evaluate an abnormal-behavior detector (reporting AUC-ROC) and use findings to repair models and reduce crashes. These constitute substantive diagnostic analyses of model internals and statistical failure patterns (consistent with the metric's \"Medium\" level). However, the paper stops short of attributing failures to underlying vulnerabilities like dataset bias, spurious features, or overfitting, so it does not meet the \"High\" criterion.",
                    "evidence": "Page 378, Abstract: \"We show inspiring findings that crash-triggering states, though they look normal, induce distinct neuron activation patterns compared with normal states. We further develop an abnormal behavior detector to harden all the evaluated models and repair them with the findings of MDPFuzz to significantly enhance their robustness without sacrificing accuracy.\"; Page 382, Section 5.1 (Sensitivity) / Alg. 2: \"Sensitivity adds small random permutations \u0394S to an initial state S0 ... The local sensitivity of the model at S0 can thus be estimated by |r\u2212r\u0394| / ||\u0394S||2.\"; Page 387, Section 7.3 (Root Cause Analysis): \"The neuron activations of crash-triggering state sequences found by MDPFuzz have a clear boundary with normal and randomly mutated sequences, which shows that the crash-triggering states can trigger the models\u2019 abnormal internal logics.\"; Page 387, Section 7.4 (Enhance Model Robustness): \"The area under the receiver operating characteristic curve (AUC-ROC) is above 0.78 for each model, indicating that our detector can simultaneously achieve good precision and recall in detecting abnormal model logics.\"; Page 387, Section 7.4 (repair results): \"... the #crash detected by MDPFuzz in 12 hours after model repairing is substantially lower, at 29, than 139 before repairing.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight into the discovered failures by analyzing observable model internals, specifically the neuron activation patterns. The authors demonstrate that crash-triggering state sequences induce neuron activation patterns that are distinctly different from those of normal or randomly mutated sequences. This analysis links the failures to the model's 'abnormal internal logics'. The paper visualizes these distinct activation clusters using t-SNE plots (Figure 9) and uses this insight to build a detector for abnormal behaviors, thus characterizing the failures beyond merely reporting them. This aligns with the 'Medium' criterion, which includes providing insight through the analysis of neuron characteristics.",
                    "evidence": "Page 11, Section 7.3, RQ3: Root Cause Analysis: 'Setup. Inspired by contemporary DNN testing criteria [58, 67, 71, 91], we characterize crash-triggering states by measuring their induced neuron activation patterns in all tested models.' and 'Results. ... The neuron activations of crash-triggering state sequences found by MDPFuzz have a clear boundary with normal and randomly mutated sequences, which shows that the crash-triggering states can trigger the models\u2019 abnormal internal logics.' and 'Answer to RQ3: Though crash-triggering initial states are considered normal from the MDP environments\u2019 perspective, they induce distinct neuron activation patterns. This shows that MDPFuzz can cover the model\u2019s corner internal logics and trigger its abnormal behaviors more efficiently than randomly-mutated state sequences.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper provides diagnostic insight into why failures occur by analyzing observable model internals and statistical failure patterns, but it does not fully tie failures to higher-level root causes such as dataset bias, spurious features, or overfitting. Concretely, the authors (1) analyze neuron activation patterns for crash-triggering vs. normal sequences (t-SNE visualizations and clustering), (2) quantify sensitivity of seeds via a local sensitivity measure (|r - r\u0394| / ||\u0394S||2) to prioritize inputs, and (3) build and evaluate an abnormal-behavior detector (reporting AUC-ROC) and use findings to repair models and reduce crashes. These constitute substantive diagnostic analyses of model internals and statistical failure patterns (consistent with the metric's \"Medium\" level). However, the paper stops short of attributing failures to underlying vulnerabilities like dataset bias, spurious features, or overfitting, so it does not meet the \"High\" criterion.",
                    "evidence": "Page 378, Abstract: \"We show inspiring findings that crash-triggering states, though they look normal, induce distinct neuron activation patterns compared with normal states. We further develop an abnormal behavior detector to harden all the evaluated models and repair them with the findings of MDPFuzz to significantly enhance their robustness without sacrificing accuracy.\"; Page 382, Section 5.1 (Sensitivity) / Alg. 2: \"Sensitivity adds small random permutations \u0394S to an initial state S0 ... The local sensitivity of the model at S0 can thus be estimated by |r\u2212r\u0394| / ||\u0394S||2.\"; Page 387, Section 7.3 (Root Cause Analysis): \"The neuron activations of crash-triggering state sequences found by MDPFuzz have a clear boundary with normal and randomly mutated sequences, which shows that the crash-triggering states can trigger the models\u2019 abnormal internal logics.\"; Page 387, Section 7.4 (Enhance Model Robustness): \"The area under the receiver operating characteristic curve (AUC-ROC) is above 0.78 for each model, indicating that our detector can simultaneously achieve good precision and recall in detecting abnormal model logics.\"; Page 387, Section 7.4 (repair results): \"... the #crash detected by MDPFuzz in 12 hours after model repairing is substantially lower, at 29, than 139 before repairing.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Provides correlation with internal coverage",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility during fuzzing by (1) restricting mutations to the MDP initial state rather than arbitrary intermediate states, (2) applying only small/bounded random perturbations, and (3) validating every mutated initial state using environment validation modules and scenario-specific bounds (e.g., speed limits). These design choices ensure mutated inputs remain realistic at each mutation step. However, the paper does not provide an explicit human evaluation or task-specific perceptual/semantic quality metrics (e.g., LPIPS/SSIM, PESQ/STOI, Perplexity) to empirically quantify the perceptual/semantic realism of the final crash-inducing inputs. Nor does it present quantitative perceptual measures for the final fault-inducing inputs; instead it reports internal neuron-activation differences and detection AUCs. Therefore the work fits the \"Medium\" level: plausibility is enforced in mutation/validation steps, but formal empirical perceptual assessments of final fault-inducing inputs are not reported.",
                    "evidence": "Introduction (p. 378): \"Second, instead of mutating arbitrary intermediate states, MDPFuzz only mutates the initial state conservatively to ensure that the sequence is realistic. The mutated initial states are also validated in a deliberate way to ensure realism.\"; Section 6, 'MDP Initial State Sampling, Mutation, and Validation' (p. 385): \"When sampling, the initial states are randomly selected from the state space P. MDPFuzz randomly mutates initial states with small noise. All the mutated states are validated in an automated and deliberate way (see below). That is, we confirm that all mutated initial states are valid and solvable.\"; Section 5 (Assumptions / Alg.1 description) (pp. 382\u2013383): \"We clarify that this assumption is realistic... MDPFuzz only mutates the initial state S0 of an MDP, under the constraint that it is still realistic and solvable after mutation. See Sec. 6 on how mutated initial states are validated in this study.\"; RQ3 Results (p. 387): \"we emphasize that the crashes found by MDPFuzz are not due to unrealistic states. Rather, they share similar visual appearances with normal states and can occur in real-world scenarios.\"; (Counter-evidence / absence) No sections report human studies or quantitative perceptual/linguistic quality metrics (e.g., LPIPS/SSIM, PESQ/STOI, Perplexity) for final fault-inducing inputs\u2014the evaluation focuses on neuron-activation patterns (t-SNE) and detection AUC (Section 7.3\u20137.4)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, MDPFuzz, is designed to maintain the realism of test inputs, which are state sequences in a Markov Decision Process (MDP). It achieves this by exclusively mutating the initial state of a sequence, arguing this preserves the natural temporal coherence of subsequent states generated by the environment's dynamics. Crucially, every mutated initial state undergoes an explicit, automated validation step to ensure it is both plausible and solvable within the task domain. For instance, in the CARLA driving simulator, it relies on the simulator's built-in validation to reject abnormal states, and for the ACAS Xu aircraft collision system, it applies domain-specific constraints like speed limits. This systematic validation of the inputs that trigger failure sequences fulfills a key requirement of the metric. However, the paper does not meet the 'High' criteria because it lacks an independent, empirical evaluation (e.g., a human study or quantitative perceptual metrics) to confirm the realism of the fault-inducing initial states; it only claims they are realistic because they pass the environment's validation checks.",
                    "evidence": "Page 9, Section 6, 'MDP Initial State Sampling, Mutation, and Validation': 'All the mutated states are validated in an automated and deliberate way (see below). That is, we confirm that all mutated states are valid and solvable.'\n\nPage 6, Section 4, 'Design Overview': 'Moreover, we only mutate the initial state rather than arbitrary states to generate more realistic scenarios... For each MDP scenario, we validate mutated initial states in a deliberate way to ensure they are realistic and solvable for an optimal model \u03c0*.'\n\nPage 9, Section 6, 'MDP Initial State Sampling, Mutation, and Validation': 'Particularly, in CARLA, we change the initial positions and angles of all 100 vehicles... Note that CARLA validates and rejects abnormal initial states: all mutated initial states passed its validation... In DNN-based ACAS Xu, we mutate the initial positions and speeds... Moreover, we bound the maximal speed of all airplanes below 1,100 ft/sec, which is within the range of normal speed... We guarantee that there exist optimal actions to avoid the crash and solve the initial states, and we do not use an initial state that is not solvable.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility during fuzzing by (1) restricting mutations to the MDP initial state rather than arbitrary intermediate states, (2) applying only small/bounded random perturbations, and (3) validating every mutated initial state using environment validation modules and scenario-specific bounds (e.g., speed limits). These design choices ensure mutated inputs remain realistic at each mutation step. However, the paper does not provide an explicit human evaluation or task-specific perceptual/semantic quality metrics (e.g., LPIPS/SSIM, PESQ/STOI, Perplexity) to empirically quantify the perceptual/semantic realism of the final crash-inducing inputs. Nor does it present quantitative perceptual measures for the final fault-inducing inputs; instead it reports internal neuron-activation differences and detection AUCs. Therefore the work fits the \"Medium\" level: plausibility is enforced in mutation/validation steps, but formal empirical perceptual assessments of final fault-inducing inputs are not reported.",
                    "evidence": "Introduction (p. 378): \"Second, instead of mutating arbitrary intermediate states, MDPFuzz only mutates the initial state conservatively to ensure that the sequence is realistic. The mutated initial states are also validated in a deliberate way to ensure realism.\"; Section 6, 'MDP Initial State Sampling, Mutation, and Validation' (p. 385): \"When sampling, the initial states are randomly selected from the state space P. MDPFuzz randomly mutates initial states with small noise. All the mutated states are validated in an automated and deliberate way (see below). That is, we confirm that all mutated initial states are valid and solvable.\"; Section 5 (Assumptions / Alg.1 description) (pp. 382\u2013383): \"We clarify that this assumption is realistic... MDPFuzz only mutates the initial state S0 of an MDP, under the constraint that it is still realistic and solvable after mutation. See Sec. 6 on how mutated initial states are validated in this study.\"; RQ3 Results (p. 387): \"we emphasize that the crashes found by MDPFuzz are not due to unrealistic states. Rather, they share similar visual appearances with normal states and can occur in real-world scenarios.\"; (Counter-evidence / absence) No sections report human studies or quantitative perceptual/linguistic quality metrics (e.g., LPIPS/SSIM, PESQ/STOI, Perplexity) for final fault-inducing inputs\u2014the evaluation focuses on neuron-activation patterns (t-SNE) and detection AUC (Section 7.3\u20137.4)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Applies bounded noise",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "chatgpt": {
                    "value": "Medium",
                    "why": "MDPFuzz mutates MDP environment/state parameters (semantic, metamorphic transformations) rather than performing fine-grained in-memory tensor perturbations, so the generated inputs are produced by the simulator (rendered frames, environment states) and are inherently more stable than ephemeral floating-point tensor mutations. However, the paper does not describe any explicit handling or checks for standard I/O/serialization effects (quantization, rounding, clipping) nor does it claim to generate inputs directly in serialized formats to ensure persistence of fault-inducing perturbations. Under the metric definition, this corresponds to \u201csemantic-preserving metamorphic transformations without explicit enforcement of reproducibility under serialization,\u201d which maps to Medium.",
                    "evidence": "Section 4 (p. 381) \u2014 \"Mutating Initial States vs. Mutating Intermediate States. ... We thus decide to only mutate the initial state (e.g., re-arrange the environment at timestep 0), allowing to preserve the coherence of the entire MDP state sequence.\" \n\nSection 6 (p. 385) \u2014 \"We add random noise to the initial states and the noise type depends on type of the original data, e.g., adding small Gaussian float numbers from N(0, 1) to vehicles' initial positions, and adding small uniform integers from U(0, 4) to the ground type in BipedalWalker.\" \n\nSection 6 (p. 385) \u2014 \"Particularly, in CARLA, we change the initial positions and angles of all 100 vehicles, including the model-controlled vehicle. Note that CARLA validates and rejects abnormal initial states: all mutated initial states passed its validation.\" \n\n(Throughout the paper, mutation and validation are described as operating on environment/state parameters and simulator-rendered frames; there is no discussion of handling standard I/O serialization effects such as pixel quantization, rounding, clipping, nor any explicit mechanism to constrain perturbations to survive common serialization.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The method, MDPFuzz, mutates the initial state of a Markov Decision Process (MDP) by adding small numerical noise. For several of the evaluated scenarios, such as autonomous driving in CARLA and aircraft collision avoidance in ACAS Xu, the mutation involves perturbing continuous state variables (e.g., positions, speeds) with \"small Gaussian float numbers\". The paper emphasizes that mutated states are validated to ensure they are \"valid and solvable\" within the simulation environment. However, there is no mention of considering the effects of standard I/O operations like quantization, rounding, or clipping. The introduced floating-point perturbations are susceptible to being lost or altered during serialization, meaning the discovered failures might not be reproducible. The metric defines this as a 'Low' level of failure reproducibility because it ignores I/O effects, allowing fault-inducing perturbations to be lost after standard serialization.",
                    "evidence": "Page 9, Section 6, \"MDP Initial State Sampling, Mutation, and Validation\":\n\n- \"MDPFuzz randomly mutates initial states with small noise.\"\n- \"We add random noise to the initial states and the noise type depends on type of the original data, e.g., adding small Gaussian float numbers from N(0, 1) to vehicles\u2019 initial positions, and adding small uniform integers from U(0, 4) to the ground type in BipedalWalker.\"\n- \"All the mutated states are validated in an automated and deliberate way (see below). That is, we confirm that all mutated states are valid and solvable.\" (The subsequent text clarifies that validation ensures the state is solvable within the simulation, not that the perturbation is robust to I/O operations)."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "MDPFuzz\u2019s mutations operate at the level of environment/state parameters (e.g. shifting vehicle positions, angles, ground types) rather than injecting ephemeral floating-point noise directly into the model\u2019s tensor inputs. These are semantic, metamorphic transformations that naturally persist through the simulator\u2019s rendering pipeline (and any associated rounding or quantization) but the paper does not describe any explicit handling or analysis of standard I/O effects (such as pixel\u2010level quantization, rounding or clipping). Under the Failure Reproducibility metric, this precisely matches the \u201csemantic\u2010preserving metamorphic transformations without explicit enforcement of reproducibility under serialization\u201d category, which corresponds to a Medium rating.",
                    "evidence": "Section 4 (p. 381): \u201cMutating Initial States vs. Mutating Intermediate States. \u2026 We thus decide to only mutate the initial state (e.g., re-arrange the environment at timestep 0), allowing to preserve the coherence of the entire MDP state sequence.\u201d\n\nSection 6 (p. 385): \u201cMDPFuzz randomly mutates initial states with small noise. \u2026 adding small Gaussian float numbers from N(0, 1) to vehicles\u2019 initial positions, and adding small uniform integers from U(0, 4) to the ground type in BipedalWalker.\u201d\n\nMetric Definition: \u201cMedium: Considers serialization effects partially (e.g., clipping but not rounding), or applies semantic\u2010preserving metamorphic transformations without explicit enforcement of reproducibility under serialization.\u201d"
                },
                "manual": {
                    "value": "High",
                    "why": "Perturbs continuous-valued initial state vectors within valid physical bounds and validates each mutated state through the simulator",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate attack transferability. MDPFuzz evaluates and finds crash-triggering initial states per target model (it reports crashes separately for five different models/scenarios), uses those findings to train per-model detectors and to repair individual models, and explicitly operates in blackbox settings without building surrogate models. The paper contains no experiment where fault-inducing inputs found on one model are reused to test another model, nor does it describe mechanisms (e.g., surrogate-model attacks, cross-model validation, or explicit cross-model transfer tests) intended to discover transferable attacks across different model implementations.",
                    "evidence": "Abstract (p.378): \"We evaluate five state-of-the-art models for solving MDPs, including supervised DNN, RL, IL, and multi-agent RL.\" (shows multiple models were evaluated but not that inputs were reused across them). \n\nSection 7.1 RQ1 / Results (p.386): \"Table 3 summarizes the findings of fuzzing each model... Overall, MDPFuzz detects a significant number of crashes from all test cases.\" (reports per-model crash counts; no cross-model reuse reported). \n\nSection 5 Assumptions (Alg.1 / p.382): \"In blackbox settings, the internal of the agent model (our testing target) is not revealed to MDPFuzz... MDPFuzz can collect the state sequence {S_t} and obtain the cumulative reward r from the environment.\" (describes blackbox testing and lack of building surrogate/internal models). \n\nSection 7.4 (p.387): \"we repair the models used by DNN-based ACAS Xu with the findings of MDPFuzz...\" (describes per-model repair using findings, not transfer of fault-inducing inputs to other models). \n\nSection 8 Discussion (p.388): \"Another threat is that bugs may be potentially overlooked since MDPFuzz only mutates the entry states (Sec. 5). Nevertheless ...\" (limits of their approach are discussed; no claim of enabling transferable/substitute-model attacks)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper evaluates the proposed fuzzer, MDPFuzz, on five distinct models, including a Reinforcement Learning (RL) and an Imitation Learning (IL) model for the same autonomous driving task (CARLA). However, the evaluation is conducted independently for each model. The paper reports the number of crash-triggering inputs found for each model in isolation and does not perform any experiments to test if the inputs that cause a failure in one model also cause a failure in another model, even those performing the same task. The design of MDPFuzz focuses on finding failures for a single model under test and lacks any mechanism explicitly designed to discover transferable attacks or shared vulnerabilities across different models.",
                    "evidence": "Page 10, Section 7.1 RQ1: Performance on Finding Crashes, Setup: \"We use the evaluation setup described in Sec. 6. That is, we launch MDPFuzz to fuzz each MDP model (listed in Table 3) and detect crashes. We collect all error-triggering inputs for analysis.\" This statement indicates that each model was fuzzed as a separate, independent target. Table 3 on the same page presents the results for each model as a distinct line item, with no cross-model evaluation or mention of transferring discovered failure-inducing inputs."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate attack transferability. MDPFuzz evaluates and finds crash-triggering initial states per target model (it reports crashes separately for five different models/scenarios), uses those findings to train per-model detectors and to repair individual models, and explicitly operates in blackbox settings without building surrogate models. The paper contains no experiment where fault-inducing inputs found on one model are reused to test another model, nor does it describe mechanisms (e.g., surrogate-model attacks, cross-model validation, or explicit cross-model transfer tests) intended to discover transferable attacks across different model implementations.",
                    "evidence": "Abstract (p.378): \"We evaluate five state-of-the-art models for solving MDPs, including supervised DNN, RL, IL, and multi-agent RL.\" (shows multiple models were evaluated but not that inputs were reused across them). \n\nSection 7.1 RQ1 / Results (p.386): \"Table 3 summarizes the findings of fuzzing each model... Overall, MDPFuzz detects a significant number of crashes from all test cases.\" (reports per-model crash counts; no cross-model reuse reported). \n\nSection 5 Assumptions (Alg.1 / p.382): \"In blackbox settings, the internal of the agent model (our testing target) is not revealed to MDPFuzz... MDPFuzz can collect the state sequence {S_t} and obtain the cumulative reward r from the environment.\" (describes blackbox testing and lack of building surrogate/internal models). \n\nSection 7.4 (p.387): \"we repair the models used by DNN-based ACAS Xu with the findings of MDPFuzz...\" (describes per-model repair using findings, not transfer of fault-inducing inputs to other models). \n\nSection 8 Discussion (p.388): \"Another threat is that bugs may be potentially overlooked since MDPFuzz only mutates the entry states (Sec. 5). Nevertheless ...\" (limits of their approach are discussed; no claim of enabling transferable/substitute-model attacks)."
                },
                "manual": {
                    "value": "Low",
                    "why": "No cross model failure pattern analysis",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided",
                "Data-driven"
            ]
        }
    }
}