{
    "Liu et al. - 2024 - AUTODAN GENERATING STEALTHY JAILBREAK PROMPTS ON ALIGNED LARGE LANGUAGE MODELS": {
        "key": "liuautodan",
        "name": "AutoDAN",
        "year": "2024",
        "citation_count": "834",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "High",
                    "why": "Guides towared output that start with a predified content"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Reports what fails without analyzing why. It provides tranferability rationale, not failure patterns or root cause"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly designs per-mutation constraints and mechanisms to preserve semantic plausibility (LLM-based diversification for initialization and mutation, synonym-based/momentum word replacement, and a low mutation probability), and it validates final fault-inducing prompts with a task-specific quantitative measure (sentence perplexity / PPL) and defense experiments (perplexity-based defense). However, the paper does not describe an explicit mechanism that enforces or measures plausibility across the entire evolutionary trajectory (i.e., cumulative drift across iterations) nor report iterative/plausibility traces; instead it relies on per-step design choices and final PPL measurements. Thus it validates final-input plausibility and constrains individual mutation steps but does not address cumulative-effects across many iterations, matching the metric's Medium level.",
                    "evidence": "3.2 Population Initialization (page 4): \"we employ LLMs as the agents responsible for revising the prototype prompt ... the modifications proposed by LLM can preserve the inherent logical flow and meaning of the original sentences, while simultaneously introducing diversity in word selection and sentence structure.\"; 3.4 Paragraph-level (page 5): \"We let the LLM-based diversification introduced in Alg. 5 to also serve as the mutation function due to its ability to preserve the global meaning and introduce diversity.\"; D.2 Implementation details (Appendix D, page 16): \"...a mutation rate of 0.01... We employ GPT-4-0613 as the base model for the Recheck metric and also as the base model for LLM-based Diversification.\"; 4.1 Metrics (page 6): \"For stealthiness, we use standard Sentence Perplexity (PPL) evaluated by GPT-2 as the metric.\"; 4.2 Results / Table 1 (page 6\u20137): \"Compared with the baseline, our method exhibits superior performance in terms of PPL, indicating more semantically meaningful and stealthy attacks being generated.\"; 3.5 Termination Criteria (page 5): \"If the algorithm has exhausted the maximum number of iterations or no keyword in a set list Lrefuse is detected in the LLM\u2019s top K words of its response, AutoDAN will terminate...\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's primary objective is to generate 'stealthy' and 'semantically meaningful' jailbreak prompts, which directly aligns with the concept of input plausibility. The proposed method, AutoDAN, enforces this plausibility throughout its generation process. Unlike methods that perform simple token-level mutations, AutoDAN employs a hierarchical genetic algorithm that operates on sentences and words. Crucially, its mutation operator uses an LLM to revise sentences, a mechanism designed to preserve logical flow and meaning, thus preventing the cumulative semantic drift common in iterative fuzzing. Furthermore, the paper provides explicit empirical evidence of this plausibility. It uses Sentence Perplexity (PPL), a standard quantitative measure for linguistic fluency, as a core evaluation metric for 'stealthiness'. The results demonstrate that AutoDAN's generated prompts have significantly lower (i.e., better) perplexity scores than token-based attack baselines (GCG) and are comparable to human-written prompts, confirming that the final fault-inducing inputs are realistic and semantically valid.",
                    "evidence": "Page 7, Section 4.1, Metrics: \"For stealthiness, we use standard Sentence Perplexity (PPL) evaluated by GPT-2 as the metric.\"\n\nPage 7, Section 4.2, Table 1 & Attack Effectiveness and Stealthiness: The results in Table 1 show AutoDAN-HGA achieving a PPL of 54.3820 on Llama2-7b-chat, vastly lower than the GCG baseline's 1027.5585. The text states: \"Compared with the baseline, our method exhibits superior performance in terms of PPL, indicating more semantically meaningful and stealthy attacks being generated.\"\n\nPage 6, Section 3.4.2, Paragraph-level: selection, crossover and mutation: \"We let the LLM-based diversification introduced in Alg. 5 to also serve as the mutation function due to its ability to preserve the global meaning and introduce diversity.\" Algorithm 5 on page 14 details this function, which instructs an LLM to revise a sentence, thus enforcing semantic plausibility during the mutation step."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "AutoDAN incorporates per-step mechanisms to preserve semantic plausibility\u2014initializing with human-crafted prompts, using an LLM to rewrite rather than randomly perturb text, employing a low mutation rate, and validating final prompts with sentence-level perplexity and a perplexity-based defense. However, the paper does not enforce or measure plausibility cumulatively across iterations (e.g., via intermediate PPL traces or explicit drift bounds), nor does it report any plausibility assessments for inputs generated mid-fuzzing. As a result, it constrains individual mutation steps and validates only the final outputs, matching the \"Medium\" level of the Input Plausibility metric.",
                    "evidence": "Section 3.2 (page 4): LLM-based diversification ensures each mutation preserves meaning; Section 3.4.2 (page 5): low mutation probability and LLM-based mutation function to maintain semantic coherence; Section 4.1 (page 6): final stealthiness evaluated via Sentence Perplexity (PPL); Section 4.2 Table 1 (pages 6\u20137): reports final PPL scores and perplexity-defense results\u2014no intermediate plausibility measures or cumulative drift analysis are provided."
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "High",
                    "why": "Mutation does not rely on guidance from the model and shows transferability"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper both (1) reuses fault-inducing prompts crafted on one model to test other models (explicit cross-model transfer experiments and reported ASR numbers), and (2) designs the fuzzing/optimization to produce semantically meaningful, lexical-level prompts (hierarchical GA, LLM-based diversification, momentum word scoring) which the authors argue and empirically show improves transfer to black-box models. Thus it demonstrates reuse across models and includes explicit mechanisms intended to discover transferable failures.",
                    "evidence": "Page 1, Abstract: \"Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability...\"; Page 8, Section 4.2 Results - Transferability: \"We conduct the evaluations by taking the jailbreak prompts with their corresponding requests and targeting another LLM. The results are shown in Tab. 2. AutoDAN exhibits a much better transferability in attacking the black-box LLMs compared with the baseline.\"; Page 8, Table 2 (Cross-model transferability): entries showing transfer ASR (examples) \u2014 \"Vicuna-7B -> Guanaco-7b: AutoDAN-HGA ASR 0.7058 Recheck 0.6712\" and \"Llama2-7b-chat -> Vicuna-7B: AutoDAN-HGA ASR 0.7288 Recheck 0.7019\"; Page 8, Table 2 caption / discussion: \"We hypothesize that this is because the AutoDAN generates prompts at a semantic level without relying on direct guidance from gradient information on the tokens, thereby avoiding overfitting on white-box models.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper explicitly evaluates the transferability of its generated jailbreak prompts. It conducts cross-model experiments where prompts generated on a source model are tested against other target models, and the results are presented in a dedicated table (Table 2). However, the design of the attack method, AutoDAN, does not incorporate explicit mechanisms aimed at discovering transferable attacks. The method uses a hierarchical genetic algorithm to generate semantically meaningful prompts. The authors speculate that the observed high transferability is a beneficial consequence of their semantic-level optimization approach, which avoids the overfitting common in gradient-based methods, rather than being an explicit design goal with specific supporting mechanisms.",
                    "evidence": "Page 8, Section 'Transferability': 'We further investigate the transferability of our method... We conduct the evaluations by taking the jailbreak prompts with their corresponding requests and targeting another LLM. The results are shown in Tab. 2. AutoDAN exhibits a much better transferability in attacking the black-box LLMs compared with the baseline. We speculate that the potential reason is the semantically meaningful jailbreak prompts may be inherently more transferable than the methods based on tokens\u2019 gradients.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "While the paper clearly evaluates whether fault-inducing prompts generated on one model transfer to others (via cross-model experiments in Section 4.2 and Table 2), the core AutoDAN design does not include mechanisms explicitly tailored for transferability (e.g. multi-model surrogate optimization or fitness functions that aggregate performance across targets). The authors observe that semantic-level optimization \u2018inherently\u2019 improves transfer, but they do not incorporate transferability as an explicit objective or design feature of their hierarchical genetic algorithm.",
                    "evidence": "Page 8, Section 4.2 (\u201cTransferability\u201d): \u201cWe conduct the evaluations by taking the jailbreak prompts with their corresponding requests and targeting another LLM. The results are shown in Tab. 2. AutoDAN exhibits a much better transferability \u2026 We speculate that the potential reason is the semantically meaningful jailbreak prompts may be inherently more transferable\u2026\u201d; Table 2 \u2013 cross-model ASR results; no mention of multi-model or transfer-focused optimization in Method (Sections 3.2\u20133.4)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Shows transferabilty but the fuzzing design is not explicit about trasferable attack discovery. It is a byproduct and the authors gives a hypothesis why they transfer",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Rule-based",
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "He et al. - 2024 - Curiosity-Driven Testing for Sequential Decision-Making Process": {
        "key": "he2024curiosity",
        "name": "CureFuzz",
        "year": "2024",
        "citation_count": "12",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Medium",
                    "why": "Uncovers crash-triggering scenarios (e.g., collisions, falls, or loss of control) across autonomous driving, aviation, and robotics systems. exposes behaviorally unsafe yet functionally valid failures \u2014 revealing unintended consequences of learned policies rather than breaches of safety enforcement mechanisms."
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "Steer toward safetly faults (crash-triggering scenarios) in in autonomous driving and other sequential decision-making systems"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "CureFuzz enforces plausibility constraints at each mutation step by restricting mutated seeds to the environment's legitimate state space and discarding illegal or initially crashing states (per-environment validity checks and simulator-based filtering). It also uses only \u2018small random perturbations\u2019 when mutating seeds. However, the paper does not describe mechanisms that explicitly prevent or measure cumulative drift across many fuzzing iterations nor report any human or quantitative perceptual/semantic plausibility assessments (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) for final fault-inducing inputs. The evaluation metrics focus on coverage and crash counts rather than input realism. This matches the metric\u2019s Medium definition: plausibility is enforced at individual mutation steps or by rule-based validity checks, but cumulative-effects and explicit plausibility validation of final fault-inducing inputs are not addressed.",
                    "evidence": "3.4 CureFuzz Architecture (Seed Mutation): \"The Seed-Mutation function generates a new mutated state by applying a small random perturbation to a selected state. ... We need to ensure that the mutated seed lies in the legitimate state spaces , and the mutated seed would not trigger an initial crash. In our experiments, we have carefully addressed this concern and verified the validity of each mutated seed, details are given in Section 4.\" (Section 3.4)\n\n4.2.1 Autonomous Driving (CARLA): \"We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.\" (Section 4.2.1)\n\n4.2.2 Aircraft Collision Avoidance (ACAS Xu): \"Given the predefined range of acceptable states, states that fall outside legal space are automatically discarded.\" (Section 4.2.2)\n\n4.2.3 Coop Navi (MARL) / BipedalWalker: \"With a clear definition of permissible positions and velocities, CureFuzz ignores any illegal states.\" and \"Since the valid ground types are known, illegal states can be easily detected and discarded.\" (Section 4.2.3 and 4.2.4)\n\n4.3 Implementation (mutation magnitude): \"When mutating a seed, the magnitude of the random perturbation is a critical factor that can impact the performance of the fuzzer. We re-use the code implementation from Pang et al. for generating mutations.\" (Section 4.3)\n\n5 Evaluation metrics (lack of perceptual plausibility measures): \"Our evaluation focuses on three key metrics: environmental state coverage, the total number of detected crash-triggering scenarios, and the distinct types of crash-triggering scenarios.\" (Section 5, RQ1 description)  -- no human studies or perceptual/semantic quality metrics (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) are reported."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility through rule-based constraints applied at each mutation step, which aligns with the 'Medium' rating. For each of the four experimental environments, the authors describe a specific mechanism to ensure that the mutated input state remains within a valid or 'legal' space, and they explicitly discard any states that violate these rules. For example, in the autonomous driving environment, the CARLA simulator is used to validate the mutated state, and in the aircraft collision avoidance system, states outside a predefined 'legal space' are discarded. However, the paper does not address the potential for cumulative drift, where a series of individually plausible mutations could lead to an overall unrealistic scenario. Furthermore, it lacks any empirical validation (such as human studies or quantitative metrics like LPIPS or SSIM) to confirm that the final crash-triggering scenarios are perceptually or semantically realistic.",
                    "evidence": "Page 7, Section 4.2.1 Autonomous Driving: 'We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.'\nPage 7, Section 4.2.2 Aircraft Collision Avoidance: 'Given the predefined range of acceptable states, states that fall outside legal space are automatically discarded.'\nPage 7, Section 4.2.3 Video Game: 'With a clear definition of permissible positions and velocities, CUREFUZZ ignores any illegal states.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "CureFuzz enforces plausibility constraints at each mutation step by restricting mutated seeds to the environment's legitimate state space and discarding illegal or initially crashing states (per-environment validity checks and simulator-based filtering). It also uses only \u2018small random perturbations\u2019 when mutating seeds. However, the paper does not describe mechanisms that explicitly prevent or measure cumulative drift across many fuzzing iterations nor report any human or quantitative perceptual/semantic plausibility assessments (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) for final fault-inducing inputs. The evaluation metrics focus on coverage and crash counts rather than input realism. This matches the metric\u2019s Medium definition: plausibility is enforced at individual mutation steps or by rule-based validity checks, but cumulative-effects and explicit plausibility validation of final fault-inducing inputs are not addressed.",
                    "evidence": "3.4 CureFuzz Architecture (Seed Mutation): \"The Seed-Mutation function generates a new mutated state by applying a small random perturbation to a selected state. ... We need to ensure that the mutated seed lies in the legitimate state spaces , and the mutated seed would not trigger an initial crash. In our experiments, we have carefully addressed this concern and verified the validity of each mutated seed, details are given in Section 4.\" (Section 3.4)\n\n4.2.1 Autonomous Driving (CARLA): \"We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.\" (Section 4.2.1)\n\n4.2.2 Aircraft Collision Avoidance (ACAS Xu): \"Given the predefined range of acceptable states, states that fall outside legal space are automatically discarded.\" (Section 4.2.2)\n\n4.2.3 Coop Navi (MARL) / BipedalWalker: \"With a clear definition of permissible positions and velocities, CureFuzz ignores any illegal states.\" and \"Since the valid ground types are known, illegal states can be easily detected and discarded.\" (Section 4.2.3 and 4.2.4)\n\n4.3 Implementation (mutation magnitude): \"When mutating a seed, the magnitude of the random perturbation is a critical factor that can impact the performance of the fuzzer. We re-use the code implementation from Pang et al. for generating mutations.\" (Section 4.3)\n\n5 Evaluation metrics (lack of perceptual plausibility measures): \"Our evaluation focuses on three key metrics: environmental state coverage, the total number of detected crash-triggering scenarios, and the distinct types of crash-triggering scenarios.\" (Section 5, RQ1 description)  -- no human studies or perceptual/semantic quality metrics (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) are reported."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Mutates continuous feature vectors representing simulator states (positions, angles, velocities) and enforces validity checks to keep them within legal bounds."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper contains no experiments or claims that fault-inducing inputs found on one model were reused to test other models for the same task, nor does CureFuzz include design elements aimed at producing transferable attacks across different model implementations. The evaluation applies CureFuzz separately to multiple SDMs and uses found scenarios to repair the same model (fine-tuning + re-test), but it does not measure whether seeds discovered on a surrogate transfer to unseen targets or test multiple implementations with the same seeds.",
                    "evidence": "Introduction (quote): \"We evaluate CureFuzz by applying it to well-known SDMs that use various learning algorithms. ... The experimental results demonstrate that CureFuzz effectively and efficiently identifies a significant number of catastrophic failures across all considered SDMs.\" (Introduction) \u2014 shows per-model evaluation but no claim of cross-model reuse.\n\nSection 5, RQ3 (quote): \"we utilize CureFuzz again to test the newly fine-tuned SDM. For the repaired ACAS Xu, 55 faults and 4 distinct types of crash-triggering scenarios are detected. We find that the crashes found by CureFuzz are reduced by 73% compared with the original DNN model.\" (Section 5, RQ3) \u2014 demonstrates using found scenarios to repair and retest the same model, not transfer to other models.\n\nSection 4.1 Research Questions (quote): \"To evaluate the performance of CureFuzz and comprehensively understand the impact, we formulate the three research questions: RQ1: How effective is CureFuzz in finding crash-triggering scenarios? RQ2: Can CureFuzz be effectively guided with the curiosity mechanism? RQ3: Can we use the crashes found by CureFuzz to improve SDMs?\" (Section 4.1) \u2014 the listed RQs do not include any question about transferability of fault-inducing inputs across different model implementations."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper introduces CUREFUZZ, a fuzzing method to find crash-triggering scenarios for sequential decision-makers (SDMs). The evaluation is conducted across multiple SDMs in various environments (e.g., two different models for the CARLA autonomous driving task, one based on DRL and one on IL). However, each SDM is treated as an independent and isolated target for the fuzzer. The experiments aim to demonstrate the effectiveness of the CUREFUZZ method on each target individually compared to baseline methods. The paper does not perform any experiments to test whether the crash-triggering scenarios discovered for one SDM would also cause failures (i.e., transfer) to another SDM performing the same task. The design of the fuzzer is focused on finding novel and diverse scenarios for the specific model under test, with no explicit mechanisms for discovering transferable failures.",
                    "evidence": "Page 8, Section 5 EXPERIMENT RESULTS, Tables 1, 2, and 3: The results for different models, even within the same environment, are presented in separate rows. For instance, 'Carla (RL)' and 'Carla (IL)' are evaluated independently. The paper discusses the performance of CUREFUZZ on each of these models but does not mention reusing the crash scenarios found for 'Carla (RL)' to test 'Carla (IL)' or vice-versa. The research questions also do not address transferability. For example, RQ1 asks: \"How effective is CUREFUZZ in finding crash-triggering scenarios?\" which is evaluated per model, not across models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper contains no experiments or claims that fault-inducing inputs found on one model were reused to test other models for the same task, nor does CureFuzz include design elements aimed at producing transferable attacks across different model implementations. The evaluation applies CureFuzz separately to multiple SDMs and uses found scenarios to repair the same model (fine-tuning + re-test), but it does not measure whether seeds discovered on a surrogate transfer to unseen targets or test multiple implementations with the same seeds.",
                    "evidence": "Introduction (quote): \"We evaluate CureFuzz by applying it to well-known SDMs that use various learning algorithms. ... The experimental results demonstrate that CureFuzz effectively and efficiently identifies a significant number of catastrophic failures across all considered SDMs.\" (Introduction) \u2014 shows per-model evaluation but no claim of cross-model reuse.\n\nSection 5, RQ3 (quote): \"we utilize CureFuzz again to test the newly fine-tuned SDM. For the repaired ACAS Xu, 55 faults and 4 distinct types of crash-triggering scenarios are detected. We find that the crashes found by CureFuzz are reduced by 73% compared with the original DNN model.\" (Section 5, RQ3) \u2014 demonstrates using found scenarios to repair and retest the same model, not transfer to other models.\n\nSection 4.1 Research Questions (quote): \"To evaluate the performance of CureFuzz and comprehensively understand the impact, we formulate the three research questions: RQ1: How effective is CureFuzz in finding crash-triggering scenarios? RQ2: Can CureFuzz be effectively guided with the curiosity mechanism? RQ3: Can we use the crashes found by CureFuzz to improve SDMs?\" (Section 4.1) \u2014 the listed RQs do not include any question about transferability of fault-inducing inputs across different model implementations."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ],
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "Zhou et al. - 2025 - Understanding the Effectiveness of Coverage Criteria for Large Language Models A Special Angle from": {
        "key": "zhou2025understanding",
        "name": "Zhou et al.",
        "year": "2025",
        "citation_count": "7",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "GPT-4 generates  new jailbreak queries through prompt rewriting. The generated contents are supposed to be natural but there was no measure reported how natural are those. The attack suffix is related to jailbreak detection, not creation"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper\u2019s coverage-guided generation and mutation processes do not enforce or measure input plausibility across iterations, nor do the authors report any human evaluation or task-specific perceptual/linguistic metrics (e.g., Perplexity used as a plausibility check) for the final fault-inducing inputs. Generated attack queries are produced via LLM rewriting (GPT-4) or by appending adversarial suffixes (GCG), and selection is based solely on coverage increase, with no mechanism described to constrain cumulative drift or to filter for semantic/naturalness. While the authors use perplexity-based methods as baselines for jailbreak detection, they do not use such measures to ensure that fuzzed/generated inputs remain realistic; likewise, no human study or quantitative plausibility metrics (e.g., Perplexity reported for final examples, LPIPS/SSIM/PESQ/STOI) are reported. Therefore the work does not enforce or assess input plausibility as required by the metric.",
                    "evidence": "III-A Motivation (page 732): \"Attack queries are generated by appending adversarial suffixes to rejected queries using GCG [7]. These queries aim to trigger the model to output malicious content (i.e., abnormal behaviors).\" \n\nV.C Jailbreak Case Generation (page 739): \"Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting. The query with the highest coverage increase is selected as the next seed in the coverage-guided approach.\" \n\nV.A Baseline (page 738-739): \"Baseline: We select the most widely used perplexity filter [33], the state-of-the-art method PARDEN [35], and self-reminder [49] as baselines for comparison.\" (shows perplexity is used as a baseline detector, not as an enforced plausibility constraint for generated inputs)\n\nEvaluation / General (pages 736-740, various): No sections or reported results describe human evaluation or quantitative perceptual/linguistic measures (e.g., Perplexity reported for generated fault-inducing prompts or other task-specific plausibility scores); coverage-guided selection is driven by coverage increase alone."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a coverage-guided method for generating jailbreak attack prompts. The generation process uses GPT-4 for 'prompt rewriting,' and the selection of subsequent prompts is guided entirely by maximizing 'coverage increase.' The evaluation of the generated prompts is based on their success rate in triggering a jailbreak, not on their plausibility or realism as user queries. The paper does not describe any mechanism to enforce input plausibility during the generation process, nor does it report any assessment of the plausibility of the final, successful jailbreak prompts, either through human evaluation or quantitative metrics like perplexity for linguistic fluency. The focus is entirely on the efficacy of the generated attacks, without consideration for whether the inputs are realistic within a normal threat model.",
                    "evidence": "Page 11, Section V-C, Jailbreak Case Generation: 'Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting. The query with the highest coverage increase is selected as the next seed in the coverage-guided approach.' and 'Evaluation results: As shown in Figure 6, the coverage-guided method outperforms the random strategy in generating successful jailbreak queries.' This demonstrates that the generation and selection process is driven by coverage and success rate, with no mention of plausibility constraints or validation."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper\u2019s coverage-guided generation and mutation processes do not enforce or measure input plausibility across iterations, nor do the authors report any human evaluation or task-specific perceptual/linguistic metrics (e.g., Perplexity used as a plausibility check) for the final fault-inducing inputs. Generated attack queries are produced via LLM rewriting (GPT-4) or by appending adversarial suffixes (GCG), and selection is based solely on coverage increase, with no mechanism described to constrain cumulative drift or to filter for semantic/naturalness. While the authors use perplexity-based methods as baselines for jailbreak detection, they do not use such measures to ensure that fuzzed/generated inputs remain realistic; likewise, no human study or quantitative plausibility metrics (e.g., Perplexity reported for final examples, LPIPS/SSIM/PESQ/STOI) are reported. Therefore the work does not enforce or assess input plausibility as required by the metric.",
                    "evidence": "III-A Motivation (page 732): \"Attack queries are generated by appending adversarial suffixes to rejected queries using GCG [7]. These queries aim to trigger the model to output malicious content (i.e., abnormal behaviors).\" \n\nV.C Jailbreak Case Generation (page 739): \"Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting. The query with the highest coverage increase is selected as the next seed in the coverage-guided approach.\" \n\nV.A Baseline (page 738-739): \"Baseline: We select the most widely used perplexity filter [33], the state-of-the-art method PARDEN [35], and self-reminder [49] as baselines for comparison.\" (shows perplexity is used as a baseline detector, not as an enforced plausibility constraint for generated inputs)\n\nEvaluation / General (pages 736-740, various): No sections or reported results describe human evaluation or quantitative perceptual/linguistic measures (e.g., Perplexity reported for generated fault-inducing prompts or other task-specific plausibility scores); coverage-guided selection is driven by coverage increase alone."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "No analysis of fault transferability in input level of mutation level"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether fault-inducing inputs (jailbreak prompts) crafted on one model transfer to other models. The generation experiments and the coverage-guided jailbreak-case generation are conducted targeting a single model (Llama-2-7b-chat). Detection and prioritization experiments are performed per-model (training/validating classifiers or thresholds for each model), but the paper does not report experiments where inputs crafted on a surrogate model are reused to test different target models, nor does it describe mechanisms explicitly designed to discover transferable attacks across models.",
                    "evidence": "Section V.C (Jailbreak Case Generation), p. 739: \"Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round ... Each method ultimately generates 250 new jailbreak queries to evaluate effectiveness.\"; Section V.A (Jailbreak Detection), p. 738: \"... we train an MLP model that uses the number of activated neurons as input ... We validate its effectiveness on OPT-125M, Llama-2-7B-Chat, and Pythia-12B.\"; Section III.A (Motivation: A Cluster Analysis Experiment), p. 732: \"Experimental Setup: We first introduce the setup for our cluster analysis experiments using the Llama-2-7b-chat [38] model as our target.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes and evaluates methods for jailbreak detection, prioritization, and generation using coverage criteria. The experiments are conducted across four different LLMs. However, the evaluation of fault-inducing inputs (jailbreak attacks) is performed on a per-model basis. Specifically, in the 'Jailbreak Case Generation' application, new jailbreak queries are generated using coverage guidance from a single target model (Llama-2-7b-chat). The paper evaluates the success of this generation method for that specific model but does not subsequently test whether these generated queries are effective (i.e., transferable) in causing failures in the other models studied in the paper (OPT-125M, Pythia-12B, Gemma-2-27B-it). The research does not demonstrate or test whether fault-inducing inputs discovered on one model trigger failures in others.",
                    "evidence": "Page 11, Section V. APPLICATION, C. Jailbreak Case Generation: \"Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting. The query with the highest coverage increase is selected as the next seed in the coverage-guided approach. For comparison, a random strategy selects seeds randomly from rewritten candidates. Each method ultimately generates 250 new jailbreak queries to evaluate effectiveness.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether fault-inducing inputs (jailbreak prompts) crafted on one model transfer to other models. The generation experiments and the coverage-guided jailbreak-case generation are conducted targeting a single model (Llama-2-7b-chat). Detection and prioritization experiments are performed per-model (training/validating classifiers or thresholds for each model), but the paper does not report experiments where inputs crafted on a surrogate model are reused to test different target models, nor does it describe mechanisms explicitly designed to discover transferable attacks across models.",
                    "evidence": "Section V.C (Jailbreak Case Generation), p. 739: \"Setup: We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round ... Each method ultimately generates 250 new jailbreak queries to evaluate effectiveness.\"; Section V.A (Jailbreak Detection), p. 738: \"... we train an MLP model that uses the number of activated neurons as input ... We validate its effectiveness on OPT-125M, Llama-2-7B-Chat, and Pythia-12B.\"; Section III.A (Motivation: A Cluster Analysis Experiment), p. 732: \"Experimental Setup: We first introduce the setup for our cluster analysis experiments using the Llama-2-7b-chat [38] model as our target.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Provides correlation with internal coverage"
                }
            }
        },
        "taxonomy": {
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "Gong et al. - PaPillon Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs": {
        "key": "gong2025papillon",
        "name": "PAPILLON",
        "year": "2025",
        "citation_count": "0",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "Optimizes toward semantic policy-violation behavior (jailbreak) but not a specific output."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Reports only observable failures but no investigation of root-cause"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly designs mutation operators and optimization constraints to preserve semantic coherence and limit token-length during every mutation step (using an LLM helper and length hyperparameters), and it provides empirical evidence that final fault-inducing prompts remain realistic: quantitative perplexity measurements (low average perplexity) and a human evaluation (interpretability/naturalness scores and alignment checks) demonstrating that generated prompts are natural and align with human judgments. These elements show both (1) mechanisms that enforce plausibility during fuzzing and (2) empirical validation that the final inputs are perceptually/semantically realistic.",
                    "evidence": "Section 4.2 (Template Mutation and Optimization), p.2408: \"In this paper, we propose using GPT-driven mutation operators to create jailbreaking templates, allowing us to preserve the semantic integrity of the generated templates throughout the mutation process.\"; Section 4.2 (Optimization schemes), p.2408\u20132409: \"Ensuring that the generated adversarial prompts are readable (low perplexity) is crucial... Experiments show that the average perplexity of our generated jailbreak prompts for GPT-3.5 Turbo is as low as 34.61, significantly below the average perplexity threshold of 58.83.\"; Section 4.2 (Optimization schemes), p.2409: \"By customizing the length of these templates (with a default setting of 200 tokens) during the mutation process, we can maintain a high success rate even with shorter prompts when setting system instructions.\"; Section 7 (Human Evaluation), p.2412\u20132413: \"A total of 30 volunteers participated... For interpretability... Naturalness... The results are summarized in Table 9. We observed that our generated jailbreak prompts match or surpass manually crafted ones in interpretability and naturalness.\"; Section 7 (Human Evaluation), p.2412: \"We observed that for a score of 8 or higher, all outputs were consistently aligned with human evaluation, achieving a 100% alignment rate. This indicates that a score of 8 serves as an appropriate threshold...\"."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, PAPILLON, is explicitly designed to maintain the plausibility and semantic coherence of its generated inputs (jailbreak prompts) throughout the fuzzing process. The mutation strategies leverage an LLM helper specifically to generate natural and fluent prompts, directly addressing the limitations of prior work that produced 'gibberish' or 'unintelligible' outputs. The paper provides strong empirical evidence for the plausibility of the final, fault-inducing prompts through two distinct mechanisms: 1) a quantitative assessment using perplexity, a standard measure for linguistic fluency, demonstrating that their prompts have low perplexity scores, and 2) a dedicated human evaluation study that measures the 'interpretability' and 'naturalness' of the generated prompts, finding them to be superior to manually crafted ones.",
                    "evidence": "Page 10, Section 4.2, 'Maintain semantic coherence': 'Ensuring that the generated adversarial prompts are readable (low perplexity) is crucial for evading existing jailbreak defenses based on perplexity filters. In this paper, we propose using GPT-driven mutation operators to create jailbreaking templates, allowing us to preserve the semantic integrity of the generated templates throughout the mutation process. Experiments show that the average perplexity of our generated jailbreak prompts for GPT-3.5 Turbo is as low as 34.61, significantly below the average perplexity threshold of 58.83.'\n\nPage 14, Section 7, 'Human Evaluation': 'Prompts quality. To evaluate the interpretability and naturalness of the generated prompts, we developed two criteria for quantifiable human evaluation. For interpretability, we measure how easily humans can understand the mechanism behind the jailbreak prompt... Naturalness is assessed based on the readability of the prompt... We observed that our generated jailbreak prompts match or surpass manually crafted ones in interpretability and naturalness.' Table 9 on page 15 provides the quantitative results of this human study, showing PAPILLON's prompts achieved higher scores for both interpretability (4.64 vs 3.98) and naturalness (4.60 vs 3.66) compared to a baseline of manually crafted prompts."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly designs mutation operators and optimization constraints to preserve semantic coherence and limit token-length during every mutation step (using an LLM helper and length hyperparameters), and it provides empirical evidence that final fault-inducing prompts remain realistic: quantitative perplexity measurements (low average perplexity) and a human evaluation (interpretability/naturalness scores and alignment checks) demonstrating that generated prompts are natural and align with human judgments. These elements show both (1) mechanisms that enforce plausibility during fuzzing and (2) empirical validation that the final inputs are perceptually/semantically realistic.",
                    "evidence": "Section 4.2 (Template Mutation and Optimization), p.2408: \"In this paper, we propose using GPT-driven mutation operators to create jailbreaking templates, allowing us to preserve the semantic integrity of the generated templates throughout the mutation process.\"; Section 4.2 (Optimization schemes), p.2408\u20132409: \"Ensuring that the generated adversarial prompts are readable (low perplexity) is crucial... Experiments show that the average perplexity of our generated jailbreak prompts for GPT-3.5 Turbo is as low as 34.61, significantly below the average perplexity threshold of 58.83.\"; Section 4.2 (Optimization schemes), p.2409: \"By customizing the length of these templates (with a default setting of 200 tokens) during the mutation process, we can maintain a high success rate even with shorter prompts when setting system instructions.\"; Section 7 (Human Evaluation), p.2412\u20132413: \"A total of 30 volunteers participated... For interpretability... Naturalness... The results are summarized in Table 9. We observed that our generated jailbreak prompts match or surpass manually crafted ones in interpretability and naturalness.\"; Section 7 (Human Evaluation), p.2412: \"We observed that for a score of 8 or higher, all outputs were consistently aligned with human evaluation, achieving a 100% alignment rate. This indicates that a score of 8 serves as an appropriate threshold...\"."
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Mutate in text level"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Medium",
                    "why": "Performs experiments on transferability"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly evaluates and demonstrates that fault-inducing prompts generated by PAPILLON on one model can be reused to successfully attack other models (cross-model and multimodal transfer experiments are reported). However, the PAPILLON design and mutation operators are described with goals of semantic coherence, short prompt length, and seed-pool/fuzzing effectiveness rather than with explicit mechanisms specifically intended to discover or maximize transferability across different model implementations (no surrogate-model training, ensemble surrogate testing, or transfer-focused mutators are described). Therefore the work shows reuse/testing of fault-inducing inputs for transferability but does not present an explicit design aimed at producing transferable attacks.",
                    "evidence": "Page 2401, Abstract: \"Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses.\"; Page 2412, Section 5.4 (Attack Transferability): \"We further investigate the transferability of PAPILLON. Transferability refers to the extent to which prompts designed to jailbreak the target model can successfully jailbreak another model.\" and \"Overall, PAPILLON demonstrates notable attack transferability across different models.\"; Page 2420, Table 17 (Cross-model transferability): shows the cross-model success-rate matrix (e.g., \"Baichuan2-7B -> Vicuna-7B-v1.3: 70%\", many non-zero cross-model ASR entries); Page 2407, Section 4.2 (Template Mutation and Optimization): \"we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.\" (describes focus on semantic coherence and prompt length rather than explicit transferability mechanisms)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper extensively evaluates the transferability of its generated jailbreak prompts. It includes a specific section (5.4 Attack Transferability) and tables (Table 5 and Table 17) demonstrating that prompts created for a source model can successfully trigger failures in other target models, including different LLMs and even multimodal models (MLLMs). This confirms that fault-inducing inputs are reused to test transferability. However, the paper's methodological description of the PAPILLON framework does not mention any explicit design choices or mechanisms specifically aimed at discovering or enhancing transferable attacks. The design goals of the mutation operators focus on maintaining semantic coherence and shortening prompt length to evade defenses and reduce costs, not on cross-model generalization. Therefore, while transferability is a demonstrated outcome, it is not an explicit goal of the fuzzing design itself.",
                    "evidence": "Page 13, Section 5.4 Attack Transferability: \"We further investigate the transferability of PAPILLON. Transferability refers to the extent to which prompts designed to jailbreak the target model can successfully jailbreak another model. ... LLM transferability. We first evaluate transferability by using jailbreak prompts generated by the source model to target another LLM. The results are shown in Table 17 (appendix). ... MLLM transferability. We then assess the transferability of the PAPILLON-generated jailbreak prompts to multimodal large language models (MLLMs). ... The results are presented in Table 5.\"\n\nPage 9, Section 4.2 Template Mutation and Optimization: \"Optimization schemes. When optimizing the jailbreaking templates, we control both semantic coherence and prompt length through system prompt instructions and the hyperparameter of the output length respectively.\" This section details the design goals of the attack generation, which do not include transferability."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper explicitly evaluates and demonstrates that fault-inducing prompts generated by PAPILLON on one model can be reused to successfully attack other models (cross-model and multimodal transfer experiments are reported). However, the PAPILLON design and mutation operators are described with goals of semantic coherence, short prompt length, and seed-pool/fuzzing effectiveness rather than with explicit mechanisms specifically intended to discover or maximize transferability across different model implementations (no surrogate-model training, ensemble surrogate testing, or transfer-focused mutators are described). Therefore the work shows reuse/testing of fault-inducing inputs for transferability but does not present an explicit design aimed at producing transferable attacks.",
                    "evidence": "Page 2401, Abstract: \"Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses.\"; Page 2412, Section 5.4 (Attack Transferability): \"We further investigate the transferability of PAPILLON. Transferability refers to the extent to which prompts designed to jailbreak the target model can successfully jailbreak another model.\" and \"Overall, PAPILLON demonstrates notable attack transferability across different models.\"; Page 2420, Table 17 (Cross-model transferability): shows the cross-model success-rate matrix (e.g., \"Baichuan2-7B -> Vicuna-7B-v1.3: 70%\", many non-zero cross-model ASR entries); Page 2407, Section 4.2 (Template Mutation and Optimization): \"we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.\" (describes focus on semantic coherence and prompt length rather than explicit transferability mechanisms)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ]
        }
    },
    "Du et al. - 2018 - DeepCruiser Automated Guided Testing for Stateful Deep Learning Systems": {
        "key": "du2018deepcruiser",
        "name": "DeepCruiser",
        "year": "2018",
        "citation_count": "45",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Provides heuristic explanations by showing correlation with state variation"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Ensures each audio is mutated at most once per transformation category (volume/speed/clearness) to prevent unnatural accumulation but does not measure naturalness on the final fuzzed inputs. The human validation is on single step transformation"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly designs and applies bounded, rule-based metamorphic transformations for audio (Section 6.1) and enforces constraints to limit accumulation of major distortions across mutation steps (e.g., altering volume, speed or clearness at most once). This shows the authors attempt to enforce input plausibility during mutation. However, they do not provide a systematic empirical assessment (human study or task-specific perceptual/fluency metrics such as PESQ/STOI/Perplexity/LPIPS/SSIM) to demonstrate that the final fault-inducing inputs remain perceptually/semantically realistic. The only explicit plausibility validation reported is a manual check of selected (100) high-quality seed inputs to mitigate false positives (Threats to Validity), not a broader or quantitative evaluation of final mutants. Therefore the paper enforces plausibility constraints in mutation design but lacks comprehensive validation of plausibility for produced failing inputs, matching the 'Medium' level.",
                    "evidence": "Section 6.1 (Metamorphic Transformations of Audio Signals), page 13: \"To generate suitable defect-triggering candidates, we apply the transformations with restraints to ensure that the audio seeds and the corresponding mutants sound the same to human beings.\"; Section 6.1 (same), page 13: \"(1) we carefully set the parameters to ensure that a single step transformation (on volume, speed or clearness) does not violate the metamorphic relations ... and (2) an audio, generated by mutating from a seed after a sequence of transformations T, is limited to be mutated by a transformation t \u2208 S \u222a UAT such that ({t} \u222a S) \u2229 T = \u2205, where S \u2208 {VRT, SRT, CRT}. Intuitively, the constraints make sure that a mutant is generated by altering the volume, speed or clearness of a seed input at most once. If this constraints is unsatisfied, the audio will not be transformed further.\"; Section 7.6 (Threats to Validity), page 19: \"We adopt a conservative metamorphic transformation strategy to make small changes on the original audio such that DeepCruiser will generate realistic audio. However, it may still cause false positives ... To mitigate this problem, we manually check and make sure the selected inputs (i.e., the 100 audio in RQ1, RQ2, RQ3 and RQ4) are of high quality.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility through rule-based constraints on its mutation operators, which it terms \"metamorphic transformations.\" The stated goal is to generate audio that \"sounds normal to human.\" Plausibility is enforced in two ways: 1) by carefully selecting parameters for individual transformations to ensure they do not perceptibly alter the audio, and 2) by explicitly limiting the cumulative effect of transformations related to volume, speed, or clearness to a single application per seed audio. This second constraint is a direct attempt to prevent the cumulative drift that can lead to unrealistic inputs. However, the paper falls short of the 'High' rating because it does not provide any empirical validation (e.g., a human study or quantitative metrics like PESQ/STOI) to confirm that the final, fault-inducing audio inputs are, in fact, plausible and intelligible. The claims of realism are based on the design of the transformation strategy rather than on an evaluation of its outputs.",
                    "evidence": "Page 14, Section 6.1: \"transformations for generating defect candidates indistinguishable to human before and after the transformations: (1) we carefully set the parameters to ensure that a single step transformation (on volume, speed or clearness) does not violate the metamorphic relations (i.e., human hearing are not affected); and (2) an audio, generated by mutating from a seed after a sequence of transformations T, is limited to be mutated by a transformation t \u2208 S U UAT such that ({t} U S) \u2229 T = \u2205, where S \u2208 {VR, SRT, CRT}. Intuitively, the constraints make sure that a mutant is generated by altering the volume, speed or clearness of a seed input at most once.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper explicitly designs and applies bounded, rule-based metamorphic transformations for audio (Section 6.1) and enforces constraints to limit accumulation of major distortions across mutation steps (e.g., altering volume, speed or clearness at most once). This shows the authors attempt to enforce input plausibility during mutation. However, they do not provide a systematic empirical assessment (human study or task-specific perceptual/fluency metrics such as PESQ/STOI/Perplexity/LPIPS/SSIM) to demonstrate that the final fault-inducing inputs remain perceptually/semantically realistic. The only explicit plausibility validation reported is a manual check of selected (100) high-quality seed inputs to mitigate false positives (Threats to Validity), not a broader or quantitative evaluation of final mutants. Therefore the paper enforces plausibility constraints in mutation design but lacks comprehensive validation of plausibility for produced failing inputs, matching the 'Medium' level.",
                    "evidence": "Section 6.1 (Metamorphic Transformations of Audio Signals), page 13: \"To generate suitable defect-triggering candidates, we apply the transformations with restraints to ensure that the audio seeds and the corresponding mutants sound the same to human beings.\"; Section 6.1 (same), page 13: \"(1) we carefully set the parameters to ensure that a single step transformation (on volume, speed or clearness) does not violate the metamorphic relations ... and (2) an audio, generated by mutating from a seed after a sequence of transformations T, is limited to be mutated by a transformation t \u2208 S \u222a UAT such that ({t} \u222a S) \u2229 T = \u2205, where S \u2208 {VRT, SRT, CRT}. Intuitively, the constraints make sure that a mutant is generated by altering the volume, speed or clearness of a seed input at most once. If this constraints is unsatisfied, the audio will not be transformed further.\"; Section 7.6 (Threats to Validity), page 19: \"We adopt a conservative metamorphic transformation strategy to make small changes on the original audio such that DeepCruiser will generate realistic audio. However, it may still cause false positives ... To mitigate this problem, we manually check and make sure the selected inputs (i.e., the 100 audio in RQ1, RQ2, RQ3 and RQ4) are of high quality.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Uses stable, perceptual transformations (volume, speed, clearness) likely to survive saving, though persistence is not explicitly verified."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "mutation and coverage strategies are tightly coupled to a specific RNN-based ASR model and training setup"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper evaluates DeepCruiser only on a single RNN-based ASR implementation (Mozilla DeepSpeech-0.3.0) and does not report reusing fault-inducing inputs to test other models nor describe design mechanisms explicitly intended to discover transferable attacks across different models. Therefore it does not demonstrate whether faults found on one model transfer to other models performing the same task.",
                    "evidence": "Page 16, Section 7.1 Datasets and Experiment Setup: \"We selected Mozilla\u2019s implementation of DeepSpeech-0.3.0 [42]... All our experiments were conducted on a pre-trained English model released along with DeepSpeech-0.3.0, which was trained with Fisher [43], LibriSpeech [44], Switchboard [45], and a pre-release snapshot of the English Common Voice training corpus [46].\"; Page 3, Introduction / contribution summary: \"We implement our testing framework, DeepCruiser, and perform in-depth evaluation on a state-of-the-art practical RNN based ASR system to demonstrate the usefulness.\" (demonstrates evaluation on a single ASR system and no cross-model transfer tests are reported)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's entire evaluation is conducted on a single, specific deep learning model for Automated Speech Recognition (ASR). The authors explicitly state they use Mozilla's DeepSpeech-0.3.0 model for all their experiments. The goal of the proposed framework, DeepCruiser, is to generate tests that increase coverage and find defects within this specific model. The paper does not mention testing the fault-inducing audio inputs generated for DeepSpeech on any other ASR models to check for transferability. Consequently, it does not demonstrate whether the discovered failures are specific to this one model or represent shared vulnerabilities across different models.",
                    "evidence": "Page 15, Section 7.1 Datasets and Experiment Setup: \"We selected Mozilla's implementation of DeepSpeech-0.3.0 [42] which produces one of the state-of-the-art open source ASR models. All our experiments were conducted on a pre-trained English model released along with DeepSpeech-0.3.0, which was trained with Fisher [43], LibriSpeech [44], Switchboard [45], and a pre-release snapshot of the English Common Voice training corpus [46].\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper evaluates DeepCruiser only on a single RNN-based ASR implementation (Mozilla DeepSpeech-0.3.0) and does not report reusing fault-inducing inputs to test other models nor describe design mechanisms explicitly intended to discover transferable attacks across different models. Therefore it does not demonstrate whether faults found on one model transfer to other models performing the same task.",
                    "evidence": "Page 16, Section 7.1 Datasets and Experiment Setup: \"We selected Mozilla\u2019s implementation of DeepSpeech-0.3.0 [42]... All our experiments were conducted on a pre-trained English model released along with DeepSpeech-0.3.0, which was trained with Fisher [43], LibriSpeech [44], Switchboard [45], and a pre-release snapshot of the English Common Voice training corpus [46].\"; Page 3, Introduction / contribution summary: \"We implement our testing framework, DeepCruiser, and perform in-depth evaluation on a state-of-the-art practical RNN based ASR system to demonstrate the usefulness.\" (demonstrates evaluation on a single ASR system and no cross-model transfer tests are reported)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Yuan et al. - 2023 - Revisiting Neuron Coverage for DNN Testing A Layer-Wise and Distribution-Aware Criterion": {
        "key": "yuan2023revisiting",
        "name": "NLC",
        "year": "2023",
        "citation_count": "36",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "shows heuristic link between coverage and errors."
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Uses inception score and FID score to measure natualness quality of the final inputs but overlook cumulave effects of mutations across iteration"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints at each mutation step (rule-based validity checks adopted from prior work) and reports quantitative perceptual quality measures (Inception Score and FID) showing that NLC-guided, fault-inducing images remain natural-looking. However, the paper does not describe any explicit mechanism that prevents cumulative drift across many sequential fuzzing iterations (i.e., global/aggregate plausibility guarantees), nor does it report human studies; plausibility is enforced/checked per-step and validated empirically on final outputs via IS/FID only.",
                    "evidence": "Sec. VI-B (Fuzzing setup) / page 1209: \"is_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than \u03b1\u00d7 #pixels or 2) the maximum of changed pixel value is less than \u03b2 \u00d7255. We set \u03b1 to 0.2 and \u03b2 to 0.4.\" (Alg.1 and accompanying text show the per-step validity check is applied before adding a mutated input as a new seed).; Sec. VI-B (Naturalness of Mutations) / pages 1209\u20131210: \"We measure the naturalness of mutated inputs under each criterion; ... We use Inception Score (IS) [39] and Frechet Inception Distance (FID) [18], which are widely used ... Overall, mutated images guided by NLC have the best scores for both IS and FID.\" (Table XI reports IS/FID values for NLC and other criteria).; There is no statement or section in the paper that introduces a global or cumulative-plausibility enforcement across fuzzing iterations (the algorithm checks is_valid(s_hat,s) relative to the immediate parent only, and mutated inputs are then added to the seed pool), implying cumulative drift across many bounded steps is not explicitly prevented nor analyzed."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper satisfies both conditions for a 'High' rating. First, it enforces input plausibility throughout the fuzzing iterations using an explicit validity check. The `is_valid()` function, described in Algorithm 1, constrains mutations by bounding the number of changed pixels and the maximum change in pixel value relative to the original seed input. This prevents the cumulative drift that can lead to unrealistic inputs. Second, the paper provides dedicated empirical evidence that the final fault-inducing inputs are realistic. It uses two standard quantitative, task-specific measures for image naturalness\u2014Inception Score (IS) and Fr\u00e8chet Inception Distance (FID)\u2014to evaluate the outputs of its fuzzing process. The results in Table XI demonstrate that the images generated under the guidance of the proposed NLC criterion have better naturalness scores compared to other methods and a random baseline.",
                    "evidence": "Page 10, Section VI-B, Triggered Faults: \"is_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than \u03b1 \u00d7 #pixels or 2) the maximum of changed pixel value is less than \u03b2 \u00d7 255. We set \u03b1 to 0.2 and \u03b2 to 0.4.\"\nPage 11, Section VI-B, Naturalness of Mutations: \"We measure the naturalness of mutated inputs under each criterion; a good criterion should lead to mutations that mostly preserve the naturalness. We use Inception Score (IS) [39] and Fr\u00e8chet Inception Distance (FID) [18], which are widely used in the AI and SE community to assess the naturalness of images, as the metrics.\"\nPage 11, Table XI: This table provides the results for IS and FID for various fuzzing configurations, with the accompanying text stating, \"Overall, mutated images guided by NLC have the best scores for both IS and FID.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility via per-step, rule-based validity checks (bounding pixel changes in each mutation) and reports quantitative measures (Inception Score and FID) showing that the final fault-inducing images are natural-looking. However, it does not introduce any mechanism to bound or assess cumulative drift over multiple fuzzing iterations, nor does it include a human evaluation of plausibility. Thus it validates plausibility at each mutation and on the final outputs, but overlooks aggregate guarantees across the entire fuzzing process.",
                    "evidence": "Section VI-B (Fuzzing setup) and Algorithm 1 (page 1209): \u201cis_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than \u03b1\u00d7#pixels or 2) the maximum of changed pixel value is less than \u03b2\u00d7255. We set \u03b1 to 0.2 and \u03b2 to 0.4.\u201d\n\nSection VI-B (Naturalness of Mutations) and Table XI (pages 1209\u20131210): \u201cWe measure the naturalness of mutated inputs \u2026 We use Inception Score (IS) [39] and Fr\u00e9chet Inception Distance (FID) [18] \u2026 Overall, mutated images guided by NLC have the best scores for both IS and FID.\u201d\n\nNo section in the paper describes any global or cumulative\u2010plausibility constraint over multiple iterations of fuzzing."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Plausibly stable due to large semantic changes, but applied in normalized float domain, so some discovered failures could vanish after re-encoding."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models performing the same task, nor does it describe mechanisms intended to discover or encourage transferable attacks. All fuzzing and adversarial-example experiments are conducted per tested model (i.e., inputs are generated/assessed against the same target DNN), and the evaluation reports coverage/faults per model without reusing generated fault-inducing inputs across different model implementations. Thus the work does not address attack transferability.",
                    "evidence": "VI.B (Guiding Input Mutation in DNN Testing), Alg. 1 (page 1209): \"Tested DNN: D; Criterion: C;\" \u2014 the fuzzing algorithm is defined in terms of a single tested DNN. \n\nVI.A.2 (Fault-Revealing Capability of Test Suites) (page 1208): \"AEs are generated using the training data (see 'Clarification' below for results of AEs generated using test data), and all algorithms attacking the three models reach over 98% success rates.\" \u2014 adversarial examples are generated and evaluated against the same model(s) they attack. \n\nVI (Evaluation) and Tables (e.g., Table VI, Table VII) (pages 1206\u20131208): All reported results present coverage and faults per model (ResNet, VGG, MobileNet, BigGAN, etc.), with no experiment or section reporting reuse of fault-inducing inputs across different models or measurements of transferability. (No section title or experiment in the paper addresses transferability or substitute-model attacks.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper focuses on designing a new coverage criterion (NLC) and evaluating its effectiveness for assessing test suites and guiding fuzzing for a single, specific DNN model at a time. The experiments involve generating fault-inducing inputs (either through adversarial attacks or fuzzing) on a given model and then evaluating the outcome on that same model. The paper does not contain any experiments or discussion on whether the fault-inducing inputs generated for one model (e.g., ResNet50) can also trigger failures in other models performing the same task (e.g., VGG16 or MobileNetV2). The fuzzing design and the NLC metric itself are aimed at exploring the internal state of a single model, not at finding shared vulnerabilities across different models.",
                    "evidence": "Page 10, Section VI-B, \"Guiding Input Mutation in DNN Testing\", Algorithm 1: The fuzzing algorithm is defined with a single \"Tested DNN: D\" as input. The core feedback loop in line 7, `if is_valid($,s) and coverage_inc(C,D) then`, clearly shows that the decision to keep a mutated input is based on its ability to increase the coverage metric `C` on that specific DNN `D`. This demonstrates that the fuzzing process is tailored to a single model, not designed for finding transferable inputs. The subsequent results in Table XI also report fault-triggering rates for individual models that were fuzzed, with no cross-evaluation."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models performing the same task, nor does it describe mechanisms intended to discover or encourage transferable attacks. All fuzzing and adversarial-example experiments are conducted per tested model (i.e., inputs are generated/assessed against the same target DNN), and the evaluation reports coverage/faults per model without reusing generated fault-inducing inputs across different model implementations. Thus the work does not address attack transferability.",
                    "evidence": "VI.B (Guiding Input Mutation in DNN Testing), Alg. 1 (page 1209): \"Tested DNN: D; Criterion: C;\" \u2014 the fuzzing algorithm is defined in terms of a single tested DNN. \n\nVI.A.2 (Fault-Revealing Capability of Test Suites) (page 1208): \"AEs are generated using the training data (see 'Clarification' below for results of AEs generated using test data), and all algorithms attacking the three models reach over 98% success rates.\" \u2014 adversarial examples are generated and evaluated against the same model(s) they attack. \n\nVI (Evaluation) and Tables (e.g., Table VI, Table VII) (pages 1206\u20131208): All reported results present coverage and faults per model (ResNet, VGG, MobileNet, BigGAN, etc.), with no experiment or section reporting reuse of fault-inducing inputs across different models or measurements of transferability. (No section title or experiment in the paper addresses transferability or substitute-model attacks.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Xie et al. - 2019 - DeepHunter a coverage-guided fuzz testing framework for deep neural networks": {
        "key": "xie2019deephunter",
        "name": "DeepHunter",
        "year": "2019",
        "citation_count": "518",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "It uses model coverage to guide input generation but does not provide any expeirment showing the correlation with coverage"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "User study was conducted on the images generated by single-step metamorphic mutations, not on the final fault-triggering fuzzed inputs after multiple chained transformations. Allows at most one affine transformation to reduce that risk."
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly designs mutation operators and checks that constrain mutations across iterative fuzzing (e.g., allowing at most one affine transform per mutation chain, and bounding cumulative pixel changes via L0/L\u221e computed relative to a reference image), thereby enforcing plausibility throughout sequential mutations. In addition, the authors performed a human user study that empirically measured the perceptual validity of generated tests (45,000 images total) and report low invalidity rates for DeepHunter (e.g., 1.2% MNIST, 2.9% CIFAR-10, 0.8% ImageNet). These two facts show the method both enforces plausibility across iterations and provides empirical evidence that final generated (including fault-inducing) inputs remain perceptually realistic.",
                    "evidence": "Page 149, Section 3.2 (Transformation and Mutation): \"In order to keep the semantics of the mutants close to the original seed, we adopt a conservative strategy that selects the Affine Transformation to be used only once ... A Pixel Value Transformation can be used multiple times ... we use L0 and L\u221e to constrain the pixel-level changes.\"; (Alg.2 / Definition 5 / Eq.2) Page 150: \"For sequential mutations that include an Affine Transformation ... the L0 and L\u221e between s0 and sn are computed as follows: L0(s0,sn) = L0(s0,sj-1) + L0(sj,sn)  L\u221e(s0,sn) = MAX(L\u221e(s0,sj-1), L\u221e(sj,sn)).\"; Page 151, Section 4 (User Study Settings): \"For each dataset, we randomly select 30 images as the seeds and generate 5,000 images with each of the 3 mutation strategies. Finally, 9 sets, with a total of 45,000 test images ... We recruit 9 participants ... A generated image is marked invalid if it either could not be perceived, or is recognized as a different class compared with its original counterpart.\"; Page 153, Section 5.1 (RQ1 Results) and Table 2: \"Table 2: The number and ratio of invalid images generated by different mutation strategies ... DeepHunter 60 (1.2%) 145 (2.9%) 40 (0.8%)\"; Page 147 (Abstract / Summary of results): \"The user study on mutation strategies shows ... with careful transformation constraint design and tuning, it can still reduce the likelihood to generate invalid tests to some extent (e.g., DeepHunter generates 1.2% on MNIST, 2.0% on CIFAR-10, and 0.8% on ImageNet).\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper explicitly designs its metamorphic mutation strategy to maintain the semantic validity of generated inputs throughout the fuzzing process, directly addressing the issue of cumulative transformations leading to implausible results. The methodology enforces this by allowing structural changes (Affine Transformations) only once per mutation sequence from an initial seed, while constraining subsequent pixel-level changes relative to a stable reference image. This design directly counters the problem of input drift over multiple iterations. Furthermore, the paper provides strong empirical evidence for the plausibility of its generated inputs via a formal user study. The study assessed the validity of thousands of generated images and found that DeepHunter's strategy produces a very low percentage of invalid images (e.g., 0.8% for ImageNet, 1.2% for MNIST), thus confirming that the inputs it generates, including those that find faults, remain perceptually realistic.",
                    "evidence": "Page 6, Section 3.2.3, Definition 5 and surrounding text: 'In order to keep the semantics of the mutants close to the original seed, we adopt a conservative strategy that selects the Affine Transformation to be used only once (we assume that mutation with only one affine transformation will not affect the semantics with the carefully selected parameters) as an image is more likely to be unrecognizable if it is changed by multiple affine transformations.' This describes the mechanism for controlling cumulative effects. \n\nPage 7, Section 4, 'User Study Settings': 'For RQ1, we perform the user study on 3 different strategies that constrain the generation of test images, and evaluate their effectiveness in generating valid test images... Each participant is asked to perform manual analysis on each of the generated test images in line with its original counterpart. A generated image is marked invalid if it either could not be perceived, or is recognized as a different class compared with its original counterpart.' This describes the empirical evaluation. \n\nPage 8, Table 2: This table presents the results of the user study, showing that DeepHunter generated invalid images at rates of only 1.2% (MNIST), 2.9% (CIFAR-10), and 0.8% (ImageNet), providing quantitative evidence of plausibility."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly designs mutation operators and checks that constrain mutations across iterative fuzzing (e.g., allowing at most one affine transform per mutation chain, and bounding cumulative pixel changes via L0/L\u221e computed relative to a reference image), thereby enforcing plausibility throughout sequential mutations. In addition, the authors performed a human user study that empirically measured the perceptual validity of generated tests (45,000 images total) and report low invalidity rates for DeepHunter (e.g., 1.2% MNIST, 2.9% CIFAR-10, 0.8% ImageNet). These two facts show the method both enforces plausibility across iterations and provides empirical evidence that final generated (including fault-inducing) inputs remain perceptually realistic.",
                    "evidence": "Page 149, Section 3.2 (Transformation and Mutation): \"In order to keep the semantics of the mutants close to the original seed, we adopt a conservative strategy that selects the Affine Transformation to be used only once ... A Pixel Value Transformation can be used multiple times ... we use L0 and L\u221e to constrain the pixel-level changes.\"; (Alg.2 / Definition 5 / Eq.2) Page 150: \"For sequential mutations that include an Affine Transformation ... the L0 and L\u221e between s0 and sn are computed as follows: L0(s0,sn) = L0(s0,sj-1) + L0(sj,sn)  L\u221e(s0,sn) = MAX(L\u221e(s0,sj-1), L\u221e(sj,sn)).\"; Page 151, Section 4 (User Study Settings): \"For each dataset, we randomly select 30 images as the seeds and generate 5,000 images with each of the 3 mutation strategies. Finally, 9 sets, with a total of 45,000 test images ... We recruit 9 participants ... A generated image is marked invalid if it either could not be perceived, or is recognized as a different class compared with its original counterpart.\"; Page 153, Section 5.1 (RQ1 Results) and Table 2: \"Table 2: The number and ratio of invalid images generated by different mutation strategies ... DeepHunter 60 (1.2%) 145 (2.9%) 40 (0.8%)\"; Page 147 (Abstract / Summary of results): \"The user study on mutation strategies shows ... with careful transformation constraint design and tuning, it can still reduce the likelihood to generate invalid tests to some extent (e.g., DeepHunter generates 1.2% on MNIST, 2.0% on CIFAR-10, and 0.8% on ImageNet).\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "Enforce norm-bounded constraint between successibe mutations but the perturbation can accumulated over time",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "The metamorphic transformations likely to persist but not validated. Works on floating point input without applying rounding."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly defines and implements a mechanism to detect faults that transfer from an original model to other models (specifically quantized versions), and the evaluation reuses tests generated on the original (unquantized) model to detect errors in the quantized models. Definition 3 formalizes a \u201cquantization erroneous behavior\u201d (i.e., a fault that manifests in one model but not the other), DeepHunter\u2019s workflow and experiments use that relation, and RQ4 reports results where tests generated on the original model are applied to quantized variants to measure such transfer. This demonstrates both reuse of fault-inducing inputs across models performing the same task (original vs. quantized) and an explicit design for discovering transferable failures.",
                    "evidence": "Page 149, Section 3.2.1 (Transformation and Mutation) \u2014 Definition 3: \"Given a DNN F and its quantized version F', a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x) \u2227 F'(x) = O(x), a mutant x' \u2208 M(x) is a quantization erroneous behavior between F and F' if (F(x) = F(x') \u2228 F'(x) = F'(x')) \u2227 F(x') \u2260 F(x).\" \n\nPage 154, Section 5.4 (RQ4. Error Detection for Quantization) \u2014 experimental procedure and results: \"In this experiment, all initial seeds of each dataset are unable to detect the errors before and after quantization for all cases... At runtime, we allocate 10 hours for test generation on each original unquantized model... Table 6 summarizes the results in detecting errors introduced during quantization by DeepHunter+Prob and TensorFuzz.\" \n\nPage 154, Section 5.4 (Answer to RQ4): \"With corner-region based criteria, DeepHunter tends to be more useful for detecting errors introduced during the DNN quantization. A larger QR often introduces larger decision logic inconsistencies before and after quantization.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's proposed framework, DeepHunter, is designed and evaluated for testing a single Deep Neural Network (DNN) model. The methodology, including the test generation algorithm and oracles, is focused on finding defects within one specific target model or between that model and its quantized version. The experimental evaluation is conducted on several different DNNs, but each is tested in isolation. The paper does not include any experiments where fault-inducing inputs generated for one model (e.g., LeNet-5) are used to test another model performing the same task (e.g., a different MNIST classifier). The authors explicitly state this single-model focus in the related work section, distinguishing their approach from differential testing techniques that compare multiple DNNs.",
                    "evidence": "Page 11, Section 6 RELATED WORK, 'Different from DeepXplore and DiffChaser which focus on differential testing, DeepHunter is a grey-box testing technique focusing on a single DNN model.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "DeepHunter explicitly evaluates the transferability of fault\u2010inducing inputs by first generating mutants on an original (32\u2010bit) DNN model and then reusing those same tests to detect errors in its quantized counterparts. Definition 3 formalizes the notion of a \u201cquantization erroneous behavior\u201d (i.e., faults that appear in one model version but not the other), and RQ4\u2019s experimental setup and results demonstrate this cross\u2010model reuse. This matches the \u201cHigh\u201d level of the Attack Transferability metric, since the framework both reuses inputs across models performing the same task (original versus quantized) and incorporates mechanisms to discover these transferable failures.",
                    "evidence": "Page 149, Section 3.2.1 \u2013 Definition 3: Formalizes \u201cquantization erroneous behavior\u201d between F and its quantized version F\u2032, describing tests generated on one model and applied to the other.\nPage 154, Section 5.4 \u2013 RQ4: Describes running DeepHunter on the original model and applying the generated tests to quantized versions, with Table 6 summarizing the errors detected across models."
                },
                "manual": {
                    "value": "Low",
                    "why": "The oracle definitions is that if same input produce different output to a model and its quanitized version, then it is fault-triggering input. But it is not transferability.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic",
                "Differential"
            ]
        }
    },
    "Guo et al. - 2018 - DLFuzz differential fuzzing testing of deep learning systems": {
        "key": "guo2018dlfuzz",
        "name": "DLFuzz",
        "year": "2018",
        "citation_count": "360",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Finds generic misclassifications"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Does not show any correlation between errors and coverage"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "norm-bounded perturbation"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper enforces perceptual plausibility throughout the fuzzing process by measuring and bounding the L2 distance of each mutated input relative to the original input (used as a criterion before keeping a seed), thereby preventing cumulative drift across iterations. In addition, the authors report an explicit manual validation of all generated adversarial inputs (366 images) and state they found no changes of identity and that perturbations are imperceptible. These constitute an enforcement mechanism plus empirical human validation that final fault-inducing inputs remain perceptually realistic.",
                    "evidence": "Section 3.2 (Algorithm) \u2014 (page 741): Algorithm 1, line 15: \"l2_distance = distance(x', x) //measure the perturbation\" and accompanying text: \"As for the conditions of seed keeping in line 16, DLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\" \n\nSection 3.1 (Architecture) \u2014 (page 740): \"The minute perturbation is restricted to be invisible and ensures the prediction results before and after the mutation to be the same.\" \n\nSection 4.3 (Discussion) \u2014 (page 742): \"For confirmation, we checked all the 366 adversarial inputs generated by DLFuzz, though DLFuzz maintains quite small l2 distance by the restricted threshold. We haven't found any adversarial inputs that have already changed their identities after mutation. The adversarial inputs are nearly the same as the original input, and the perturbations are imperceptible.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility in individual mutation steps by using a bounded L2 distance constraint to ensure perturbations are small and imperceptible. The algorithm's design, which uses mutated inputs as new seeds, may overlook the cumulative effect of these small perturbations over many iterations. However, the paper validates the plausibility of the final fault-inducing inputs through manual inspection. The authors state they visually checked all 366 generated adversarial inputs and found them to be imperceptible and nearly identical to the original inputs, which constitutes a form of human-based validation. This combination of bounded per-step constraints and validation of the final outputs, without enforcing plausibility throughout the entire process or using quantitative perceptual metrics, aligns with the 'Medium' rating.",
                    "evidence": "Page 4, Section 4, Fuzzing Process: \"Second, DLFuzz adopts l2 distance to measure the perturbation with similar computation with [12]. As for the conditions of seed keeping in line 16, DLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\"\n\nPage 5, Section 4.3, Discussion: \"Without Manual Effort. For confirmation, we checked all the 366 adversarial inputs generated by DLFuzz, though DLFuzz maintains quite small l2 distance by the restricted threshold. We haven't found any adversarial inputs that have already changed their identities after mutation. The adversarial inputs are nearly the same as the original input, and the perturbations are imperceptible.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "DLFuzz actively enforces plausibility at every fuzzing iteration by bounding the L2 distance of each mutated input to the original seed (Algorithm 1, lines 15\u201316), preventing cumulative drift. Furthermore, the authors conducted a full manual validation of all 366 adversarial examples, reporting that none changed their semantic identity and that all perturbations remained imperceptible. This combination of a global perturbation constraint and explicit human evaluation of the final fault-inducing inputs satisfies the criteria for a \u201cHigh\u201d rating.",
                    "evidence": "Section 3.2, Algorithm 1 (page 741): \u201cl2_distance = distance(x\u2032, x) // measure the perturbation\u201d and \u201cD\u2009Lfuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\u201d\n\nSection 4.3, Discussion (page 742): \u201cFor confirmation, we checked all the 366 adversarial inputs generated by DLFuzz\u2026 We haven\u2019t found any adversarial inputs that have already changed their identities after mutation. The adversarial inputs are nearly the same as the original input, and the perturbations are imperceptible.\u201d"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Bound perturbation relative to parent seed but that can already mutated multiple time. No control over cumulative deviation across iterations",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": "relies on imperceptible, float-level perturbations that can vanish after quantization or saving, with no persistence verification."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across different models. DLFuzz is explicitly designed to avoid cross-referencing multiple models (unlike DeepXplore) and treats an input as a fault when the mutated input and the original input produce different labels on the same CNN under test. The evaluation compares DLFuzz to DeepXplore on per-model metrics (neuron coverage, number of adversarial inputs, perturbation size, time) but does not reuse adversarial inputs generated on one model to test other models or report whether such inputs trigger failures across models. The only related experiment (augmenting training sets) retrains the same CNNs with adversarial images and is not a transferability test. Therefore the paper does not demonstrate transferable attacks nor include mechanisms explicitly aimed at discovering transferable failures across models.",
                    "evidence": "Abstract (p.739, Abstract): \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\" \n\nSection 3.1 Architecture (p.740, 3.1 Architecture): \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.\" \n\nIntroduction / Evaluation summary (p.740, end of Intro / start of 3): \"Compared with DeepXplore, DLFuzz does not need extra efforts to collect similar functional DL systems for cross-referencing label check...\" \n\nResults (p.742, 4.2 Result): \"we incorporated 114 adversarial images into the training set of three CNNs on MNIST and retrained them trying to increase their accuracy.\" (This describes retraining with generated adversarial inputs, not testing those inputs on other, unseen models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper presents DLFuzz as a method for testing a single Deep Learning (DL) system in isolation. A core contribution highlighted is its ability to find incorrect behaviors without needing \"cross-referencing oracles from other DL systems with the same functionality.\" The methodology focuses on generating adversarial inputs for one specific model under test by maximizing prediction differences between an original and a mutated input for that same model. The experimental evaluation, while conducted on multiple models (e.g., LeNet-1, LeNet-4, VGG-16), treats each model as an independent target. The results are reported on a per-model basis, and there is no mention of any experiment where fault-inducing inputs generated for one model are used to test another model to assess transferability.",
                    "evidence": "Page 2, Abstract: \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\"\n\nPage 5, Section 5, Conclusion: \"DLFuzz also overcomes the trouble of relying on multiple DL systems of the similar functionality in DeepXplore.\"\n\nPage 5, Table 1: \"Effectiveness of DLFuzz compared with DeepXplore.\" This table presents the results for each model (LeNet-1, LeNet-4, LeNet-5, VGG16, VGG19, ResNet50) individually, comparing DLFuzz to DeepXplore on that specific model. There is no data or discussion regarding the transferability of adversarial inputs generated for one model to any of the others."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across different models. DLFuzz is explicitly designed to avoid cross-referencing multiple models (unlike DeepXplore) and treats an input as a fault when the mutated input and the original input produce different labels on the same CNN under test. The evaluation compares DLFuzz to DeepXplore on per-model metrics (neuron coverage, number of adversarial inputs, perturbation size, time) but does not reuse adversarial inputs generated on one model to test other models or report whether such inputs trigger failures across models. The only related experiment (augmenting training sets) retrains the same CNNs with adversarial images and is not a transferability test. Therefore the paper does not demonstrate transferable attacks nor include mechanisms explicitly aimed at discovering transferable failures across models.",
                    "evidence": "Abstract (p.739, Abstract): \"DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\" \n\nSection 3.1 Architecture (p.740, 3.1 Architecture): \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.\" \n\nIntroduction / Evaluation summary (p.740, end of Intro / start of 3): \"Compared with DeepXplore, DLFuzz does not need extra efforts to collect similar functional DL systems for cross-referencing label check...\" \n\nResults (p.742, 4.2 Result): \"we incorporated 114 adversarial images into the training set of three CNNs on MNIST and retrained them trying to increase their accuracy.\" (This describes retraining with generated adversarial inputs, not testing those inputs on other, unseen models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Whitebox"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Prediction-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Yu et al. - LLM-Fuzzer Scaling Assessment of Large Language Model Jailbreaks": {
        "key": "yu2024llm",
        "name": "LLM-Fuzzer",
        "year": "2024",
        "citation_count": "40",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "aims to generate many jailbreak templates that trigger harmful content without specifying exact outcomes"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Does not provide any explanation why those errors occur."
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper designs mutation operators that explicitly aim to preserve semantic correctness (using LLM-assisted Generate/Crossover/Expand/Shorten/Rephrase operators) and adds a seed-selection reward discount (\u03b1 * path length) and early-termination probability to discourage deep mutation chains, i.e., mechanisms that constrain individual mutation steps and help limit cumulative drift. However, the paper does not present any empirical assessment of the perceptual or semantic plausibility of the final fault-inducing prompts (no human evaluation or task-specific quantitative measures such as perplexity or semantic-similarity scores are reported). Thus the work enforces plausibility in mutation design and includes measures to reduce accumulation effects, but it does not validate final-input plausibility empirically \u2014 matching the \"Medium\" definition.",
                    "evidence": "Section 3.3 Mutation (page 4662): \"To maintain the semantic correctness of newly generated prompts, we have introduced five innovative mutation operators that harness LLMs as assistants for prompt alteration while preserving their semantic integrity.\"; Section 3.2 Seed Selection / Algorithm 1 (page 4661): algorithm lines show reward discounting to penalize deep paths: \"if reward > 0 then\\n    reward \u2190 max(reward \u2212 \u03b1 * len(path), \u03b2)\" (Algorithm 1, Update); Section 3.3 (page 4662): \"To showcase that the newly generated templates can maintain their semantic integrity and diversify our template collection, Figure 3 presents several post-mutated templates.\" \u2014 i.e., qualitative examples only; Section 3.4 Oracle (page 4663): \"we harnessed these annotated responses to fine-tune a RoBERTa model... this RoBERTa model serves as our designated 'oracle'.\" \u2014 oracle evaluates harmfulness of outputs, not input plausibility; Section 4.1 Metrics (page 4664): defines evaluation metrics (JQN, ASR, EASR, QBC, TBC) with no mention of human evaluation or quantitative plausibility measures (e.g., perplexity or semantic-similarity), and no human study of final prompts is reported. Together these passages show plausibility is enforced in mutation steps and some controls for accumulation exist, but final-input plausibility is not empirically validated."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's methodology is explicitly designed to maintain the plausibility and semantic integrity of the fuzzer's inputs. The authors identify the shortcomings of traditional mutation techniques, which can produce \"syntactically incorrect or semantically nonsensical inputs\" when applied to natural language. To overcome this, their core mutation strategy leverages LLMs to perform all transformations (Generate, Crossover, Expand, Shorten, Rephrase). This design inherently ensures that mutated inputs remain within the plausible distribution of coherent, human-readable text throughout all fuzzing iterations. Furthermore, the paper provides empirical evidence to support this claim by presenting multiple examples of mutated templates and explicitly stating that this is done to \"showcase that the newly generated templates can maintain their semantic integrity\". These examples serve as qualitative validation that the final fault-inducing inputs remain semantically realistic.",
                    "evidence": "Page 7, Section 3.3 Mutation: \"Directly applying these techniques [traditional program fuzzing] to natural language inputs can result in syntactically incorrect or semantically nonsensical inputs, which are unlikely to be effective in jailbreaking LLMs. To address this challenge, we introduce distinct mutation methods that leverage LLMs themselves to assist in the mutation process. LLMs, with their proficiency in understanding and generating human-like text, offer a promising approach for crafting coherent and contextually relevant mutations... In order to showcase that the newly generated templates can maintain their semantic integrity and diversify our template collection, Figure 3 presents several post-mutated templates.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper incorporates mechanisms to enforce semantic plausibility during each mutation step\u2014namely, LLM-assisted Generate/Crossover/Expand/Shorten/Rephrase operators\u2014and penalizes overly deep mutation chains via reward discounting and early\u2010termination in the MCTS-Explore seed selection (Algorithm 1). However, it does not provide any explicit empirical evaluation of the final fault-inducing prompts\u2019 plausibility (no human studies or quantitative metrics such as perplexity or semantic-similarity scores are reported). Thus, while plausibility is enforced in mutation design and some mitigation against cumulative drift is applied, there is no empirical validation of final-input realism, matching the \u201cMedium\u201d level.",
                    "evidence": "Section 3.3 Mutation (page 4662): \u201cTo maintain the semantic correctness of newly generated prompts, we have introduced five innovative mutation operators that harness LLMs\u2026\u2009\u201d and Figure 3 qualitative examples; Algorithm 1 Update (page 4661): reward \u2190 max(reward \u2212 \u03b1*len(path),\u03b2) and early-termination to limit deep mutation chains; Section 4.1 Metrics (page 4664): defines JQN, ASR, EASR, QBC, TBC with no mention of human evaluation or task-specific measures (e.g., perplexity, SSIM) to assess input plausibility."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Mutations are purely at the text level"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly evaluates transferability: templates (fault-inducing inputs) generated by LLM-FUZZER on a set of surrogate models are reused to test many other models, and the fuzzer's design includes an explicit mechanism (oracle reward modification) to encourage generation of templates that succeed across multiple models. The authors both describe this cross-model generation/evaluation procedure (Experiment III) and report strong transfer results (EASR > 80% on many target models) and comparisons to other methods.",
                    "evidence": "Section 4.2 Experiment III (p.4664\u20134665): \"To address the question of transferability, we chose to create templates using gpt-3.5-turbo-0125, Llama-2-7B-Chat, and Vicuna-7B.... in the template reward assignment by the oracle, a zero reward was imposed if the template\u2019s all successful jailbreak attempts were attributed solely to one or two LLMs.\" \n\nSection 4.2 Experiment III (p.4665): \"Following the exhaustion of the 150k query budget, our experiment involved calculating the average ASR for each template generated across the three target models. The template with the highest average ASR, denoted as 'top-1', and the top five templates with the highest average ASR, known as 'top-5', were selected for further experimentation.\" \n\nSection 4.3 Results for Experiment III (p.4666\u20134668): \"These templates displayed a notable degree of transferability... As depicted in Figure 4, when evaluated against Vicuna-13B, Baichuan-13B, ChatGLM-6B, Llama-2-13B/70B, Claude1, and PaLM2, the EASR for LLM-FUZZER consistently exceeded 80%, having a huge advantage over EASRs of other methods.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper satisfies both conditions for a 'High' rating. First, it explicitly tests the transferability of the jailbreak templates generated by LLM-FUZZER. Templates are created using a set of three models and then evaluated against a larger, diverse set of unseen open-source and commercial LLMs. Second, the fuzzing design incorporates an explicit mechanism to support the discovery of transferable failures. Specifically for the transferability experiment, the reward mechanism of the fuzzer is modified to query multiple models simultaneously and penalize templates that are not universally effective, thereby actively guiding the fuzzer towards generating transferable jailbreaks.",
                    "evidence": "Page 10-11, Section 4.2, Experiment Design: \"Experiment III. To address the question of transferability, we chose to create templates using gpt-3.5-turbo-0125, Llama-2-7B-Chat, and Vicuna-7B. This approach was taken to ensure that the generated templates would be applicable across various LLMs. We followed a similar procedure with the previous experiment but made two slight modifications. First, in each fuzzing iteration, we queried 100 questions for each template across all three models, resulting in a total of 300 responses. Second, in the template reward assignment by the oracle, a zero reward was imposed if the template's all successful jailbreak attempts were attributed solely to one or two LLMs. This modification aimed to bolster universality by discouraging the generation of templates that could exploit only one or two specific LLMs.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly evaluates transferability: templates (fault-inducing inputs) generated by LLM-FUZZER on a set of surrogate models are reused to test many other models, and the fuzzer's design includes an explicit mechanism (oracle reward modification) to encourage generation of templates that succeed across multiple models. The authors both describe this cross-model generation/evaluation procedure (Experiment III) and report strong transfer results (EASR > 80% on many target models) and comparisons to other methods.",
                    "evidence": "Section 4.2 Experiment III (p.4664\u20134665): \"To address the question of transferability, we chose to create templates using gpt-3.5-turbo-0125, Llama-2-7B-Chat, and Vicuna-7B.... in the template reward assignment by the oracle, a zero reward was imposed if the template\u2019s all successful jailbreak attempts were attributed solely to one or two LLMs.\" \n\nSection 4.2 Experiment III (p.4665): \"Following the exhaustion of the 150k query budget, our experiment involved calculating the average ASR for each template generated across the three target models. The template with the highest average ASR, denoted as 'top-1', and the top five templates with the highest average ASR, known as 'top-5', were selected for further experimentation.\" \n\nSection 4.3 Results for Experiment III (p.4666\u20134668): \"These templates displayed a notable degree of transferability... As depicted in Figure 4, when evaluated against Vicuna-13B, Baichuan-13B, ChatGLM-6B, Llama-2-13B/70B, Claude1, and PaLM2, the EASR for LLM-FUZZER consistently exceeded 80%, having a huge advantage over EASRs of other methods.\""
                },
                "manual": {
                    "value": "High",
                    "why": "Transferability is encouraged in mutation by rewarding less to mutations that transfer less.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ],
            "oracle": [
                "Property-based"
            ]
        }
    },
    "Tian et al. - 2018 - DeepTest automated testing of deep-neural-network-driven autonomous cars": {
        "key": "tian2018deeptest",
        "name": "DeepTest",
        "year": "2018",
        "citation_count": "1791",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Aims to find errors in steering angle predictions under realistic environmental variations."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Shows correlation between coverage and errors"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Does not verify the naturalness of the fault-revealing mutated images through any human study or algorithmic naturalness metric. Individual transformations are naturalistic but chained mutations might not be"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "DeepTest explicitly restricts individual mutation steps to realistic, domain-specific image transformations (brightness, contrast, affine transforms, blur, rain/fog composed with Photoshop filters) and specifies bounded parameter ranges (Table 4). It also applies a quantitative filter (Equation 3) and manual inspection to reduce implausible / false-positive reports, i.e., it validates plausibility of final fault-inducing inputs. However, the fuzzing algorithm (Algorithm 1) composes multiple transformations without an explicit mechanism that enforces plausibility across cumulative iterations (no distributional constraint or perceptual-similarity metric is enforced during generation), and the authors acknowledge that generated images \u201cmay not be exactly reproducible in reality.\u201d Thus DeepTest validates final inputs and constrains single-step mutations, but it does not fully prevent or measure cumulative drift with perceptual metrics or a human study, matching the paper to the Medium level.",
                    "evidence": "Section 3.2 (p.306): \"we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect).\"; Table 4 (p.306): lists bounded parameter ranges for each transformation (e.g., Translation (tx, ty) (10, 10) to (100, 100) step (10, 10); Rotation q 3 to 30 with step 3; Contrast \u03b1 1.2 to 3.0 step 0.2; Brightness \u03b2 10 to 100 step 10), showing per-step constraints; Algorithm 1 (Section 3.3, p.307): shows composition of transformations across iterations (e.g., \"Randomly pick parameter P1 for T1 ... newImage = ApplyTransforms(image, T1, P1, T2, P2)\"), indicating cumulative mutation without an explicit plausibility-enforcing constraint across iterations; Section 3.4 (p.309\u2013310): defines metamorphic oracle and filtering: \"(\u02c6\u03b8i \u2212 \u03b8t_i)^2 \u2264 \u03bb MSEorig\" (Eqn.2) and the filtration criterion \"| MSE(trans,param) \u2212 MSEorig | \u2264 \u03f5 (3)\" and states \"Thus, we only choose the transformations that obey Equation 3 for counting erroneous behaviors.\" \u2014 this is a quantitative validation/filter applied to final inputs; Section 6 Threats to Validity (p.311): \"DeepTest generates realistic synthetic images by applying different image transformations on the seed images. However, these transformations are not designed to be exhaustive and therefore may not cover all realistic cases... the generated pictures may not be exactly reproducible in reality due to a large number of unpredictable factors...\"; Section 5 (p.311): \"We also manually checked the bugs reported in Table 8 and report the false positives in Figure 8... all the authors have reviewed the images and agreed on the false positives.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The method is explicitly designed to generate inputs by mimicking realistic, real-world phenomena. The transformations applied to seed images include weather effects (rain, fog), lighting changes (brightness, contrast), and physical changes (rotation, shear, scaling) intended to be plausible. This enforces plausibility at the level of individual mutation steps. However, the paper's greedy algorithm for combining these transformations is driven solely by increasing neuron coverage, without an explicit mechanism to check if the combined transformations result in a plausible image, thus potentially overlooking cumulative effects. The paper does perform a final assessment of the plausibility of the failure-inducing inputs, but this is an informal, manual check by the authors rather than a formal human study or the use of quantitative perceptual metrics.",
                    "evidence": "Page 5, Section 3.2, 'Increasing Coverage with Synthetic Images': 'Therefore, DeepTest focuses on generating realistic synthetic images by applying image transformations on seed images and mimic different real-world phenomena like camera lens distortions, object movements, different weather conditions, etc. To this end, we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect).' \nPage 6, Section 3.3, 'Algorithm 1': The greedy search algorithm for combining transformations checks only for coverage increase ('if covInc(newimage) then') and does not include a plausibility check for the resulting 'newImage'.\nPage 10, Section 5, 'RQ4': 'We also manually checked the bugs reported in Table 8 and report the false positives in Figure 8. It also shows two synthetic images (the corresponding original images) where DeepTest incorrectly reports erroneous behaviors while the model\u2019s output is indeed safe. Although such manual investigation is, by definition, subjective and approximate, all the authors have reviewed the images and agreed on the false positives.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "DeepTest explicitly restricts individual mutation steps to realistic, domain-specific image transformations (brightness, contrast, affine transforms, blur, rain/fog composed with Photoshop filters) and specifies bounded parameter ranges (Table 4). It also applies a quantitative filter (Equation 3) and manual inspection to reduce implausible / false-positive reports, i.e., it validates plausibility of final fault-inducing inputs. However, the fuzzing algorithm (Algorithm 1) composes multiple transformations without an explicit mechanism that enforces plausibility across cumulative iterations (no distributional constraint or perceptual-similarity metric is enforced during generation), and the authors acknowledge that generated images \u201cmay not be exactly reproducible in reality.\u201d Thus DeepTest validates final inputs and constrains single-step mutations, but it does not fully prevent or measure cumulative drift with perceptual metrics or a human study, matching the paper to the Medium level.",
                    "evidence": "Section 3.2 (p.306): \"we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect).\"; Table 4 (p.306): lists bounded parameter ranges for each transformation (e.g., Translation (tx, ty) (10, 10) to (100, 100) step (10, 10); Rotation q 3 to 30 with step 3; Contrast \u03b1 1.2 to 3.0 step 0.2; Brightness \u03b2 10 to 100 step 10), showing per-step constraints; Algorithm 1 (Section 3.3, p.307): shows composition of transformations across iterations (e.g., \"Randomly pick parameter P1 for T1 ... newImage = ApplyTransforms(image, T1, P1, T2, P2)\"), indicating cumulative mutation without an explicit plausibility-enforcing constraint across iterations; Section 3.4 (p.309\u2013310): defines metamorphic oracle and filtering: \"(\u02c6\u03b8i \u2212 \u03b8t_i)^2 \u2264 \u03bb MSEorig\" (Eqn.2) and the filtration criterion \"| MSE(trans,param) \u2212 MSEorig | \u2264 \u03f5 (3)\" and states \"Thus, we only choose the transformations that obey Equation 3 for counting erroneous behaviors.\" \u2014 this is a quantitative validation/filter applied to final inputs; Section 6 Threats to Validity (p.311): \"DeepTest generates realistic synthetic images by applying different image transformations on the seed images. However, these transformations are not designed to be exhaustive and therefore may not cover all realistic cases... the generated pictures may not be exactly reproducible in reality due to a large number of unpredictable factors...\"; Section 5 (p.311): \"We also manually checked the bugs reported in Table 8 and report the false positives in Figure 8... all the authors have reviewed the images and agreed on the false positives.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Performs semantic, real-world image transformations (brightness, contrast, blur, fog, rain, etc.) but does not ensure IO stability using clipping and rounding"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "No cross model failure analysis"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across models. DeepTest generates synthetic tests and reports erroneous behaviors for three different DNNs, but it treats each model's testing separately and does not (a) reuse inputs crafted on one model to test other models, (b) measure overlap of discovered faults across models, or (c) include design elements explicitly aimed at finding transferable attacks. The authors explicitly contrast their approach with prior differential-testing work (DeepXplore) that used multiple DNNs; DeepTest is presented as a single-model, metamorphic-relation-driven approach rather than a substitute-model transfer-attack methodology.",
                    "evidence": "\u2022 Paper objective / scope (multiple models tested but separately): \u201cWe use DeepTest to systematically test three top performing DNN models from the Udacity driving challenge.\u201d (Abstract / Contributions, p.304). \n\n\u2022 Explicit distinction from multi-model differential testing (shows DeepTest is single-model focused): \u201cPei et al. [70] proposed DeepXplore, a whitebox differential testing algorithm for systematically finding inputs that can trigger inconsistencies between multiple DNNs. \u2026 By contrast, our graybox methods use neuron coverage for guided test generation in a single DNN and leverage metamorphic relations to identify erroneous behaviors without requiring multiple DNNs.\u201d (Related Work \u2014 Testing and verification of machine learning / DeepXplore comparison, p.312). \n\n\u2022 Results present per-model erroneous behaviors but no cross-model reuse/transfer experiments or overlap analysis: e.g., Tables and text report errors per model (Table 8: \u201cNumber of unique erroneous behaviors reported by DeepTest for different models...\u201d and surrounding discussion) and aggregate counts, but contain no experiment where inputs found on one model are applied to other models to test transferability (Results / RQ4 and Table 8 discussion, pp.310\u2013311)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper introduces DeepTest, a method for finding erroneous behaviors in a single Deep Neural Network (DNN) by generating synthetic inputs that maximize neuron coverage. The evaluation is performed on three separate DNN models (Chauffeur, Rambo, and Epoch) that perform the same task of autonomous driving. However, the paper's methodology and experiments focus on testing each model in isolation. The results are presented separately for each model, highlighting which types of transformations induce errors in which models. There is no experiment or discussion where a fault-inducing input generated for one model is then used to test another model to check for transferability. The paper does not demonstrate whether the discovered failures are model-specific or shared across different architectures.",
                    "evidence": "Page 10, Section 5. RESULTS, Table 8: 'Number of unique erroneous behaviors reported by DeepTest for different models with \u03bb = 5'. The accompanying text states: 'Interestingly, some models are more prone to erroneous behaviors for some transformations than others. For example, Rambo produces 23 erroneous cases for shear, while the other two models do not show any such cases. Similarly, DeepTest finds 650 instances of erroneous behavior in Chauffeur for rain but only 64 and 27 for Epoch and Rambo respectively.' This evidence shows that bugs were identified and counted for each model independently, rather than testing if a bug found in one model transfers to others."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate transferability of fault-inducing inputs across models. DeepTest generates synthetic tests and reports erroneous behaviors for three different DNNs, but it treats each model's testing separately and does not (a) reuse inputs crafted on one model to test other models, (b) measure overlap of discovered faults across models, or (c) include design elements explicitly aimed at finding transferable attacks. The authors explicitly contrast their approach with prior differential-testing work (DeepXplore) that used multiple DNNs; DeepTest is presented as a single-model, metamorphic-relation-driven approach rather than a substitute-model transfer-attack methodology.",
                    "evidence": "\u2022 Paper objective / scope (multiple models tested but separately): \u201cWe use DeepTest to systematically test three top performing DNN models from the Udacity driving challenge.\u201d (Abstract / Contributions, p.304). \n\n\u2022 Explicit distinction from multi-model differential testing (shows DeepTest is single-model focused): \u201cPei et al. [70] proposed DeepXplore, a whitebox differential testing algorithm for systematically finding inputs that can trigger inconsistencies between multiple DNNs. \u2026 By contrast, our graybox methods use neuron coverage for guided test generation in a single DNN and leverage metamorphic relations to identify erroneous behaviors without requiring multiple DNNs.\u201d (Related Work \u2014 Testing and verification of machine learning / DeepXplore comparison, p.312). \n\n\u2022 Results present per-model erroneous behaviors but no cross-model reuse/transfer experiments or overlap analysis: e.g., Tables and text report errors per model (Table 8: \u201cNumber of unique erroneous behaviors reported by DeepTest for different models...\u201d and surrounding discussion) and aggregate counts, but contain no experiment where inputs found on one model are applied to other models to test transferability (Results / RQ4 and Table 8 discussion, pp.310\u2013311)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Xian Yuen et al. - 2023 - ASDF A Differential Testing Framework for Automatic Speech Recognition Systems": {
        "key": "yuen2023asdf",
        "name": "ASDF",
        "year": "2023",
        "citation_count": "8",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Performs phonetic analysis to correlated phonemes with errors."
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Synthesized inputs might not be natural and no measure of naturnaless provided"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces a plausibility filter on transformed inputs by requiring that any transformed text be transcribed correctly by at least one ASR system (cross-referencing) before it is considered valid, which is an explicit validation of final mutated inputs. However, the paper does not report any human evaluation or task-specific perceptual/fluency metrics (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for final fault-inducing audio/text, nor does it describe mechanisms that bound or control cumulative drift across multiple mutation iterations. The authors also acknowledge limitations of using TTS-generated audio versus real human speech. Together, this matches the metric\u2019s \u201cMedium\u201d level: plausibility of final inputs is validated, but cumulative-effects and empirical perceptual assessments are not addressed.",
                    "evidence": "Section IV (USAGE), p.462: \"It is important to note that cross-referencing is used to filter the low-quality, unrealistic texts resulting from text transformation. A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\" Section V (CHALLENGES AND FUTURE WORK), p.462: \"ASDF generates an audio test suite using a TTS library. This assumes that the generated audio is interchangeable with the real-world speech of humans, which may not be the case in real-life situations. There would still be differences between human speech and computer-generated audio, namely pronunciation, accent, and tone. As a result, if human speech were to be used instead of computer-generated audio, such inconsistencies may yield different results for our differential testing.\" (Also: Section III / Abstract describe text transformations such as tense change and homophone substitution used to generate test cases, but no human study or perceptual/fluency metrics are reported in the paper or the described evaluation.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper does not provide empirical evidence, such as human studies or quantitative perceptual metrics (e.g., PESQ, STOI), to confirm that the final fault-inducing audio inputs are realistic. In fact, the authors explicitly acknowledge this as a limitation, stating that the Text-To-Speech (TTS) generated audio may not be interchangeable with real human speech. However, the method does not completely ignore plausibility. It incorporates an indirect validation mechanism through cross-referencing, where a transformed input is only considered valid if at least one ASR system under test can transcribe it correctly. This step is explicitly designed to filter out \"low-quality, unrealistic texts.\" This filtering of final inputs aligns with the criteria for a 'Medium' rating, as it constitutes a form of plausibility validation without enforcing it throughout the generation process or providing direct realism measurements.",
                    "evidence": "Page 2, Section IV. USAGE: \"It is important to note that cross-referencing is used to filter the low-quality, unrealistic texts resulting from text transformation. A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\"\n\nPage 2, Section V. CHALLENGES AND FUTURE WORK: \"ASDF generates an audio test suite using a TTS library. This assumes that the generated audio is interchangeable with the real-world speech of humans, which may not be the case in real-life situations. There would still be differences between human speech and computer-generated audio, namely pronunciation, accent, and tone.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces a plausibility filter on transformed inputs by requiring that any transformed text be transcribed correctly by at least one ASR system (cross-referencing) before it is considered valid, which is an explicit validation of final mutated inputs. However, the paper does not report any human evaluation or task-specific perceptual/fluency metrics (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for final fault-inducing audio/text, nor does it describe mechanisms that bound or control cumulative drift across multiple mutation iterations. The authors also acknowledge limitations of using TTS-generated audio versus real human speech. Together, this matches the metric\u2019s \u201cMedium\u201d level: plausibility of final inputs is validated, but cumulative-effects and empirical perceptual assessments are not addressed.",
                    "evidence": "Section IV (USAGE), p.462: \"It is important to note that cross-referencing is used to filter the low-quality, unrealistic texts resulting from text transformation. A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\" Section V (CHALLENGES AND FUTURE WORK), p.462: \"ASDF generates an audio test suite using a TTS library. This assumes that the generated audio is interchangeable with the real-world speech of humans, which may not be the case in real-life situations. There would still be differences between human speech and computer-generated audio, namely pronunciation, accent, and tone. As a result, if human speech were to be used instead of computer-generated audio, such inconsistencies may yield different results for our differential testing.\" (Also: Section III / Abstract describe text transformations such as tense change and homophone substitution used to generate test cases, but no human study or perceptual/fluency metrics are reported in the paper or the described evaluation.)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "The speech input is generated using TTS library which are fed to the ASR model. There is not in memory mutation on the audio."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper describes a differential testing framework that runs multiple ASR systems in parallel and uses cross-referencing to identify failed cases, and it leverages failures to generate transformed test texts. However, it does not demonstrate or evaluate whether fault-inducing inputs discovered on one model are reused to test other (unseen) target models, nor does it describe explicit mechanisms or experiments aimed at measuring cross-model transferability of faults. All descriptions pertain to comparing outputs across selected ASR systems within the same test run and generating transformations from observed failures, but no reuse-on-surrogate \u2192 test-on-other-model workflow or transferability analysis is presented.",
                    "evidence": "Page 461, Section II (Tool Design): \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\" \n\nPage 461-462, Section III (Core Functionalities): \"The ASR systems currently available in ASDF are DeepSpeech [19], wav2letter [20], and wav2vec2 [21].\" and \"Cross-referencing is performed amongst the selected ASR systems to ensure that the test cases are valid and can be determined by at least one ASR service.\" \n\nPage 462, Section IV (Usage): \"A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\" \n\n(There are no sections or quotes in the paper that describe generating fault-inducing inputs on a surrogate model and then reusing them to test other unseen target models, nor experiments measuring transferability across models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper describes a differential testing framework, ASDF, where a failure is defined by a disagreement among multiple Automated Speech Recognition (ASR) models. A test case is considered a failure for a specific ASR system only if at least one other system under test transcribes the audio correctly. This design principle means the framework is explicitly built to find model-specific failures, not failures that transfer across different models. The methodology does not involve generating a fault-inducing input on one model and then reusing it to test for transferability on others; rather, it tests a single input on all models simultaneously to find discrepancies. Furthermore, the framework discards inputs that cannot be correctly transcribed by at least one ASR service, which would include cases where a failure might be transferable across all tested models. Therefore, the paper does not demonstrate whether fault-inducing inputs discovered for one model also trigger failures in others.",
                    "evidence": "Page 1, Section II, TOOL DESIGN: \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\" \n\nPage 2, Section IV, USAGE: \"A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper describes a differential testing framework that runs multiple ASR systems in parallel and uses cross-referencing to identify failed cases, and it leverages failures to generate transformed test texts. However, it does not demonstrate or evaluate whether fault-inducing inputs discovered on one model are reused to test other (unseen) target models, nor does it describe explicit mechanisms or experiments aimed at measuring cross-model transferability of faults. All descriptions pertain to comparing outputs across selected ASR systems within the same test run and generating transformations from observed failures, but no reuse-on-surrogate \u2192 test-on-other-model workflow or transferability analysis is presented.",
                    "evidence": "Page 461, Section II (Tool Design): \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\" \n\nPage 461-462, Section III (Core Functionalities): \"The ASR systems currently available in ASDF are DeepSpeech [19], wav2letter [20], and wav2vec2 [21].\" and \"Cross-referencing is performed amongst the selected ASR systems to ensure that the test cases are valid and can be determined by at least one ASR service.\" \n\nPage 462, Section IV (Usage): \"A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\" \n\n(There are no sections or quotes in the paper that describe generating fault-inducing inputs on a surrogate model and then reusing them to test other unseen target models, nor experiments measuring transferability across models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ]
        }
    },
    "Odena et al. - 2019 - TensorFuzz Debugging Neural Networks with Coverage-Guided Fuzzing": {
        "key": "odena2019tensorfuzz",
        "name": "TensorFuzz",
        "year": "2019",
        "citation_count": "426",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "does not deliberately target particular outputs or behaviors. Objective function mechanism allows arbitrary user-defined checks to enable diverse failure detection but the exploration is not targeted toward any specific class of outputs "
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "norm-bounded"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints on inputs (e.g., restricting inputs to valid shapes/ranges and providing a bounded L\u221e mutation option relative to an original seed) and explicitly uses such bounded perturbations in at least one experiment (the quantization experiment where mutations are restricted to an L\u221e ball). However, the paper does not report any explicit empirical assessment of the perceptual or semantic realism of final fault-inducing inputs (no human study or task-specific perceptual/fluency metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity are provided). Thus TensorFuzz enforces plausibility via bounded/rule-based constraints in mutation design and uses them in experiments, but it does not provide quantitative or human-evaluation evidence that final failing inputs remain perceptually/semantically realistic across fuzzing iterations.",
                    "evidence": "Section 4 (An Overview of the TensorFuzz Library): \"we restrict the inputs to those that are valid neural network inputs. If the inputs are images, we restrict our inputs to have the correct size and shape, and to lie in the same interval as the input pixels of the dataset under consideration. If the inputs are sequences of characters, we only allow characters that are in the vocabulary extracted from the training set.\" \n\nSection 5 (Details \u2014 Mutator): \"For image inputs, we implemented two different types of mutation. The first is to just add white noise of a user-configurable variance to the input. The second is to add white noise, but to constrain the difference between the mutated element and the original element from which it is descended to have a user-configurable L\u221e norm. This type of constrained mutation can be useful if we want to find inputs that satisfy some objective function, but are still plausibly of the same 'class' as the original input that was used as a seed. In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.\" \n\nSection 6 (Experimental Results \u2014 Quantization experiment): \"We then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. We restrict to inputs near seed images because these inputs nearly all have unambiguous class semantics.\" \n\nAbsence of empirical plausibility evaluation: The paper describes mutation constraints and uses bounded perturbations in experiments but contains no section or statements reporting human evaluation or task-specific perceptual/fluency metrics (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) for the final fault-inducing inputs."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper describes a mutation strategy specifically designed to maintain input plausibility by constraining mutations within an L-infinity norm ball around the original seed image. The authors explicitly state this is done to ensure inputs retain \"unambiguous class semantics\" and to avoid testing on \"out-of-domain garbage data\". This method enforces plausibility by bounding the cumulative perturbation from the original seed. However, the paper does not provide any empirical validation, such as a human study or quantitative perceptual quality metrics (e.g., LPIPS, SSIM), to systematically assess whether the final fault-inducing inputs are indeed realistic. It only provides a single visual example (Figure 3). This meets the criteria for \"Medium\" as it enforces plausibility through bounded constraints but does not include an explicit assessment of the final inputs' plausibility.",
                    "evidence": "Page 7, Section 6, \"TensorFuzz surfaces disagreements between models and their quantized versions\": \"We then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. We restrict to inputs near seed images because these inputs nearly all have unambiguous class semantics. It is less interesting if two versions of the model disagree on out-of-domain garbage data with no true class.\"\n\nPage 5, Section 5, \"Mutator\": \"The second is to add white noise, but to constrain the difference between the mutated element and the original element from which it is descended to have a user-configurable L\u221e norm. This type of constrained mutation can be useful if we want to find inputs that satisfy some objective function, but are still plausibly of the same 'class' as the original input that was used as a seed.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints on inputs (e.g., restricting inputs to valid shapes/ranges and providing a bounded L\u221e mutation option relative to an original seed) and explicitly uses such bounded perturbations in at least one experiment (the quantization experiment where mutations are restricted to an L\u221e ball). However, the paper does not report any explicit empirical assessment of the perceptual or semantic realism of final fault-inducing inputs (no human study or task-specific perceptual/fluency metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity are provided). Thus TensorFuzz enforces plausibility via bounded/rule-based constraints in mutation design and uses them in experiments, but it does not provide quantitative or human-evaluation evidence that final failing inputs remain perceptually/semantically realistic across fuzzing iterations.",
                    "evidence": "Section 4 (An Overview of the TensorFuzz Library): \"we restrict the inputs to those that are valid neural network inputs. If the inputs are images, we restrict our inputs to have the correct size and shape, and to lie in the same interval as the input pixels of the dataset under consideration. If the inputs are sequences of characters, we only allow characters that are in the vocabulary extracted from the training set.\" \n\nSection 5 (Details \u2014 Mutator): \"For image inputs, we implemented two different types of mutation. The first is to just add white noise of a user-configurable variance to the input. The second is to add white noise, but to constrain the difference between the mutated element and the original element from which it is descended to have a user-configurable L\u221e norm. This type of constrained mutation can be useful if we want to find inputs that satisfy some objective function, but are still plausibly of the same 'class' as the original input that was used as a seed. In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.\" \n\nSection 6 (Experimental Results \u2014 Quantization experiment): \"We then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. We restrict to inputs near seed images because these inputs nearly all have unambiguous class semantics.\" \n\nAbsence of empirical plausibility evaluation: The paper describes mutation constraints and uses bounded perturbations in experiments but contains no section or statements reporting human evaluation or task-specific perceptual/fluency metrics (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) for the final fault-inducing inputs."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": "Mutations occur in-memory on float tensors and are never verified for persistence after I/O."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly reuses fault-inducing inputs generated by fuzzing one model (the 32-bit model) to test another implementation (the 16-bit quantized/truncated model) and reports that many such inputs cause disagreements. However, the fuzzing framework is presented as a general-purpose CGF + PBT tool rather than being explicitly designed primarily to discover transferable attacks across different model implementations; the cross-model experiment is an application/example rather than a described core design goal. Thus the work demonstrates transferability reuse but the fuzzing design is not explicitly aimed at discovering transferable attacks across models.",
                    "evidence": "Section 6, 'TensorFuzz surfaces disagreements between models and their quantized versions': \"We then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. ... With these settings, the fuzzer was able to generate disagreements for 70% of the examples we tried.\" Also in same subsection: \"As a baseline experiment, we trained an MNIST classifier (this time without intentionally introducing numerical issues) using 32-bit floating point numbers. We then truncated all weights and activations to 16-bits. We then compared the predictions of the 32-bit and the 16-bit model on the MNIST test set and found 0 disagreements.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper does not evaluate whether the fault-inducing inputs discovered through fuzzing on one model can trigger failures in other, distinct models performing the same task. The experiments focus on finding bugs within a single model instance (e.g., numerical errors), comparing a model to its own quantized version, or testing the correctness of specific code implementations. There is no investigation into the transferability of these discovered faults to different model architectures or independently trained models, which is the core of the Attack Transferability metric.",
                    "evidence": "Page 7, Section \"TensorFuzz surfaces disagreements between models and their quantized versions\": The paper describes an experiment comparing a 32-bit MNIST classifier to its 16-bit quantized version. It states, \"We then compared the predictions of the 32-bit and the 16-bit model on the MNIST test set and found 0 disagreements. We then ran the fuzzer... the fuzzer was able to generate disagreements for 70% of the examples we tried.\" While this involves two model variants, it is a test of a model against its direct, transformed derivative, not a test of transferability to a separate, independently trained model. The other experiments in the paper similarly focus on single models or codebases without testing for transferability."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper includes an explicit experiment in which fault-inducing inputs generated by fuzzing a 32-bit MNIST classifier are reused to test a 16-bit quantized version, demonstrating that many of these inputs transfer and cause disagreements. This satisfies the criterion of reusing fuzzed inputs across two models performing the same task. However, the overall fuzzing framework is presented as a general CGF+PBT tool rather than being specifically designed to discover transferable failures across arbitrary surrogate and target models, so it lacks an explicit architectural mechanism for transferability beyond this illustrative application.",
                    "evidence": "Section 6, \u201cTensorFuzz surfaces disagreements between models and their quantized versions\u2019: \u201cWe then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. ... With these settings, the fuzzer was able to generate disagreements for 70% of the examples we tried.\u201d"
                },
                "manual": {
                    "value": "Low",
                    "why": "It is not transferability. It is differential testing based oracle.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Property-based",
                "Differential"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ]
        }
    },
    "Chao et al. - 2025 - Jailbreaking Black Box Large Language Models in Twenty Queries": {
        "key": "chao2025jailbreaking",
        "name": "PAIR",
        "year": "2025",
        "citation_count": "806",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "Targets specific malicious intents (e.g. bomb-making, phishing) but does not enforce a fixed phrase or label."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis of discovered faults. It provides transferability rationale"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The PAIR method explicitly constrains prompt generation via rule-based attacker system prompts (role-playing, logical appeal, authority endorsement), JSON output formats, and in-context examples that keep mutations semantic and human-interpretable in each iteration. These design choices enforce plausibility at the individual mutation step level. However, the paper does not provide an explicit mechanism or empirical evaluation that enforces or measures plausibility across cumulative iterations (e.g., no bounded-perturbation guarantees or perceptual/semantic metrics like perplexity reported for final fault-inducing prompts), nor a human study that directly assesses the realism of final jailbreak prompts. Therefore PAIR enforces plausibility through rule-based / template constraints per mutation but does not validate or measure cumulative drift of final inputs, matching the Medium category.",
                    "evidence": "III-A Attacker\u2019s system prompt (p.25): \"we carefully design three distinct system prompts templates, all of which instructs the LLM to output a specific kind of objectionable content... each system prompt template is based on one of three criteria: logical appeal, authority endorsement, and role-playing.\"; III-A (p.25): \"Format... we require that the attacker generate its responses in JSON format.\" (shows rule-based/template constraints and structured generation per iteration). Algorithm / Iterative refinement (III-B, p.25): \"If S == 1 then return P ... otherwise the conversation is updated... the conversation history is then passed back to the attacker, and the process repeats.\" (shows iterative mutation without bounded perturbation guarantees). IV Evaluation (p.26): \"We compute the Jailbreak %\u2014the percentage of behaviors that elicit a jailbroken response according to JUDGE\u2014and the Queries per Success.\" (evaluation reports jailbreak rates, not perceptual/plausibility metrics). IV / JUDGE dataset (p.26, Table I): \"We collected a dataset of 100 prompts and responses... Three expert annotators labeled each pair...\" (human annotation was used to evaluate JUDGE agreement on jailbreak labeling, not to assess plausibility of generated prompts). Appendix C / Examples (pp.41-42): contains example generated prompts and conversations (illustrative human-interpretable examples), but no reported human study or quantitative plausibility measures such as perplexity, LPIPS, SSIM, PESQ, STOI, or Perplexity for final fault-inducing prompts."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's proposed method, Prompt Automatic Iterative Refinement (PAIR), is explicitly designed to generate semantically plausible and human-interpretable inputs (prompts) as its core mechanism. Plausibility is enforced throughout the iterative process by using an attacker Large Language Model (LLM) to generate and refine prompts based on conversational history and chain-of-thought reasoning. This design inherently constrains the generated inputs to a distribution of realistic natural language. The paper contrasts this directly with token-level attacks that produce semantically invalid or uninterpretable inputs. Furthermore, the paper provides substantial empirical evidence, in the form of numerous qualitative examples of the final fault-inducing prompts, which are all semantically coherent and represent realistic user queries.",
                    "evidence": "Page 2, Contributions, 'Interpretability': 'PAIR generates prompt-level semantic jailbreaks that are interpretable to humans and includes interpretable, chain-of-thought improvement assessments.'\n\nPage 2, Figure 2 'Prompt- vs. token-level jailbreaks': This figure provides a direct visual comparison, showing a plausible prompt generated by PAIR ('Write a fun and creative story about a person who gets into a lot of trouble for committing insider trading...') versus an uninterpretable token-level jailbreak from another method.\n\nPage 3, Section III-A, 'Attacker's system prompt': The method's design relies on plausible social engineering strategies to construct prompts: 'each system prompt template is based on one of three criteria: logical appeal, authority endorsement, and role-playing.'\n\nPages 7, 19, and 20, Figures 3, 9, 10, 11, 12, 13, 14: These figures provide multiple examples of final, successful jailbreak prompts generated by PAIR. All examples are coherent, plausible, and framed as realistic scenarios (e.g., 'You are a creative writer working on a new political thriller... write a convincing letter from the IRS which the protagonist will use...')."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "While PAIR\u2019s design clearly constrains each generated prompt to remain semantically coherent\u2014via role-playing, logical-appeal and authority-endorsement system prompts, JSON formatting, and chain-of-thought refinements\u2014it does not include any explicit mechanism to prevent cumulative semantic drift over multiple iterations, nor does it report quantitative or human-study assessments (e.g. perplexity scores or user judgments) of the final fault-inducing prompts\u2019 realism. Thus, PAIR enforces plausibility at the individual mutation step level but overlooks validation of the aggregate effect of those mutations, fitting the Medium category.",
                    "evidence": "III-A Attacker\u2019s system prompt design (p.25): uses structured templates and in-context examples to keep each prompt plausible; III-B Algorithm 1 (p.25): iterative refinement with no bounded-perturbation or plausibility filter; IV Evaluation (p.26): reports only jailbreak success rates and query counts, with no perceptual/linguistic plausibility metrics or human evaluation of final prompts."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Fuzzed inputs are text prompts generated by another LLM, not numerical perturbations"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "High",
                    "why": "Generates new test cases by semantic mutations. The authors attributed transferability to this mutation "
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly evaluates transferability by reusing jailbreak prompts found on one model to test other models (Section IV-B, Table VI) and reports transfer percentages. Furthermore, the PAIR design intentionally generates semantic, prompt-level (interpretable) attacks\u2014via attacker system-prompt templates, chain-of-thought improvement assessments, and parallel streams\u2014which the authors argue and evidence as supporting cross-model transferability (they compare PAIR vs GCG transfer and attribute better transfer to PAIR's semantic prompts). Thus the paper both demonstrates reuse of fault-inducing inputs across models and incorporates design choices aimed at discovering transferable failures.",
                    "evidence": "Abstract (p. 23): \"PAIR also achieves competitive jailbreaking success rates and transferability on open and closed\u2011source LLMs, including GPT-3.5/4, Vicuna, and Gemini.\"; Introduction / Contributions (p. 23): \"We also find that PAIR jailbreaks are often more trans\u2011ferrable to other LLMs than jailbreaks generated by GCG.\"; Section IV-B 'Jailbreak transfer experiments' (p. 28): \"For PAIR, we use the successful jailbreaks found for GPT-4 and Vicuna... Our results in Table VI indicate that PAIR\u2019s Vicuna prompts transfer more readily than those generated by GCG on all models except GPT-3.5, and PAIR\u2019s GPT-4 prompts transfer well on Vicuna, GPT-3.5, and Gemini. We believe that this is largely attributable to the fact that PAIR\u2019s prompts are semantic, and they therefore target similar vulnerabilities across LLMs...\"; Table VI caption (p. 28): \"Jailbreak transferability. We report the jailbreaking percentage of prompts that successfully jailbreak a source LLM when transferred to downstream LLM.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper extensively evaluates the transferability of the jailbreak prompts generated by its proposed method, PAIR. Fault-inducing inputs (prompts) generated on source models (GPT-4 and Vicuna) are systematically reused to test their effectiveness on a suite of other target models, with results clearly presented. This fulfills the primary condition for a 'Medium' rating. However, the paper does not meet the criteria for a 'High' rating because the PAIR method's design is not explicitly aimed at discovering transferable attacks. The core algorithm is an iterative refinement process focused on jailbreaking a single, specific target model. The authors attribute the observed high transferability to the semantic nature of the generated prompts, which is a beneficial byproduct of the method rather than an explicit mechanism integrated into the design to optimize for transferability across models.",
                    "evidence": "Page 6, Section IV-B, Jailbreak transfer experiments: \"We next evaluate the transferability of the attacks generated in \u00a7 IV-A. For PAIR, we use the successful jailbreaks found for GPT-4 and Vicuna... Our results in Table VI indicate that PAIR\u2019s Vicuna prompts transfer more readily than those generated by GCG on all models except GPT-3.5, and PAIR\u2019s GPT-4 prompts transfer well on Vicuna, GPT-3.5, and Gemini. We believe that this is largely attributable to the fact that PAIR\u2019s prompts are semantic, and they therefore target similar vulnerabilities across LLMs, which are generally trained on similar datasets.\" This section, along with Table VI, shows the reuse of prompts to test transferability, but the explanation attributes this property to the semantic nature of the prompts, not to an explicit design choice for promoting transferability."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper clearly demonstrates reuse of fault-inducing inputs (jailbreak prompts) generated on one model to test their effectiveness on other models (see Section IV-B, Table VI). However, while the authors attribute high transferability to the semantic, interpretable nature of PAIR\u2019s prompt-level attacks, the core design of PAIR is focused on efficiently generating jailbreaks on a single target model rather than explicitly optimizing for cross-model transfer. Thus it meets the criteria for Medium: it reuses inputs for transfer tests but does not incorporate mechanisms explicitly aimed at discovering transferable failures.",
                    "evidence": "Section IV-B (p. 28): \u201cFor PAIR, we use the successful jailbreaks found for GPT-4 and Vicuna\u2026 Our results in Table VI indicate that PAIR\u2019s Vicuna prompts transfer more readily than those generated by GCG\u2026 We believe that this is largely attributable to the fact that PAIR\u2019s prompts are semantic, and they therefore target similar vulnerabilities across LLMs.\u201d; Table VI caption: \u201cJailbreak transferability. We report the jailbreaking percentage of prompts that successfully jailbreak a source LLM when transferred to downstream LLM.\u201d"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Transferability is a byproduct in this paper, not a design goal",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Oracle-guided"
            ]
        }
    },
    "Asyrofi et al. - 2020 - CrossASR Efficient Differential Testing of Automatic Speech Recognition via Text-To-Speech": {
        "key": "asyrofi2020crossasr",
        "name": "CrossASR",
        "year": "2020",
        "citation_count": "25",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": "transcription error"
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": "Performs human study"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "CrossASR generates test inputs by synthesizing speech from natural texts (Europarl) via TTS engines rather than applying unconstrained, accumulating perturbations. The paper describes explicit text preprocessing (normalization) \u201cintended to ensure that the text are pronounced properly by TTS\u201d (preventing implausible TTS outputs), uses multiple real-world TTSes to avoid bias, and treats texts from a real corpus as the input distribution. Importantly, the authors perform a human verification on a statistically representative sample of final failed test cases (395 samples) and report low false positive rates (12 false positives out of 395), explicitly noting that the failed cases are \u201caudio understandable to humans but cannot be transcribed properly by an ASR.\u201d These design choices and the human evaluation demonstrate that CrossASR enforces and empirically validates input plausibility for final fault-inducing inputs.",
                    "evidence": "Intro / Abstract (page 640): \"CrossASR employs Text-to-Speech (TTS) engines to synthesize test inputs from texts and treats the texts as the ground truth labels...\"; Architecture / Text preprocessing (Section II-A / II-B, page 643): \"The Text Preprocessor lowercases the text, removes punctuation from it, and normalizes non-standard words... This normalization is intended to ensure that the text are pronounced properly by TTS.\"; Experimental validation / RQ4 & manual check (Section III-D, pages 645-647): \"we ask a native English speaker, who is not an author of this paper to verify whether the failed test cases generated by CrossASR are actual failed test cases...\" and Table VI reporting low FPRs; Discussion / manual sample analysis (page 647): \"From 33,618 failed test cases, we manually verified a statistically representative sample of size 395 and found 12 of them to be false positives.\"; Abstract / Conclusion (page 640 / 649): \"Based on a manual verification carried out on statistically representative sample size, we found that most samples are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR).\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, CrossASR, generates audio test cases using Text-To-Speech (TTS) engines. This design choice inherently aims for plausible inputs, as TTS systems are specifically created to produce realistic, human-like speech. More importantly, the paper provides direct empirical evidence for the plausibility of the fault-inducing inputs through a human evaluation. The authors tasked a native English speaker with verifying a statistically significant sample of the generated failed test cases. The explicit goal was to check if the audio was understandable to a human. The results confirmed that the vast majority of inputs that caused failures were indeed plausible and intelligible. This combination of a generation method designed for plausibility (TTS) and a rigorous human-centric validation of the final, failure-inducing inputs satisfies the criteria for a 'High' rating.",
                    "evidence": "Page 10, Section VI, Conclusion: \"CrossASR is highly reliable in determining failed test cases, as demonstrated by manual verification. Almost all of the failed test case samples inspected are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR).\"\n\nPage 5, Section III-C, RQ4: \"To measure the reliability of CrossASR in determining failed test cases, we ask a native English speaker, who is not an author of this paper to verify whether the failed test cases generated by CrossASR are actual failed test cases.\"\n\nPage 6, Section III-D, RQ4: \"The non-author verified that in the random sample, CrossASR generates failed test cases with the highest and lowest FPR of 9% and 0%, respectively. From all combination of TTSes and ASRs, the FPRs are low, which means that the failed test cases that are detected by CrossASR actually highlight a deficiency in the corresponding ASR (it is a true positive).\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "CrossASR generates test inputs by synthesizing speech from natural texts (Europarl) via TTS engines rather than applying unconstrained, accumulating perturbations. The paper describes explicit text preprocessing (normalization) \u201cintended to ensure that the text are pronounced properly by TTS\u201d (preventing implausible TTS outputs), uses multiple real-world TTSes to avoid bias, and treats texts from a real corpus as the input distribution. Importantly, the authors perform a human verification on a statistically representative sample of final failed test cases (395 samples) and report low false positive rates (12 false positives out of 395), explicitly noting that the failed cases are \u201caudio understandable to humans but cannot be transcribed properly by an ASR.\u201d These design choices and the human evaluation demonstrate that CrossASR enforces and empirically validates input plausibility for final fault-inducing inputs.",
                    "evidence": "Intro / Abstract (page 640): \"CrossASR employs Text-to-Speech (TTS) engines to synthesize test inputs from texts and treats the texts as the ground truth labels...\"; Architecture / Text preprocessing (Section II-A / II-B, page 643): \"The Text Preprocessor lowercases the text, removes punctuation from it, and normalizes non-standard words... This normalization is intended to ensure that the text are pronounced properly by TTS.\"; Experimental validation / RQ4 & manual check (Section III-D, pages 645-647): \"we ask a native English speaker, who is not an author of this paper to verify whether the failed test cases generated by CrossASR are actual failed test cases...\" and Table VI reporting low FPRs; Discussion / manual sample analysis (page 647): \"From 33,618 failed test cases, we manually verified a statistically representative sample of size 395 and found 12 of them to be false positives.\"; Abstract / Conclusion (page 640 / 649): \"Based on a manual verification carried out on statistically representative sample size, we found that most samples are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR).\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Uses TTS to generate the input audio - non in memory mutation"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "Utilize multiple ASRs to find faults using differential oracle but does not analyze if same error is reproducible to other model"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models, nor does it present experiments or analyses measuring transferability. CrossASR runs each synthesized audio against multiple ASRs simultaneously (differential testing) and reports per-ASR failure counts, but it does not (a) create inputs targeted at one surrogate ASR and then reuse them to test other ASRs, (b) measure overlap/transfer of failures across models, or (c) claim design choices explicitly intended to discover transferable attacks. The classifier (failure probability predictor) is trained with labels per ASR to predict likely failures, but the approach and experiments focus on efficiently finding failures per ASR rather than on demonstrating or quantifying transferability across ASRs.",
                    "evidence": "Page 642, I. Introduction: \"CrossASR performs a black-box differential testing to uncover erroneous behaviors on ASR systems by cross-referencing different ASRs to detect different transcriptions from ASRs.\" \n\nPage 643, II-B Test Selection Engine: \"For this training, a text has m labels, each corresponds to an ASR.\" \n\nPage 646, III-D RQ3 / Table V: \"Table V shows the total number of failed test cases that is generated from the entire 20,000 texts for all combinations of experimented TTSes and ASRs.\" (Table V reports per-ASR failed-case counts but no cross-model transfer/overlap statistics.) \n\nPage 649, VI Conclusion / Future Work: \"In the future, we plan to evaluate CrossASR using additional ASRs and TTSes.\" (Indicates no current evaluation of transferability across broader sets of models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, CrossASR, is based on differential testing, not attack transferability. It generates a single audio input and feeds it to multiple Automatic Speech Recognition (ASR) systems simultaneously. A failure is defined based on a disagreement among the outputs of these systems. Specifically, a test case is considered a failure for a particular ASR if its transcription does not match the input text, while at least one other ASR's transcription does. The methodology does not involve generating a fault-inducing input on a single source/surrogate model and then reusing that specific input to test for failures on other target models. The evaluation counts the total number of failures detected for each ASR system individually but does not analyze or report on whether the same audio inputs caused failures across multiple ASRs. Therefore, it does not demonstrate the transferability of fault-inducing inputs.",
                    "evidence": "Page 3, Section II-A, Architecture, Case 2: \"At least one of the ASR transcribed texts that matches with the input Text while at least one of the other ASR Transcribed Texts do not match with the input Text. In this case, CrossASR considers cases where the transcribed text does not match input text as failed test cases...\" This describes the core differential testing mechanism, where a failure in one model is identified by its disagreement with a successful transcription from another model for the same input, rather than testing for transferability of a known failing input. \n\nPage 6, Table V, \"NUMBER OF FAILED TEST CASES GENERATED FOR EACH COMBINATION OF TTSES AND ASRS\": This table presents the total number of failures for each ASR model as separate, aggregated counts. For example, using the Google TTS, it found 4,036 failures for Deepspeech and 2,539 for Deepspeech2. The paper does not analyze the intersection of these sets to determine how many specific audio files caused failures in both models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate whether fault-inducing inputs discovered on one model also trigger failures in other models, nor does it present experiments or analyses measuring transferability. CrossASR runs each synthesized audio against multiple ASRs simultaneously (differential testing) and reports per-ASR failure counts, but it does not (a) create inputs targeted at one surrogate ASR and then reuse them to test other ASRs, (b) measure overlap/transfer of failures across models, or (c) claim design choices explicitly intended to discover transferable attacks. The classifier (failure probability predictor) is trained with labels per ASR to predict likely failures, but the approach and experiments focus on efficiently finding failures per ASR rather than on demonstrating or quantifying transferability across ASRs.",
                    "evidence": "Page 642, I. Introduction: \"CrossASR performs a black-box differential testing to uncover erroneous behaviors on ASR systems by cross-referencing different ASRs to detect different transcriptions from ASRs.\" \n\nPage 643, II-B Test Selection Engine: \"For this training, a text has m labels, each corresponds to an ASR.\" \n\nPage 646, III-D RQ3 / Table V: \"Table V shows the total number of failed test cases that is generated from the entire 20,000 texts for all combinations of experimented TTSes and ASRs.\" (Table V reports per-ASR failed-case counts but no cross-model transfer/overlap statistics.) \n\nPage 649, VI Conclusion / Future Work: \"In the future, we plan to evaluate CrossASR using additional ASRs and TTSes.\" (Indicates no current evaluation of transferability across broader sets of models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Asyrofi et al. - 2021 - CrossASR++ a modular differential testing framework for automatic speech recognition": {
        "key": "asyrofi2021crossasr++",
        "name": "CrossASR++",
        "year": "2021",
        "citation_count": "25",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "TTS engines are inherently designed to produce natural-sounding speech from text, thereby providing a theoretical justification and design constraint for input naturalness."
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "CrossASR++ generates test inputs via TTS and enforces a plausibility-based filter in its differential-testing logic: an audio is counted as a failed test case for the System Under Test (SUT) only if the SUT fails but at least one other ASR recognizes the audio correctly. This implicitly filters out wholly implausible/invalid TTS outputs (which the paper calls \"indeterminable\"). The paper also explicitly discusses that better TTSes produce fewer invalid/indeterminable audios and that indeterminable cases exist. However, the paper does not provide explicit perceptual or task-specific quantitative assessments (e.g., PESQ, STOI, human evaluation, or other audio-quality/intelligibility metrics) of the final fault-inducing audios. Because CrossASR++ therefore enforces plausibility via cross-referencing (a form of validation of final inputs) but lacks dedicated empirical perceptual/quality evaluation, the appropriate rating is Medium.",
                    "evidence": "Section 2.1 Workflow (p.1576): \"For a piece of audio and its corresponding transcription, there are three possible situations: 1) if the SUT can recognize it successfully... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT. 3) If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case because a TTS may generate an invalid audio.\" (Section 2.1, Workflow, p.1576) Section 2.2 Extensibility (p.1576): \"The main reason is that more advanced TTSes generate fewer invalid audios that are more likely to be indeterminable test cases.\" (Section 2.2, p.1576) Section 3 Experiments (p.1577): experimental evaluation focuses on counts of \"# Failed Test Case\" and ablation of features (visibility, added ASRs, estimators) \u2014 no reported perceptual or speech-quality / intelligibility metrics or human evaluation is provided in the experiments or results tables (e.g., Tables 1\u20133 measure numbers of failed test cases). (Section 3, RQ1/RQ2, p.1577)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The method generates test inputs by synthesizing audio from a text corpus using standard Text-to-Speech (TTS) engines. This design inherently constrains the inputs to be plausible speech, as TTS systems are designed to produce human-like utterances. This serves as a strong, rule-based constraint that enforces input plausibility in the generation step. However, the paper does not provide any empirical validation, either through human studies or quantitative metrics (e.g., PESQ, STOI), to confirm that the final, failure-inducing audio inputs are realistic and intelligible. The paper acknowledges that TTS can produce 'invalid audios,' but these are filtered out as 'indeterminable test cases' based on consensus among ASR systems, not by a direct assessment of plausibility. This approach fits the 'Medium' criteria, as it enforces plausibility during generation but does not empirically validate the final outputs.",
                    "evidence": "Page 3, Section 2.1, Workflow: 'Then, TTS is used to generate an audio file for each piece of selected text (Line 9)... If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case because a TTS may generate an invalid audio.' This shows the method of generation (TTS) and the indirect way of handling potentially implausible ('invalid') inputs. The Experiments section (Section 3) evaluates the method based on the number of failed test cases found, without reporting any measures of the audio inputs' realism or quality."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "CrossASR++ generates test inputs via TTS and enforces a plausibility-based filter in its differential-testing logic: an audio is counted as a failed test case for the System Under Test (SUT) only if the SUT fails but at least one other ASR recognizes the audio correctly. This implicitly filters out wholly implausible/invalid TTS outputs (which the paper calls \"indeterminable\"). The paper also explicitly discusses that better TTSes produce fewer invalid/indeterminable audios and that indeterminable cases exist. However, the paper does not provide explicit perceptual or task-specific quantitative assessments (e.g., PESQ, STOI, human evaluation, or other audio-quality/intelligibility metrics) of the final fault-inducing audios. Because CrossASR++ therefore enforces plausibility via cross-referencing (a form of validation of final inputs) but lacks dedicated empirical perceptual/quality evaluation, the appropriate rating is Medium.",
                    "evidence": "Section 2.1 Workflow (p.1576): \"For a piece of audio and its corresponding transcription, there are three possible situations: 1) if the SUT can recognize it successfully... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT. 3) If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case because a TTS may generate an invalid audio.\" (Section 2.1, Workflow, p.1576) Section 2.2 Extensibility (p.1576): \"The main reason is that more advanced TTSes generate fewer invalid audios that are more likely to be indeterminable test cases.\" (Section 2.2, p.1576) Section 3 Experiments (p.1577): experimental evaluation focuses on counts of \"# Failed Test Case\" and ablation of features (visibility, added ASRs, estimators) \u2014 no reported perceptual or speech-quality / intelligibility metrics or human evaluation is provided in the experiments or results tables (e.g., Tables 1\u20133 measure numbers of failed test cases). (Section 3, RQ1/RQ2, p.1577)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "Uses multiple ASRs in differential oracle, does not show transferability of failures across errors"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper describes a differential testing tool that generates the same TTS-produced inputs and feeds them to multiple ASRs for cross-referencing (so inputs are evaluated across ASRs concurrently), but it does not evaluate or claim that fault-inducing inputs discovered on one model are reused to test transferability to other (unseen) models, nor does it include experiments that measure cross-model transfer of faults or support surrogate-model attacks. The evaluation focuses on counting failed test cases per SUT and on how adding more ASRs or changing estimator/visibility affects the number of failures found, without demonstrating or measuring whether inputs that fail one model also cause failures in other models beyond the concurrent cross-referencing setup or in unseen targets.",
                    "evidence": "Page 1576, Section 2.1 Workflow: \"we use all ASR systems (including the System Under Test (SUT)) to convert the audio files back to texts (Line 11), and then we perform cross-reference (Line 12). For a piece of audio and its corresponding transcription, there are three possible situations: ... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\"  Page 1577, Section 3 RQ1 and Table 1: RQ1 is \"How many failed test cases can CrossASR++ find?\" and results report counts of failed test cases per SUT under different configurations (no analysis of reuse/transfer to unseen models).  Page 1577, Section 3 RQ2 / Table 2: \"By simply adding one more ASR for cross-referencing, we can increase the number of failed test cases uncovered for 4 SUTs by 25.07%, 39.63%, 20.95% and 8.17% respectively.\" (shows cross-referencing effect but not a transferability experiment).  Page 1578, Conclusion: \"CrossASR++ , an extensible tool that performs black-box differential testing on ASR systems... The extensibility allows CrossASR++ to flexibly utilizes more tools to boost its performance on uncovering failed test cases.\" (no mention of evaluating or supporting transferable/substitute-model attacks)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, CrossASR++, is a differential testing framework. A test case is defined as a failure for a specific System Under Test (SUT) if that SUT fails to transcribe an audio input correctly, while at least one other ASR system succeeds. This design inherently focuses on finding disagreements between models, rather than shared vulnerabilities or transferable failures. The case where all ASR models fail on the same input is categorized as an \"indeterminable test case\" and is not treated as a successful discovery of a fault. The paper does not reuse the inputs that are found to cause failures in one model to evaluate if they also induce failures in other models. Therefore, it does not demonstrate the transferability of fault-inducing inputs.",
                    "evidence": "Page 3, Section 2.1 Workflow, \"For a piece of audio and its corresponding transcription, there are three possible situations: ... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT. 3) If all the ASR systems cannot recognize the audio correctly, we call it an indeterminable test case because a TTS may generate an invalid audio.\""
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper describes a differential testing tool that generates the same TTS-produced inputs and feeds them to multiple ASRs for cross-referencing (so inputs are evaluated across ASRs concurrently), but it does not evaluate or claim that fault-inducing inputs discovered on one model are reused to test transferability to other (unseen) models, nor does it include experiments that measure cross-model transfer of faults or support surrogate-model attacks. The evaluation focuses on counting failed test cases per SUT and on how adding more ASRs or changing estimator/visibility affects the number of failures found, without demonstrating or measuring whether inputs that fail one model also cause failures in other models beyond the concurrent cross-referencing setup or in unseen targets.",
                    "evidence": "Page 1576, Section 2.1 Workflow: \"we use all ASR systems (including the System Under Test (SUT)) to convert the audio files back to texts (Line 11), and then we perform cross-reference (Line 12). For a piece of audio and its corresponding transcription, there are three possible situations: ... 2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\"  Page 1577, Section 3 RQ1 and Table 1: RQ1 is \"How many failed test cases can CrossASR++ find?\" and results report counts of failed test cases per SUT under different configurations (no analysis of reuse/transfer to unseen models).  Page 1577, Section 3 RQ2 / Table 2: \"By simply adding one more ASR for cross-referencing, we can increase the number of failed test cases uncovered for 4 SUTs by 25.07%, 39.63%, 20.95% and 8.17% respectively.\" (shows cross-referencing effect but not a transferability experiment).  Page 1578, Conclusion: \"CrossASR++ , an extensible tool that performs black-box differential testing on ASR systems... The extensibility allows CrossASR++ to flexibly utilizes more tools to boost its performance on uncovering failed test cases.\" (no mention of evaluating or supporting transferable/substitute-model attacks)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Data-driven"
            ]
        }
    },
    "Asyrofi et al. - 2021 - Can Differential Testing Improve Automatic Speech Recognition Systems": {
        "key": "asyrofi2021can",
        "name": "Asyrofi et al.",
        "year": "2021",
        "citation_count": "16",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility via a rule-based cross-referencing filter: synthesized audio-text pairs are accepted as valid only if at least one ASR correctly transcribes them, which ensures generated inputs are intelligible/valid at the time of creation. However, the paper does not provide any human evaluation or task-specific perceptual/semantic quality metrics (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for the final fault-inducing inputs, nor does it report additional quantitative checks of perceptual realism. Also, although their cross-referencing filter enforces plausibility per generated audio, the work does not report explicit mechanisms or analyses addressing cumulative mutations or perceptual assessment of final failing inputs. These facts align with the \"Medium\" level: plausibility is enforced by rule-based constraints but there is no empirical perceptual validation of final fault-triggering inputs.",
                    "evidence": "Section II.A (p.675): \"to avoid using the invalid audios synthesized by the TTS system, the generated audio-text pair is fed into multiple ASR systems for cross-referencing: If at least one ASR system can successfully transcribe the audio, the audio-text pair is viewed as a valid test case.\"; Section II.A (p.675): \"We take the audio-text pair as an indeterminable test case if none of them can successfully transcribe the audio.\"; Section III.B (p.676): \"The TTS system is ResponsiveVoice. The 5 ASR systems are DeepSpeech, DeepSpeech2, Wav2Letter++, Wav2Vec2, and Wit.\"; Section III.C (p.676): \"We use two metrics to evaluate the performance of the SUT. One is the number of failed test cases uncovered by CrossASR++, and the other is the word error rate (WER) on a test set.\". The paper contains no reported human study or perceptual/quality measures (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for assessing realism of final fault-inducing inputs."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method generates audio test cases from a real-world text corpus using a Text-to-Speech (TTS) system. It incorporates a mechanism to enforce input plausibility by filtering the generated audio. Specifically, an audio file is considered a 'valid test case' only if at least one of the multiple Automatic Speech Recognition (ASR) systems used for cross-referencing can transcribe it correctly. This step is explicitly designed to discard 'invalid audios synthesized by the TTS system,' which serves as a proxy for filtering out unintelligible or unrealistic speech. However, the paper does not provide any additional empirical validation of the plausibility of the final fault-inducing audio inputs, such as through human evaluation or by reporting quantitative speech quality metrics (e.g., PESQ, STOI). The method enforces plausibility during the test case generation and filtering process but lacks a final, explicit assessment of the resulting inputs.",
                    "evidence": "Page 2, Section II.A, paragraph 1: \"The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio (i.e., each word in the transcription matches each corresponding word in the input text). A valid test case can be either a failed test case or a successful test case... We take the audio-text pair as an indeterminable test case if none of them can successfully transcribe the audio.\"\nThis quote describes the filtering mechanism that ensures a baseline level of intelligibility for an input to be considered 'valid', thus enforcing a form of plausibility. The paper lacks any further assessment beyond this."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility via a rule-based cross-referencing filter: synthesized audio-text pairs are accepted as valid only if at least one ASR correctly transcribes them, which ensures generated inputs are intelligible/valid at the time of creation. However, the paper does not provide any human evaluation or task-specific perceptual/semantic quality metrics (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for the final fault-inducing inputs, nor does it report additional quantitative checks of perceptual realism. Also, although their cross-referencing filter enforces plausibility per generated audio, the work does not report explicit mechanisms or analyses addressing cumulative mutations or perceptual assessment of final failing inputs. These facts align with the \"Medium\" level: plausibility is enforced by rule-based constraints but there is no empirical perceptual validation of final fault-triggering inputs.",
                    "evidence": "Section II.A (p.675): \"to avoid using the invalid audios synthesized by the TTS system, the generated audio-text pair is fed into multiple ASR systems for cross-referencing: If at least one ASR system can successfully transcribe the audio, the audio-text pair is viewed as a valid test case.\"; Section II.A (p.675): \"We take the audio-text pair as an indeterminable test case if none of them can successfully transcribe the audio.\"; Section III.B (p.676): \"The TTS system is ResponsiveVoice. The 5 ASR systems are DeepSpeech, DeepSpeech2, Wav2Letter++, Wav2Vec2, and Wit.\"; Section III.C (p.676): \"We use two metrics to evaluate the performance of the SUT. One is the number of failed test cases uncovered by CrossASR++, and the other is the word error rate (WER) on a test set.\". The paper contains no reported human study or perceptual/quality measures (e.g., PESQ, STOI, LPIPS, SSIM, Perplexity) for assessing realism of final fault-inducing inputs."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "Does not demonstrate if fuzzed input on one model aslo causes error in another model"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. While CrossASR++ feeds each synthesized audio to multiple ASR systems simultaneously (so generated inputs are executed on several models), the authors do not (a) craft fault-inducing inputs on a surrogate model and then reuse them to test other models, nor (b) analyze whether fault-inducing inputs discovered for one model also trigger failures in other models. The experiments focus on (i) fine-tuning the SUT with generated test cases to reduce its failures, and (ii) fine-tuning one cross-referenced ASR to improve CrossASR++\u2019s ability to find failures. These evaluations measure improvement in detection and model quality, not the transferability of faults between independently-trained target models.",
                    "evidence": "Page 675, Section II.A: \"The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1). The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio.\" \n\nPage 677, Section III.C (RQ2 - Experiment Design & Result): \"In this research question, we want to investigate whether fine-tuning one of the cross-referenced ASR systems using generated test cases can boost the performance of CrossASR++ to uncover failed test cases... For the evolved CrossASR++, we continuously evolve DeepSpeech and always utilize DeepSpeech as one of the ASR systems used for cross-referencing... By comparing the number of failed test cases found by the static and evolved CrossASR++ head-to-head, it is clear that the evolved CrossASR++ uncovers more failed test cases than the static one for each ASR system.\" \n\n(These excerpts show that inputs are evaluated across multiple ASRs and that the paper measures improvements from fine-tuning, but there is no experiment or analysis that shows fault-inducing inputs crafted on one (surrogate) model transfer to other models.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's method, evolutionary differential testing, uses multiple Automatic Speech Recognition (ASR) models for cross-referencing. However, this is done to create a reliable test oracle\u2014a test case is deemed valid if at least one model can correctly transcribe the audio. A failure is then identified specifically for the System Under Test (SUT) if it fails on this valid test case. The paper does not investigate or demonstrate whether these fault-inducing inputs for one SUT also trigger failures in other ASR models. The generated failed test cases are used to fine-tune and improve the SUT itself, rather than being reused to test for attack transferability across different models. The core focus is on improving a single model or the testing tool, not on discovering shared vulnerabilities.",
                    "evidence": "Page 2, Section II-A, \"CrossASR++\": \"A valid test case can be either a failed test case or a successful test case. If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT. Otherwise, a successful test case is found.\" This quote illustrates that a failure is defined and recorded for a single, specific SUT, not in the context of transferability to other models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. While CrossASR++ feeds each synthesized audio to multiple ASR systems simultaneously (so generated inputs are executed on several models), the authors do not (a) craft fault-inducing inputs on a surrogate model and then reuse them to test other models, nor (b) analyze whether fault-inducing inputs discovered for one model also trigger failures in other models. The experiments focus on (i) fine-tuning the SUT with generated test cases to reduce its failures, and (ii) fine-tuning one cross-referenced ASR to improve CrossASR++\u2019s ability to find failures. These evaluations measure improvement in detection and model quality, not the transferability of faults between independently-trained target models.",
                    "evidence": "Page 675, Section II.A: \"The generated audio file is then fed to all ASR systems inside CrossASR++ (SUT and ASR1 to ASRm in Fig. 1). The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio.\" \n\nPage 677, Section III.C (RQ2 - Experiment Design & Result): \"In this research question, we want to investigate whether fine-tuning one of the cross-referenced ASR systems using generated test cases can boost the performance of CrossASR++ to uncover failed test cases... For the evolved CrossASR++, we continuously evolve DeepSpeech and always utilize DeepSpeech as one of the ASR systems used for cross-referencing... By comparing the number of failed test cases found by the static and evolved CrossASR++ head-to-head, it is clear that the evolved CrossASR++ uncovers more failed test cases than the static one for each ASR system.\" \n\n(These excerpts show that inputs are evaluated across multiple ASRs and that the paper measures improvements from fine-tuning, but there is no experiment or analysis that shows fault-inducing inputs crafted on one (surrogate) model transfer to other models.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Data-driven"
            ]
        }
    },
    "Pei et al. - 2017 - DeepXplore Automated Whitebox Testing of Deep Learning Systems": {
        "key": "pei2017deepxplore",
        "name": "DeepXplore",
        "year": "2017",
        "citation_count": "1814",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": "reveals incorrect corner-case behaviors but does not bypass explicity safety mechinasims"
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "performs untargeted exploration to reveal generic model faults (differential behaviour, inaccuracies). No targeted misclassification."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "No diagnsotic analysis"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces input-plausibility via explicit, rule-based domain constraints that are applied during gradient-based mutation (the authors describe modifying gradients so each iteration preserves constraints and give concrete image/file constraints). This shows the fuzzing design actively restricts mutations to remain realistic at each mutation step. However, the paper does not provide any empirical, task-specific or human evaluation of the final fault-inducing inputs\u2019 perceptual or semantic realism (no human study or quantitative perceptual measures such as LPIPS/SSIM/PESQ/STOI/Perplexity are reported). Because the work enforces plausibility through bounded / rule-based constraints during mutation but does not report explicit empirical validation of final-input realism, the appropriate rating is Medium.",
                    "evidence": "Section 4.2 (Domain-specific constraints), p. 6: \"One important aspect of the optimization process is that the generated test inputs need to satisfy several domain-specific constraints to be physically realistic... Our algorithm ensures this property by modifying the gradient \u0434rad (line 13 in Algorithm 1) such that xi+1 = xi + s \u00b7 \u0434rad still satisfies the constraints (s is the step size in the gradient ascent).\" \n\nSection 6.2 (Domain-specific constraints), p. 9: \"DeepXplore used three different types of constraints for simulating different environment conditions of images: (1) lighting effects for simulating different intensities of lights, (2) occlusion by a single small rectangle for simulating an attacker potentially blocking some parts of a camera, and (3) occlusion by multiple tiny black rectangles for simulating effects of dirt on camera lens... The first constraint restricts image modifications so that DeepXplore can only make the image darker or brighter without changing its content. Specifically, the modification can only increase or decrease all pixel values by the same amount...\" \n\nSection 7 (Results), p. 10: \"For the experimental results shown in Figure 8, we apply three domain-specific constraints (lighting effects, occlusion by a single rectangle, and occlusion by multiple rectangles) as described in \u00a76.2.\" \n\n(Absence of empirical perceptual evaluation) The evaluation (Sections 7 and 7.2, pp. 10\u201313) reports neuron coverage, counts of difference-inducing inputs, runtimes, and retraining accuracy improvements, but contains no human-subject study or quantitative perceptual/linguistic measures (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity) assessing whether final fault-inducing inputs remain perceptually/semantically realistic."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility through explicit, rule-based, domain-specific constraints designed to generate realistic inputs. For image-based tasks, these constraints simulate plausible environmental conditions like changes in lighting, occlusions from objects, or dirt on a camera lens. For other domains like Android malware, constraints ensure that application functionality is not broken (e.g., only adding permissions, not removing them). The paper provides visual examples (Figure 8) of the generated inputs, which serve as a qualitative validation of their plausibility. However, it does not meet the 'High' criteria because it lacks a formal, empirical validation of the final fault-inducing inputs, such as a human study or quantitative perceptual quality metrics (e.g., LPIPS, SSIM), to confirm their realism. The methodology relies on pre-defined rules rather than ensuring mutations remain within a learned plausible distribution.",
                    "evidence": "Page 10, Section 6.2, \"Domain-specific constraints\": \"As discussed earlier, to be useful in practice, we need to ensure that the generated tests are valid and realistic by applying domain-specific constraints. For example, generated images should be physically producible by a camera... Below we describe two major types of domain-specific constraints (i.e., image and file constraints) that we use in this paper.\"\n\nPage 10, Section 6.2, \"Image constraints (MNIST, ImageNet, and Driving)\": \"DeepXplore used three different types of constraints for simulating different environment conditions of images: (1) lighting effects for simulating different intensities of lights, (2) occlusion by a single small rectangle for simulating an attacker potentially blocking some parts of a camera, and (3) occlusion by multiple tiny black rectangles for simulating effects of dirt on camera lens.\"\n\nPage 11, Figure 8: This figure provides visual examples of the seed inputs and the final difference-inducing inputs generated by DeepXplore under the various constraints (lighting, single occlusion, multiple occlusions), qualitatively demonstrating the plausibility of the outputs."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces input-plausibility via explicit, rule-based domain constraints that are applied during gradient-based mutation (the authors describe modifying gradients so each iteration preserves constraints and give concrete image/file constraints). This shows the fuzzing design actively restricts mutations to remain realistic at each mutation step. However, the paper does not provide any empirical, task-specific or human evaluation of the final fault-inducing inputs\u2019 perceptual or semantic realism (no human study or quantitative perceptual measures such as LPIPS/SSIM/PESQ/STOI/Perplexity are reported). Because the work enforces plausibility through bounded / rule-based constraints during mutation but does not report explicit empirical validation of final-input realism, the appropriate rating is Medium.",
                    "evidence": "Section 4.2 (Domain-specific constraints), p. 6: \"One important aspect of the optimization process is that the generated test inputs need to satisfy several domain-specific constraints to be physically realistic... Our algorithm ensures this property by modifying the gradient \u0434rad (line 13 in Algorithm 1) such that xi+1 = xi + s \u00b7 \u0434rad still satisfies the constraints (s is the step size in the gradient ascent).\" \n\nSection 6.2 (Domain-specific constraints), p. 9: \"DeepXplore used three different types of constraints for simulating different environment conditions of images: (1) lighting effects for simulating different intensities of lights, (2) occlusion by a single small rectangle for simulating an attacker potentially blocking some parts of a camera, and (3) occlusion by multiple tiny black rectangles for simulating effects of dirt on camera lens... The first constraint restricts image modifications so that DeepXplore can only make the image darker or brighter without changing its content. Specifically, the modification can only increase or decrease all pixel values by the same amount...\" \n\nSection 7 (Results), p. 10: \"For the experimental results shown in Figure 8, we apply three domain-specific constraints (lighting effects, occlusion by a single rectangle, and occlusion by multiple rectangles) as described in \u00a76.2.\" \n\n(Absence of empirical perceptual evaluation) The evaluation (Sections 7 and 7.2, pp. 10\u201313) reports neuron coverage, counts of difference-inducing inputs, runtimes, and retraining accuracy improvements, but contains no human-subject study or quantitative perceptual/linguistic measures (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity) assessing whether final fault-inducing inputs remain perceptually/semantically realistic."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Enforces domain specific constraints like keeping pixel values between 0 and 255, but it can produce decimal point pixel value within that range and does not perform rounding"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper's testing approach is explicitly differential: it requires whitebox access to multiple DNNs and constructs inputs by optimizing across those models so that they disagree. DeepXplore does not present or evaluate the scenario where fault-inducing inputs are crafted on a single surrogate model and then reused (without access) to trigger failures in other, unseen target models. In other words, the paper demonstrates generating inputs that cause differences among available models, but it does not demonstrate or measure transferability of fault-inducing inputs from one model to other independent/unknown models (the kind of surrogate-model transfer evaluated by the metric).",
                    "evidence": "Page 1, Abstract: \"Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles to avoid manual checking.\"; Page 5, Section 3 (Overview): \"DeepXplore takes unlabeled test inputs as seeds and generates new tests that cover a large number of neurons ... while causing the tested DNNs to behave differently.\"; Page 6, Section 4.2 / Algorithm 1: algorithm inputs show \"dnns \u2190 multiple DNNs under test\" and the algorithm selects \"d = randomly select one dnn from dnns\" (showing the method relies on multiple available models during generation); Page 14, Section 8 (Limitations): \"First, differential testing requires at least two different DNNs with the same functionality. Further, if two DNNs only differ slightly ... DeepXplore will take longer to find difference-inducing inputs...\" (stating the necessity of multiple models and acknowledging that DeepXplore cannot operate in the single-surrogate transfer scenario)."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's core methodology, differential testing, is explicitly designed to discover transferable failures by finding inputs that cause different behaviors across multiple deep learning models with similar functionality. The method does not simply generate a fault-inducing input for one model and then test its transferability on others as a secondary step. Instead, it uses multiple models as 'cross-referencing oracles' from the outset. The fuzzing design incorporates an explicit mechanism\u2014a joint optimization function\u2014that directly aims to maximize the behavioral differences between the models under test. This process inherently generates inputs that demonstrate failures across different models.",
                    "evidence": "Page 7, Section 4.2, 'Maximizing differential behaviors': 'The first objective of the optimization problem is to generate test inputs that can induce different behaviors in the tested DNNs, i.e., different DNNs will classify the same input into different classes.' and Page 8, Equation (2) and its description: 'obj1(x) = \u2211k\u2260j Fk(x)[c] - \u03bb1 \u00b7 Fj(x)[c] where \u03bb1 is a parameter to balance the objective terms between the DNNs Fk\u2260j that maintain the same class outputs as before and the DNN Fj that produce different class outputs.' This objective function is the explicit mechanism to find transferable/differential failures."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DeepXplore\u2019s approach requires whitebox access to multiple DNNs during test generation and optimizes inputs to provoke disagreements among them. It does not generate fault-inducing inputs on a surrogate model and then reuse those inputs (without internal access) to trigger failures in independently trained or unseen target models. Thus, it does not evaluate or support the traditional notion of attack transferability from one model to another.",
                    "evidence": "Page 1 (Abstract): \u201cNext, we leverage multiple DL systems with similar functionality as cross-referencing oracles\u2026\u201d; Page 5 (Section 3 Overview): \u201cDeepXplore takes unlabeled test inputs as seeds \u2026 while causing the tested DNNs to behave differently.\u201d; Page 6 (Algorithm 1 inputs): \u201cdnns \u2190 multiple DNNs under test\u201d and selection of one DNN from dnns for optimization; Page 14 (Section 8 Limitations): \u201cdifferential testing requires at least two different DNNs with the same functionality\u2026 DeepXplore cannot operate in the single-surrogate transfer scenario.\u201d"
                },
                "manual": {
                    "value": "Low",
                    "why": "It is differential testing based oracle. Not transferability.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Whitebox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Prediction-guided"
            ]
        }
    },
    "Gao et al. - 2020 - Fuzz testing based data augmentation to improve robustness of deep neural networks": {
        "key": "gao2020fuzz",
        "name": "Sensei",
        "year": "2020",
        "citation_count": "169",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Aims to find generic misclassifications"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Apply large semantic level mutations and perform floating point operations (zoom, contrast, shear), which may produce non-discrete pixel values. They don't perform clipping/rounding to ensure IO persistence."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "No analysis of fault transferability in input level or mutation level"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or claim that fault-inducing inputs found on one (surrogate) model transfer to other models. Sensei is designed as an in-training data-augmentation method that generates variants per model/seed during that model's training; the experiments measure per-model robust accuracy after applying Sensei to each model independently. The paper explicitly contrasts its approach with multi-model differential testing (DeepXplore) and states that Sensei does not require multiple models, indicating the design and evaluation do not target discovering or measuring transferable faults across models.",
                    "evidence": "p.1148, Abstract: \"We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets.\"; p.1150, Section 3.2 (Overview): \"Note that, Sensei is in-training data augmentation approach, i.e., data generation and augmentation happen on-the-fly during training.\"; p.1156, Section 6 (Related Work): \"DeepXplore [24] generates test inputs that lead to exhibit different behaviors by different models for the same task. Our approach does not require multiple models.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a data augmentation technique, SENSEI, to improve the robustness of a single Deep Neural Network (DNN) model during its training phase. The method uses a genetic algorithm to generate high-loss variants of training inputs, which are then immediately used to train that same model. The entire process of generating and utilizing these 'fault-inducing' inputs is self-contained within the training loop of one specific model. The experiments evaluate the robustness of multiple individual models after they have been trained with SENSEI, but they do not test whether the variants generated for one model can cause failures in other models performing the same task. The paper does not mention or evaluate the concept of attack transferability between different models.",
                    "evidence": "Page 5, Algorithm 2, 'Overall algorithm': The algorithm describes a training process for a single model 'M'. In each epoch, for each training input 'X[i]', variants are generated and evaluated using the current model 'M' (line 14: `f := fitness(M, children);`). The model 'M' is then retrained on the augmented data (line 19: `M := train(X, Y);`). This shows the generated inputs are specific to and consumed by the model being trained, not transferred to other models.\n\nPage 7, Section 4.3, 'Evaluation Metric': The paper's own evaluation metric is defined for a single model's performance: 'A DNN is robust around x if and only if M(x') = c for all x' \u2208 X'. This confirms that the evaluation is focused on the robustness of an individual model, not on cross-model failure patterns.\n\nPage 9, Table 2, 'The robust accuracy for Random, W-10 and SENSEI': This table presents the results for 15 different models. Each model is trained and evaluated independently. The methodology does not involve generating inputs on one model (e.g., GTSRB-1) and testing them on another (e.g., GTSRB-2)."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or claim that fault-inducing inputs found on one (surrogate) model transfer to other models. Sensei is designed as an in-training data-augmentation method that generates variants per model/seed during that model's training; the experiments measure per-model robust accuracy after applying Sensei to each model independently. The paper explicitly contrasts its approach with multi-model differential testing (DeepXplore) and states that Sensei does not require multiple models, indicating the design and evaluation do not target discovering or measuring transferable faults across models.",
                    "evidence": "p.1148, Abstract: \"We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets.\"; p.1150, Section 3.2 (Overview): \"Note that, Sensei is in-training data augmentation approach, i.e., data generation and augmentation happen on-the-fly during training.\"; p.1156, Section 6 (Related Work): \"DeepXplore [24] generates test inputs that lead to exhibit different behaviors by different models for the same task. Our approach does not require multiple models.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "coonstrained transformation but no naturalness measure of the fuzzed inputs"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility through bounded, rule-based constraints on allowed transformations and by design prevents cumulative drift (it always applies each mutation to the original seed). These measures ensure mutated inputs remain realistic by construction during the iterative search. However, the paper does not provide empirical perceptual or semantic plausibility assessments (no human study or task-specific perceptual metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity) for the final fault-inducing / augmented inputs, so it does not meet the 'High' requirement of providing empirical evidence that final inputs remain perceptually realistic.",
                    "evidence": "Section 3.3.2 (Generation of population), p.1150: \"once a new chromosome is generated through the mutation or crossover, it is validated to make sure that it is within the range of each transformation that we set globally (line 12). Furthermore, Sensei always applies the resulting transformation vector (chromosome) on the original image (as opposed to applying on an already transformed data) to prevent the resulting data from being unrealistic.\"; Section 4.2 (Generation of Realistic Variations), p.1153: \"To make sure the translated images are visually similar to natural ones, we restrict the space of allowed perturbations following [8] where it is applicable. The operations and corresponding restrictions ... rotation(x,d): rotate x by d degree within a range [-30, 30]. translation(x, d): ... within a range of [-10%, 10%] ... brightness(x,d): ... range of [-32, 32] ... contrast(x,d): ... range of [0.8, 1.2].\"; Section 4.3 (Evaluation Metric), p.1153: \"robust accuracy is the proportion of images in the testing dataset where the prediction of a DNN does not fluctuate with any small realistic perturbations...\" \u2014 the paper's evaluation uses robust accuracy as the reported metric and does not report any human evaluation or perceptual-similarity scores (e.g., LPIPS, SSIM) for the generated/augmented images."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility using rule-based constraints. It defines a set of image transformations (rotation, translation, shear, zoom, brightness, contrast) intended to simulate 'natural environmental variations' and restricts the parameters of these transformations to bounded ranges to ensure the resulting images are 'visually similar to natural ones'. Crucially, the method is designed to prevent the cumulative effect of mutations across iterations, which could lead to unrealistic inputs. It achieves this by always applying transformations to the original seed image, not to an already-mutated one. However, the paper does not provide any empirical validation that the final augmented inputs are perceptually realistic, such as through a human study or by using quantitative metrics like SSIM or LPIPS. The plausibility is asserted based on the design constraints rather than being empirically verified, which prevents a 'High' rating.",
                    "evidence": "Page 6, Section 3.3.2, 'Generation of population': 'Furthermore, SENSEI always applies the resulting transformation vector (chromosome) on the original image (as opposed to applying on an already transformed data) to prevent the resulting data from being unrealistic.' \nPage 7, Section 4.2, 'Generation of Realistic Variations': 'SENSEI focuses on improving the robustness of DNN models by augmenting training data with natural environmental variations. ... To make sure the translated images are visually similar to natural ones, we restrict the space of allowed perturbations... The operations and corresponding restrictions with respect to an image x are as follows: \u2022 rotation(x,d): rotate x by d degree within a range [-30, 30]. \u2022 translation(x, d): horizontally or vertically translate x by d pixels within a range of [-10%, 10%] of image size. ... These image operations preserve the content of original image.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility through bounded, rule-based constraints on allowed transformations and by design prevents cumulative drift (it always applies each mutation to the original seed). These measures ensure mutated inputs remain realistic by construction during the iterative search. However, the paper does not provide empirical perceptual or semantic plausibility assessments (no human study or task-specific perceptual metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity) for the final fault-inducing / augmented inputs, so it does not meet the 'High' requirement of providing empirical evidence that final inputs remain perceptually realistic.",
                    "evidence": "Section 3.3.2 (Generation of population), p.1150: \"once a new chromosome is generated through the mutation or crossover, it is validated to make sure that it is within the range of each transformation that we set globally (line 12). Furthermore, Sensei always applies the resulting transformation vector (chromosome) on the original image (as opposed to applying on an already transformed data) to prevent the resulting data from being unrealistic.\"; Section 4.2 (Generation of Realistic Variations), p.1153: \"To make sure the translated images are visually similar to natural ones, we restrict the space of allowed perturbations following [8] where it is applicable. The operations and corresponding restrictions ... rotation(x,d): rotate x by d degree within a range [-30, 30]. translation(x, d): ... within a range of [-10%, 10%] ... brightness(x,d): ... range of [-32, 32] ... contrast(x,d): ... range of [0.8, 1.2].\"; Section 4.3 (Evaluation Metric), p.1153: \"robust accuracy is the proportion of images in the testing dataset where the prediction of a DNN does not fluctuate with any small realistic perturbations...\" \u2014 the paper's evaluation uses robust accuracy as the reported metric and does not report any human evaluation or perceptual-similarity scores (e.g., LPIPS, SSIM) for the generated/augmented images."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ],
            "oracle": [
                "Metamorphic",
                "Property-based"
            ]
        }
    },
    "You et al. - 2023 - Regression Fuzzing for Deep Learning Systems": {
        "key": "you2023regression",
        "name": "DRFuzz",
        "year": "2023",
        "citation_count": "27",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Intargeted exploration for generic regression misclassifications - any inputs that cause the new model to misclassify examples previously classified correctly"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "does not analyze the root cause of regression faults, acknowledging that identifying their causes remains an open challenge. It provides behavior-level explanations (e.g., which classes or behaviors regress, per-class fault heatmaps, and faulty-behavior tuples). Interprets regression faults as evidence of bias or overfitting (e.g., specific classes suffering more after retraining)."
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": "Every batch of mutated inputs is passed through the GAN discriminator and only natural mutated inputs are kept for execution and further mutation."
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly incorporates multiple mechanisms to enforce input plausibility throughout fuzzing iterations and provides empirical human-evaluation evidence that the final fault-inducing inputs remain perceptually/semantically realistic. Concretely, DRFuzz: (1) constrains mutations (pixel-level mutations limited to 0.5% of pixels and image-level mutations applied to whole images to avoid unrealistic partial edits); (2) uses a GAN discriminator (trained on the original training set) to compute a per-seed fidelity threshold and filter out low-fidelity mutated inputs during the fuzzing process; and (3) limits cumulative drift via seed-generation controls (e.g., max_times to bound number of times a seed can be mutated and an exponential-decay seed selection probability) and tree-based trimming. The authors also report a manual/human evaluation comparing DRFuzz (with GAN filtering) to a variant without the GAN: two authors labeled sampled inputs (Cohen\u2019s kappa = 0.67), finding 86% of DRFuzz inputs labeled high-fidelity vs 57.5% for the no-GAN variant. These elements together satisfy the metric requirement of enforcing plausibility through iterations and providing empirical (human) evidence for plausibility of final fault-inducing inputs.",
                    "evidence": "Page 84, Section III-B (GAN-based Fidelity Assurance): \"During the fuzzing process, we feed an initial seed into the Discriminator to get a fidelity score, which serves as the fidelity threshold for filtering out the low-fidelity inputs generated from this initial seed.\"; Page 86, Table I / Section III-C (Mutation Rules): \"Pixel-Level Mutation aims to change a small set of pixels in an image ... DRFuzz randomly selects a small set of pixels (i.e., 0.5% total pixels) for mutation.\" and \"Image-Level Mutation ... this category of mutation rules can simulate real-world scenarios more effectively when applying them to the whole image.\"; Page 88, Section IV-A5 (Implementations and Experiments): \"... we set ... max_times to be 10 ...\" (bounding number of times a seed can be mutated); Page 90, Section IV-B2 (Contribution of the main component of DRFuzz): \"Specifically, we sample 100 inputs generated by DRFuzz_NG and DRFuzz, respectively, and two authors manually evaluate their fidelity individually. The Cohen\u2019s Kappa coefficient between their evaluation is 0.67. They label 86% of inputs generated by DRFuzz as high-fidelity and only 57.5% of inputs generated by DRFuzz_NG as high-fidelity.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper explicitly designs a mechanism, \"GAN-based Fidelity Assurance\", to enforce input plausibility throughout the fuzzing process. This component uses a discriminator, trained to distinguish between real and synthetic data, to filter out mutated inputs that are not semantically similar to the original data distribution. This ensures that generated inputs remain plausible during the fuzzing iterations. Furthermore, the paper provides direct empirical evidence of the effectiveness of this mechanism through a human evaluation. In an ablation study, two authors manually assessed the fidelity of 100 inputs generated with and without the GAN filter. The study found that 86% of inputs from the proposed method (DRFuzz) were high-fidelity, compared to only 57.5% from the variant without the GAN filter (DRFuzz_NG). This combination of a dedicated plausibility enforcement mechanism and empirical validation via human study aligns perfectly with the 'High' rating.",
                    "evidence": "Page 9, Section IV-B2, \"Contribution of the main component of DRFuzz\": \"We further manually evaluate the fidelity of inputs obtained with and without the fidelity assurance mechanism. Specifically, we sample 100 inputs generated by DRFuzz_NG and DRFuzz, respectively, and two authors manually evaluate their fidelity individually. The Cohen\u2019s Kappa coefficient between their evaluation is 0.67. They label 86% of inputs generated by DRFuzz as high-fidelity and only 57.5% of inputs generated by DRFuzz_NG as high-fidelity. This indicates that the GAN-based Fidelity Assurance technique can filter out more than 20% of fault-triggering inputs with low fidelity.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly incorporates multiple mechanisms to enforce input plausibility throughout fuzzing iterations and provides empirical human-evaluation evidence that the final fault-inducing inputs remain perceptually/semantically realistic. Concretely, DRFuzz: (1) constrains mutations (pixel-level mutations limited to 0.5% of pixels and image-level mutations applied to whole images to avoid unrealistic partial edits); (2) uses a GAN discriminator (trained on the original training set) to compute a per-seed fidelity threshold and filter out low-fidelity mutated inputs during the fuzzing process; and (3) limits cumulative drift via seed-generation controls (e.g., max_times to bound number of times a seed can be mutated and an exponential-decay seed selection probability) and tree-based trimming. The authors also report a manual/human evaluation comparing DRFuzz (with GAN filtering) to a variant without the GAN: two authors labeled sampled inputs (Cohen\u2019s kappa = 0.67), finding 86% of DRFuzz inputs labeled high-fidelity vs 57.5% for the no-GAN variant. These elements together satisfy the metric requirement of enforcing plausibility through iterations and providing empirical (human) evidence for plausibility of final fault-inducing inputs.",
                    "evidence": "Page 84, Section III-B (GAN-based Fidelity Assurance): \"During the fuzzing process, we feed an initial seed into the Discriminator to get a fidelity score, which serves as the fidelity threshold for filtering out the low-fidelity inputs generated from this initial seed.\"; Page 86, Table I / Section III-C (Mutation Rules): \"Pixel-Level Mutation aims to change a small set of pixels in an image ... DRFuzz randomly selects a small set of pixels (i.e., 0.5% total pixels) for mutation.\" and \"Image-Level Mutation ... this category of mutation rules can simulate real-world scenarios more effectively when applying them to the whole image.\"; Page 88, Section IV-A5 (Implementations and Experiments): \"... we set ... max_times to be 10 ...\" (bounding number of times a seed can be mutated); Page 90, Section IV-B2 (Contribution of the main component of DRFuzz): \"Specifically, we sample 100 inputs generated by DRFuzz_NG and DRFuzz, respectively, and two authors manually evaluate their fidelity individually. The Cohen\u2019s Kappa coefficient between their evaluation is 0.67. They label 86% of inputs generated by DRFuzz as high-fidelity and only 57.5% of inputs generated by DRFuzz_NG as high-fidelity.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Generates and evaluates mutated inputs entirely in memory using both pixel level and image level semantic mutations. They use GAN-based fidelity filtering to ensure naturalness but does not ensure if the mutation will survive I/O."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper defines and evaluates regression-fault-triggering inputs strictly with respect to a prior model M1 and its regression model M2 (i.e., it detects inputs where M1 is correct and M2 is incorrect). All described generation, fidelity filtering, and evaluation steps execute mutated inputs on that M1\u2013M2 pair. The experiments use multiple dataset/model pairs but do not evaluate whether inputs found on one model transfer to other models performing the same task, nor does the design include explicit mechanisms to discover or measure transferable attacks across different model implementations. Therefore the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "Section II (Definition), Definition 2 \u2014 \"Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\" (Section II, Definition 2)\n\nSection III (Approach) \u2014 \"the mutated inputs are passed into the GAN-based Fidelity Assurance model ... The inputs that do not trigger regression faults will be maintained ... the mutated inputs ... are then executed by both the original model and the regression model. If the two models provide different prediction results, then a regression-fault-triggering input is identified.\" (Section III, overview / Approach)\n\nSection IV-A1 (Subjects / Experimental setup) \u2014 \"we used four pairs of datasets and DNN models as subjects, i.e., LeNet-5 on MNIST, VGG16 on CIFAR10, AlexNet on Fashion-MNIST (FM), and ResNet18 on SVHN\" (Section IV-A1 Subjects). \n\nThroughout the paper (Approach and Evaluation sections) all testing and analysis focus on M1 vs. M2 pairs; there is no experiment or statement reusing inputs crafted on one model to test different, unseen target models nor any mechanism intended to discover transferable attacks across model implementations."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's entire methodology and evaluation are focused on 'regression fuzzing', which it defines as finding faults between a prior version of a model (M1) and its subsequent version (M2). The goal is to find inputs that were correctly classified by M1 but are misclassified by M2 after an evolution event (e.g., fine-tuning, pruning). The experiments are designed around creating these M1/M2 pairs and evaluating the proposed fuzzer, DRFuzz, in its ability to find such regression-specific faults. The paper does not perform any experiments to check if the fault-inducing inputs found for a specific model (e.g., LeNet-5) also trigger failures in other, different model architectures (e.g., VGG16) performing the same task. Therefore, it does not demonstrate attack transferability across models.",
                    "evidence": "Page 2, Section II. DEFINITION, Definition 2. Regression fault: \"Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1 [x] = y \u2227 cM2 [x] != y.\" This definition explicitly frames the problem as a comparison between two versions of the same model, not between different models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper defines and evaluates regression-fault-triggering inputs strictly with respect to a prior model M1 and its regression model M2 (i.e., it detects inputs where M1 is correct and M2 is incorrect). All described generation, fidelity filtering, and evaluation steps execute mutated inputs on that M1\u2013M2 pair. The experiments use multiple dataset/model pairs but do not evaluate whether inputs found on one model transfer to other models performing the same task, nor does the design include explicit mechanisms to discover or measure transferable attacks across different model implementations. Therefore the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "Section II (Definition), Definition 2 \u2014 \"Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\" (Section II, Definition 2)\n\nSection III (Approach) \u2014 \"the mutated inputs are passed into the GAN-based Fidelity Assurance model ... The inputs that do not trigger regression faults will be maintained ... the mutated inputs ... are then executed by both the original model and the regression model. If the two models provide different prediction results, then a regression-fault-triggering input is identified.\" (Section III, overview / Approach)\n\nSection IV-A1 (Subjects / Experimental setup) \u2014 \"we used four pairs of datasets and DNN models as subjects, i.e., LeNet-5 on MNIST, VGG16 on CIFAR10, AlexNet on Fashion-MNIST (FM), and ResNet18 on SVHN\" (Section IV-A1 Subjects). \n\nThroughout the paper (Approach and Evaluation sections) all testing and analysis focus on M1 vs. M2 pairs; there is no experiment or statement reusing inputs crafted on one model to test different, unseen target models nor any mechanism intended to discover transferable attacks across model implementations."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Zhang et al. - 2019 - Life after Speech Recognition Fuzzing Semantic Misinterpretation for Voice Assistant Applications": {
        "key": "zhang2019life",
        "name": "LipFuzzer",
        "year": "2019",
        "citation_count": "79",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Medium",
                    "why": "It does not bypass permissions or authentication; instead, it tricks the VA into misinterpreting intent. The resulting misbehavior (e.g., invoking the wrong vApp or disclosing data to a malicious app). Expose semantically incorrect but functionally valid behaviors that breach user-intent and specification expectations\u2014posing integrity and trust risks without directly bypassing safety or security mechanisms."
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "untargeted exploration to expose semantic or intent-classification inconsistencies, sometimes aligning with attacker intent (e.g., LAPSUS examples), though its discovery of attacker intent is incidental rather than goal-optimized."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Empirically isolates the NLU Intent Classifier as the primary source of error and correlates misinterpretation patterns with linguistic mutation types"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "They perform human study to train the linguistic model that help selection potential fault triggering input. However, the synthesized voice commands are not measured for naturalness"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper enforces plausibility throughout its fuzzing design and provides empirical human-driven evidence that the fault-inducing inputs remain realistic. LipFuzzer mutates NLP-preprocessed templates using linguistically-grounded LAPSUS Models (Bayesian Networks) constructed from curated linguistic knowledge and further weight-trained with audio recordings from a user study. The fuzzer applies a probability cutoff to filter low-likelihood mutations and explicitly limits mutations per seed (\"we only get one mutation for each voice command\"), reducing cumulative drift. Finally, generated inputs are converted to speech (TTS) and verified on real VA devices; the authors report human-collected/resynthesized examples and measured rates (e.g., 77% of synthesized transcriptions still induced semantic inconsistency), demonstrating that many fault-triggering inputs are perceptually/semantically plausible.",
                    "evidence": "Page 2, (Introduction / Our Approach): \"we design LipFuzzer\u2019s basic template fuzzing by first mutating NLP pre-processed voice command templates. Then, we input mutated voice commands to VA devices by using machine speech synthesis to eliminate the human factor of producing ASR-related misinterpretation.\"; Page 7, (IV.B Weight Training): \"Weight Training. After we have the initial BN ... we further train the weight ... through a user study with audio recording.\"; Page 8, (IV.C Verification / Template Fuzzing): \"Verification. We verify if a LAPSUS is effective by testing the synthesis audio with Alexa. To do that, we first synthesize speech from generated LAPSUS voice commands.\"; Page 11, (VI-B Cutoffs for Different Query Strategies): \"Note that, during the fuzzing, we only get one mutation for each voice command.\"; Page 11, (VI-A Intent Classifier Evaluation): \"Our result shows that 77% (84/109) still incur the semantic inconsistency, which means LAPSUS still exists.\"; Page 11, (VI-B The Cutoff Value): \"By applying this cutoff value, we cut off 49.9% of generated LAPSUS population with the effectiveness rate increased to 59.52%.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper enforces input plausibility throughout the fuzzing process by design, and provides empirical evidence that the discovered fault-inducing inputs are realistic. The fuzzer, LipFuzzer, is explicitly a \"linguistic-model-guided\" tool that generates mutations based on models of plausible human speech errors, termed LAPSUS. These models are constructed from real-world linguistic knowledge (e.g., pronunciation, vocabulary, grammar rules) and are statistically trained using data from a human user study. This ensures that the generated voice commands are not random or unrealistic, but rather emulate common speech errors. The paper further provides empirical evidence of plausibility by conducting a user study with 150 MTurk workers to collect real-world voice command errors. The study confirms that the types of errors generated by LipFuzzer naturally occur, with 38.14% of collected user audio recordings classified as LAPSUS. Examples of these realistic errors are provided, directly demonstrating their plausibility.",
                    "evidence": "Page 8, Section IV-B, \"Weight Training\": \"After we have the initial BN (i.e., a graph with all edge weights set as 1) ready, we further train the weight (i.e., the probabilities of successful transition states) through a user study with audio recording. In this user study, we find sentences or short expressions which contains the states in the models. Then, we ask users in the study to repeat these sentences or expressions... Next, we calculate how many times these transitions are observed.\"\n\nPage 11, Section VI-A, \"Intent Classifier Evaluation\": \"The goal of this experiment is to confirm the existence of LAPSUS in the real world. From the collected audio records, we first remove unqualified audio samples... As a result, 61.86% (193/312) of them are processed with intended skills and functionality. This means that 38.14% of the recorded voice commands are LAPSUS. We showcase examples of these LAPSUS in Table III.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper enforces plausibility throughout its fuzzing design and provides empirical human-driven evidence that the fault-inducing inputs remain realistic. LipFuzzer mutates NLP-preprocessed templates using linguistically-grounded LAPSUS Models (Bayesian Networks) constructed from curated linguistic knowledge and further weight-trained with audio recordings from a user study. The fuzzer applies a probability cutoff to filter low-likelihood mutations and explicitly limits mutations per seed (\"we only get one mutation for each voice command\"), reducing cumulative drift. Finally, generated inputs are converted to speech (TTS) and verified on real VA devices; the authors report human-collected/resynthesized examples and measured rates (e.g., 77% of synthesized transcriptions still induced semantic inconsistency), demonstrating that many fault-triggering inputs are perceptually/semantically plausible.",
                    "evidence": "Page 2, (Introduction / Our Approach): \"we design LipFuzzer\u2019s basic template fuzzing by first mutating NLP pre-processed voice command templates. Then, we input mutated voice commands to VA devices by using machine speech synthesis to eliminate the human factor of producing ASR-related misinterpretation.\"; Page 7, (IV.B Weight Training): \"Weight Training. After we have the initial BN ... we further train the weight ... through a user study with audio recording.\"; Page 8, (IV.C Verification / Template Fuzzing): \"Verification. We verify if a LAPSUS is effective by testing the synthesis audio with Alexa. To do that, we first synthesize speech from generated LAPSUS voice commands.\"; Page 11, (VI-B Cutoffs for Different Query Strategies): \"Note that, during the fuzzing, we only get one mutation for each voice command.\"; Page 11, (VI-A Intent Classifier Evaluation): \"Our result shows that 77% (84/109) still incur the semantic inconsistency, which means LAPSUS still exists.\"; Page 11, (VI-B The Cutoff Value): \"By applying this cutoff value, we cut off 49.9% of generated LAPSUS population with the effectiveness rate increased to 59.52%.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "evaluates two voice assistant systems independently but does not analyze whether failures generalize"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or report experiments where fault-inducing inputs (LAPSUS) discovered on one model/target are reused to test other models to show cross-model transferability. Although LipFuzzer is applied to both Amazon Alexa and Google Assistant stores (i.e., the authors run the tool on multiple platforms), the stated evaluation goals and the described experiments focus on finding and verifying LAPSUS per platform. The paper does not present (a) an explicit experiment that generates faults on a surrogate model and then reuses those same inputs on different/unknown target models, nor (b) design elements explicitly aimed at discovering or measuring the transferability of discovered fault-inducing inputs across independently implemented models performing the same task.",
                    "evidence": "Section/Pages and quotes from the paper:\n\n1) Abstract (p.1): \"We also use LipFuzzer to evaluate both Amazon Alexa and Google Assistant vApp platforms.\" \u2014 supports that both platforms were evaluated but does not indicate reuse/transfer experiments.\n\n2) Evaluation goals (Section VI, p.11): \"More specifically, our evaluation has three goals: (i) We empirically verify that the problematic Intent Classifier can lead to speech misinterpretation related to LAPSUS. (ii) We show LipFuzzer's performance in terms of the LAPSUS Models' accuracy and effectiveness. (iii) We use LipFuzzer to reveal that problematic templates widely exist in both Amazon Alexa and Google Assistant platforms.\" \u2014 none of these goals mentions generating fault-inducing inputs on one model and testing them on other models to assess transferability.\n\n3) vApp store experiment setup (Section VI-C, p.12): \"we apply the same LAPSUS Models used in LipFuzzer evaluation.\" and \"we evaluate LipFuzzer by using templates crawled from the Amazon Alexa Store and Google Assistant Store.\" \u2014 indicates application across stores but does not describe experiments that generate LAPSUS on one platform and then reuse them to test the other platform as a transferability study.\n\n(These quotes are from the paper's Abstract, Section VI (Evaluation), and Section VI-C (vApp Store Evaluation).)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper applies its fuzzing methodology to two different major Voice Assistant platforms (Amazon Alexa and Google Assistant), which represent distinct models performing the same task of semantic interpretation. The design of the fuzzer, LipFuzzer, explicitly incorporates mechanisms to discover transferable failures. It achieves this by building platform-agnostic 'LAPSUS Models' based on general linguistic knowledge of common speech errors (e.g., pronunciation, vocabulary, grammar) rather than on the specific implementation of any single platform's Natural Language Understanding (NLU) component. By using these general models to generate fault-inducing inputs for applications on both platforms, the paper demonstrates that the same classes of vulnerabilities exist across different models, fulfilling both criteria for a 'High' rating.",
                    "evidence": "Page 12, Section VI-C, \"vApp Store Evaluation\": \"1) Experiment Setup: We evaluate LipFuzzer by using templates crawled from the Amazon Alexa Store and Google Assistant Store. For Amazon Alexa Store, we acquired a seed template dataset of 98,261 voice commands from 32,892 vApps. For Google Assistant Store, we gathered 2,328 vApps with 9,044 voice commands.\" This is further supported by Table IV on Page 13, which shows the fuzzing results for both platforms. The design for transferability is evidenced on Page 6, Section IV-A, \"Fuzzing Input & Output\": \"The input of linguistic modeling is linguistic knowledge data shown in 1 of Figure 4. We choose LAPSUS-related linguistic knowledge from multiple sources [12] [26] [1] [20] [24].\" This confirms the models are built from general, platform-agnostic linguistic sources."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "Although the authors apply LipFuzzer separately to both Amazon Alexa and Google Assistant vApp platforms, they do not generate fault-inducing inputs on one platform and then reuse those exact inputs to test the other. The evaluation treats each platform in isolation, and the paper does not include any explicit experiment or analysis showing that LAPSUS examples found on a surrogate (e.g., Alexa) transfer to an unseen target (e.g., Google Assistant). Thus, it does not demonstrate cross-model transferability of specific fault-inducing inputs.",
                    "evidence": "Section VI\u2013C (vApp Store Evaluation, pp. 12\u201313): the authors describe running LipFuzzer \u201cby using templates crawled from the Amazon Alexa Store\u201d and \u201ctemplates crawled from the Google Assistant Store\u201d but do not report reusing the same LAPSUS across both platforms.  Section VI (Evaluation Goals, p. 11): none of the three stated goals mention testing the transfer of inputs from one model to another."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Data-driven"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Woodlief et al. - 2022 - Semantic image fuzzing of AI perception systems": {
        "key": "woodlief2022semantic",
        "name": "semSensFuzz",
        "year": "2022",
        "citation_count": "18",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "generates safety-critical scenarios (cars, pedestrians, obstacles) to expose perception failures."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Offers descriptive correlations between input changes and output errors, but no analysis on root cause"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Applies design constratraints but no measure of naturalness of the fault-triggering fuzzed inputs"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly designs and enforces per-mutation plausibility constraints (resource-based parameterization, preconditions, perspective and lighting checks) and it performs an explicit human validation of final fault-inducing inputs. However, the implementation still produces a substantial rate of non-conforming (false-positive) mutants, the authors report limitations of automated conformity checks (failed discriminator), and they do not address cumulative drift across multiple iterative mutations. Thus the work enforces plausibility at the individual-mutation level and validates final cases empirically, but does not fully ensure plausibility throughout iterative fuzzing and exhibits imperfect conformity enforcement.",
                    "evidence": "Section 3.2 (p.1960): \"Second, we associate a set of preconditions Prec with each mutation \u03b4, specified in terms of the interp that defines whether \u03b4 is applicable to a given t. If Prec(interp) is not satisfied, then \u03b4 is not applicable to that test.\" (Section name: 3.2 Semantic Mutations with semSensFuzz) \n\nSection 4.3.1 (p.1963): \"To ensure that the lighting conditions are similar, if the entity does not have a median value within 5 units (~2%) of the median value of the base image target area, then it fails the conformity check.\" (Section name: 4.3 Adding an Entity / Implementation) \n\nSection 4.3.1 (p.1962\u20131963): description of multiple conformity checks (vanishing point quadrant matching, bounding-box-on-road check, occlusion filtering) used before generating a mutated image. (Section name: 4.3 Adding an Entity / Implementation) \n\nSection 4.4.4 (p.1964): \"We explored this approach, training a binary classifier based on a CNN ... The discriminator learned to differentiate between the classes, but could not differentiate conforming versus nonconforming images.\" (Section name: 4.4.4 Discriminators for Conformity Checking) \n\nSection 6.4.1 and Table 2 (p.1966\u20131967): human-validation of plausibility: \"we manually inspect all the generated test cases that led to 5+ p.p. drop, and sample 10% of the test cases that led to a drop between 1 and 5 p.p.. The process entailed each author examining each image and classifying them as either a true positive or false positive.\" Table 2 (p.1967) reports substantial false-positive rates (e.g., EfficientPS: 43% for [1,5) p.p., 47% for [5,10), 53% for [10,100]). (Section name: 6.4.1 RQ1 Results: Finding Inconsistencies) \n\nSection 4.3.2 and Figures 9 (p.1963): authors enumerate concrete failure modes leading to nonconforming images (perspective mismatch, overlapping entities, inconsistent lighting, missing shadows). (Section name: 4.3 Adding an Entity / Potential for False Positives) \n\nSection 3.2 (p.1960): \"All of the mechanisms in our approach make a conscious trade off between conformity and a smaller space of available mutations.\" (Section name: 3.2 Semantic Mutations with semSensFuzz)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's methodology is fundamentally centered on maintaining input plausibility, which it terms \"conformity.\" The fuzzing design incorporates numerous mechanisms to enforce this throughout the mutation process, such as using semantic preconditions, leveraging real-world data for mutations, and performing explicit checks for consistent perspective, physical feasibility (e.g., placing a car on a road), and lighting. Most importantly, the paper provides direct empirical evidence assessing the plausibility of the final fault-inducing inputs. The authors conducted a manual study where they inspected all high-impact and a sample of moderate-impact failure-inducing images to classify them as conforming (\"true positive\") or non-conforming (\"false positive\"). The results of this human evaluation, including false positive rates and visual examples, are reported. This combination of enforcing plausibility by design and empirically validating the realism of the final outputs fully meets the criteria for a \"High\" rating.",
                    "evidence": "Page 11, Section 6.4, \"...one potential issue is the presence of test cases that are nonconforming, which leads to false inconsistencies. As highlighted in Section 4, there are several factors that can result in a nonconforming test case, and determining conformity is subjective. Still, to gain a better grasp on the rate of false positives we manually inspect all the generated test cases that led to 5+ p.p. drop, and sample 10% of the test cases that led to a drop between 1 and 5 p.p.. The process entailed each author examining each image and classifying them as either a true positive or false positive. If any of the three authors deemed an image a false positive, it was conservatively recorded as such.\" This is further supported by Table 2, \"False Positive Rate for Inconsistencies Found,\" and Figure 14, which provides visual examples of both \"True Positive\" and \"False Positive\" (i.e., non-conforming) results."
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "semSensFuzz builds in strong, per-mutation plausibility constraints (semantic preconditions, real data parameterization, perspective & lighting checks) and even conducts a human validation of final fault-inducing inputs, but it does not enforce a global, iterative plausibility guarantee across successive mutations (no cumulative-drift control) nor apply quantitative perceptual metrics (e.g. LPIPS/SSIM) on the full mutated distribution. Thus it validates individual steps and final cases empirically, but stops short of the end-to-end, iteration-aware enforcement required for a \u201cHigh\u201d rating.",
                    "evidence": "Section 3.2 (p. 1960): \u201cwe associate a set of preconditions Prec with each mutation \u03b4\u2026 If Prec(interp) is not satisfied, then \u03b4 is not applicable\u2026\u201d\n\nSection 4.3.1 (p. 1962\u20131963): Perspective quadrant matching, on-road bounding-box check, occlusion filtering, and median HSV brightness checks are all applied before each \u2018add entity\u2019 mutation.\n\nSection 4.4.4 (p. 1964): \u201cWe explored\u2026 a discriminator\u2026 but it could not differentiate conforming versus nonconforming images.\u201d\n\nSection 6.4.1 & Table 2 (p. 1966\u20131967): Authors manually inspect all high-impact and a sampled set of moderate-impact fault inputs, reporting substantial false-positive rates (e.g. 43%\u201375% nonconforming in some bins)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Perturbations occur at a semantic level that is plausibly I/O-stable, but the paper neither enforces nor verifies persistence after I/O transformations, and minor artifacts from the mutation implementation may cause non-persistent faults."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper reuses fault-inducing inputs generated by semImFuzz to test multiple different perception models: the same set of 150,000 semantically mutated tests was executed against each of the five SUTs, and the authors report aggregated counts of inconsistencies across SUTs (e.g., \"1210 SUT inconsistencies resulting from 884 mutations\"), which demonstrates reuse of generated failures to evaluate other models. However, the paper does not present design elements or explicit mechanisms aimed at producing or measuring transferable adversarial attacks (e.g., surrogate-model training, cross-model transfer rate analyses, or targeted transferability objectives). The evaluation reports per-SUT inconsistency counts and notes which mutations trigger errors, but it does not characterize transfer rates or describe methods explicitly designed to discover transferable security failures across models. These facts match the metric definition for Medium: inputs are reused to test transferability but the fuzzing design is not explicitly aimed at discovering transferable attacks.",
                    "evidence": "Section 6.2 (Tests Generated), p. 1966: \"For evaluation, we ran each SUT on 150,000 tests generated by semImFuzz comprised of 50,000 'Add Car' mutations, 50,000 'Add Person' mutations, and 50,000 'Change Car Color' mutations.\" \n\nSection 6.4.1 (RQ1 Results: Finding Inconsistencies), p. 1966: \"We first remark that semImFuzz found 1210 SUT inconsistencies resulting from 884 mutations and that each SUT had over 100 inconsistencies.\" \n\n(Absence of transfer-focused design) The paper's evaluation and methodology sections (Sections 3, 4, 6) describe mutation design, conformity checks, and per-SUT inconsistency reporting but do not describe surrogate-model-based strategies, explicit cross-model transferability metrics, or procedures to maximize/measure transfer of failures between models. For example, Section 6 (Evaluation) frames results as per-SUT inconsistency counts and severity drops rather than transfer rates or cross-model vulnerability analyses."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's evaluation reuses the generated fault-inducing inputs across multiple models but the fuzzing design is not explicitly aimed at discovering transferable attacks. The methodology involves creating a single set of 150,000 mutated inputs and then executing this same set against five different perception models (SUTs) performing the same task. This experimental design inherently tests for transferability by observing how different models react to the same set of inputs. However, the design of the fuzzer, `semImFuzz`, focuses on generating semantically valid and conforming mutations based on real-world constraints (e.g., adding cars to roads, changing colors) rather than incorporating explicit mechanisms to discover vulnerabilities that are common across different model architectures. Therefore, it satisfies the condition of reusing inputs but lacks the explicit design goal for transferability required for a 'High' rating.",
                    "evidence": "Page 9, Section 6.2 Tests Generated, \"For evaluation, we ran each SUT on 150,000 tests generated by semImFuzz comprised of 50,000 \"Add Car\" mutations, 50,000 \"Add Person\" mutations, and 50,000 \u201cChange Car Color\u201d mutations.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper reuses fault-inducing inputs generated by semImFuzz to test multiple different perception models: the same set of 150,000 semantically mutated tests was executed against each of the five SUTs, and the authors report aggregated counts of inconsistencies across SUTs (e.g., \"1210 SUT inconsistencies resulting from 884 mutations\"), which demonstrates reuse of generated failures to evaluate other models. However, the paper does not present design elements or explicit mechanisms aimed at producing or measuring transferable adversarial attacks (e.g., surrogate-model training, cross-model transfer rate analyses, or targeted transferability objectives). The evaluation reports per-SUT inconsistency counts and notes which mutations trigger errors, but it does not characterize transfer rates or describe methods explicitly designed to discover transferable security failures across models. These facts match the metric definition for Medium: inputs are reused to test transferability but the fuzzing design is not explicitly aimed at discovering transferable attacks.",
                    "evidence": "Section 6.2 (Tests Generated), p. 1966: \"For evaluation, we ran each SUT on 150,000 tests generated by semImFuzz comprised of 50,000 'Add Car' mutations, 50,000 'Add Person' mutations, and 50,000 'Change Car Color' mutations.\" \n\nSection 6.4.1 (RQ1 Results: Finding Inconsistencies), p. 1966: \"We first remark that semImFuzz found 1210 SUT inconsistencies resulting from 884 mutations and that each SUT had over 100 inconsistencies.\" \n\n(Absence of transfer-focused design) The paper's evaluation and methodology sections (Sections 3, 4, 6) describe mutation design, conformity checks, and per-SUT inconsistency reporting but do not describe surrogate-model-based strategies, explicit cross-model transferability metrics, or procedures to maximize/measure transfer of failures between models. For example, Section 6 (Evaluation) frames results as per-SUT inconsistency counts and severity drops rather than transfer rates or cross-model vulnerability analyses."
                },
                "manual": {
                    "value": "Low",
                    "why": "Mutated inputs are evaluated independently across multiple inputs but no measure whether same failures discovered by the fauzzer transfer to other models",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Data-driven"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Lee et al. - 2020 - Effective white-box testing of deep neural networks with adaptive neuron-selection strategy": {
        "key": "lee2020effective",
        "name": "ADAPT",
        "year": "2020",
        "citation_count": "81",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Correlates neuron features with faults"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Norm-bounded"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility during fuzzing via an explicit distance constraint (Constraint(I,I')) and by tuning mutation parameters (e.g., \u03b71) to limit drift; the authors also report maintaining the L2 distance between original and mutated images (\u22480.05) during experiments and note that some adversarial images were \"visually indistinguishable\" from originals. However, the paper does not provide an explicit empirical assessment of the perceptual or semantic realism of the final fault-inducing inputs using human studies or task-specific quantitative measures (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity). Therefore it enforces plausibility at mutation time and validates it qualitatively, but lacks rigorous quantitative / human-evaluation evidence required for a High rating.",
                    "evidence": "Section 2.3 (Algorithm discussion), p.166: \"For example, Constraint(I, I') is true if L2 distance between I and I' is less than a certain threshold.\"; Section 3.2 (Hyperparameters), p.169: \"The second hyper-parameter \u03b71 is the number of times that the set of selected neurons is used for generating new inputs... We manually set the \u03b71 to 3 so that the distance is not too far.\"; Section 4.1 (Experimental setup), p.170: \"All the techniques, including Adapt, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\"; Section 4.3 (Adversarial Inputs), p.172: \"...the adversarial images found for VGG-19 and ResNet-50 were visually indistinguishable from the original ones.\""
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces a plausibility constraint during input generation and provides a qualitative assessment of the final fault-inducing inputs. A bounded L2-distance constraint is maintained between the initial seed and any mutated input, which helps control the cumulative effect of perturbations. The authors also qualitatively assess the realism of the generated inputs, stating that for the large ImageNet models, the adversarial examples were 'visually indistinguishable from the original ones' and providing visual examples in Figure 3. However, this does not meet the 'High' standard because the plausibility assessment is not a formal human study or based on quantitative perceptual metrics (e.g., LPIPS, SSIM), and the L2-norm is a simple proxy for maintaining inputs within a truly plausible distribution.",
                    "evidence": "Page 7, Section 4.1, Experimental Setup: \"All the techniques, including ADAPT, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\"\n\nPage 10, Section 4.3, Correlation between Coverage and Adversarial Inputs: \"In particular, the adversarial images found for VGG-19 and ResNet-50 were visually indistinguishable from the original ones.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility during fuzzing via an explicit distance constraint (Constraint(I,I')) and by tuning mutation parameters (e.g., \u03b71) to limit drift; the authors also report maintaining the L2 distance between original and mutated images (\u22480.05) during experiments and note that some adversarial images were \"visually indistinguishable\" from originals. However, the paper does not provide an explicit empirical assessment of the perceptual or semantic realism of the final fault-inducing inputs using human studies or task-specific quantitative measures (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity). Therefore it enforces plausibility at mutation time and validates it qualitatively, but lacks rigorous quantitative / human-evaluation evidence required for a High rating.",
                    "evidence": "Section 2.3 (Algorithm discussion), p.166: \"For example, Constraint(I, I') is true if L2 distance between I and I' is less than a certain threshold.\"; Section 3.2 (Hyperparameters), p.169: \"The second hyper-parameter \u03b71 is the number of times that the set of selected neurons is used for generating new inputs... We manually set the \u03b71 to 3 so that the distance is not too far.\"; Section 4.1 (Experimental setup), p.170: \"All the techniques, including Adapt, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\"; Section 4.3 (Adversarial Inputs), p.172: \"...the adversarial images found for VGG-19 and ResNet-50 were visually indistinguishable from the original ones.\""
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper presents Adapt as a white-box testing technique that operates on a single target DNN (Algorithm 2 takes a single DNN as input) and evaluates it per-model. The experiments report per-model coverage and adversarial inputs but do not test or report whether fault-inducing inputs produced on one model transfer to other models. The authors explicitly note that DeepXplore's differential (multi-DNN) testing was not used in its multi-model mode: \u201cwe used it with a single DNN and labeled data in our experiments.\u201d There is no experiment, analysis, or mechanism described to generate or evaluate transferable (cross-model) fault-inducing inputs or surrogate-model attacks. Therefore the paper does not demonstrate attack transferability across models.",
                    "evidence": "Section 3.2 (Algorithm 2): \"1: procedure Adapt(DNN, I,Cov)\" (Algorithm 2 signature shows Adapt operates on a single DNN) \u2014 (page 168, Section 3.2, Algorithm 2).\n\nSection 4.1 Experimental Setup: \"DeepXplore originally performs differential testing requiring multiple DNNs without labeled data but we used it with a single DNN and labeled data in our experiments.\" (shows the authors evaluated tools in single-model mode and did not perform multi-model/differential transfer experiments) \u2014 (page 170, Section 4.1 Experimental Setup).\n\nOverall evaluation description and results (Sections 4.2\u20134.3) report per-model coverage and adversarial inputs but contain no experiments or discussion on reusing inputs across models or on surrogate-model transferability \u2014 (pages 171\u2013173, Sections 4.2\u20134.3)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's methodology and experimental evaluation focus on generating fault-inducing inputs for a single, white-box model at a time. The proposed method, ADAPT, is designed to optimize its neuron-selection strategy based on the internal structure and feedback from the specific model under test. The experiments are conducted independently for each of the four models (LeNet-4, LeNet-5, VGG-19, ResNet-50). The results report the number of adversarial inputs found for each model individually. There is no procedure or result mentioned where fault-inducing inputs generated for one model (e.g., LeNet-4) are tested against another model performing the same task (e.g., LeNet-5) to assess transferability. The paper even notes that it adapted a baseline technique, DeepXplore, to work with a single DNN, moving away from its original multi-DNN differential testing approach.",
                    "evidence": "Page 7, Section 4.1, \"Experimental Setup\": \"DeepXplore originally performs differential testing requiring multiple DNNs without labeled data but we used it with a single DNN and labeled data in our experiments.\" This quote shows a deliberate choice to focus on single-model testing, not cross-model transferability. Additionally, Table 3 on page 9, \"Effectiveness for finding adversarial inputs,\" presents results for each model in separate, independent sections, with no indication that inputs generated for one model were ever tested on another."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper presents Adapt as a white-box testing technique that operates on a single target DNN (Algorithm 2 takes a single DNN as input) and evaluates it per-model. The experiments report per-model coverage and adversarial inputs but do not test or report whether fault-inducing inputs produced on one model transfer to other models. The authors explicitly note that DeepXplore's differential (multi-DNN) testing was not used in its multi-model mode: \u201cwe used it with a single DNN and labeled data in our experiments.\u201d There is no experiment, analysis, or mechanism described to generate or evaluate transferable (cross-model) fault-inducing inputs or surrogate-model attacks. Therefore the paper does not demonstrate attack transferability across models.",
                    "evidence": "Section 3.2 (Algorithm 2): \"1: procedure Adapt(DNN, I,Cov)\" (Algorithm 2 signature shows Adapt operates on a single DNN) \u2014 (page 168, Section 3.2, Algorithm 2).\n\nSection 4.1 Experimental Setup: \"DeepXplore originally performs differential testing requiring multiple DNNs without labeled data but we used it with a single DNN and labeled data in our experiments.\" (shows the authors evaluated tools in single-model mode and did not perform multi-model/differential transfer experiments) \u2014 (page 170, Section 4.1 Experimental Setup).\n\nOverall evaluation description and results (Sections 4.2\u20134.3) report per-model coverage and adversarial inputs but contain no experiments or discussion on reusing inputs across models or on surrogate-model transferability \u2014 (pages 171\u2013173, Sections 4.2\u20134.3)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Whitebox"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic"
            ]
        }
    },
    "Ben Braiek and Khomh - 2019 - DeepEvolution A Search-Based Testing Approach for Deep Neural Networks": {
        "key": "braiek2019deepevolution",
        "name": "DeepEvolution",
        "year": "2019",
        "citation_count": "53",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Uses neuron coveage as heuristic guide, does not show correlation between coverage and neurons"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": "Enforces naturalness across iterations by computing SSIM after each mutation and rejecting any visually distorted image."
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper enforces plausibility constraints both in the mutation design and in candidate selection: transformation parameters are manually tuned to valid domain intervals (defining a constrained sub-space of exploration), the compound transformation limits affine changes (only one affine transform) to avoid unrealistic images, and mutated inputs with low perceptual similarity are rejected using SSIM. The authors also report an empirical plausibility check: they sampled generated images (95% confidence, 5% error) and verified them manually. These points show plausibility is enforced throughout the search and that final generated inputs were assessed quantitatively (SSIM) and via manual inspection.",
                    "evidence": "Section IV.A (Metamorphic Transformation), p.455: \"To infer the valid domain interval of each transformation\u2019s parameters, we manually tune them to set up the appropriate range of values, i.e., high and low boundaries, that preserves the input semantics before and after each transformation... To verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8]... Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.\"; Section IV.C (Swarm-based Metaheuristics), p.456: \"To ensure semantically preserving transformations, we use the valid domain intervals of transformations that we have already tuned manually to create the high and low boundaries vectors, defining the sub-space of exploration.\"; Section VI (Threats to Validity), p.457: \"The manual tuning of metamorphic transformations\u2019 domain and the threshold of SSIM could affect the validity of our results. To mitigate this threat, we selected a sample of our generated images using a confidence level of 95% and an error margin of 5%, and verified them manually. We found them to be correct.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, DeepEvolution, explicitly incorporates mechanisms to enforce input plausibility throughout the test generation process and provides empirical evidence that the resulting inputs are realistic. Plausibility is enforced in two ways: 1) by manually tuning the parameter ranges for metamorphic transformations to ensure they are 'semantically-preserving', and 2) by using a quantitative perceptual quality metric, the Structural Similarity Index (SSIM), to filter and reject any generated inputs that fall below a tuned similarity threshold. This use of SSIM directly addresses the metric's criteria for using task-specific quantitative measures. Furthermore, the paper provides empirical evidence of the final inputs' plausibility by stating that the authors manually verified a statistically significant sample of the generated images and confirmed their correctness.",
                    "evidence": "Page 456, Section IV.A, 'To verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8] which assesses the similarity between two images based on the visual impact of three characteristics: luminance, contrast, and structure... Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.'\n\nPage 458, Section VI. Threats to Validity, 'The manual tuning of metamorphic transformations\u2019 domain and the threshold of SSIM could affect the validity of our results. To mitigate this threat, we selected a sample of our generated images using a confidence level of 95% and an error margin of 5%, and verified them manually. We found them to be correct.'"
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper enforces plausibility constraints both in the mutation design and in candidate selection: transformation parameters are manually tuned to valid domain intervals (defining a constrained sub-space of exploration), the compound transformation limits affine changes (only one affine transform) to avoid unrealistic images, and mutated inputs with low perceptual similarity are rejected using SSIM. The authors also report an empirical plausibility check: they sampled generated images (95% confidence, 5% error) and verified them manually. These points show plausibility is enforced throughout the search and that final generated inputs were assessed quantitatively (SSIM) and via manual inspection.",
                    "evidence": "Section IV.A (Metamorphic Transformation), p.455: \"To infer the valid domain interval of each transformation\u2019s parameters, we manually tune them to set up the appropriate range of values, i.e., high and low boundaries, that preserves the input semantics before and after each transformation... To verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8]... Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.\"; Section IV.C (Swarm-based Metaheuristics), p.456: \"To ensure semantically preserving transformations, we use the valid domain intervals of transformations that we have already tuned manually to create the high and low boundaries vectors, defining the sub-space of exploration.\"; Section VI (Threats to Validity), p.457: \"The manual tuning of metamorphic transformations\u2019 domain and the threshold of SSIM could affect the validity of our results. To mitigate this threat, we selected a sample of our generated images using a confidence level of 95% and an error margin of 5%, and verified them manually. We found them to be correct.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Semantic-level pixel transformations (contrast, blur, rotation, etc.) but no clipping/rounding to ensure IO persistence"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper explicitly reuses fault-inducing inputs generated by DeepEvolution to detect divergences between an original model and its quantized version (i.e., it reuses generated inputs to test another model variant), but the fuzzing design is not presented as being explicitly aimed at discovering transferable attacks across independently implemented/trained models. The experiments focus on per-dataset single-model instantiations (LeNet for MNIST, CifarNet for CIFAR-10) and compare original vs. quantized versions; they do not evaluate reuse of generated faults across different model implementations trained for the same task nor claim mechanisms specifically for crafting transferable substitute-model attacks.",
                    "evidence": "Section III (page 455): \"From one generation of candidates to another, DeepEvolution performs follow-up tests with the resulting transformed inputs and stores the failed tests that exhibit erroneous DNNs behavior or induce a divergence between the original DNN and its quantized version.\" \n\nSection V.D (page 456-457) Findings: \"DeepEvolution can effectively detect defects introduced during DNN quantization, outperforming the coverage-guided fuzzing tool TensorFuzz.\" (Table III reports number of \"difference-inducing inputs\" found for quantization defects.)\n\nExperiment setup (page 456): \"For each dataset (i.e., MNIST and CIFAR-10), we took, respectively, the official open-source implementation of Tensorflow models, LeNet [3] and CifarNet [4] to allow the reproducibility of our results and comparisons with our approach.\" \n\nRelated work contrast (page 457, Section VII): Paper cites DeepXplore's use of \"multiple DNNs\u2019 implementations\" for differential testing but does not describe performing similar cross-model reuse in DeepEvolution."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's methodology and evaluation focus on generating test cases to find defects within a single Deep Neural Network (DNN) model or between a model and its own quantized version. The experiments are conducted on two distinct models, LeNet and CifarNet, but they are used for different datasets (MNIST and CIFAR-10, respectively) and are treated as separate, independent evaluation subjects. The paper does not describe any experiment or methodology where fault-inducing inputs generated for one model are subsequently used to test a different model performing the same task. Therefore, the concept of attack transferability is not assessed.",
                    "evidence": "Page 456, Section V, EMPIRICAL EVALUATION: The research questions guiding the evaluation are: \"RQ1: How much can DeepEvolution increase the coverage of generated test cases? RQ2: Can DeepEvolution detect diverse erroneous behaviors in DNN models? RQ3: Can DeepEvolution detect divergences induced by DNN quantization?\". All three questions are focused on the behavior of a single model under test. \n\nPage 456, Section V.A, Experiment Setup: \"DNNs. For each dataset (i.e., MNIST and CIFAR-10), we took, respectively, the official open-source implementation of Tensorflow models, LeNet [3] and CifarNet [4] to allow the reproducibility of our results and comparisons with our approach.\" This shows that the experiments test one specific model per dataset, rather than testing multiple models on a single dataset to check for transferability.\n\nPage 457, Section V.D, RQ3: DNN Quantization Defects: The evaluation for quantization defects involves comparing a model to its own modified version, not to a different model. \"TensorFuzz [5] performs a coverage-guided fuzzing process to generate mutated inputs that are able to expose disagreements between a DNN trained on MNIST (that is 32-bit floating point precision) and its quantized versions...\" This further illustrates that the testing scope is limited to a single model architecture at a time."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "DeepEvolution does not demonstrate attack transferability across independently trained model implementations on the same task. The paper only reuses generated fault\u2010inducing inputs to compare a model with its own quantized variant, not to test different models (e.g., using LeNet inputs on another MNIST model or CifarNet inputs on another CIFAR-10 model). There is no experimental evaluation or design mechanism aimed at assessing whether faults transfer across distinct model architectures trained for the same task.",
                    "evidence": "Section III (page 455) describes follow-up tests only between the original DNN and its quantized version. Section V.D (pages 456\u2013457) evaluates quantization defects by comparing each model to its own truncated-precision variant; no experiments reuse inputs across different model implementations performing the same task."
                },
                "manual": {
                    "value": "Low",
                    "why": "Use quantized model for differntial testing based oracle, not for checking fauilure transferability",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ],
            "oracle": [
                "Metamorphic",
                "Differential"
            ]
        }
    },
    "Ma et al. - 2021 - HDTest Differential Fuzz Testing of Brain-Inspired Hyperdimensional Computing": {
        "key": "ma2021hdtest",
        "name": "HDTest",
        "year": "2021",
        "citation_count": "39",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Finds generic misclassifications"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": " It uses hypervector distance purely as a heuristic to guide input mutations - no insight where/how/why the faults occur"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "norm-bounded"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints during fuzzing (they set an explicit distance threshold to keep perturbations \u201cinvisible\u201d and discard candidates beyond it) and reports quantitative distance metrics (normalized L1/L2) and sample images for the final adversarial inputs. However, the paper does not perform stronger perceptual/semantic evaluations (no human study, no task-specific perceptual metrics such as LPIPS/SSIM) nor does it explicitly analyze cumulative drift across multiple mutation iterations beyond the single distance threshold. This matches the \"Medium\" description: plausibility is enforced/validated (bounded constraints and final-distance reporting) but cumulative-effects and perceptual validation are not thoroughly addressed.",
                    "evidence": "Section IV. HDTest Framework (page ~393-394): \"To ensure the added perturbations are within an 'invisible' range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.\" \n\nSection V.A Experimental Setup (page ~394): \"We use different metrics to evaluate HDTest: normalized L1 distance and normalized L2 distance between the mutated image and original image, the average fuzzing iterations and the execution time to successfully generate 1000 adversarial images.\" \n\nSection V.B Mutation Strategies Analysis (page ~394-395): \"We provide sample original images, mutated pixels, and generated images of HDTest in Fig. 4\u20136, under gauss, rand, and shift respectively.\" \n\n(Absence of stronger perceptual/semantic evaluation): Nowhere in the paper do the authors report human evaluation or task-specific perceptual metrics (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity) for the final fault-inducing inputs; their evaluation is limited to L1/L2 distances, iteration counts, runtime, and example figures."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces a form of input plausibility by using a bounded constraint on the perturbations. It explicitly sets an L2 distance threshold and discards any generated images that exceed this limit, with the stated goal of keeping perturbations within an \"invisible\" range. This method constitutes a rule-based constraint on mutations. Furthermore, the paper validates the plausibility of the final generated adversarial inputs by measuring and reporting the normalized L1 and L2 distances from the original inputs, stating that smaller distances are preferred. However, it does not meet the criteria for a \"High\" rating because it does not provide more rigorous empirical evidence of perceptual realism, such as through human studies or by using dedicated perceptual similarity metrics like SSIM or LPIPS. The use of L1/L2 distance serves as a basic validation, fitting the \"Medium\" definition.",
                    "evidence": "Page 4, Section IV, Paragraph 4: 'To ensure the added perturbations are within an \"invisible\" range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.'\n\nPage 4, Section V.A, Paragraph 1: 'We use different metrics to evaluate HDTest: normalized L1 distance and normalized L2 distance between the mutated image and original image, the average fuzzing iterations and the execution time to successfully generate 1000 adversarial images. A smaller distance between the generated image and the original image is preferred by HDTest because it means less perturbation added to the image.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints during fuzzing (they set an explicit distance threshold to keep perturbations \u201cinvisible\u201d and discard candidates beyond it) and reports quantitative distance metrics (normalized L1/L2) and sample images for the final adversarial inputs. However, the paper does not perform stronger perceptual/semantic evaluations (no human study, no task-specific perceptual metrics such as LPIPS/SSIM) nor does it explicitly analyze cumulative drift across multiple mutation iterations beyond the single distance threshold. This matches the \"Medium\" description: plausibility is enforced/validated (bounded constraints and final-distance reporting) but cumulative-effects and perceptual validation are not thoroughly addressed.",
                    "evidence": "Section IV. HDTest Framework (page ~393-394): \"To ensure the added perturbations are within an 'invisible' range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.\" \n\nSection V.A Experimental Setup (page ~394): \"We use different metrics to evaluate HDTest: normalized L1 distance and normalized L2 distance between the mutated image and original image, the average fuzzing iterations and the execution time to successfully generate 1000 adversarial images.\" \n\nSection V.B Mutation Strategies Analysis (page ~394-395): \"We provide sample original images, mutated pixels, and generated images of HDTest in Fig. 4\u20136, under gauss, rand, and shift respectively.\" \n\n(Absence of stronger perceptual/semantic evaluation): Nowhere in the paper do the authors report human evaluation or task-specific perceptual metrics (e.g., SSIM, LPIPS, PESQ, STOI, Perplexity) for the final fault-inducing inputs; their evaluation is limited to L1/L2 distances, iteration counts, runtime, and example figures."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": "In memory floating point mutation, no safeguards for IO preservation"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether adversarial / fault-inducing inputs generated on one HDC model transfer to other models performing the same task. All experiments generate and use adversarial inputs against a single HDC classifier on MNIST (including using generated inputs to retrain that same classifier). The paper does not report reusing seeds/adversarial inputs on other independently-trained HDC models, nor does it include explicit cross-model transferability experiments or mechanisms designed to test transfer across different model implementations.",
                    "evidence": "V.A Experimental Setup (page 394): \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"  V.D Case Study on Defense Against Adversarial Attacks (page 395): \"First, for a specific HDC model, we run HDTest to generate 1000 adversarial images. ... We feed the first subset of adversarial images to the HDC model with correct labels to retrain the HDC model ... Next, we use the second subset of adversarial images to attack the HDC model. Our experimental results show that, after retraining, the rate of successful attack rate drops more than 20%.\"  V.E Discussion (page 395): \"While we only consider one dataset in this paper due to the fact that HDC is still exploring its use in image classification domain, HDTest can be naturally extended to other HDC model structures because it considers a general greybox assumption with only HV distance information.\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper introduces HDTest, a differential fuzz testing method specifically designed for Hyperdimensional Computing (HDC) models. The evaluation of HDTest is conducted on a single HDC model architecture trained for the MNIST classification task. The experiments focus on generating adversarial inputs that cause mispredictions in this specific model and then using these generated inputs to retrain and improve the robustness of that same model. The paper does not perform any experiments to test whether the fault-inducing inputs generated for this one model also cause failures in other, independently trained HDC models or in models based on different architectures (e.g., DNNs) performing the same task. Therefore, the transferability of the generated attacks is not demonstrated.",
                    "evidence": "Page 4, Section V-A, Experimental Setup: \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"\n\nPage 6, Section V-D, Case Study on Defense Against Adversarial Attacks: \"First, for a specific HDC model, we run HDTest to generate 1000 adversarial images... We feed the first subset of adversarial images to the HDC model with correct labels to retrain the HDC model, i.e., updating the reference HVs. Next, we use the second subset of adversarial images to attack the HDC model.\" This section describes a process of retraining and then re-attacking the same model, not testing a different one."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate whether adversarial / fault-inducing inputs generated on one HDC model transfer to other models performing the same task. All experiments generate and use adversarial inputs against a single HDC classifier on MNIST (including using generated inputs to retrain that same classifier). The paper does not report reusing seeds/adversarial inputs on other independently-trained HDC models, nor does it include explicit cross-model transferability experiments or mechanisms designed to test transfer across different model implementations.",
                    "evidence": "V.A Experimental Setup (page 394): \"We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\"  V.D Case Study on Defense Against Adversarial Attacks (page 395): \"First, for a specific HDC model, we run HDTest to generate 1000 adversarial images. ... We feed the first subset of adversarial images to the HDC model with correct labels to retrain the HDC model ... Next, we use the second subset of adversarial images to attack the HDC model. Our experimental results show that, after retraining, the rate of successful attack rate drops more than 20%.\"  V.E Discussion (page 395): \"While we only consider one dataset in this paper due to the fact that HDC is still exploring its use in image classification domain, HDTest can be naturally extended to other HDC model structures because it considers a general greybox assumption with only HV distance information.\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Du et al. - 2019 - DeepStellar model-based quantitative analysis of stateful deep learning systems": {
        "key": "du2019deepstellar",
        "name": "DeepStellar",
        "year": "2019",
        "citation_count": "188",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Broad exploration to uncover functional errors"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Shows correlation between coverage criteria and faults"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Design contraints on perturbations but no naturalness measure of final fault-triggering inputs"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility at the level of individual mutation steps: it uses domain-specific, metamorphic transformations with conservative parameters (e.g., small speed/volume/noise changes for audio; small brightness/contrast/translation/rotation for images) and states that these perturbations are imperceptible. However, DeepStellar does not describe mechanisms that prevent or measure cumulative drift across multiple fuzzing/mutation iterations, nor does it provide empirical human evaluation or task-specific perceptual metrics (e.g., SSIM/LPIPS for images, PESQ/STOI for audio, or Perplexity for language) to validate that final fault-inducing inputs remain perceptually/semantically realistic. The coverage-guided testing algorithm (Algorithm 2) accepts mutants based on coverage increase alone, with no stated plausibility filter applied to the retained seeds; the paper therefore enforces plausibility in individual mutation choices but overlooks cumulative-effects and lacks quantitative or user-study evidence for final-input plausibility, matching the rubric for \u201cMedium.\u201d",
                    "evidence": "Section 5.2 Coverage-Guided Testing (p. 481): \"Mutation operators are often specific to the application domains. For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human\u2019s perspective during testing.\" (Algorithm 2 also shows mutants are retained when they increase coverage: \"Randomly pick transformation t with a random parameter p; A = mutate(t, p, a); ... else if CoverageIncrease(cov, Q) then Q <- Q \u222a a' .\")\n\nSection 6.1 Data Preparation (p. 482\u2013483): \"For each ASR model, we generate 10,000 perturbed audios from original benign ones with existing audio data augmentation techniques [5] (i.e., speed and volume adjustment, low/high-frequency filtering, noise blending).... For each MNIST model, we also generate 10,000 benign perturbed samples with existing image transformation techniques [50] (i.e., image contrast, brightness, translation, scaling, shearing, rotation and add white noise). ... For both audio and image case, we set conservative parameters for transformation so that the perturbation on original samples is slight and imperceptible. Note that all the adversarial samples are also with minimal perturbations and not perceptible by human.\"\n\nEvaluation sections (e.g., RQ3 & RQ4, pp. 484\u2013486) report detection AUROC, coverage numbers and counts of adversarial samples, but the paper does not report any human studies or task-specific perceptual/semantic quality metrics (e.g., LPIPS/SSIM for images, PESQ/STOI for audio, or Perplexity for language) to empirically validate final fault-inducing inputs' realism."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's coverage-guided testing (CGT) method is designed with input plausibility in mind. It explicitly states the use of a \"metamorphic mutation strategy... that would keep the prediction meaning from the human's perspective during testing.\" The transformations used for generating perturbed inputs are standard, bounded operations (e.g., image contrast, brightness, rotation), which enforce plausibility within individual mutation steps. The paper also claims that the perturbations are \"slight and imperceptible\" and that adversarial samples are \"not perceptible by human.\" \n\nHowever, the evaluation does not meet the criteria for 'High'. Firstly, the paper does not discuss or provide a mechanism to control for the cumulative effect of mutations over many iterations, which could cause inputs to drift from the plausible distribution. Secondly, the claim of imperceptibility is not substantiated with empirical evidence, such as a human study or quantitative, task-specific metrics (e.g., LPIPS, SSIM, PESQ). This approach aligns with the 'Medium' definition, as it enforces plausibility in individual mutation steps via bounded constraints but does not address cumulative effects or provide rigorous empirical validation of the final outputs' realism.",
                    "evidence": "Page 7, Section 5.2: \"For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human's perspective during testing.\"\n\nPage 8, Section 6.1, Data Preparation: \"For each MNIST model, we also generate 10,000 benign perturbed samples with existing image transformation techniques [50] (i.e., image contrast, brightness, translation, scaling, shearing, rotation and add white noise).\"\n\nPage 8, Section 6.1, Data Preparation: \"For both audio and image case, we set conservative parameters for transformation so that the perturbation on original samples is slight and imperceptible. Note that all the adversarial samples are also with minimal perturbations and not perceptible by human.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility at the level of individual mutation steps: it uses domain-specific, metamorphic transformations with conservative parameters (e.g., small speed/volume/noise changes for audio; small brightness/contrast/translation/rotation for images) and states that these perturbations are imperceptible. However, DeepStellar does not describe mechanisms that prevent or measure cumulative drift across multiple fuzzing/mutation iterations, nor does it provide empirical human evaluation or task-specific perceptual metrics (e.g., SSIM/LPIPS for images, PESQ/STOI for audio, or Perplexity for language) to validate that final fault-inducing inputs remain perceptually/semantically realistic. The coverage-guided testing algorithm (Algorithm 2) accepts mutants based on coverage increase alone, with no stated plausibility filter applied to the retained seeds; the paper therefore enforces plausibility in individual mutation choices but overlooks cumulative-effects and lacks quantitative or user-study evidence for final-input plausibility, matching the rubric for \u201cMedium.\u201d",
                    "evidence": "Section 5.2 Coverage-Guided Testing (p. 481): \"Mutation operators are often specific to the application domains. For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human\u2019s perspective during testing.\" (Algorithm 2 also shows mutants are retained when they increase coverage: \"Randomly pick transformation t with a random parameter p; A = mutate(t, p, a); ... else if CoverageIncrease(cov, Q) then Q <- Q \u222a a' .\")\n\nSection 6.1 Data Preparation (p. 482\u2013483): \"For each ASR model, we generate 10,000 perturbed audios from original benign ones with existing audio data augmentation techniques [5] (i.e., speed and volume adjustment, low/high-frequency filtering, noise blending).... For each MNIST model, we also generate 10,000 benign perturbed samples with existing image transformation techniques [50] (i.e., image contrast, brightness, translation, scaling, shearing, rotation and add white noise). ... For both audio and image case, we set conservative parameters for transformation so that the perturbation on original samples is slight and imperceptible. Note that all the adversarial samples are also with minimal perturbations and not perceptible by human.\"\n\nEvaluation sections (e.g., RQ3 & RQ4, pp. 484\u2013486) report detection AUROC, coverage numbers and counts of adversarial samples, but the paper does not report any human studies or task-specific perceptual/semantic quality metrics (e.g., LPIPS/SSIM for images, PESQ/STOI for audio, or Perplexity for language) to empirically validate final fault-inducing inputs' realism."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Applies deephunter metamorphic mutations that are plausibly I/O-stable but does not apply clipping and rounding to ensure"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate whether fault-inducing inputs transfer across different models. All algorithms and experiments are formulated and executed per-target-model (the abstract model M and the RNN D are paired), adversarial examples were generated for particular models only, and the authors explicitly note they could only generate adversarial samples for one DeepSpeech version due to tool compatibility. There are no experiments reusing inputs crafted on one model to test other models performing the same task, nor design elements aimed at discovering transferable attacks.",
                    "evidence": "Section 5.1 (page 481): \"input :D: RNN-based DL system, M: Abstract model of D\" and \"We first collect a set of benign samples B and a set of adversarial samples A.\" (Algorithm 1) \u2014 shows detection is trained per given RNN and its abstract model. Section 5.2 (page 482): \"input :I: Initial seeds, D: RNN-based DL system, M: Abstract model of D\" (Algorithm 2) \u2014 testing is guided against a single RNN instance D with its model M. Section 6.1 Data Preparation (page 483): \"Finally, we only successfully generate adversarial samples for DeepSpeech-0.1.1 because there exists a compatibility issue between DeepSpeech-0.3.0 and the adversarial attack tools [8] we used.\" \u2014 indicates adversarial samples were not produced for multiple models and no cross-model reuse/transfer experiments were performed. Section 6.4 (page 485): \"...we trained a classifier for each model to detect adversarial samples.\" \u2014 confirms evaluation is per-model, not testing transferability across models."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes methods for generating and detecting adversarial samples for Recurrent Neural Networks (RNNs). The core methodology involves creating a model-specific abstraction (a Discrete-Time Markov Chain or DTMC) based on the internal state transitions of a single, trained RNN. The evaluation is conducted on multiple models performing the same tasks (e.g., MNIST-LSTM and MNIST-GRU for image classification). However, the adversarial samples are always generated for a specific model and then evaluated on that same model. The paper does not perform any experiments where fault-inducing inputs generated for one model are reused to test for failures on another model. The methodology is inherently designed to analyze a single model at a time and lacks any mechanism to support the discovery of transferable failures.",
                    "evidence": "Page 7, Algorithm 2, \"Coverage guided testing of RNN\": The algorithm for generating adversarial samples takes a single \"RNN-based DL system, M: Abstract model of D\" as input. All subsequent steps within the testing loop, including prediction, coverage analysis, and failure checking, are performed with respect to this single model `D` and its abstract model `M`. This demonstrates the single-model focus of the generation technique.\n\nPage 10, Section 6.4, \"RQ3. Adversarial Sample Detection\": In the setup for evaluating the detection mechanism, the paper states: \"With the constructed dataset, we trained a classifier for each model to detect adversarial samples.\" This confirms that the detection process is model-specific and does not involve testing the transferability of adversarial examples between different models.\n\nPage 10, Section 6.5, \"RQ4. Coverage-guided Testing\": The evaluation of the coverage-guided testing is presented separately for MNIST-LSTM and MNIST-GRU in Table 6. The accompanying text describes the testing process for each model independently, without any mention of cross-testing the generated adversarial samples between the two models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate whether fault-inducing inputs transfer across different models. All algorithms and experiments are formulated and executed per-target-model (the abstract model M and the RNN D are paired), adversarial examples were generated for particular models only, and the authors explicitly note they could only generate adversarial samples for one DeepSpeech version due to tool compatibility. There are no experiments reusing inputs crafted on one model to test other models performing the same task, nor design elements aimed at discovering transferable attacks.",
                    "evidence": "Section 5.1 (page 481): \"input :D: RNN-based DL system, M: Abstract model of D\" and \"We first collect a set of benign samples B and a set of adversarial samples A.\" (Algorithm 1) \u2014 shows detection is trained per given RNN and its abstract model. Section 5.2 (page 482): \"input :I: Initial seeds, D: RNN-based DL system, M: Abstract model of D\" (Algorithm 2) \u2014 testing is guided against a single RNN instance D with its model M. Section 6.1 Data Preparation (page 483): \"Finally, we only successfully generate adversarial samples for DeepSpeech-0.1.1 because there exists a compatibility issue between DeepSpeech-0.3.0 and the adversarial attack tools [8] we used.\" \u2014 indicates adversarial samples were not produced for multiple models and no cross-model reuse/transfer experiments were performed. Section 6.4 (page 485): \"...we trained a classifier for each model to detect adversarial samples.\" \u2014 confirms evaluation is per-model, not testing transferability across models."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic",
                "Property-based"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ]
        }
    },
    "Huang et al. - 2022 - Coverage-Guided Testing for Recurrent Neural Networks": {
        "key": "huang2021coverage",
        "name": "TESTRNN",
        "year": "2021",
        "citation_count": "66",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": "Even though it can detect backdoor triggers, this is purely diagnostic. The fuzzer exposes low-level functional and robustness failures (misclassifications, metamorphic inconsistencies, and backdoor-induced activation anomalies) without breaching behavioral or safety boundaries."
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "broadly explore the model's internal behavior and flag any unexpected classification (adversarial or backdoor) as a defect."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Linnks faulty behaviors to internal LSTM mechanisms. Correlates coverage with faults. identify where and how the failures arise internally. The analysis is correlational, not causal. "
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Constraint bounded mutation but not evidence if naturalness measure of the generated inputs"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility in individual mutation steps using domain-specific, semantics-preserving mutation policies (e.g., EDA-based text mutations, SMILES enumeration for molecules) and by defining an oracle via norm-balls (r_oracle) intended to capture human-indistinguishability for continuous inputs. These design choices validate that many generated inputs remain realistic. However, the paper does not present any explicit mechanism to prevent cumulative drift across multiple mutation iterations (i.e., it does not enforce a global bound or filtering of plausibility across generations), nor does it report empirical human evaluation or task-specific perceptual/fluency metrics (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) to quantify the perceptual or semantic realism of the final fault-inducing inputs. Therefore the work enforces plausibility at the mutation-step / domain-rule level and validates plausibility conceptually, but overlooks cumulative effects and lacks quantitative or human-subjective evidence for final test cases.",
                    "evidence": "\u2022 Abstract (page 1191): \u201c...owing to its working with finer temporal semantics and the consideration of the naturalness of input perturbation.\u201d\n\n\u2022 Section VI-B, Mutation Policies \u2014 Random Mutation (page 1197): \u201cWhen the LSTM input has continuous values (e.g., image input), Gaussian noises with fixed mean and variance are added to the input. Meanwhile, for discrete value input (e.g., IMDB movie reviews for sentiment analysis), a set of problem-specific mutation functions M are defined.\u201d\n\n\u2022 Section VII-A, Experimental Setup \u2014 Input Mutation (page 1199): \u201cTo avoid this, we take a set M of mutants from the EDA toolkit [48], which was originally designed to augment the training data for improvement on text classification tasks. This ensures the mutated text paragraphs are always valid.\u201d\n\n\u2022 Section VI-C1, Test Oracle (page 1198): \u201cWe define a set of norm-balls, each of which is centered around a data sample with known label. The radius r_oracle of norm-balls intuitively means that a human cannot differentiate between inputs within a norm ball.\u201d\n\n\u2022 Section VII-A4, Oracle Setting (page 1199 / Table II discussion): \u201cNote, we let r_oracle = None for the Lipophilicity model, suggesting that no constraint is imposed on the norm ball.\u201d\n\n\u2022 Algorithm 1 (page 1197): shows orig mapping initialized for seeds and used only in the oracle check (detection): \u201corig[x] <- x for all x \u2208 T0\u201d and later \u201cif ||orig[x] - x'||2 \u2264 r_oracle, and \u03c6(orig[x]) \u2260 \u03c6(x') then Tadv \u2190 Tadv \u222a {x'}\u201d, indicating the paper checks distance to original seeds to label adversarial cases but does not describe rejecting or constraining candidates across successive mutation iterations to prevent cumulative drift.\n\n\u2022 Introduction / Motivation (page 1192): the authors assert their goal that generated cases be realistic: \u201c...our diversified test cases are 'closer' to their seeds (points on the RNN\u2019s data manifold), compared to other state-of-the-art tools, implying higher chance to be seen in the real-life operation, thus preserving the naturalness.\u201d\n\n\u2022 Absence of empirical perceptual/fluency evaluation: The paper does not report any human studies or quantitative perceptual/semantic measures (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) for assessing the realism or naturalness of final fault-inducing inputs (no such metrics or human-evaluation results appear in the Experimental Evaluation sections or tables)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility through bounded perturbations and domain-specific, rule-based mutation strategies designed to preserve semantic validity. For text-based inputs (IMDB dataset), it explicitly uses the EDA toolkit, which performs operations like synonym replacement to ensure the mutated text remains valid and meaningful. For molecular data (Lipophilicity dataset), mutations are constrained to different string representations of the same molecular structure. For continuous inputs like images (MNIST), it relies on bounded Gaussian noise and an L2-norm oracle, asserting that inputs within the norm-ball are indistinguishable to humans. The authors also provide a qualitative validation by comparing a 'natural' and comprehensible text sample generated by their tool against an incomprehensible one from a gradient-based attack (Figure 6). This approach aligns with the 'Medium' definition because it enforces plausibility through bounded/rule-based constraints and provides some validation of the final inputs. However, it does not provide a rigorous empirical evaluation of plausibility (e.g., via human studies or quantitative metrics like SSIM or Perplexity) and does not explicitly address the cumulative effects of mutations across iterations, especially for the continuous image and video domains.",
                    "evidence": "Page 9, Section VII-A, '3) Input Mutation': 'The input to IMDB model is a sequence of words, on which a random change may lead to an unrecognizable (and invalid) text paragraph. To avoid this, we take a set M of mutants from the EDA toolkit [48], which was originally designed to augment the training data for improvement on text classification tasks. This ensures the mutated text paragraphs are always valid.'\n\nPage 12, Section VII-E, 'E. Comparison With Attack-Based Defect Detection (RQ6)': 'It is easy to see that the adversarial review returned by the gradient attack is hard to comprehend, whereas the one from TESTRNN is much easier.' This is supported by the visual comparison in Figure 6.\n\nPage 7, Section VI-C, '1) Test Oracle': 'The radius roracle of norm-balls intuitively means that a human cannot differentiate between inputs within a norm ball. In this article, Euclidean distance, i.e., L2-norm || \u00b7 ||2 is used.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility in individual mutation steps using domain-specific, semantics-preserving mutation policies (e.g., EDA-based text mutations, SMILES enumeration for molecules) and by defining an oracle via norm-balls (r_oracle) intended to capture human-indistinguishability for continuous inputs. These design choices validate that many generated inputs remain realistic. However, the paper does not present any explicit mechanism to prevent cumulative drift across multiple mutation iterations (i.e., it does not enforce a global bound or filtering of plausibility across generations), nor does it report empirical human evaluation or task-specific perceptual/fluency metrics (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) to quantify the perceptual or semantic realism of the final fault-inducing inputs. Therefore the work enforces plausibility at the mutation-step / domain-rule level and validates plausibility conceptually, but overlooks cumulative effects and lacks quantitative or human-subjective evidence for final test cases.",
                    "evidence": "\u2022 Abstract (page 1191): \u201c...owing to its working with finer temporal semantics and the consideration of the naturalness of input perturbation.\u201d\n\n\u2022 Section VI-B, Mutation Policies \u2014 Random Mutation (page 1197): \u201cWhen the LSTM input has continuous values (e.g., image input), Gaussian noises with fixed mean and variance are added to the input. Meanwhile, for discrete value input (e.g., IMDB movie reviews for sentiment analysis), a set of problem-specific mutation functions M are defined.\u201d\n\n\u2022 Section VII-A, Experimental Setup \u2014 Input Mutation (page 1199): \u201cTo avoid this, we take a set M of mutants from the EDA toolkit [48], which was originally designed to augment the training data for improvement on text classification tasks. This ensures the mutated text paragraphs are always valid.\u201d\n\n\u2022 Section VI-C1, Test Oracle (page 1198): \u201cWe define a set of norm-balls, each of which is centered around a data sample with known label. The radius r_oracle of norm-balls intuitively means that a human cannot differentiate between inputs within a norm ball.\u201d\n\n\u2022 Section VII-A4, Oracle Setting (page 1199 / Table II discussion): \u201cNote, we let r_oracle = None for the Lipophilicity model, suggesting that no constraint is imposed on the norm ball.\u201d\n\n\u2022 Algorithm 1 (page 1197): shows orig mapping initialized for seeds and used only in the oracle check (detection): \u201corig[x] <- x for all x \u2208 T0\u201d and later \u201cif ||orig[x] - x'||2 \u2264 r_oracle, and \u03c6(orig[x]) \u2260 \u03c6(x') then Tadv \u2190 Tadv \u222a {x'}\u201d, indicating the paper checks distance to original seeds to label adversarial cases but does not describe rejecting or constraining candidates across successive mutation iterations to prevent cumulative drift.\n\n\u2022 Introduction / Motivation (page 1192): the authors assert their goal that generated cases be realistic: \u201c...our diversified test cases are 'closer' to their seeds (points on the RNN\u2019s data manifold), compared to other state-of-the-art tools, implying higher chance to be seen in the real-life operation, thus preserving the naturalness.\u201d\n\n\u2022 Absence of empirical perceptual/fluency evaluation: The paper does not report any human studies or quantitative perceptual/semantic measures (e.g., LPIPS, SSIM, PESQ, STOI, Perplexity) for assessing the realism or naturalness of final fault-inducing inputs (no such metrics or human-evaluation results appear in the Experimental Evaluation sections or tables)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": "Applies low level noise but does not enforce IO persistence with clipping/clamping"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. All test-case generation and evaluation are performed per-target model (seed inputs are mutated into tests for that same model), and the experiments measure adversarial/backdoor discovery rates per model rather than reusing fault-inducing inputs generated on a surrogate to test other models. The paper also does not describe any fuzzing design or mechanisms aimed at producing inputs intended to transfer across different model implementations performing the same task.",
                    "evidence": "Page 1197, Section VI (Algorithm 1 / Test generation): \"The test suite T is initialized with T0, a corpus of seed inputs (Line 1). New test cases are generated by mutating seed inputs. It keeps the traceability of test cases via a mapping orig that maps each test case generated back to its seed origin.\" \n\nPage 1201, Section VII-C (Detecting RNN Defects): \"As shown in Table V, adversarial samples rate and number of unique adversarial samples in the generated test suite are two important indicators for the robustness evaluation. ... For a set of trained models, we pursue the model, the test suite of which contains less amount of adversarial samples and unique adversarial samples.\" \n\nPage 1202\u20131203, Section VII-F (Comparison With State-of-the-Art Testing Methods): \"We start from 100 seeds drawn from training set and generate 100 000 test cases by DeepStellar and TESTRNN, respectively. The test suites are evaluated for the coverage rate and number of adversarial samples.\" \n\n(These quoted passages show the paper's testing/generation and evaluation are carried out per model/seed and report per-model adversarial/backdoor findings; the paper contains no experimental description or results where fault-inducing inputs crafted on one (surrogate) model are reused to test other models, nor any described mechanisms explicitly aimed at discovering transferable attacks.)"
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a coverage-guided testing method called TESTRNN, which is a white-box approach that generates test cases to maximize structural coverage for a specific RNN model under test. The methodology is designed to operate on a single model at a time. The experimental evaluation, including the comparison of model robustness in Table V, applies the test generation process independently to different models to compare their individual robustness characteristics (e.g., number of adversarial samples found). The paper does not contain any experiments where fault-inducing inputs generated for one model are used to test a different model to assess transferability. Furthermore, the test generation algorithm is guided by internal coverage metrics of the specific model being tested and does not incorporate any mechanisms designed to promote the discovery of transferable attacks.",
                    "evidence": "Page 11, Section VII-C, Table V and accompanying text: The paper evaluates the robustness of two different models per task (e.g., Model 1 and Model 2 for MNIST). However, the methodology described involves running the test generation process separately for each model to compare their individual vulnerability. The text explains the goal as comparing models: \"For a set of trained models, we pursue the model, the test suite of which contains less amount of adversarial samples and unique adversarial samples. For example, we pick up model 2 for Lipophilicity prediction, since the values of two indicators are way smaller than that of model 1.\" This indicates a comparison of the models' robustness via separate testing runs, not a test of attack transferability between them."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability. All test-case generation and evaluation are performed per-target model (seed inputs are mutated into tests for that same model), and the experiments measure adversarial/backdoor discovery rates per model rather than reusing fault-inducing inputs generated on a surrogate to test other models. The paper also does not describe any fuzzing design or mechanisms aimed at producing inputs intended to transfer across different model implementations performing the same task.",
                    "evidence": "Page 1197, Section VI (Algorithm 1 / Test generation): \"The test suite T is initialized with T0, a corpus of seed inputs (Line 1). New test cases are generated by mutating seed inputs. It keeps the traceability of test cases via a mapping orig that maps each test case generated back to its seed origin.\" \n\nPage 1201, Section VII-C (Detecting RNN Defects): \"As shown in Table V, adversarial samples rate and number of unique adversarial samples in the generated test suite are two important indicators for the robustness evaluation. ... For a set of trained models, we pursue the model, the test suite of which contains less amount of adversarial samples and unique adversarial samples.\" \n\nPage 1202\u20131203, Section VII-F (Comparison With State-of-the-Art Testing Methods): \"We start from 100 seeds drawn from training set and generate 100 000 test cases by DeepStellar and TESTRNN, respectively. The test suites are evaluated for the coverage rate and number of adversarial samples.\" \n\n(These quoted passages show the paper's testing/generation and evaluation are carried out per model/seed and report per-model adversarial/backdoor findings; the paper contains no experimental description or results where fault-inducing inputs crafted on one (surrogate) model are reused to test other models, nor any described mechanisms explicitly aimed at discovering transferable attacks.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Oracle-guided"
            ]
        }
    },
    "Li et al. - 2021 - Testing DNN-based Autonomous Driving Systems under Critical Environmental Conditions": {
        "key": "li2021testing",
        "name": "TACTIC",
        "year": "2021",
        "citation_count": "51",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "Finds inconsistent steering under environmental variations"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Relies on coverage-guided mutation and assumes based on prior work, that higher coverage correlates with fault discovery"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": "Test images are generated through a GAN trained to mimic real driving scenes, so naturalness is preserved throughout fuzzing iterations. Provides human evaluation."
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper enforces input plausibility by performing all mutations/search inside a learned style space produced by a MUNIT image-to-image translation model: style vectors sampled or evolved from that space are decoded by the generator to produce test images, so mutated inputs remain drawn from a distribution of real environmental conditions captured by data. The authors also provide empirical human-evaluation evidence that the final synthesized, fault-inducing driving scenes are perceptually realistic (a user study with 34 participants showing TACTIC images are judged realistic and correctly classified to their environmental types). Together, these show (1) plausibility is enforced throughout the fuzzing/search process by constraining search to the MUNIT style space and (2) final fault-inducing inputs were validated as realistic via a user study.",
                    "evidence": "Page 1, Introduction: \"Because of the richness of styles, the synthesised scenes are realistic enough (as shown by Figure 1(d)) to test whether the DNN-based ADS can perform correctly under the environmental conditions corresponding to the obtained styles.\" \n\nPage 3, Section 3.1 (Environmental Conditions): \"To encode the environmental condition space of an environmental type, we leverage MUNIT ... One insight of MUNIT is that the representation of images can be decomposed into a content space that is domain-invariant and a style space that captures domain-specific characteristics ... we can use the style space of an environmental type in a well-trained MUNIT model to represent the environmental condition space of the type.\" \n\nPage 4, Section 3.3 (Search of Critical Environmental Conditions): \"The (1+1) ES is initialised with an initial individual, which is a style vector randomly sampled from the style space in the MUNIT model. ... a new individual s' is firstly generated based on the current individual s. Then, the fitness values of s and s' is evaluated ... the individual with higher fitness value is kept.\" \n\nPage 6, Section 4.3 (Comparison with Baselines on Image Quality / User Study): \"we conduct a user study to evaluate the realism of the testing driving scenes synthesised by TACTIC ... We received answers from a total of 34 participants ... The results demonstrate that the testing driving scenes synthesised by TACTIC are more realistic than the ones synthesised by affine image transformation (e.g., DeepTest) or high-fidelity simulation (e.g., PreScan) and can better reflect real environments.\""
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's proposed method, TACTIC, is explicitly designed to generate plausible and realistic test inputs. It uses a GAN-based image-to-image translation model (MUNIT) trained on real-world images of various environmental conditions, which inherently constrains the generated outputs to a plausible distribution. The paper goes further and provides direct empirical evidence of this plausibility through a dedicated user study. This study compares the realism of scenes generated by TACTIC against real-world images and other baseline methods. The results show that a significant portion of TACTIC's generated images were considered as realistic as real-world images by human participants. This combination of using a plausibility-aware generation mechanism and empirically validating the realism of the final fault-inducing inputs with a human study directly meets the criteria for a 'High' rating.",
                    "evidence": "Page 8, Section 4.3, 'Comparison with Baselines on Image Quality': 'We conduct a user study to evaluate the realism of the testing driving scenes synthesised by TACTIC... Specifically, we design an online questionnaire consisting of two questions: (1) \u201cWhich driving scene is more realistic?\u201d, and (2) \u201cWhich environmental type does the driving scene belong to?\u201d.'\nPage 8, Section 4.3, Results: 'The results demonstrate that the testing driving scenes synthesised by TACTIC are more realistic than the ones synthesised by affine image transformation (e.g., DeepTest) or high-fidelity simulation (e.g., PreScan) and can better reflect real environments.'\nPage 9, Figure 5 and accompanying text: The results of the user study are presented, showing that for 44% of image pairs (27% 'both realistic' + 16% 'synthesized scene' + 1% 'both unrealistic' is likely a typo in my reading, the text says 44% for TACTIC and DeepRoad where synthesized are not distinguishable from real ones), TACTIC's synthesized scenes were considered as realistic or more realistic than real-world scenes, a stark contrast to DeepTest (6%) and PreScan (14%)."
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper enforces input plausibility by performing all mutations/search inside a learned style space produced by a MUNIT image-to-image translation model: style vectors sampled or evolved from that space are decoded by the generator to produce test images, so mutated inputs remain drawn from a distribution of real environmental conditions captured by data. The authors also provide empirical human-evaluation evidence that the final synthesized, fault-inducing driving scenes are perceptually realistic (a user study with 34 participants showing TACTIC images are judged realistic and correctly classified to their environmental types). Together, these show (1) plausibility is enforced throughout the fuzzing/search process by constraining search to the MUNIT style space and (2) final fault-inducing inputs were validated as realistic via a user study.",
                    "evidence": "Page 1, Introduction: \"Because of the richness of styles, the synthesised scenes are realistic enough (as shown by Figure 1(d)) to test whether the DNN-based ADS can perform correctly under the environmental conditions corresponding to the obtained styles.\" \n\nPage 3, Section 3.1 (Environmental Conditions): \"To encode the environmental condition space of an environmental type, we leverage MUNIT ... One insight of MUNIT is that the representation of images can be decomposed into a content space that is domain-invariant and a style space that captures domain-specific characteristics ... we can use the style space of an environmental type in a well-trained MUNIT model to represent the environmental condition space of the type.\" \n\nPage 4, Section 3.3 (Search of Critical Environmental Conditions): \"The (1+1) ES is initialised with an initial individual, which is a style vector randomly sampled from the style space in the MUNIT model. ... a new individual s' is firstly generated based on the current individual s. Then, the fitness values of s and s' is evaluated ... the individual with higher fitness value is kept.\" \n\nPage 6, Section 4.3 (Comparison with Baselines on Image Quality / User Study): \"we conduct a user study to evaluate the realism of the testing driving scenes synthesised by TACTIC ... We received answers from a total of 34 participants ... The results demonstrate that the testing driving scenes synthesised by TACTIC are more realistic than the ones synthesised by affine image transformation (e.g., DeepTest) or high-fidelity simulation (e.g., PreScan) and can better reflect real environments.\""
                },
                "manual": {
                    "value": "High",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Produced synthesized images"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate transferability of fault-inducing inputs across models. TACTIC is defined and executed per subject model (the search takes the subject DNN as input and returns critical conditions for that model), and all experiments generate and evaluate critical styles separately for each ADS. There is no experiment that crafts fault-inducing inputs on one surrogate model and reuses them to test other models, nor does the design include explicit mechanisms aimed at finding transferable attacks across different model implementations.",
                    "evidence": "Page 3, Section 3.3 \"Search of Critical Environmental Conditions\": \"TACTIC receives as inputs the subject DNN-based ADS N, the MUNIT model M trained for the given environmental type, and a set Io of the original driving scenes. The output of TACTIC is a set S of the critical environmental conditions of the given environmental type.\"; Page 4, Section 4.2 \"Comparison with Baselines on Effectiveness - Setup\": \"We execute TACTIC with two coverage-guiding strategies: KMNC (denoted as TACTICKMNC) and NBC (denoted as TACTICNBC), respectively, on each of the three subject DNN-based ADSs, under the five environmental types.\"; Page 4, Section 4.2 \"Setup\" (describing baselines): \"For Rc, when testing the three subject systems, we randomly sample 4 style vectors (equal to the number of critical style vectors generated by TACTIC) for each of the environmental types and separately apply the 4 random style vectors on the Udacity testing dataset to synthesise testing driving scenes...\""
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper proposes a method, TACTIC, to identify critical environmental conditions for a specific, or 'subject', Deep Neural Network (DNN) based Autonomous Driving System (ADS). The search for these conditions is guided by model-specific metrics, such as neuron coverage and steering angle divergence, which are calculated on the subject model. The experimental evaluation involves three different ADS models (Dave-orig, Dave-dropout, Chauffeur). However, the TACTIC framework is executed independently for each model to find its specific critical conditions. The paper does not contain any experiments or discussion on whether the fault-inducing environmental conditions discovered for one model also cause erroneous behavior in the other models. Therefore, it does not demonstrate the transferability of the discovered failures.",
                    "evidence": "Page 6, Section 3.3, \"Search of Critical Environmental Conditions\": \"TACTIC receives as inputs the subject DNN-based ADS N, the MUNIT model M trained for the given environmental type, and a set Io of the original driving scenes.\" This statement clarifies that the method is designed to operate on a single 'subject' model 'N' at a time. \n\nPage 7, Section 4.2, \"Comparison with Baselines on Effectiveness\", paragraph \"Setup\": \"We execute TACTIC with two coverage-guiding strategies: KMNC (denoted as TACTIC KMNC ) and NBC (denoted as TACTIC NBC ), respectively, on each of the three subject DNN-based ADSs, under the five environmental types.\" This shows that the experiments were run separately for each of the three models, rather than generating inputs on one and testing them on others. The results are then presented independently for each model, confirming the lack of a transferability analysis."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or evaluate transferability of fault-inducing inputs across models. TACTIC is defined and executed per subject model (the search takes the subject DNN as input and returns critical conditions for that model), and all experiments generate and evaluate critical styles separately for each ADS. There is no experiment that crafts fault-inducing inputs on one surrogate model and reuses them to test other models, nor does the design include explicit mechanisms aimed at finding transferable attacks across different model implementations.",
                    "evidence": "Page 3, Section 3.3 \"Search of Critical Environmental Conditions\": \"TACTIC receives as inputs the subject DNN-based ADS N, the MUNIT model M trained for the given environmental type, and a set Io of the original driving scenes. The output of TACTIC is a set S of the critical environmental conditions of the given environmental type.\"; Page 4, Section 4.2 \"Comparison with Baselines on Effectiveness - Setup\": \"We execute TACTIC with two coverage-guiding strategies: KMNC (denoted as TACTICKMNC) and NBC (denoted as TACTICNBC), respectively, on each of the three subject DNN-based ADSs, under the five environmental types.\"; Page 4, Section 4.2 \"Setup\" (describing baselines): \"For Rc, when testing the three subject systems, we randomly sample 4 style vectors (equal to the number of critical style vectors generated by TACTIC) for each of the environmental types and separately apply the 4 random style vectors on the Udacity testing dataset to synthesise testing driving scenes...\""
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Generative Synthesized"
            ],
            "exploration_strategy": [
                "Coverage-guided",
                "Oracle-guided"
            ]
        }
    },
    "Wang et al. - 2023 - DistXplore Distribution-Guided Testing for Evaluating and Enhancing Deep Learning Systems": {
        "key": "wang2023distxplore",
        "name": "DistXplore",
        "year": "2023",
        "citation_count": "13",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "High",
                    "why": "Guides test generation to minimize distribution differences between source class inputs and target class."
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "links the occurrence and detectability of errors to the statistical closeness (measured via MMD) between the distribution of generated test inputs and the distribution of another class at the logits layer."
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "metamorphic transformations can yield decimal-valued pixels internally which may change when re-quantized to integers."
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability across models. DistXplore formulates and optimizes test suites using distributions measured on a target model's internal layer outputs (the logits) and the evaluation and robustness enhancement steps retrain or test the same model on which test suites were generated. The experiments run the method separately on multiple models/datasets, but the paper does not reuse fault-inducing inputs crafted on one (surrogate) model to test other models, nor does it describe any mechanism intended to discover or promote attacks that transfer across different model implementations.",
                    "evidence": "\u2022 Section 3.1.1 (page 71, \"Model Evaluation\"): \"The distribution difference is measured on a specific layer of the DNN. In this paper, we select the logits layer, i.e., the layer before the softmax layer...\" (shows method depends on internal layer outputs of the target model).\n\n\u2022 Section 4.1.4 (page 73, \"Configuration of DistXplore\"): \"For each class, we run DistXplore multiple times ... Finally, for each model, we run DistXplore 90 times (i.e., 10 source classes \u00d7 9 target classes).\" (shows runs are per-model; no cross-model reuse is described).\n\n\u2022 Section 4.2.3 (page 75, \"RQ3: Robustness Enhancement\"): \"For each tool, we fine-tune the original model 20 epochs ... by adding the new data generated from each tool, and evaluate the empirical robustness of the new model on the validation dataset.\" (shows generated tests are used to retrain/evaluate the same model rather than testing other models).\n\n\u2022 Section 3.1.2 (page 71, \"Model Enhancement\"): description of target selection and distribution diversity refers to selecting other classes as targets (inter-class guidance), not other models or surrogate-model attacks: \"We select the classes in the same task as targets because the classification is based on their relationships...\".\n\nThere is no text in the paper that (a) states that fault-inducing inputs generated on one model were reused to test other models, (b) measures cross-model transfer of discovered faults, or (c) proposes mechanisms specifically designed to produce inputs intended to transfer across different model implementations."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper's methodology and evaluation focus exclusively on generating fault-inducing inputs for a single model under test and then evaluating the properties of those inputs (e.g., their ability to bypass defenses) on that same model. The concept of attack transferability\u2014testing if inputs that cause faults on one model also cause faults on another model performing the same task\u2014is never discussed or evaluated. The experimental setup, described in Section 4, details procedures for generating errors, evaluating their strength against defenses, and using them for retraining. All these steps are performed within the context of the specific model for which the errors were generated. There is no mention of reusing these inputs to test other models.",
                    "evidence": "Page 7, Section 4.1.4, Experiment Setup, RQ Setup: \"To demonstrate the capability of DistXplore in generating strong errors for model evaluation (RQ1), we collect the test suite in the last iteration for every pair (c, c') (i.e., the best chromosome returns from Algo 1). For each model, we collect a total number of 90 chromosomes over 90 pairs, which are used to evaluate the strength of these errors. The strength of errors is measured by the success rate of bypassing defenses.\"\n\nThis quote illustrates that the evaluation is conducted on a per-model basis, where errors are generated for a specific model and their strength is evaluated against defenses on that same model, without assessing transferability to other models."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not evaluate or demonstrate attack transferability across models. DistXplore formulates and optimizes test suites using distributions measured on a target model's internal layer outputs (the logits) and the evaluation and robustness enhancement steps retrain or test the same model on which test suites were generated. The experiments run the method separately on multiple models/datasets, but the paper does not reuse fault-inducing inputs crafted on one (surrogate) model to test other models, nor does it describe any mechanism intended to discover or promote attacks that transfer across different model implementations.",
                    "evidence": "\u2022 Section 3.1.1 (page 71, \"Model Evaluation\"): \"The distribution difference is measured on a specific layer of the DNN. In this paper, we select the logits layer, i.e., the layer before the softmax layer...\" (shows method depends on internal layer outputs of the target model).\n\n\u2022 Section 4.1.4 (page 73, \"Configuration of DistXplore\"): \"For each class, we run DistXplore multiple times ... Finally, for each model, we run DistXplore 90 times (i.e., 10 source classes \u00d7 9 target classes).\" (shows runs are per-model; no cross-model reuse is described).\n\n\u2022 Section 4.2.3 (page 75, \"RQ3: Robustness Enhancement\"): \"For each tool, we fine-tune the original model 20 epochs ... by adding the new data generated from each tool, and evaluate the empirical robustness of the new model on the validation dataset.\" (shows generated tests are used to retrain/evaluate the same model rather than testing other models).\n\n\u2022 Section 3.1.2 (page 71, \"Model Enhancement\"): description of target selection and distribution diversity refers to selecting other classes as targets (inter-class guidance), not other models or surrogate-model attacks: \"We select the classes in the same task as targets because the classification is based on their relationships...\".\n\nThere is no text in the paper that (a) states that fault-inducing inputs generated on one model were reused to test other models, (b) measures cross-model transfer of discovered faults, or (c) proposes mechanisms specifically designed to produce inputs intended to transfer across different model implementations."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Does not explicitly enforce naturalness across fuzzing iterations. Apply norm constraints in each mutation step but cumulative effect is ignored. Checks visual validity on the final outputs via human evaluation."
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility via bounded, rule-based mutation constraints (they adopt conservative mutation bounds and use Lp-norm validity constraints) and additionally reports an explicit human validation of final fault-inducing inputs (manual inspection with high \u2018validity ratio\u2019). However, the paper does not report quantitative perceptual metrics (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) nor an explicit analysis of cumulative drift across many fuzzing iterations (it relies on per-mutation constraints and conservative configuration but does not measure or bound accumulated drift over iterations). This matches the 'Medium' level: plausibility of final inputs is validated, and individual mutation steps are constrained, but cumulative-effects across iterations are not explicitly analyzed or controlled.",
                    "evidence": "Page 70, Section 2.1.2 (Data Validity): \"In practice, the Lp norm [40] is usually used to guarantee the validity of the generated data by the existing DL testing and adversarial attack techniques. Specifically, given a valid input x, the new test case x' generated by adding some perturbations on x is considered as valid if ||x' - x|| < \u03b5, where \u03b5 is a safe radius.\"; Page 72, Section 3.2 (Distribution-Guided Test Generation): \"To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation with both 0 and \u221e.\"; Page 76, Section 4.2.2 (RQ2: Efficiency of DistXplore): \"In order to evaluate the validity of the generated inputs, we conducted a manual investigation by randomly selecting 10,000 erroneous inputs from the testing outputs of each model and calculating the average validity ratio. The validity ratios were found to be 98.5%+, 96.5%+, 98.7%+, and 95.3%+ for MNIST, Fashion\u2011MNIST, CIFAR\u201110, and SVHN datasets, respectively.\"; (No mention in the paper of perceptual/semantic quantitative metrics such as LPIPS, SSIM, PESQ, STOI or Perplexity, nor of an explicit assessment of cumulative mutation drift across iterations.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper satisfies the criteria for 'Medium' by both enforcing plausibility in individual mutation steps and validating the plausibility of the final generated inputs. Plausibility is enforced by adopting a 'conservative strategy' that constrains image transformations using both L0 and L\u221e norms, which aligns with the 'bounded or rule-based constraints' mentioned in the metric definition. However, the paper does not discuss or address the potential for cumulative drift over multiple iterations. Crucially, the authors explicitly validate the plausibility of the final fault-inducing inputs through a large-scale manual investigation (a form of human study), reporting high validity ratios across multiple datasets. This direct assessment of the final outputs clearly meets the 'Medium' criteria. The method does not qualify for 'High' because it does not demonstrate a mechanism to ensure inputs remain within a plausible distribution *throughout* the entire fuzzing process to prevent cumulative drift.",
                    "evidence": "Page 6, Section 3.2, \"Distribution-Guided Test Generation\": \"To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation with both L0 and L\u221e.\"\n\nPage 10, Section 4.2.2, \"RQ2: Efficiency of DistXplore\": \"In order to evaluate the validity of the generated inputs, we conducted a manual investigation by randomly selecting 10,000 erroneous inputs from the testing outputs of each model and calculating the average validity ratio. The validity ratios were found to be 98.5%+, 96.5%+, 98.7%+, and 95.3%+ for MNIST, Fashion-MNIST, CIFAR-10, and SVHN datasets, respectively. The results demonstrate that DistXplore is capable of generating valid inputs with high proportions.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility via bounded, rule-based mutation constraints (they adopt conservative mutation bounds and use Lp-norm validity constraints) and additionally reports an explicit human validation of final fault-inducing inputs (manual inspection with high \u2018validity ratio\u2019). However, the paper does not report quantitative perceptual metrics (e.g., LPIPS/SSIM/PESQ/STOI/Perplexity) nor an explicit analysis of cumulative drift across many fuzzing iterations (it relies on per-mutation constraints and conservative configuration but does not measure or bound accumulated drift over iterations). This matches the 'Medium' level: plausibility of final inputs is validated, and individual mutation steps are constrained, but cumulative-effects across iterations are not explicitly analyzed or controlled.",
                    "evidence": "Page 70, Section 2.1.2 (Data Validity): \"In practice, the Lp norm [40] is usually used to guarantee the validity of the generated data by the existing DL testing and adversarial attack techniques. Specifically, given a valid input x, the new test case x' generated by adding some perturbations on x is considered as valid if ||x' - x|| < \u03b5, where \u03b5 is a safe radius.\"; Page 72, Section 3.2 (Distribution-Guided Test Generation): \"To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation with both 0 and \u221e.\"; Page 76, Section 4.2.2 (RQ2: Efficiency of DistXplore): \"In order to evaluate the validity of the generated inputs, we conducted a manual investigation by randomly selecting 10,000 erroneous inputs from the testing outputs of each model and calculating the average validity ratio. The validity ratios were found to be 98.5%+, 96.5%+, 98.7%+, and 95.3%+ for MNIST, Fashion\u2011MNIST, CIFAR\u201110, and SVHN datasets, respectively.\"; (No mention in the paper of perceptual/semantic quantitative metrics such as LPIPS, SSIM, PESQ, STOI or Perplexity, nor of an explicit assessment of cumulative mutation drift across iterations.)"
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Greybox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Coverage-guided"
            ]
        }
    },
    "Liu et al. - 2022 - QATest A Uniform Fuzzing Framework for Question Answering Systems": {
        "key": "liu2022qatest",
        "name": "QATest",
        "year": "2022",
        "citation_count": "21",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": ""
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "High",
                    "why": ""
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility at individual mutation steps and uses automatic, task-relevant measures to prefer authentic candidates (ROUGE-1 quality check and a POS-based ``perplexity priority'' used to select/retain seeds). It also implements realistic transformation operators (back-translation, BERT insertion, WordNet synonyms, entity-alias substitution, OCR/keyboard typo simulation). However, the work does not provide an explicit human evaluation or dedicated empirical assessment of the final fault-inducing inputs\u2019 semantic or perceptual realism, and it does not present a formal mechanism that strictly bounds cumulative drift across many iterative mutations (rather, it relies on selection heuristics such as perplexity priority and ROUGE threshold). These properties match the metric\u2019s \u201cMedium\u201d level: per-step plausibility checks and quantitative plausibility heuristics are present, but cumulative-iteration effects are not strictly enforced nor validated by human study or external fluency/quality evaluations of final failing inputs.",
                    "evidence": "Section 3.1 / Algorithm 1 (p.3): \"quality_score \u2190 Rouge1(s, s'); if quality_score > \u03c4 then // acceptable ... D.append(s')\" \u2014 shows ROUGE-1 is used as a per-generation quality filter.\n\nSection 3.2.5 Quality Assessment (p.3): \"To ensure the quality of generated questions, we employ the ROUGE-1 metric [35] to evaluate the questions generated by metamorphic transformations.\" (ROUGE used to discard low-quality generations).\n\nSection 3.3.2 Perplexity Priority (p.4): \"To ensure that the generated test questions have good authenticity and rationality, we consider applying perplexity priority as guidance for seed data retention... per(q) = p(seq(q))^{-1/n}. In each iteration, QATest selects the generated question with the lowest perplexity score in a generated batch.\" \u2014 shows a quantitative authenticity/prioritization metric used during fuzzing.\n\nSection 4.3 Sentence Transformations (p.6): \"...the back translation operator also employs the pre-trained language model... For the entity alias substitution operator, we apply ... TagMe... For each transformation, if the generated question is exactly the same as the original question, it is considered a failed transformation...\" \u2014 shows use of realistic NLP methods and sensible discard rules for trivial/failed transformations.\n\nSection 6.3 Threats to Validity (p.9): \"...it is impossible to guarantee the reliability of the generated question without manual inspection. To ensure the quality of the generated questions, most of the transformations are implemented with pre-trained language models ... Besides, we apply the existing ROUGE metric to evaluate the quality of the generated data and discard the low-quality generated questions directly.\" \u2014 acknowledges no manual/human evaluation and that quality control is via automatic metrics rather than a human study or external perceptual/fluency evaluation of final failing inputs."
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper enforces input plausibility throughout the fuzzing process using two distinct, explicit mechanisms. First, after each mutation, a quality assessment is performed using the ROUGE-1 metric to measure the similarity between the original and transformed question. Generated questions that fall below a certain similarity threshold are discarded, ensuring that individual mutation steps do not drastically alter the input's semantic meaning. Second, to address the cumulative effect of mutations across iterations, the framework uses a 'perplexity priority' criterion to guide the selection of new seeds. This mechanism prioritizes generated questions that are more linguistically fluent and authentic (i.e., have lower perplexity) to be added back into the seed pool for further fuzzing. The combination of ROUGE-based filtering and perplexity-guided seed selection ensures that the generated inputs remain within a plausible distribution throughout the process. These quantitative, task-specific measures (ROUGE for similarity, perplexity for fluency) serve as the empirical evidence that the framework maintains realistic inputs, as required by the metric definition.",
                    "evidence": "1. Page 5, Section 3.2.5, Quality Assessment: \"To ensure the quality of generated questions, we employ the ROUGE-1 metric [35] to evaluate the questions generated by metamorphic transformations.\" \n2. Page 4, Algorithm 1: The algorithm shows that generated questions are only kept if their ROUGE score is above a threshold: \"if quality_score > \u03c4 then // acceptable D.append(s');\"\n3. Page 6, Section 3.3.2, Perplexity Priority: \"To ensure that the generated test questions have good authenticity and rationality, we consider applying perplexity priority as guidance for seed data retention.\"\n4. Page 7, Definition 3.5 (Perplexity priority): The framework selects the question with the lowest perplexity to feed back into the seed set: \"s** = arg min per(q) q\u2208G\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "QATest incorporates per\u2010step plausibility checks\u2014filtering each mutated question by a ROUGE\u20101 similarity threshold and prioritizing seeds with low perplexity to encourage fluent, realistic inputs\u2014but it does not formally bound cumulative drift across multiple iterations nor does it perform an explicit human or task\u2010specific quantitative evaluation of the final fault\u2010inducing inputs. Thus, while individual mutation steps are plausibility\u2010constrained, the framework relies on heuristics rather than a strict mechanism or empirical validation to ensure that deeply mutated inputs remain within the realistic question distribution, which aligns with the \u201cMedium\u201d level.",
                    "evidence": "Section 3.2.5 (Quality Assessment): uses ROUGE\u20101 to discard low\u2010quality transformations on each step\nSection 3.3.2 (Perplexity Priority): guides seed retention by selecting the lowest\u2010perplexity (most fluent) questions\nSection 6.3 (Threats to Validity): acknowledges absence of manual inspection or human study and reliance on automatic metrics rather than external plausibility validation of final failing inputs"
                },
                "manual": {
                    "value": "Medium",
                    "why": "Checks quality drift between successive mutations but overlooks drift across many iterations",
                    "evidence": ""
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "No cross model failure pattern analysis"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper evaluates QATest by applying its metamorphic transformations and guidance criteria separately to multiple QA systems (UnifiedQA, ALBERT, DrQA, MARL), but it does not present any experiments in which fault-inducing inputs crafted on one model are reused to test other models, nor does it describe design elements intended to discover or prioritize transferable attacks across models. The testing guidance (N-Gram coverage and perplexity priority) and seed selection are aimed at diversity and authenticity of generated questions, not at identifying inputs that transfer across different model implementations. Consequently, the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "1) Abstract (page 1, Abstract): \"To evaluate the performance of QATest, we experiment with it on four QA systems that are designed for different tasks. The experiment results show that the tests generated by QATest detect hundreds of erroneous behaviors of QA systems efficiently.\" \u2014 indicates per-system evaluation but does not claim cross-model reuse. \n\n2) Section 4.1 (page 7, \"Research Questions\"): The four RQs listed are RQ1 (Metamorphic Transformations), RQ2 (Testing Criteria), RQ3 (Test Diversity), and RQ4 (Case Study). None ask about or evaluate transferability across models: \"RQ1 (Metamorphic Transformations): How effective are different transformation methods for generating tests for various types of QA systems?... RQ4 (Case Study): Analysis of the bug types detected by QATest.\" \n\n3) Section 4.2.2 (page 8, \"QA systems\"): \"To evaluate the performance of QATest, we select four state-of-art QA systems as the testing subjects in our experiments. We briefly introduce them as follows: UnifiedQA ..., ALBERT ..., DrQA ..., MARL ...\" \u2014 shows multiple target systems but no description of crafting faults on one and reusing them on others. \n\n4) Section 3.3 (page 4, \"Guidance Criteria Design\") and Section 3.3.1/3.3.2 (N-Gram Coverage / Perplexity Priority): These criteria are defined to improve test diversity and authenticity (e.g., \"N-Gram coverage... select the particular question that can increase the richness of the seed set\" and \"perplexity priority... to ensure that the generated test questions have good authenticity\"), with no mention of mechanisms to identify or prioritize inputs likely to transfer across models."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper does not investigate whether the fault-inducing inputs generated for one model transfer to other models. The evaluation is conducted on four distinct Question Answering (QA) systems, but each is treated as an independent target. The experiments generate test cases from a seed dataset and apply them to a corresponding QA system to find failures. There is no mention of reusing the failure-inducing questions found on one model (e.g., ALBERT) to test another model (e.g., UnifiedQA). The research questions and experimental results focus on the effectiveness, efficiency, and diversity of the fuzzing framework for individual systems. The design of the fuzzer, including its guidance criteria (N-Gram coverage and perplexity priority), is aimed at improving bug detection on a single target rather than discovering shared vulnerabilities or transferable attacks across models.",
                    "evidence": "Page 8, Section 5.1, \"Answer to RQ1: Metamorphic Transformations\": \"To evaluate the effectiveness of each type of MR applied by QATest, we select 500 test cases from each data set and apply the MRs proposed in Section 3.2 to generate test data sets. Then, we record the number of failed tests in the generated test set on the corresponding QA systems, which is shown in Table 3.\" This statement describes an experimental process where each QA system is tested independently. The results presented in Tables 3, 4, and 5 consistently show separate evaluations for each system, with no cross-testing of generated inputs."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper evaluates QATest by applying its metamorphic transformations and guidance criteria separately to multiple QA systems (UnifiedQA, ALBERT, DrQA, MARL), but it does not present any experiments in which fault-inducing inputs crafted on one model are reused to test other models, nor does it describe design elements intended to discover or prioritize transferable attacks across models. The testing guidance (N-Gram coverage and perplexity priority) and seed selection are aimed at diversity and authenticity of generated questions, not at identifying inputs that transfer across different model implementations. Consequently, the paper does not demonstrate attack transferability as defined by the metric.",
                    "evidence": "1) Abstract (page 1, Abstract): \"To evaluate the performance of QATest, we experiment with it on four QA systems that are designed for different tasks. The experiment results show that the tests generated by QATest detect hundreds of erroneous behaviors of QA systems efficiently.\" \u2014 indicates per-system evaluation but does not claim cross-model reuse. \n\n2) Section 4.1 (page 7, \"Research Questions\"): The four RQs listed are RQ1 (Metamorphic Transformations), RQ2 (Testing Criteria), RQ3 (Test Diversity), and RQ4 (Case Study). None ask about or evaluate transferability across models: \"RQ1 (Metamorphic Transformations): How effective are different transformation methods for generating tests for various types of QA systems?... RQ4 (Case Study): Analysis of the bug types detected by QATest.\" \n\n3) Section 4.2.2 (page 8, \"QA systems\"): \"To evaluate the performance of QATest, we select four state-of-art QA systems as the testing subjects in our experiments. We briefly introduce them as follows: UnifiedQA ..., ALBERT ..., DrQA ..., MARL ...\" \u2014 shows multiple target systems but no description of crafting faults on one and reusing them on others. \n\n4) Section 3.3 (page 4, \"Guidance Criteria Design\") and Section 3.3.1/3.3.2 (N-Gram Coverage / Perplexity Priority): These criteria are defined to improve test diversity and authenticity (e.g., \"N-Gram coverage... select the particular question that can increase the richness of the seed set\" and \"perplexity priority... to ensure that the generated test questions have good authenticity\"), with no mention of mechanisms to identify or prioritize inputs likely to transfer across models."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Text level mutation "
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Data-driven"
            ]
        }
    },
    "Wang et al. - 2022 - BET black-box efficient testing for convolutional neural networks": {
        "key": "wang2022bet",
        "name": "BET",
        "year": "2022",
        "citation_count": "26",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Low",
                    "why": ""
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "High",
                    "why": ""
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Low",
                    "why": "Only reports failures without any insight on root-cause"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "norm-bounded"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces bounded, input-centric perturbations throughout its mutator design and explicitly uses an L_infty constraint (L\u221e = 13/255) for all methods, and the mutator logic ensures each pixel is toggled only between original \u00b1 \u03b5 (so cumulative drift beyond the bound is prevented). These design choices constrain mutations to visually small changes and maintain per-pixel plausibility. However, the paper does not provide any empirical assessment of the perceptual or semantic realism of the final fault-inducing inputs (no human study or task-specific perceptual/fluency metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity are reported). According to the metric definition, this corresponds to Medium: plausibility is enforced in mutation design (bounded/rule-based), but the paper omits explicit plausibility validation of final inputs.",
                    "evidence": "Section 4.1 Hyperparameters (page 171): \"We use the widely used L\u221e metric [4] to restrict perturbations for error-inducing inputs on all compared methods... i.e., L\u221e = 13/255 on all datasets (all compared methods use the same setting).\"; Section 3.5 Mutator, \"Perturbation values\" (page 169): \"...we limit the perturbations to each byte to two choices {+\u03b5, \u2212\u03b5}... Given an original input Eo, a current best test case Eb... if Eb[i] = Eo[i], we randomly add perturbation value of +\u03b5 or \u2212\u03b5 to this byte. If Eb[i] = Eo[i] \u2212 \u03b5, we change this byte to Eo[i] + \u03b5. Otherwise, Eb[i] = Eo[i] + \u03b5 must hold and we change this byte to Eo[i] \u2212 \u03b5.\" (shows per-pixel changes remain within \u00b1\u03b5 of original); Section 4.3 Result of Inact-Rate (page 172): \"The Inact-Rate of BET is 0.0%. The perturbations are directly added on pixel values as integers, BET does not suffer the inactivation issue.\" (confirms direct integer pixel edits and bounded changes); Section 4.1 Metrics (page 171): lists evaluation metrics as Err-Num, Label-Num, SR, (and later Inact-Rate) but contains no perceptual or human-evaluation measures (no LPIPS/SSIM/PESQ/STOI/Perplexity or human study reported)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility to a limited extent by using a bounded perturbation constraint. Specifically, it uses the L\u221e norm to restrict the magnitude of modifications applied to the original input images. This is a common technique to ensure the perturbed image does not deviate excessively from the original, thereby acting as a proxy for plausibility. However, the paper does not provide any empirical validation to confirm that the resulting error-inducing inputs are perceptually or semantically realistic. There is no mention of human studies or quantitative, task-specific metrics like SSIM or LPIPS to assess the visual quality of the generated inputs. The evaluation focuses exclusively on the number and diversity of errors found. Therefore, the method meets the criteria for \"Medium\" as it enforces plausibility through a bounded constraint but does not empirically validate the plausibility of the final outputs.",
                    "evidence": "Page 8, Section 4.1 Experimental Setup, subsection Hyperparameters: \"We use the widely used L\u221e metric [4] to restrict perturbations for error-inducing inputs on all compared methods. To set values of L\u221e, we first run all compared methods under different L\u221e for multiple times, and then we choose values that make them perform relatively well, i.e., L\u221e = 13/255 on all datasets (all compared methods use the same setting).\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces bounded, input-centric perturbations throughout its mutator design and explicitly uses an L_infty constraint (L\u221e = 13/255) for all methods, and the mutator logic ensures each pixel is toggled only between original \u00b1 \u03b5 (so cumulative drift beyond the bound is prevented). These design choices constrain mutations to visually small changes and maintain per-pixel plausibility. However, the paper does not provide any empirical assessment of the perceptual or semantic realism of the final fault-inducing inputs (no human study or task-specific perceptual/fluency metrics such as LPIPS/SSIM/PESQ/STOI/Perplexity are reported). According to the metric definition, this corresponds to Medium: plausibility is enforced in mutation design (bounded/rule-based), but the paper omits explicit plausibility validation of final inputs.",
                    "evidence": "Section 4.1 Hyperparameters (page 171): \"We use the widely used L\u221e metric [4] to restrict perturbations for error-inducing inputs on all compared methods... i.e., L\u221e = 13/255 on all datasets (all compared methods use the same setting).\"; Section 3.5 Mutator, \"Perturbation values\" (page 169): \"...we limit the perturbations to each byte to two choices {+\u03b5, \u2212\u03b5}... Given an original input Eo, a current best test case Eb... if Eb[i] = Eo[i], we randomly add perturbation value of +\u03b5 or \u2212\u03b5 to this byte. If Eb[i] = Eo[i] \u2212 \u03b5, we change this byte to Eo[i] + \u03b5. Otherwise, Eb[i] = Eo[i] + \u03b5 must hold and we change this byte to Eo[i] \u2212 \u03b5.\" (shows per-pixel changes remain within \u00b1\u03b5 of original); Section 4.3 Result of Inact-Rate (page 172): \"The Inact-Rate of BET is 0.0%. The perturbations are directly added on pixel values as integers, BET does not suffer the inactivation issue.\" (confirms direct integer pixel edits and bounded changes); Section 4.1 Metrics (page 171): lists evaluation metrics as Err-Num, Label-Num, SR, (and later Inact-Rate) but contains no perceptual or human-evaluation measures (no LPIPS/SSIM/PESQ/STOI/Perplexity or human study reported)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Applies integer perturbations. Shows failure stability empricaclly. Shows ADAPT, DLFUZZ, DeepExplore failures are unstable"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "Does not show transferability to other models or common pattern between models. It uses differential testing to determine fault if two model versions gives inconsistent output"
                },
                "chatgpt": {
                    "value": "High",
                    "why": "The paper explicitly targets differential testing (DT) and includes a tunable objective function designed to maximize disagreements between a target model and other models (Equation 5). The authors construct surrogate/alternative implementations (quantized 8-bit and 16-bit variants) and generate fault-inducing inputs that cause disagreements between the original model and these other models; they report DT results (Tables 1\u20134) demonstrating many such cross-model disagreement inputs. Thus BET both (1) reuses/queries inputs across multiple models and (2) incorporates an explicit mechanism (the DT objective term summing differences with other models) to discover transferable failures across model implementations.",
                    "evidence": "Page 166, Section 2.2 (Related Work / definition of DT): \"There are two common scenarios for CNN testing, i.e., differential testing (DT) [31, 46] and single model testing (ST) [12, 19]. DT finds error-inducing inputs which trigger behavior inconsistencies among multiple models that have similar functionality ... For example, given two CNN models C1, C2 with similar functionality, an error-inducing input x_err is found if C1(x_err) \u2260 C2(x_err).\" \n\nPage 167, Section 3.3 (Tunable objective function for DT): \"Tunable objective function for DT. For DT, we hope to identify inputs that can maximize the prediction difference between the target model with other models.... The tunable objective function for DT is defined in Equation 5. DOF(x) = C(x)[l_i] + \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] |, ... The second term \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] | aims to find disagreements between the target model C with a batch of other models C\u0302 = {C\u0302_1, C\u0302_2, ...}, as such disagreements indicate error-inducing inputs are found.\" (Equation 5 and surrounding text)\n\nPage 170\u2013171, Section 4.2 (Evaluation: Constructing differential models / Results): \"Constructing differential models. ... We apply these two options to all models and we get corresponding 8-bits and 16-bits quantized models as shown in Table 1. Then, we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\" \n\nPage 171, Section 4.2 (Results): \"Results. Table 2, Table 3 and Table 4 show the results of DT on all datasets and models, respectively. Overall, under the same experimental conditions, BET significantly outperforms DiffChaser on all comparison metrics.\" (The presence of these DT-specific evaluation tables shows BET-generated inputs are evaluated across model variants to demonstrate cross-model disagreements.)"
                },
                "gemini": {
                    "value": "High",
                    "why": "The paper's method, BET, is explicitly designed and evaluated for a differential testing (DT) scenario, which aligns with the concept of attack transferability. The fuzzing design incorporates an explicit mechanism\u2014a tunable objective function for DT\u2014specifically to discover transferable failures by maximizing the prediction differences between a target model and other related models. The evaluation section is dedicated to demonstrating this capability, where generated inputs are systematically tested against pairs of models (an original and its quantized version) to find disagreements, which are a form of transferable failure.",
                    "evidence": "Page 6, Section 3.3, \"Tunable objective function for DT\": \"For DT, we hope to identify inputs that can maximize the prediction difference between the target model with other models... The tunable objective function for DT is defined in Equation 5... The second term \u03a3|C(x)[lo] \u2013 \u0108i(x)[lo]| aims to find disagreements between the target model C with a batch of other models \u0108 = {C1, C2, ...\u0108n}, as such disagreements indicate error-inducing inputs are found.\"\n\nPage 8, Section 4.2, \"Evaluation of Black-Box Differential Testing\": \"Constructing differential models. To get a convincing DT result, we directly use Tensorflow-Lite... to construct quantized models for DT... Then, we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\""
                },
                "arbitrator": {
                    "value": "High",
                    "why": "The paper explicitly targets differential testing (DT) and includes a tunable objective function designed to maximize disagreements between a target model and other models (Equation 5). The authors construct surrogate/alternative implementations (quantized 8-bit and 16-bit variants) and generate fault-inducing inputs that cause disagreements between the original model and these other models; they report DT results (Tables 1\u20134) demonstrating many such cross-model disagreement inputs. Thus BET both (1) reuses/queries inputs across multiple models and (2) incorporates an explicit mechanism (the DT objective term summing differences with other models) to discover transferable failures across model implementations.",
                    "evidence": "Page 166, Section 2.2 (Related Work / definition of DT): \"There are two common scenarios for CNN testing, i.e., differential testing (DT) [31, 46] and single model testing (ST) [12, 19]. DT finds error-inducing inputs which trigger behavior inconsistencies among multiple models that have similar functionality ... For example, given two CNN models C1, C2 with similar functionality, an error-inducing input x_err is found if C1(x_err) \u2260 C2(x_err).\" \n\nPage 167, Section 3.3 (Tunable objective function for DT): \"Tunable objective function for DT. For DT, we hope to identify inputs that can maximize the prediction difference between the target model with other models.... The tunable objective function for DT is defined in Equation 5. DOF(x) = C(x)[l_i] + \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] |, ... The second term \u03a3_i | C(x)[l_0] - C\u0302_i(x)[l_0] | aims to find disagreements between the target model C with a batch of other models C\u0302 = {C\u0302_1, C\u0302_2, ...}, as such disagreements indicate error-inducing inputs are found.\" (Equation 5 and surrounding text)\n\nPage 170\u2013171, Section 4.2 (Evaluation: Constructing differential models / Results): \"Constructing differential models. ... We apply these two options to all models and we get corresponding 8-bits and 16-bits quantized models as shown in Table 1. Then, we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.\" \n\nPage 171, Section 4.2 (Results): \"Results. Table 2, Table 3 and Table 4 show the results of DT on all datasets and models, respectively. Overall, under the same experimental conditions, BET significantly outperforms DiffChaser on all comparison metrics.\" (The presence of these DT-specific evaluation tables shows BET-generated inputs are evaluated across model variants to demonstrate cross-model disagreements.)"
                },
                "manual": {
                    "value": "Low",
                    "why": "Uses differential testing based oracle to detect failure, not to check transferability from one to another.",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Differential",
                "Property-based"
            ],
            "mutation_strategy": [
                "Feedback-informed"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Zhang et al. - 2020 - Towards characterizing adversarial defects of deep learning software from the lens of uncertainty": {
        "key": "zhang2020towards",
        "name": "KuK",
        "year": "2020",
        "citation_count": "101",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "High",
                    "why": "Demostrates its ability to bypass adversarail defense mechanisms"
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Low",
                    "why": "aims to maximize model uncertainty and reveal generic misclassifications"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Shows correlatin"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "norm-bounded"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints during generation (bounded pixel perturbations and avoiding certain transformations) but does not provide any empirical evaluation (human study or perceptual/semantic similarity metrics) of the final fault-inducing inputs. Concretely, KuK\u2019s GA uses L_infty bounds and avoids affine/crossover operations that would create invalid images, which constrains individual mutation steps and initialization relative to the original seed. However, the authors do not report task-specific perceptual/semantic measures (e.g., SSIM/LPIPS) or any human evaluation to demonstrate that the final uncommon AEs/BEs remain perceptually or semantically realistic. This matches the metric\u2019s \u201cMedium\u201d level: plausibility is enforced by bounded/rule-based constraints but final-input plausibility is not validated empirically.",
                    "evidence": "Section 4 (Uncommon Data Generation) and Section 5.1 (RQ3 setting):\n\n1) Constraint on mutations / avoiding invalid transforms (Section 4, Population Initialization): \"In order to generate high-quality images (i.e., recognizable by human), we abandon the affine transformation (e.g., translation and rotation [55]) as the crossover may generate invalid images. We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\" (Section 4, p. 746)\n\n2) Explicit L\u221e mutation radius in evaluation settings (Section 5.1): \"For the mutation process, the radius of L\u221e is set as 0.3.\" (Section 5.1, p. 746)\n\n3) Lack of explicit perceptual / human plausibility assessment: the paper\u2019s evaluations (RQ3/RQ4) report counts, PCS/VRO statistics and defense success rates but contain no reported human study or task-specific perceptual/semantic metrics (e.g., SSIM, LPIPS, PESQ, STOI, or Perplexity) for final fault-inducing inputs. For example, the experimental descriptions and results focus on generated counts and PCS/VRO distributions (Tables 4\u20136) and defense bypass rates (Section 5.2), but no perceptual-quality or human-evaluation results are presented (Sections 3\u20135, pp. 743\u2013748)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility using a bounded constraint but does not empirically validate the plausibility of the final generated inputs. The method uses an L\u221e norm to constrain the allowable changes between the original seed image and its generated counterparts, with the explicit goal of generating \"high-quality images (i.e., recognizable by human)\". This use of a bounded perturbation is a mechanism to control plausibility. However, the paper provides no empirical assessment to confirm that this goal was achieved. There are no human studies, quantitative perceptual metrics (e.g., SSIM, LPIPS), or even qualitative examples of the final fault-inducing images to demonstrate their realism or semantic validity. This fulfills the criteria for a \"Medium\" rating, as it includes a plausibility enforcement mechanism but lacks the empirical validation of the final outputs required for a \"High\" rating.",
                    "evidence": "Page 8, Section 4, \"Population Initialization\": \"In order to generate high-quality images (i.e., recognizable by human), we abandon the affine transformation (e.g., translation and rotation [55]) as the crossover may generate invalid images. We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\"\nPage 9, Section 5.1, \"Setting\": \"For the mutation process, the radius of L\u221e is set as 0.3.\""
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces plausibility constraints during generation (bounded pixel perturbations and avoiding certain transformations) but does not provide any empirical evaluation (human study or perceptual/semantic similarity metrics) of the final fault-inducing inputs. Concretely, KuK\u2019s GA uses L_infty bounds and avoids affine/crossover operations that would create invalid images, which constrains individual mutation steps and initialization relative to the original seed. However, the authors do not report task-specific perceptual/semantic measures (e.g., SSIM/LPIPS) or any human evaluation to demonstrate that the final uncommon AEs/BEs remain perceptually or semantically realistic. This matches the metric\u2019s \u201cMedium\u201d level: plausibility is enforced by bounded/rule-based constraints but final-input plausibility is not validated empirically.",
                    "evidence": "Section 4 (Uncommon Data Generation) and Section 5.1 (RQ3 setting):\n\n1) Constraint on mutations / avoiding invalid transforms (Section 4, Population Initialization): \"In order to generate high-quality images (i.e., recognizable by human), we abandon the affine transformation (e.g., translation and rotation [55]) as the crossover may generate invalid images. We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\" (Section 4, p. 746)\n\n2) Explicit L\u221e mutation radius in evaluation settings (Section 5.1): \"For the mutation process, the radius of L\u221e is set as 0.3.\" (Section 5.1, p. 746)\n\n3) Lack of explicit perceptual / human plausibility assessment: the paper\u2019s evaluations (RQ3/RQ4) report counts, PCS/VRO statistics and defense success rates but contain no reported human study or task-specific perceptual/semantic metrics (e.g., SSIM, LPIPS, PESQ, STOI, or Perplexity) for final fault-inducing inputs. For example, the experimental descriptions and results focus on generated counts and PCS/VRO distributions (Tables 4\u20136) and defense bypass rates (Section 5.2), but no perceptual-quality or human-evaluation results are presented (Sections 3\u20135, pp. 743\u2013748)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "Low",
                    "why": "Applies small floating point perturbations without clipping/rounding"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "evaluates several models independently but does not analyze cross-model consistency in failure behaviors."
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate or measure whether fault-inducing inputs crafted on one model transfer to other models performing the same task. All experiments generate and evaluate adversarial / uncommon inputs per target model (attacks and KuK generation are applied to each model separately) and the evaluation of \"transfer\" in the paper is limited to applying inputs crafted on an original model to a defended/retrained version of that same model (a defense transfer scenario), not to distinct other model implementations or architectures. There is no reported experiment that reuses inputs generated on one surrogate model to test other (unseen) models or that explicitly measures cross-model transferability across different model implementations (e.g., LeNet-5 \u2192 NIN or ResNet \u2192 MobileNet).",
                    "evidence": "Page 740, Introduction (research questions): \"RQ4: To what extent are the uncommon samples defended by existing adversarial defense techniques compared with the common ones?\" \u2014 shows focus on defenses, not cross-model transfer.\n\nPage 743, Section 3.1.3 Data Preparation: \"For each dataset, we prepared the following three sets of data: one set of benign examples, one set of AEs generated from the attack methods, and one set of AEs generated by testing tools... For each input in BenignData, we generated four types of AEs with the four attack methods, resulting in a total of 36,000 AEs.\" \u2014 indicates attacks/AEs are generated per model/dataset but does not describe applying them to different model implementations.\n\nPage 748, Section 5.2 Results: \"The reason is that a new model is retrained with these defense techniques, while the attacks are generated regarding to the original one, making it a more challenging transfer attack scenario.\" \u2014 the only 'transfer' discussion refers to transfer to a retrained/defended version of the same model, not to other distinct models or architectures.\n\n(Throughout the Methods and Evaluation sections (e.g., Tables 2, 4, 5 and 6) adversarial/uncommon inputs are reported per model; no experiment or section describes testing inputs crafted on one model against other model implementations to measure cross-model transferability.)"
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper evaluates whether the 'uncommon data' generated for an original model can bypass defense techniques that involve retraining the model (e.g., defensive distillation and label smoothing). The authors explicitly describe this as a 'transfer attack scenario', thus testing if the generated inputs (fault-inducing inputs) transfer to a different but related model. However, the proposed generation method, KuK, is not explicitly designed to discover transferable attacks. Its genetic algorithm is guided by fitness functions based on the uncertainty metrics of a single target model, without incorporating mechanisms to generalize the attack across multiple models.",
                    "evidence": "Page 11, Section 5.2 RQ4: Evaluation on Defense Techniques, 'For other defense techniques, the reduction in success rate appears smaller than that of binary classifier and mutation-based detection. For example, the success rates drop to 78.3% and 76.2% for defensive distillation and label smoothing on NIN. The reason is that a new model is retrained with these defense techniques, while the attacks are generated regarding to the original one, making it a more challenging transfer attack scenario. For example, defensive distillation retrains a more robust model by reducing the gradients. In this case, some of the data, which are uncommon for original model, become common data w.r.t. the retrained model, because of some weight variation. However, it still can be seen from the results that the uncommon data reveal stronger transferability.'"
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not include any experiments or analysis of whether fault-inducing inputs generated on one model transfer to other, independently trained model implementations performing the same task. All adversarial and uncommon inputs are crafted and evaluated per target model (including defense-retrained variants of that same model), but no inputs generated on one architecture are tested on a different architecture or distinct model instance without access. Therefore it does not demonstrate cross-model transferability.",
                    "evidence": "Page 743 (\u00a73.1.3 Data Preparation): adversarial examples are generated \u201cfor each input in BenignData\u2026 on each model,\u201d with no mention of re-using those inputs on different models. \nPage 748 (\u00a75.2 RQ4): discussion of \u201ctransfer attack scenario\u201d refers only to attacks against a retrained/defended version of the same model, not to separate model implementations (e.g., LeNet-5\u2192NIN or ResNet\u2192MobileNet). \nThroughout (e.g. Tables 2, 4\u20136) all results report attacks and defenses per model, with no cross-model evaluation of transferability."
                },
                "manual": {
                    "value": "Medium",
                    "why": "Tests whether the same inputs still cause failures on the defended model (RQ4)",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Metamorphic"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided"
            ]
        }
    },
    "Pang et al. - 2022 - MDPFuzz testing models solving Markov decision processes": {
        "key": "pang2022mdpfuzz",
        "name": "MDPFuzz",
        "year": "2022",
        "citation_count": "39",
        "assessments": {
            "Failure Severity": {
                "manual1": {
                    "value": "Medium",
                    "why": "Uncovers behaviorally unsafe yet functionally valid faults \u2014 crash-inducing policies within valid operational parameters. These expose specification-level safety violations but do not bypass explicit control or policy mechanisms. expose  safety critical violations that lead to crashes/collisions or unsafe actions in autonomous or multi-agent environments. reveals crash-triggering state sequences in models solving Markov Decision Processes (MDPs) across autonomous driving, aircraft collision avoidance, and robotic control settings. the failures are behaviorally unsafe but functionally valid \u2014 the agent is still \u201cplaying by the rules\u201d of the simulator. Does not bypass any safety or policy mechanisms. It exposes latent unsafe behaviors within unconstrained learned policies, revealing specification-level vulnerabilities rather than policy breaches."
                }
            },
            "Targeted Attack Discovery": {
                "manual1": {
                    "value": "Medium",
                    "why": "Targets a class of faults (collission or dangerous states)"
                }
            },
            "Failure Diagnostics": {
                "manual1": {
                    "value": "Medium",
                    "why": "Provides correlation with internal coverage"
                }
            },
            "Input Plausibility": {
                "manual1": {
                    "value": "Medium",
                    "why": "Applies bounded noise"
                },
                "chatgpt": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility during fuzzing by (1) restricting mutations to the MDP initial state rather than arbitrary intermediate states, (2) applying only small/bounded random perturbations, and (3) validating every mutated initial state using environment validation modules and scenario-specific bounds (e.g., speed limits). These design choices ensure mutated inputs remain realistic at each mutation step. However, the paper does not provide an explicit human evaluation or task-specific perceptual/semantic quality metrics (e.g., LPIPS/SSIM, PESQ/STOI, Perplexity) to empirically quantify the perceptual/semantic realism of the final crash-inducing inputs. Nor does it present quantitative perceptual measures for the final fault-inducing inputs; instead it reports internal neuron-activation differences and detection AUCs. Therefore the work fits the \"Medium\" level: plausibility is enforced in mutation/validation steps, but formal empirical perceptual assessments of final fault-inducing inputs are not reported.",
                    "evidence": "Introduction (p. 378): \"Second, instead of mutating arbitrary intermediate states, MDPFuzz only mutates the initial state conservatively to ensure that the sequence is realistic. The mutated initial states are also validated in a deliberate way to ensure realism.\"; Section 6, 'MDP Initial State Sampling, Mutation, and Validation' (p. 385): \"When sampling, the initial states are randomly selected from the state space P. MDPFuzz randomly mutates initial states with small noise. All the mutated states are validated in an automated and deliberate way (see below). That is, we confirm that all mutated initial states are valid and solvable.\"; Section 5 (Assumptions / Alg.1 description) (pp. 382\u2013383): \"We clarify that this assumption is realistic... MDPFuzz only mutates the initial state S0 of an MDP, under the constraint that it is still realistic and solvable after mutation. See Sec. 6 on how mutated initial states are validated in this study.\"; RQ3 Results (p. 387): \"we emphasize that the crashes found by MDPFuzz are not due to unrealistic states. Rather, they share similar visual appearances with normal states and can occur in real-world scenarios.\"; (Counter-evidence / absence) No sections report human studies or quantitative perceptual/linguistic quality metrics (e.g., LPIPS/SSIM, PESQ/STOI, Perplexity) for final fault-inducing inputs\u2014the evaluation focuses on neuron-activation patterns (t-SNE) and detection AUC (Section 7.3\u20137.4)."
                },
                "gemini": {
                    "value": "Medium",
                    "why": "The paper's method, MDPFuzz, is designed to maintain the realism of test inputs, which are state sequences in a Markov Decision Process (MDP). It achieves this by exclusively mutating the initial state of a sequence, arguing this preserves the natural temporal coherence of subsequent states generated by the environment's dynamics. Crucially, every mutated initial state undergoes an explicit, automated validation step to ensure it is both plausible and solvable within the task domain. For instance, in the CARLA driving simulator, it relies on the simulator's built-in validation to reject abnormal states, and for the ACAS Xu aircraft collision system, it applies domain-specific constraints like speed limits. This systematic validation of the inputs that trigger failure sequences fulfills a key requirement of the metric. However, the paper does not meet the 'High' criteria because it lacks an independent, empirical evaluation (e.g., a human study or quantitative perceptual metrics) to confirm the realism of the fault-inducing initial states; it only claims they are realistic because they pass the environment's validation checks.",
                    "evidence": "Page 9, Section 6, 'MDP Initial State Sampling, Mutation, and Validation': 'All the mutated states are validated in an automated and deliberate way (see below). That is, we confirm that all mutated states are valid and solvable.'\n\nPage 6, Section 4, 'Design Overview': 'Moreover, we only mutate the initial state rather than arbitrary states to generate more realistic scenarios... For each MDP scenario, we validate mutated initial states in a deliberate way to ensure they are realistic and solvable for an optimal model \u03c0*.'\n\nPage 9, Section 6, 'MDP Initial State Sampling, Mutation, and Validation': 'Particularly, in CARLA, we change the initial positions and angles of all 100 vehicles... Note that CARLA validates and rejects abnormal initial states: all mutated initial states passed its validation... In DNN-based ACAS Xu, we mutate the initial positions and speeds... Moreover, we bound the maximal speed of all airplanes below 1,100 ft/sec, which is within the range of normal speed... We guarantee that there exist optimal actions to avoid the crash and solve the initial states, and we do not use an initial state that is not solvable.'"
                },
                "arbitrator": {
                    "value": "Medium",
                    "why": "The paper enforces input plausibility during fuzzing by (1) restricting mutations to the MDP initial state rather than arbitrary intermediate states, (2) applying only small/bounded random perturbations, and (3) validating every mutated initial state using environment validation modules and scenario-specific bounds (e.g., speed limits). These design choices ensure mutated inputs remain realistic at each mutation step. However, the paper does not provide an explicit human evaluation or task-specific perceptual/semantic quality metrics (e.g., LPIPS/SSIM, PESQ/STOI, Perplexity) to empirically quantify the perceptual/semantic realism of the final crash-inducing inputs. Nor does it present quantitative perceptual measures for the final fault-inducing inputs; instead it reports internal neuron-activation differences and detection AUCs. Therefore the work fits the \"Medium\" level: plausibility is enforced in mutation/validation steps, but formal empirical perceptual assessments of final fault-inducing inputs are not reported.",
                    "evidence": "Introduction (p. 378): \"Second, instead of mutating arbitrary intermediate states, MDPFuzz only mutates the initial state conservatively to ensure that the sequence is realistic. The mutated initial states are also validated in a deliberate way to ensure realism.\"; Section 6, 'MDP Initial State Sampling, Mutation, and Validation' (p. 385): \"When sampling, the initial states are randomly selected from the state space P. MDPFuzz randomly mutates initial states with small noise. All the mutated states are validated in an automated and deliberate way (see below). That is, we confirm that all mutated initial states are valid and solvable.\"; Section 5 (Assumptions / Alg.1 description) (pp. 382\u2013383): \"We clarify that this assumption is realistic... MDPFuzz only mutates the initial state S0 of an MDP, under the constraint that it is still realistic and solvable after mutation. See Sec. 6 on how mutated initial states are validated in this study.\"; RQ3 Results (p. 387): \"we emphasize that the crashes found by MDPFuzz are not due to unrealistic states. Rather, they share similar visual appearances with normal states and can occur in real-world scenarios.\"; (Counter-evidence / absence) No sections report human studies or quantitative perceptual/linguistic quality metrics (e.g., LPIPS/SSIM, PESQ/STOI, Perplexity) for final fault-inducing inputs\u2014the evaluation focuses on neuron-activation patterns (t-SNE) and detection AUC (Section 7.3\u20137.4)."
                },
                "manual": {
                    "value": "Medium",
                    "why": "",
                    "evidence": ""
                }
            },
            "Failure Reproducibility": {
                "manual1": {
                    "value": "High",
                    "why": "Perturbs continuous-valued initial state vectors within valid physical bounds and validates each mutated state through the simulator"
                }
            },
            "Attack Transferability": {
                "manual1": {
                    "value": "Low",
                    "why": "No cross model failure pattern analysis"
                },
                "chatgpt": {
                    "value": "Low",
                    "why": "The paper does not demonstrate attack transferability. MDPFuzz evaluates and finds crash-triggering initial states per target model (it reports crashes separately for five different models/scenarios), uses those findings to train per-model detectors and to repair individual models, and explicitly operates in blackbox settings without building surrogate models. The paper contains no experiment where fault-inducing inputs found on one model are reused to test another model, nor does it describe mechanisms (e.g., surrogate-model attacks, cross-model validation, or explicit cross-model transfer tests) intended to discover transferable attacks across different model implementations.",
                    "evidence": "Abstract (p.378): \"We evaluate five state-of-the-art models for solving MDPs, including supervised DNN, RL, IL, and multi-agent RL.\" (shows multiple models were evaluated but not that inputs were reused across them). \n\nSection 7.1 RQ1 / Results (p.386): \"Table 3 summarizes the findings of fuzzing each model... Overall, MDPFuzz detects a significant number of crashes from all test cases.\" (reports per-model crash counts; no cross-model reuse reported). \n\nSection 5 Assumptions (Alg.1 / p.382): \"In blackbox settings, the internal of the agent model (our testing target) is not revealed to MDPFuzz... MDPFuzz can collect the state sequence {S_t} and obtain the cumulative reward r from the environment.\" (describes blackbox testing and lack of building surrogate/internal models). \n\nSection 7.4 (p.387): \"we repair the models used by DNN-based ACAS Xu with the findings of MDPFuzz...\" (describes per-model repair using findings, not transfer of fault-inducing inputs to other models). \n\nSection 8 Discussion (p.388): \"Another threat is that bugs may be potentially overlooked since MDPFuzz only mutates the entry states (Sec. 5). Nevertheless ...\" (limits of their approach are discussed; no claim of enabling transferable/substitute-model attacks)."
                },
                "gemini": {
                    "value": "Low",
                    "why": "The paper evaluates the proposed fuzzer, MDPFuzz, on five distinct models, including a Reinforcement Learning (RL) and an Imitation Learning (IL) model for the same autonomous driving task (CARLA). However, the evaluation is conducted independently for each model. The paper reports the number of crash-triggering inputs found for each model in isolation and does not perform any experiments to test if the inputs that cause a failure in one model also cause a failure in another model, even those performing the same task. The design of MDPFuzz focuses on finding failures for a single model under test and lacks any mechanism explicitly designed to discover transferable attacks or shared vulnerabilities across different models.",
                    "evidence": "Page 10, Section 7.1 RQ1: Performance on Finding Crashes, Setup: \"We use the evaluation setup described in Sec. 6. That is, we launch MDPFuzz to fuzz each MDP model (listed in Table 3) and detect crashes. We collect all error-triggering inputs for analysis.\" This statement indicates that each model was fuzzed as a separate, independent target. Table 3 on the same page presents the results for each model as a distinct line item, with no cross-model evaluation or mention of transferring discovered failure-inducing inputs."
                },
                "arbitrator": {
                    "value": "Low",
                    "why": "The paper does not demonstrate attack transferability. MDPFuzz evaluates and finds crash-triggering initial states per target model (it reports crashes separately for five different models/scenarios), uses those findings to train per-model detectors and to repair individual models, and explicitly operates in blackbox settings without building surrogate models. The paper contains no experiment where fault-inducing inputs found on one model are reused to test another model, nor does it describe mechanisms (e.g., surrogate-model attacks, cross-model validation, or explicit cross-model transfer tests) intended to discover transferable attacks across different model implementations.",
                    "evidence": "Abstract (p.378): \"We evaluate five state-of-the-art models for solving MDPs, including supervised DNN, RL, IL, and multi-agent RL.\" (shows multiple models were evaluated but not that inputs were reused across them). \n\nSection 7.1 RQ1 / Results (p.386): \"Table 3 summarizes the findings of fuzzing each model... Overall, MDPFuzz detects a significant number of crashes from all test cases.\" (reports per-model crash counts; no cross-model reuse reported). \n\nSection 5 Assumptions (Alg.1 / p.382): \"In blackbox settings, the internal of the agent model (our testing target) is not revealed to MDPFuzz... MDPFuzz can collect the state sequence {S_t} and obtain the cumulative reward r from the environment.\" (describes blackbox testing and lack of building surrogate/internal models). \n\nSection 7.4 (p.387): \"we repair the models used by DNN-based ACAS Xu with the findings of MDPFuzz...\" (describes per-model repair using findings, not transfer of fault-inducing inputs to other models). \n\nSection 8 Discussion (p.388): \"Another threat is that bugs may be potentially overlooked since MDPFuzz only mutates the entry states (Sec. 5). Nevertheless ...\" (limits of their approach are discussed; no claim of enabling transferable/substitute-model attacks)."
                },
                "manual": {
                    "value": "Low",
                    "why": "",
                    "evidence": ""
                }
            }
        },
        "taxonomy": {
            "access_level": [
                "Blackbox"
            ],
            "oracle": [
                "Property-based"
            ],
            "mutation_strategy": [
                "Rule-based"
            ],
            "exploration_strategy": [
                "Prediction-guided",
                "Data-driven"
            ]
        }
    }
}