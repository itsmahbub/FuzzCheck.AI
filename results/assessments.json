{
  "Liu et al. - 2024 - AUTODAN GENERATING STEALTHY JAILBREAK PROMPTS ON ALIGNED LARGE LANGUAGE MODELS": {
    "key": "liuautodan",
    "name": "AutoDAN",
    "year": "2024",
    "citation_count": "834",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "High",
          "why": "Guides towared output that start with a predified content"
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": "Reports what fails without analyzing why. It provides tranferability rationale, not failure patterns or root cause"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "mutation_strategy": [
        "Rule-based",
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ],
      "oracle": [
        "Property-based"
      ]
    }
  },
  "He et al. - 2024 - Curiosity-Driven Testing for Sequential Decision-Making Process": {
    "key": "he2024curiosity",
    "name": "CureFuzz",
    "year": "2024",
    "citation_count": "12",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Medium",
          "why": "Uncovers crash-triggering scenarios (e.g., collisions, falls, or loss of control) across autonomous driving, aviation, and robotics systems. exposes behaviorally unsafe yet functionally valid failures \u2014 revealing unintended consequences of learned policies rather than breaches of safety enforcement mechanisms."
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Medium",
          "why": "Steer toward safetly faults (crash-triggering scenarios) in in autonomous driving and other sequential decision-making systems"
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "High",
          "why": "Mutates continuous feature vectors representing simulator states (positions, angles, velocities) and enforces validity checks to keep them within legal bounds."
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ],
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Property-based"
      ]
    }
  },
  "Zhou et al. - 2025 - Understanding the Effectiveness of Coverage Criteria for Large Language Models A Special Angle from": {
    "key": "zhou2025understanding",
    "name": "Zhou et al.",
    "year": "2025",
    "citation_count": "7",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Medium",
          "why": ""
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "GPT-4 generates  new jailbreak queries through prompt rewriting. The generated contents are supposed to be natural but there was no measure reported how natural are those. The attack suffix is related to jailbreak detection, not creation"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": "No analysis of fault transferability in input level of mutation level"
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Medium",
          "why": "Provides correlation with internal coverage"
        }
      }
    },
    "taxonomy": {
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ],
      "access_level": [
        "Greybox"
      ],
      "oracle": [
        "Property-based"
      ]
    }
  },
  "Gong et al. - PAPILLON Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs": {
    "key": "gong2025papillon",
    "name": "PAPILLON",
    "year": "2025",
    "citation_count": "0",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Medium",
          "why": "Optimizes toward semantic policy-violation behavior (jailbreak) but not a specific output."
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": "Reports only observable failures but no investigation of root-cause"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "High",
          "why": "Mutate in text level"
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "High",
          "why": "Performs experiments on transferability"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Property-based"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Oracle-guided"
      ]
    }
  },
  "Du et al. - 2018 - DeepCruiser Automated Guided Testing for Stateful Deep Learning Systems": {
    "key": "du2018deepcruiser",
    "name": "DeepCruiser",
    "year": "2018",
    "citation_count": "45",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Medium",
          "why": "Provides heuristic explanations by showing correlation with state variation"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "Ensures each audio is mutated at most once per transformation category (volume/speed/clearness) to prevent unnatural accumulation but does not measure naturalness on the final fuzzed inputs. The human validation is on single step transformation"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Medium",
          "why": "Uses stable, perceptual transformations (volume, speed, clearness) likely to survive saving, though persistence isn\u2019t explicitly verified."
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": "mutation and coverage strategies are tightly coupled to a specific RNN-based ASR model and training setup"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ],
      "oracle": [
        "Metamorphic"
      ]
    }
  },
  "Yuan et al. - 2023 - Revisiting Neuron Coverage for DNN Testing A Layer-Wise and Distribution-Aware Criterion": {
    "key": "yuan2023revisiting",
    "name": "NLC",
    "year": "2023",
    "citation_count": "36",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Medium",
          "why": "shows heuristic link between coverage and errors."
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "Uses inception score and FID score to measure natualness quality of the final inputs but overlook cumulave effects of mutations across iteration"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Medium",
          "why": "Plausibly stable due to large semantic changes, but applied in normalized float domain, so some discovered failures could vanish after re-encoding."
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ],
      "oracle": [
        "Metamorphic"
      ]
    }
  },
  "Xie et al. - 2019 - DeepHunter a coverage-guided fuzz testing framework for deep neural networks": {
    "key": "xie2019deephunter",
    "name": "DeepHunter",
    "year": "2019",
    "citation_count": "518",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": "It uses model coverage to guide input generation but does not provide any expeirment showing the correlation with coverage"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "User study was conducted on the images generated by single-step metamorphic mutations, not on the final fault-triggering fuzzed inputs after multiple chained transformations. Allows at most one affine transformation to reduce that risk."
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Medium",
          "why": "The metamorphic transformations likely to persist but not validated. Works on floating point input without applying rounding."
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ],
      "oracle": [
        "Metamorphic",
        "Differential"
      ]
    }
  },
  "Guo et al. - 2018 - DLFuzz differential fuzzing testing of deep learning systems": {
    "key": "guo2018dlfuzz",
    "name": "DLFuzz",
    "year": "2018",
    "citation_count": "360",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": "Finds generic misclassifications"
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": "Does not show any correlation between errors and coverage"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "norm-bounded perturbation"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Low",
          "why": "relies on imperceptible, float-level perturbations that can vanish after quantization or saving, with no persistence verification."
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Whitebox"
      ],
      "mutation_strategy": [
        "Feedback-informed"
      ],
      "exploration_strategy": [
        "Coverage-guided",
        "Prediction-guided"
      ],
      "oracle": [
        "Metamorphic"
      ]
    }
  },
  "Yu et al. - LLM-Fuzzer Scaling Assessment of Large Language Model Jailbreaks": {
    "key": "yu2024llm",
    "name": "LLM-Fuzzer",
    "year": "2024",
    "citation_count": "40",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Medium",
          "why": "aims to generate many jailbreak templates that trigger harmful content without specifying exact outcomes"
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": "Does not provide any explanation why those errors occur."
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "High",
          "why": "Mutations are purely at the text level"
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Oracle-guided"
      ],
      "oracle": [
        "Property-based"
      ]
    }
  },
  "Tian et al. - 2018 - DeepTest automated testing of deep-neural-network-driven autonomous cars": {
    "key": "tian2018deeptest",
    "name": "DeepTest",
    "year": "2018",
    "citation_count": "1791",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": "Aims to find errors in steering angle predictions under realistic environmental variations."
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Medium",
          "why": "Shows correlation between coverage and errors"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "Does not verify the naturalness of the fault-revealing mutated images through any human study or algorithmic naturalness metric. Individual transformations are naturalistic but chained mutations might not be"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Medium",
          "why": "Performs semantic, real-world image transformations (brightness, contrast, blur, fog, rain, etc.) but does not ensure IO stability using clipping and rounding"
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": "No cross model failure analysis"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ],
      "oracle": [
        "Metamorphic"
      ]
    }
  },
  "Xian Yuen et al. - 2023 - ASDF A Differential Testing Framework for Automatic Speech Recognition Systems": {
    "key": "yuen2023asdf",
    "name": "ASDF",
    "year": "2023",
    "citation_count": "8",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Medium",
          "why": "Performs phonetic analysis to correlated phonemes with errors."
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "Synthesized inputs might not be natural and no measure of naturnaless provided"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "High",
          "why": "The speech input is generated using TTS library which are fed to the ASR model. There is not in memory mutation on the audio."
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Differential"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Oracle-guided"
      ]
    }
  },
  "Odena et al. - 2019 - TensorFuzz Debugging Neural Networks with Coverage-Guided Fuzzing": {
    "key": "odena2019tensorfuzz",
    "name": "TensorFuzz",
    "year": "2019",
    "citation_count": "426",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": "does not deliberately target particular outputs or behaviors. Objective function mechanism allows arbitrary user-defined checks to enable diverse failure detection but the exploration is not targeted toward any specific class of outputs "
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": "No diagnsotic analysis"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "norm-bounded"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Low",
          "why": "Mutations occur in-memory on float tensors and are never verified for persistence after I/O."
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "oracle": [
        "Property-based",
        "Differential"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ]
    }
  },
  "Chao et al. - 2025 - Jailbreaking Black Box Large Language Models in Twenty Queries": {
    "key": "chao2025jailbreaking",
    "name": "PAIR",
    "year": "2025",
    "citation_count": "806",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Medium",
          "why": "Targets specific malicious intents (e.g. bomb-making, phishing) but does not enforce a fixed phrase or label."
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": "No diagnsotic analysis of discovered faults. It provides transferability rationale"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "High",
          "why": "Fuzzed inputs are text prompts generated by another LLM, not numerical perturbations"
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Property-based"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Oracle-guided"
      ]
    }
  },
  "Asyrofi et al. - 2020 - CrossASR Efficient Differential Testing of Automatic Speech Recognition via Text-To-Speech": {
    "key": "asyrofi2020crossasr",
    "name": "CrossASR",
    "year": "2020",
    "citation_count": "25",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": "transcription error"
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": "No diagnsotic analysis"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "High",
          "why": "Performs human study"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "High",
          "why": "Uses TTS to generate the input audio - non in memory mutation"
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": "Utilize multiple ASRs to find faults using differential oracle but does not analyze if same error is reproducible to other model"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Differential"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ]
    }
  },
  "Asyrofi et al. - 2021 - CrossASR++ a modular differential testing framework for automatic speech recognition": {
    "key": "asyrofi2021crossasr++",
    "name": "CrossASR++",
    "year": "2021",
    "citation_count": "25",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": "No diagnsotic analysis"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "TTS engines are inherently designed to produce natural-sounding speech from text, thereby providing a theoretical justification and design constraint for input naturalness."
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": "Uses multiple ASRs in differential oracle, does not show transferability of failures across errors"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Differential"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Data-driven"
      ]
    }
  },
  "Asyrofi et al. - 2021 - Can Differential Testing Improve Automatic Speech Recognition Systems": {
    "key": "asyrofi2021can",
    "name": "Asyrofi et al.",
    "year": "2021",
    "citation_count": "16",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": "No diagnsotic analysis"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": "Does not demonstrate if fuzzed input on one model aslo causes error in another model"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Differential"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Data-driven"
      ]
    }
  },
  "Pei et al. - 2017 - DeepXplore Automated Whitebox Testing of Deep Learning Systems": {
    "key": "pei2017deepxplore",
    "name": "DeepXplore",
    "year": "2017",
    "citation_count": "1814",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": "reveals incorrect corner-case behaviors but does not bypass explicity safety mechinasims"
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": "performs untargeted exploration to reveal generic model faults (differential behaviour, inaccuracies). No targeted misclassification."
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": "No diagnsotic analysis"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Medium",
          "why": "Enforces domain specific constraints like keeping pixel values between 0 and 255, but it can produce decimal point pixel value within that range and does not perform rounding"
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Whitebox"
      ],
      "oracle": [
        "Differential"
      ],
      "mutation_strategy": [
        "Feedback-informed"
      ],
      "exploration_strategy": [
        "Coverage-guided",
        "Prediction-guided"
      ]
    }
  },
  "Gao et al. - 2020 - Fuzz testing based data augmentation to improve robustness of deep neural networks": {
    "key": "gao2020fuzz",
    "name": "Sensei",
    "year": "2020",
    "citation_count": "169",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": "Aims to find generic misclassifications"
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Medium",
          "why": "Apply large semantic level mutations and perform floating point operations (zoom, contrast, shear), which may produce non-discrete pixel values. They don't perform clipping/rounding to ensure IO persistence."
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": "No analysis of fault transferability in input level or mutation level"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "coonstrained transformation but no naturalness measure of the fuzzed inputs"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ],
      "oracle": [
        "Metamorphic",
        "Property-based"
      ]
    }
  },
  "You et al. - 2023 - Regression Fuzzing for Deep Learning Systems": {
    "key": "you2023regression",
    "name": "DRFuzz",
    "year": "2023",
    "citation_count": "27",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": "Intargeted exploration for generic regression misclassifications - any inputs that cause the new model to misclassify examples previously classified correctly"
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Medium",
          "why": "does not analyze the root cause of regression faults, acknowledging that identifying their causes remains an open challenge. It provides behavior-level explanations (e.g., which classes or behaviors regress, per-class fault heatmaps, and faulty-behavior tuples). Interprets regression faults as evidence of bias or overfitting (e.g., specific classes suffering more after retraining)."
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "High",
          "why": "Every batch of mutated inputs is passed through the GAN discriminator and only natural mutated inputs are kept for execution and further mutation."
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Medium",
          "why": "Generates and evaluates mutated inputs entirely in memory using both pixel level and image level semantic mutations. They use GAN-based fidelity filtering to ensure naturalness but does not ensure if the mutation will survive I/O."
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Differential"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ]
    }
  },
  "Zhang et al. - 2019 - Life after Speech Recognition Fuzzing Semantic Misinterpretation for Voice Assistant Applications": {
    "key": "zhang2019life",
    "name": "LipFuzzer",
    "year": "2019",
    "citation_count": "79",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Medium",
          "why": "It does not bypass permissions or authentication; instead, it tricks the VA into misinterpreting intent. The resulting misbehavior (e.g., invoking the wrong vApp or disclosing data to a malicious app). Expose semantically incorrect but functionally valid behaviors that breach user-intent and specification expectations\u2014posing integrity and trust risks without directly bypassing safety or security mechanisms."
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": "untargeted exploration to expose semantic or intent-classification inconsistencies, sometimes aligning with attacker intent (e.g., LAPSUS examples), though its discovery of attacker intent is incidental rather than goal-optimized."
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Medium",
          "why": "Empirically isolates the NLU Intent Classifier as the primary source of error and correlates misinterpretation patterns with linguistic mutation types"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "They perform human study to train the linguistic model that help selection potential fault triggering input. However, the synthesized voice commands are not measured for naturalness"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": "evaluates two voice assistant systems independently but does not analyze whether failures generalize"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Data-driven"
      ],
      "oracle": [
        "Metamorphic"
      ]
    }
  },
  "Woodlief et al. - 2022 - Semantic image fuzzing of AI perception systems": {
    "key": "woodlief2022semantic",
    "name": "semSensFuzz",
    "year": "2022",
    "citation_count": "18",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Medium",
          "why": "generates safety-critical scenarios (cars, pedestrians, obstacles) to expose perception failures."
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": "Offers descriptive correlations between input changes and output errors, but no analysis on root cause"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "Applies design constratraints but no measure of naturalness of the fault-triggering fuzzed inputs"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Medium",
          "why": "Perturbations occur at a semantic level that is plausibly I/O-stable, but the paper neither enforces nor verifies persistence after I/O transformations, and minor artifacts from the mutation implementation may cause non-persistent faults."
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Data-driven"
      ],
      "oracle": [
        "Metamorphic"
      ]
    }
  },
  "Lee et al. - 2020 - Effective white-box testing of deep neural networks with adaptive neuron-selection strategy": {
    "key": "lee2020effective",
    "name": "ADAPT",
    "year": "2020",
    "citation_count": "81",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Medium",
          "why": "Correlates neuron features with faults"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "Norm-bounded"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Whitebox"
      ],
      "mutation_strategy": [
        "Feedback-informed"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ],
      "oracle": [
        "Metamorphic"
      ]
    }
  },
  "Ben Braiek and Khomh - 2019 - DeepEvolution A Search-Based Testing Approach for Deep Neural Networks": {
    "key": "braiek2019deepevolution",
    "name": "DeepEvolution",
    "year": "2019",
    "citation_count": "53",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": "Uses neuron coveage as heuristic guide, does not show correlation between coverage and neurons"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "High",
          "why": "Enforces naturalness across iterations by computing SSIM after each mutation and rejecting any visually distorted image."
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Medium",
          "why": "Semantic-level pixel transformations (contrast, blur, rotation, etc.) but no clipping/rounding to ensure IO persistence"
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ],
      "oracle": [
        "Metamorphic",
        "Differential"
      ]
    }
  },
  "Ma et al. - 2021 - HDTest Differential Fuzz Testing of Brain-Inspired Hyperdimensional Computing": {
    "key": "ma2021hdtest",
    "name": "HDTest",
    "year": "2021",
    "citation_count": "39",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": "Finds generic misclassifications"
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": " It uses hypervector distance purely as a heuristic to guide input mutations - no insight where/how/why the faults occur"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "norm-bounded"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Low",
          "why": "In memory floating point mutation, no safeguards for IO preservation"
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "oracle": [
        "Metamorphic"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ]
    }
  },
  "Du et al. - 2019 - DeepStellar model-based quantitative analysis of stateful deep learning systems": {
    "key": "du2019deepstellar",
    "name": "DeepStellar",
    "year": "2019",
    "citation_count": "188",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": "Broad exploration to uncover functional errors"
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Medium",
          "why": "Shows correlation between coverage criteria and faults"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "Design contraints on perturbations but no naturalness measure of final fault-triggering inputs"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Medium",
          "why": "Applies deephunter metamorphic mutations that are plausibly I/O-stable but does not apply clipping and rounding to ensure"
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "oracle": [
        "Metamorphic",
        "Property-based"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ]
    }
  },
  "Huang et al. - 2022 - Coverage-Guided Testing for Recurrent Neural Networks": {
    "key": "huang2021coverage",
    "name": "TESTRNN",
    "year": "2021",
    "citation_count": "66",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": "Even though it can detect backdoor triggers, this is purely diagnostic. The fuzzer exposes low-level functional and robustness failures (misclassifications, metamorphic inconsistencies, and backdoor-induced activation anomalies) without breaching behavioral or safety boundaries."
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": "broadly explore the model's internal behavior and flag any unexpected classification (adversarial or backdoor) as a defect."
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Medium",
          "why": "Linnks faulty behaviors to internal LSTM mechanisms. Correlates coverage with faults. identify where and how the failures arise internally. The analysis is correlational, not causal. "
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "Constraint bounded mutation but not evidence if naturalness measure of the generated inputs"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Low",
          "why": "Applies low level noise but does not enforce IO persistence with clipping/clamping"
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "oracle": [
        "Metamorphic"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided",
        "Oracle-guided"
      ]
    }
  },
  "Li et al. - 2021 - Testing DNN-based Autonomous Driving Systems under Critical Environmental Conditions": {
    "key": "li2021testing",
    "name": "TACTIC",
    "year": "2021",
    "citation_count": "51",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": "Finds inconsistent steering under environmental variations"
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Medium",
          "why": "Relies on coverage-guided mutation and assumes based on prior work, that higher coverage correlates with fault discovery"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "High",
          "why": "Test images are generated through a GAN trained to mimic real driving scenes, so naturalness is preserved throughout fuzzing iterations. Provides human evaluation."
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "High",
          "why": "Produced synthesized images"
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "oracle": [
        "Metamorphic"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Coverage-guided",
        "Oracle-guided"
      ]
    }
  },
  "Wang et al. - 2023 - DistXplore Distribution-Guided Testing for Evaluating and Enhancing Deep Learning Systems": {
    "key": "wang2023distxplore",
    "name": "DistXplore",
    "year": "2023",
    "citation_count": "13",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "High",
          "why": "Guides test generation to minimize distribution differences between source class inputs and target class."
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Medium",
          "why": "links the occurrence and detectability of errors to the statistical closeness (measured via MMD) between the distribution of generated test inputs and the distribution of another class at the logits layer."
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Medium",
          "why": "metamorphic transformations can yield decimal-valued pixels internally which may change when re-quantized to integers."
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "Does not explicitly enforce naturalness across fuzzing iterations. Apply norm constraints in each mutation step but cumulative effect is ignored. Checks visual validity on the final outputs via human evaluation."
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "oracle": [
        "Property-based"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ]
    }
  },
  "Liu et al. - 2022 - QATest A Uniform Fuzzing Framework for Question Answering Systems": {
    "key": "liu2022qatest",
    "name": "QATest",
    "year": "2022",
    "citation_count": "21",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Medium",
          "why": ""
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": "No cross model failure pattern analysis"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "High",
          "why": "Text level mutation "
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Metamorphic"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Data-driven"
      ]
    }
  },
  "Wang et al. - 2022 - BET black-box efficient testing for convolutional neural networks": {
    "key": "wang2022bet",
    "name": "BET",
    "year": "2022",
    "citation_count": "26",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "High",
          "why": ""
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Low",
          "why": "Only reports failures without any insight on root-cause"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "norm-bounded"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "High",
          "why": "Applies integer perturbations. Shows failure stability empricaclly. Shows ADAPT, DLFUZZ, DeepExplore failures are unstable"
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": "Does not show transferability to other models or common pattern between models. It uses differential testing to determine fault if two model versions gives inconsistent output"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Differential",
        "Property-based"
      ],
      "mutation_strategy": [
        "Feedback-informed"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ]
    }
  },
  "Zhang et al. - 2020 - Towards characterizing adversarial defects of deep learning software from the lens of uncertainty": {
    "key": "zhang2020towards",
    "name": "KuK",
    "year": "2020",
    "citation_count": "101",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "High",
          "why": "Demostrates its ability to bypass adversarail defense mechanisms"
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Low",
          "why": "aims to maximize model uncertainty and reveal generic misclassifications"
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Medium",
          "why": "Shows correlatin"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "norm-bounded"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "Low",
          "why": "Applies small floating point perturbations without clipping/rounding"
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": "evaluates several models independently but does not analyze cross-model consistency in failure behaviors or mutation patterns."
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Metamorphic"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ]
    }
  },
  "Pang et al. - 2022 - MDPFuzz testing models solving Markov decision processes": {
    "key": "pang2022mdpfuzz",
    "name": "MDPFuzz",
    "year": "2022",
    "citation_count": "39",
    "assessments": {
      "Failure Severity": {
        "manual1": {
          "value": "Medium",
          "why": "Uncovers behaviorally unsafe yet functionally valid faults \u2014 crash-inducing policies within valid operational parameters. These expose specification-level safety violations but do not bypass explicit control or policy mechanisms. expose  safety critical violations that lead to crashes/collisions or unsafe actions in autonomous or multi-agent environments. reveals crash-triggering state sequences in models solving Markov Decision Processes (MDPs) across autonomous driving, aircraft collision avoidance, and robotic control settings. the failures are behaviorally unsafe but functionally valid \u2014 the agent is still \u201cplaying by the rules\u201d of the simulator. Does not bypass any safety or policy mechanisms. It exposes latent unsafe behaviors within unconstrained learned policies, revealing specification-level vulnerabilities rather than policy breaches."
        }
      },
      "Targeted Attack Discovery": {
        "manual1": {
          "value": "Medium",
          "why": "Targets a class of faults (collission or dangerous states)"
        }
      },
      "Failure Diagnostics": {
        "manual1": {
          "value": "Medium",
          "why": "Provides correlation with internal coverage"
        }
      },
      "Input Plausibility": {
        "manual1": {
          "value": "Medium",
          "why": "Applies bounded noise"
        }
      },
      "Failure Reproducibility": {
        "manual1": {
          "value": "High",
          "why": "Perturbs continuous-valued initial state vectors within valid physical bounds and validates each mutated state through the simulator"
        }
      },
      "Attack Transferability": {
        "manual1": {
          "value": "Low",
          "why": "No cross model failure pattern analysis"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Property-based"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Prediction-guided",
        "Data-driven"
      ]
    }
  }
}