{
  "Liu et al. - 2024 - AUTODAN GENERATING STEALTHY JAILBREAK PROMPTS ON ALIGNED LARGE LANGUAGE MODELS": {
    "key": "liuautodan",
    "name": "AutoDAN",
    "year": "2024",
    "citation_count": "834",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "High",
          "why": "AutoDAN reveals the ability to bypass explicit safety and defense mechanisms (e.g. perplexity\u2010based filtering), demonstrating vulnerabilities at the system\u2010policy boundary rather than merely triggering low\u2010level functional errors or norm violations.",
          "evidence": "Page 8, Section 4.2 \"Effectiveness against defense\": \u201cAs suggested by Alon & Kamfonas (2023); Jain et al. (2023), we evaluate both our method and the baselines \u2026 Table 3 shows the perplexity defense significantly undermines the effectiveness of the token\u2010level jailbreak attack, i.e., GCG attack. However, the semantically meaningful jailbreak prompts AutoDAN (and the original handcrafted DAN) is not influenced. These findings underscore the capability of our method to generate semantically meaningful content similar to benign text, verifying the stealthiness of our method.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "AutoDAN is explicitly designed to generate 'stealthy jailbreak prompts' that bypass the safety features and alignment mechanisms of Large Language Models (LLMs). The paper defines jailbreak attacks as methods that 'elicit malicious outputs that should not be given by aligned LLMs' and 'disrupt the human-aligned values of LLMs or other constraints imposed by the model developer, compelling them to respond to malicious questions.' This directly corresponds to uncovering vulnerabilities that bypass explicit safety, policy, or security mechanisms, which is the definition of 'High' severity.",
          "evidence": "Page 1, Abstract: \"However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs... In this paper, we introduce AutoDAN, a novel jailbreak at-tack against aligned LLMs.\"\nPage 1, Introduction: \"As a consequence, jailbreak attacks have been discovered: combining the jailbreak prompt with mali-cious questions (e.g., how to steal someone's identity) can mislead the aligned LLMs to bypass the safety feature and consequently generate responses that compose harmful, discriminatory, violent, or sensitive content.\"\nPage 3, Section 3.1 Threat model: \"The main goal of this type of attack is to disrupt the human-aligned values of LLMs or other con-straints imposed by the model developer, compelling them to respond to malicious questions posed by adversaries with correct answers, rather than refusing to answer.\""
        },
        "arbitrator": {
          "value": "High",
          "why": "AutoDAN reveals the ability to bypass explicit safety and defense mechanisms (e.g. perplexity\u2010based filtering), demonstrating vulnerabilities at the system\u2010policy boundary rather than merely triggering low\u2010level functional errors or norm violations.",
          "evidence": "Page 8, Section 4.2 \"Effectiveness against defense\": \u201cAs suggested by Alon & Kamfonas (2023); Jain et al. (2023), we evaluate both our method and the baselines \u2026 Table 3 shows the perplexity defense significantly undermines the effectiveness of the token\u2010level jailbreak attack, i.e., GCG attack. However, the semantically meaningful jailbreak prompts AutoDAN (and the original handcrafted DAN) is not influenced. These findings underscore the capability of our method to generate semantically meaningful content similar to benign text, verifying the stealthiness of our method.\u201d"
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "High",
          "why": "AutoDAN is designed to steer the LLM\u2019s generation toward a very specific, predefined fault\u2014namely, producing outputs that begin with a fixed phrase (\u201cSure, here is how to [Qi]\u201d) in order to answer malicious queries. This matches the definition of a highly directed fuzzer, which targets precise outputs rather than exploring broadly for generic failures.",
          "evidence": "Section 3.1 (Formulation), \u201cThe goal of jailbreak attacks is to prompt the model to produce output starting with specific words (e.g. \u2018Sure, here is how to [Qi]\u2019), namely tokens denoted as \u27e8rm+1, rm+2, \u2026, rm+k\u27e9.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "AutoDAN's goal is to automatically generate jailbreak prompts that compel Large Language Models (LLMs) to bypass their safety features and produce malicious content, rather than refusing to answer. While the optimization process targets responses starting with a predefined affirmative prefix (e.g., 'Sure, here is how to [Qi]'), the method does not aim to fix the exact, full output of the LLM. Instead, it seeks to elicit responses that are 'predominantly answers closely associated with the malicious questions,' indicating a focus on triggering a category-level fault (unsafe behavior/jailbreak) where the specific content varies based on the malicious query, beyond the initial fixed phrase.",
          "evidence": "Page 1, Abstract: '...Can we develop an approach that can automatically generate stealthy jailbreak prompts?'\nPage 1, Introduction: '...jailbreak attacks... can mislead the aligned LLMs to bypass the safety feature and consequently generate responses that compose harmful, discriminatory, violent, or sensitive content...'\nPage 4, Section 3.1 Threat model: 'The main goal of this type of attack is to disrupt the human-aligned values of LLMs or other constraints imposed by the model developer, compelling them to respond to malicious questions posed by adversaries with correct answers, rather than refusing to answer.'\nPage 4, Section 3.1 Formulation: 'The objective of jailbreak attacks is to ensure that the responses in R are predominantly answers closely associated with the malicious questions in Q, rather than refusal messages aligned with human values. Consequently, a common solution... is to designate the target response as affirmative, such as answers beginning with \u201cSure, here is how to [Qi].\u201d'"
        },
        "arbitrator": {
          "value": "High",
          "why": "AutoDAN explicitly optimizes for the LLM to produce a predefined prefix (\u201cSure, here is how to [Qi]\u201d) before any content, steering the fuzzer toward that exact target phrase rather than broadly searching for generic failures. This precise targeting of specific tokens aligns with the definition of a highly directed fuzzing approach.",
          "evidence": "Section 3.1 (Formulation): \u201cThe goal of jailbreak attacks is to prompt the model to produce output starting with specific words (e.g. \u2018Sure, here is how to [Qi]\u2019), namely tokens denoted as \u27e8rm+1, rm+2, \u2026, rm+k\u27e9.\u201d"
        },
        "manual": {
          "value": "High",
          "why": "Guides towared output that start with a predified content"
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "AutoDAN focuses solely on generating and evaluating jailbreak prompts by measuring attack success rates and perplexity, without any diagnostic or correlational analysis of the model\u2019s internal mechanisms (e.g. activations, gradients, decision boundaries) to explain why the vulnerabilities occur. The paper does not include sections that investigate the root causes of failures or identify causal links to architectural or training weaknesses.",
          "evidence": "Section 4.2 (\u201cAttack Effectiveness and Stealthiness\u201d) reports only metrics such as ASR and PPL (Table 1) and discusses success rates against defenses; there is no analysis of internal signals or exploration of why the model fails at a mechanistic level."
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper provides correlational explanations and external interpretations of behavioral patterns to clarify where and how jailbreak failures manifest. It speculates on plausible contributing factors related to the semantic meaningfulness of the generated prompts and the optimization strategy, particularly when discussing transferability and stealthiness, rather than solely reporting what fails. However, it stops short of establishing causal links to specific architectural, training, or data-related weaknesses within the LLMs through deep diagnostic analysis of internal model signals.",
          "evidence": "Page 8, Section 4.2, 'Transferability': 'We speculate that the potential reason is the semantically meaningful jailbreak prompts may be inherently more transferable than the methods based on tokens' gradients. As GCG-like method directly optimizes the jailbreak prompt by the gradient information, it is likely for the algorithm to get relatively overfitting in the white-box model. On the contrary, since lexical-level data such as words usually cannot be updated according to specific gradient information, optimizing at the lexical-level may make it easier to generate the more general jailbreak prompts, which may be common flaws for language models. This may support our hypothesis that the semantically meaningful jailbreak prompts are usually more transferable inherently (but more difficult to optimize).'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "While AutoDAN does not perform deep instrumented analysis of model internals (e.g., activations, gradients or decision boundaries), the paper does offer heuristic and correlational explanations for why certain failures occur. In Section 4.2 (Transferability), the authors explicitly speculate on how the semantic meaningfulness of prompts versus token-level gradient\u2010guided prompts affects overfitting and transfer success\u2014an external interpretation of failure patterns that links behavioral outcomes to plausible contributing factors. This meets the criterion for an intermediate level of root-cause insight, as it clarifies where and how failures arise without establishing full causal mechanisms tied to architectural or training weaknesses.",
          "evidence": "Section 4.2 \u201cTransferability\u201d (Page 8): \u201cWe speculate that the potential reason is the semantically meaningful jailbreak prompts may be inherently more transferable than the methods based on tokens\u2019 gradients\u2026optimizing at the lexical level may make it easier to generate the more general jailbreak prompts, which may be common flaws for language models.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "Reports what fails without analyzing why. It provides tranferability rationale, not failure patterns or root cause"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "High",
          "why": "AutoDAN empirically evaluates the naturalness (stealthiness) of its generated jailbreak prompts using sentence perplexity (PPL) measured by GPT-2, demonstrating substantially lower PPL scores than the token-level baseline and comparable scores to handcrafted prompts. This quantitative validation shows that AutoDAN\u2019s outputs remain semantically meaningful and realistic.",
          "evidence": "Section 4.1 Metrics: \u201cFor stealthiness, we use standard Sentence Perplexity (PPL) evaluated by GPT-2 as the metric.\u201d; Section 4.2 Attack Effectiveness and Stealthiness, Table 1 reports PPL scores, where AutoDAN-HGA achieves PPL\u224846.47 on Vicuna-7b versus GCG\u2019s PPL\u22481532.16, indicating preserved naturalness."
        },
        "gemini": {
          "value": "High",
          "why": "The paper explicitly states the goal of preserving semantic meaningfulness and fluency (stealthiness) of generated jailbreak prompts. It quantitatively evaluates this aspect using standard Sentence Perplexity (PPL) as a metric, comparing its method (AutoDAN) against baselines. The results, presented in Table 1, demonstrate that AutoDAN achieves a much lower PPL than the baseline GCG, indicating more semantically meaningful and stealthy attacks. Furthermore, the paper shows AutoDAN's ability to bypass perplexity-based defense methods, reinforcing its effectiveness in generating natural-sounding inputs.",
          "evidence": "Page 2, Abstract: 'AutoDAN not only automates the process while preserving semantic meaningfulness... Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively.'\n\nPage 2, Section 1 Introduction: 'We aim to propose a method that preserves the meaningfulness and fluency (i.e., stealthiness) of jailbreak prompts, akin to handcrafted ones... As a result, such features ensure that our method can bypass defenses like perplexity detection while maintaining scalability and adaptability.'\n\nPage 7, Metrics: 'For stealthiness, we use standard Sentence Perplexity (PPL) evaluated by GPT-2 as the metric.'\n\nPage 7, Attack Effectiveness and Stealthiness: 'Moreover, when we see the stealthiness metric PPL, we can find that our method can achieve much lower PPL than the baseline, GCG and is comparable with Handcrafted DAN. All these results demonstrate that our method can generate stealthy jailbreak prompts successfully.'\n\nPage 7, Table 1: Attack effectiveness and Stealthiness. (Shows PPL values for Handcrafted DAN, GCG, AutoDAN-GA, AutoDAN-HGA across different models).\n\nPage 7, Effectiveness against defense: 'As demonstrated in Tab. 3, the perplexity defense significantly undermines the effectiveness of the token-level jailbreak attack, i.e., GCG attack. However, the semantically meaningful jailbreak prompts AutoDAN (and the original handcrafted DAN) is not influenced. These findings underscore the capability of our method to generate semantically meaningful content similar to benign text, verifying the stealthiness of our method.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "AutoDAN empirically evaluates the naturalness (stealthiness) of its generated jailbreak prompts using sentence perplexity (PPL) measured by GPT-2, demonstrating substantially lower PPL scores than the token-level baseline and comparable scores to handcrafted prompts. This quantitative validation shows that AutoDAN\u2019s outputs remain semantically meaningful and realistic.",
          "evidence": "Section 4.1 Metrics: \u201cFor stealthiness, we use standard Sentence Perplexity (PPL) evaluated by GPT-2 as the metric.\u201d; Section 4.2 Attack Effectiveness and Stealthiness, Table 1 reports PPL scores, where AutoDAN-HGA achieves PPL\u224846.47 on Vicuna-7b versus GCG\u2019s PPL\u22481532.16, indicating preserved naturalness."
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "High",
          "why": "AutoDAN generates semantic\u2010level text prompts\u2014discrete artifacts that do not rely on fragile floating\u2010point perturbations\u2014so the induced jailbreak failures persist unchanged when the prompts are serialized, stored, or otherwise subjected to standard I/O operations.",
          "evidence": "Section 4.2 (Attack Effectiveness and Stealthiness, p. 7): \u201cWe share the standardized Sentence Perplexity (PPL) score \u2026 Compared with the baseline, our method exhibits superior performance in terms of PPL, indicating more semantically meaningful and stealthy attacks being generated.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "AutoDAN's method generates jailbreak prompts, which are discrete text strings. The metric definition explicitly states that \"model-generated fuzzed inputs\u2014such as ... generated text\u2014are inherently stable under I/O operations because they originate from self-contained generation processes that produce finalized artifacts rather than transient numeric states.\" AutoDAN's hierarchical genetic algorithm, which operates on structured discrete text data through operations like LLM-based diversification, crossover, and word replacement, directly produces these finalized textual artifacts. These are not continuous perturbations susceptible to floating-point precision loss during I/O.",
          "evidence": "Page 1, Abstract: \"In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm.\"\nPage 2, Figure 1(a) Caption: \"We propose AutoDAN, a novel method that can generate semantically meaningful jailbreak prompts against aligned LLMs. AutoDAN employs a hierarchical genetic algorithm that we specially designed for the optimization in structured discrete data.\"\nPage 2, Section 3.4.2, Paragraph 1: \"Based on the aforementioned insights, we propose AutoDAN, a hierarchical genetic algorithm tailored specifically for structured discrete data like prompt text.\"\nMetric Definition (provided): \"model-generated fuzzed inputs\u2014such as synthesized speech, rendered images, or generated text\u2014are inherently stable under I/O operations because they originate from self-contained generation processes that produce finalized artifacts rather than transient numeric states.\""
        },
        "arbitrator": {
          "value": "High",
          "why": "AutoDAN generates semantic\u2010level text prompts\u2014discrete artifacts that do not rely on fragile floating\u2010point perturbations\u2014so the induced jailbreak failures persist unchanged when the prompts are serialized, stored, or otherwise subjected to standard I/O operations.",
          "evidence": "Section 4.2 (Attack Effectiveness and Stealthiness, p. 7): \u201cWe share the standardized Sentence Perplexity (PPL) score \u2026 Compared with the baseline, our method exhibits superior performance in terms of PPL, indicating more semantically meaningful and stealthy attacks being generated.\u201d"
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "High",
          "why": "AutoDAN empirically demonstrates that the same failing inputs (jailbreak prompts) cause equivalent erroneous behaviors across multiple models. In Table 2, prompts generated on one model (e.g., Vicuna-7B) achieve high attack success rates when applied to other models (e.g., 0.7058 ASR on Guanaco-7b), indicating input-level reproducibility of failures across models.",
          "evidence": "Section 4.2 \u201cTransferability\u201d, Table 2: \u201cAutoDAN-HGA \u2026 0.7058 ASR transferring from Vicuna-7B to Guanaco-7b\u201d \u2013 demonstrating the same prompts induce failures consistently across models."
        },
        "gemini": {
          "value": "High",
          "why": "The paper empirically demonstrates that the same failing inputs (jailbreak prompts) generated by AutoDAN on one model cause erroneous behaviors across multiple other models performing the same task. This aligns with the definition of 'High' transferability, which requires that 'the same failing inputs cause equivalent erroneous behaviors across multiple models performing the same task.' The authors explicitly evaluate this by generating prompts on a source model and then testing them against various black-box and white-box LLMs.",
          "evidence": "Page 1, Abstract: \"Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability...\nPage 8, Section 4.2, 'Transferability' subsection: \"We further investigate the transferability of our method... We conduct the evaluations by taking the jailbreak prompts with their corresponding requests and targeting another LLM. The results are shown in Tab. 2. AutoDAN exhibits a much better transferability in attacking the black-box LLMs compared with the baseline.\"\nPage 8, Table 2: 'Cross-model transferability.' This table presents ASR and Recheck scores when jailbreak prompts generated on a 'Source Model' (e.g., Vicuna-7B) are used to attack other models (e.g., Guanaco-7b, Llama2-7b-chat).\nPage 20, Table 9: 'The attack performance of AutoDAN-GA on OpanAI's GPT-3.5-turbo and GPT-4.' This table shows attack success rates for prompts transferred from Vicuna and Llama2 to GPT-3.5-turbo and GPT-4. The accompanying text states: \"To summarize, our proposed method transfers well to the March version of GPT-3.5-turbo and surpasses the baseline GCG by a large margin, but has low transferability on the latest GPT-4, like the baseline GCG.\""
        },
        "arbitrator": {
          "value": "High",
          "why": "AutoDAN empirically demonstrates that the same failing inputs (jailbreak prompts) cause equivalent erroneous behaviors across multiple models. In Table 2, prompts generated on one model (e.g., Vicuna-7B) achieve high attack success rates when applied to other models (e.g., 0.7058 ASR on Guanaco-7b), indicating input-level reproducibility of failures across models.",
          "evidence": "Section 4.2 \u201cTransferability\u201d, Table 2: \u201cAutoDAN-HGA \u2026 0.7058 ASR transferring from Vicuna-7B to Guanaco-7b\u201d \u2013 demonstrating the same prompts induce failures consistently across models."
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "mutation_strategy": [
        "Rule-based",
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ],
      "oracle": [
        "Property-based"
      ]
    }
  },
  "He et al. - 2024 - Curiosity-Driven Testing for Sequential Decision-Making Process": {
    "key": "he2024curiosity",
    "name": "CureFuzz",
    "year": "2024",
    "citation_count": "12",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "High",
          "why": "CureFuzz uncovers crash-triggering scenarios that bypass the explicit safety boundaries of sequential decision-making systems (e.g., collisions in autonomous driving, falls in robotic locomotion). These failures represent safety-critical breaches rather than mere functional errors or specification misalignments, demonstrating the fuzzer\u2019s ability to penetrate and reveal vulnerabilities in the systems\u2019 safety mechanisms.",
          "evidence": "Page 1, Abstract: \u201cto identify a diverse set of crash-triggering scenarios\u201d\nPage 4, Section 3.1 (Assumption): \u201cCureFuzz is designed to uncover crash-triggering scenarios that eventually lead to the crash of SDMs.\u201d\nPage 6, Section 4.2.1 (Autonomous Driving): \u201cin the CARLA simulator ... the SDM-controlled vehicle experiences a collision with other vehicles or buildings.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "CUREFUZZ is designed to identify 'crash-triggering scenarios' and 'catastrophic failures' in sequential decision-making processes, particularly in safety-critical applications like autonomous driving, aircraft collision avoidance, and robotic control. The paper explicitly defines these 'crashes' as incidents where an autonomous vehicle collides, an airplane collides, or a robot falls, directly bypassing explicit safety mechanisms. This aligns with the 'High' severity level of uncovering vulnerabilities that bypass explicit safety, policy, or security mechanisms, rather than mere functional errors or undesirable behaviors.",
          "evidence": "Page 1, ABSTRACT: \"SDMs remain vulnerable to learning unsafe behaviors, posing significant risks in safety-critical applications.\" \nPage 2, Contributions: \"We introduce CUREFUZZ, the first curiosity-driven black-box fuzz testing framework for DL-based sequential decision makers. CUREFUZZ aims to reveal a diverse set of crash-triggering scenarios, enhancing the safety and effectiveness of these decision-making systems.\"\nPage 4, Section 3.2 Approach Overview: \"CUREFUZZ is designed to uncover crash-triggering scenarios that eventually lead to the crash of SDMs. ... For instance, within the context of autonomous driving, a crash could be defined as an incident where an autonomous vehicle collides with pedestrians. In the context of robotics control, a crash can refer to the falling of a walking robot. ... Furthermore, our methodology focuses on catastrophic failure rather than minor deviations from optimal performance.\"\nPage 6, Section 4.2 Experiment Subject and Environment:\n- Autonomous Driving (CARLA): \"In CARLA, CUREFUZZ checks for the situations when the SDM-controlled vehicle experiences a collision with other vehicles or buildings.\"\n- Aircraft Collision Avoidance (ACAS Xu): \"In ACAS Xu, CUREFUZZ simply aims to find the scenarios when there are collisions between the SDM-controlled airplane with other airplanes.\"\n- BipedalWalker (DRL): \"We aim to find the scenarios when the robot falls.\""
        },
        "arbitrator": {
          "value": "High",
          "why": "CureFuzz uncovers crash-triggering scenarios that bypass the explicit safety boundaries of sequential decision-making systems (e.g., collisions in autonomous driving, falls in robotic locomotion). These failures represent safety-critical breaches rather than mere functional errors or specification misalignments, demonstrating the fuzzer\u2019s ability to penetrate and reveal vulnerabilities in the systems\u2019 safety mechanisms.",
          "evidence": "Page 1, Abstract: \u201cto identify a diverse set of crash-triggering scenarios\u201d\nPage 4, Section 3.1 (Assumption): \u201cCureFuzz is designed to uncover crash-triggering scenarios that eventually lead to the crash of SDMs.\u201d\nPage 6, Section 4.2.1 (Autonomous Driving): \u201cin the CARLA simulator ... the SDM-controlled vehicle experiences a collision with other vehicles or buildings.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Uncovers crash-triggering scenarios (e.g., collisions, falls, or loss of control) across autonomous driving, aviation, and robotics systems. exposes behaviorally unsafe yet functionally valid failures \u2014 revealing unintended consequences of learned policies rather than breaches of safety enforcement mechanisms."
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Medium",
          "why": "CureFuzz explicitly targets the semantic category of safety faults\u2014\u2018crash-triggering scenarios\u2019\u2014rather than performing broad untargeted exploration. However, it does not steer tests toward a single predefined crash condition or specific output, but rather any scenario that leads to an unsafe (crash) outcome, fitting the definition of medium directedness.",
          "evidence": "Abstract (page 1): \u201cwe propose CureFuzz, a novel curiosity-driven black-box fuzz testing approach for SDMs\u2026 leading to improved detection of crash-triggering scenarios.\u201d\nSection 3.1 Assumption (page 5): \u201cCureFuzz is designed to uncover crash-triggering scenarios that eventually lead to the crash of SDMs\u2026 our methodology focuses on catastrophic failure rather than minor deviations.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "CUREFUZZ is designed to uncover 'crash-triggering scenarios' that lead to 'catastrophic failures' or 'unsafe behaviors' in Sequential Decision-Making Processes (SDMs). This approach focuses on identifying a specific *category* of faults (e.g., collisions in autonomous driving, robots falling, agent collisions in games) rather than generic misclassifications or highly specific, predefined outputs. The paper explicitly states that the methodology focuses on 'catastrophic failure rather than minor deviations from optimal performance,' which aligns with triggering semantic or category-level faults without constraining the exact output.",
          "evidence": "Section 3.2 Approach Overview, Page 4: 'CUREFUZZ is designed to uncover crash-triggering scenarios that eventually lead to the crash of SDMs. It is crucial to note that the definition of a 'crash' can vary across different environments. For instance, within the context of autonomous driving, a crash could be defined as an incident where an autonomous vehicle collides with pedestrians. In the context of robotics control, a crash can refer to the falling of a walking robot. We employ the term 'crash' to represent its broader conceptual meaning. Furthermore, our methodology focuses on catastrophic failure rather than minor deviations from optimal performance.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "CureFuzz explicitly targets the semantic category of safety faults\u2014\u2018crash-triggering scenarios\u2019\u2014rather than performing broad untargeted exploration. However, it does not steer tests toward a single predefined crash condition or specific output, but rather any scenario that leads to an unsafe (crash) outcome, fitting the definition of medium directedness.",
          "evidence": "Abstract (page 1): \u201cwe propose CureFuzz, a novel curiosity-driven black-box fuzz testing approach for SDMs\u2026 leading to improved detection of crash-triggering scenarios.\u201d\nSection 3.1 Assumption (page 5): \u201cCureFuzz is designed to uncover crash-triggering scenarios that eventually lead to the crash of SDMs\u2026 our methodology focuses on catastrophic failure rather than minor deviations.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Steer toward safetly faults (crash-triggering scenarios) in in autonomous driving and other sequential decision-making systems"
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "CureFuzz is a purely black-box fuzzing approach that focuses on exposing crash-triggering scenarios without inspecting or interpreting any internal model signals (activations, gradients, feature maps) or external failure patterns to suggest diagnostic causes. It reports when and where crashes occur but does not analyze the underlying architectural, training, or data weaknesses that lead to those failures.",
          "evidence": "Section 3.1 (Approach Overview): \u201cCureFuzz performs fuzz testing in a black-box manner\u2026 CureFuzz does not require access to the SDM\u2019s internal logic, nor knowledge of the environment\u2019s transition dynamics or reward mechanism.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "CUREFUZZ goes beyond merely reporting faulty cases by providing heuristic and correlational explanations for where and how failures manifest. It leverages a 'curiosity mechanism' based on prediction error, which serves as a proxy for the model's uncertainty in exploring specific scenarios. High prediction error (curiosity) indicates regions where the model is unfamiliar or struggles, implicitly linking these areas to where failures are likely to occur due to lack of learned patterns or epistemic uncertainty. Additionally, the paper discusses external interpretations of behavioral patterns, such as the spatial distribution of crash scenarios, indicating certain models might exhibit a bias towards boundary conditions. While CUREFUZZ does not identify causal links to specific architectural or training weaknesses, it provides valuable insights into the model's behavioral patterns and regions of uncertainty.",
          "evidence": "Page 4, Section 3.3, 'A Curiosity-driven Search Strategy': 'Our curiosity mechanism calculates the extent of the fuzzer's curiosity in exploring specific scenarios and encourages the fuzzer to prioritize scenarios with higher curiosity. Specifically, when presented with a scenario, our curiosity mechanism will predict the subsequent states within its environment. The discrepancy between our prediction and the actual outcome (prediction error) reflects the level of the fuzzer's curiosity. A scenario with higher curiosity suggests that it is unfamiliar to the fuzzer, as the fuzzer cannot accurately predict it.'\n\nPage 6, Section 4.2.1, 'Autonomous Driving': 'In CARLA, CUREFUZZ checks for the situations when the SDM-controlled vehicle experiences a collision with other vehicles or buildings.' This defines what constitutes a crash.\n\nPage 7, Section 5, 'Total Number of Crashes': 'We perform additional analysis to investigate why it is happening for Coop Navi(MARL). Recall that the goal of the Coop Navi game is to find the scenarios when the agents collide with each other, in Figure 2, we select the median performance from the five repeated experiments for CUREFUZZ and G-Model, and plot the positions of the agents in the crash-triggering scenarios found by different methods. While the valid range of the agent's position is from -1 to 1, from Figure 2, we can see that G-Model tends to find the crashes that the agents are positioned around the boundary of the valid input. In contrast, CUREFUZZ covers more spread positions.' This illustrates an external interpretation of behavioral patterns.\n\nPage 9, Section 6, 'Facilitating Fault Detection through Curiosity Mechanism Optimization': 'The prediction error of the target network and the predictor network then serves as a proxy for measuring uncertainty, specifically epistemic uncertainty [28]. Epistemic uncertainty would be particularly high in regions of the input space where few similar examples were seen in the predictor network's training data. By focusing on areas of high epistemic uncertainty, where the predictor network struggles, the testing process inherently explores a wider variety of scenarios.' This connects an internal signal (prediction error/epistemic uncertainty) to where and how failures manifest."
        },
        "arbitrator": {
          "value": "Low",
          "why": "CureFuzz operates purely as a black-box fuzzer: it discovers crash-triggering inputs and uses a novelty heuristic to guide exploration, but it does not inspect or interpret any of the SDM\u2019s internal activations, gradients, or learned representations, nor does it analyze failure patterns to suggest specific architectural, training, or data causes. Consequently, it only reports when and where the model fails, without providing diagnostic insights into the root causes of those failures.",
          "evidence": "Section 3.1 (Approach Overview): \u201cCureFuzz performs fuzz testing in a black-box manner\u2026 CureFuzz does not require access to the SDM\u2019s internal logic, nor knowledge of the environment\u2019s transition dynamics or reward mechanism.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "CureFuzz enforces theoretical design constraints to preserve naturalness by applying only small, bounded perturbations within each environment\u2019s legitimate state space and discarding any mutated seeds that fall outside these valid ranges. However, it does not perform any empirical validation (e.g., LPIPS, SSIM, BLEU) or human evaluation to quantitatively demonstrate that its fuzzed inputs remain perceptually or semantically consistent with real\u2010world data.",
          "evidence": "Section 3.4 Seed Mutation: \u201cWe need to ensure that the mutated seed lies in the legitimate state spaces, and the mutated seed would not trigger an initial crash.\u201d\nSection 4.2 Experiment Subject and Environment (CARLA): \u201cWhen mutating a seed, small perturbations are randomly generated and added to the positions and angles of these vehicles. We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The fuzzer, CUREFUZZ, employs design constraints such as 'small random perturbations' and bounds on state variables (e.g., maximal speed, valid ground types) to maintain input naturalness. Crucially, it empirically validates these constraints by checking the validity of mutated states against the environment's legitimate state space and discarding any 'illegally mutated states' or 'states that fall outside legal space'. While this process ensures that fuzzed inputs remain plausible and semantically valid within the simulation's rules, it does not provide quantitative empirical evidence of perceptual or semantic consistency with real-world data (e.g., LPIPS, SSIM, BLEU metrics) or human evaluation of such naturalness, which is required for a 'High' rating. The method goes beyond mere theoretical justification by actively filtering invalid states, but it lacks the advanced naturalness validation described for the 'High' category.",
          "evidence": "Page 5, Section 3.4 'Seed Mutation': 'We need to ensure that the mutated seed lies in the legitimate state spaces, and the mutated seed would not trigger an initial crash. In our experiments, we have carefully addressed this concern and verified the validity of each mutated seed, details are given in Section 4.'\nPage 6, Section 4.2.1 'Autonomous Driving': 'When CUREFUZZ mutates a given state, small perturbations are randomly generated and added to the positions and angles of these vehicles. We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.'\nPage 6, Section 4.2.2 'Aircraft Collision Avoidance': 'For mutation, the initial positions and speeds of the SDM-controlled and the other planes slightly changed. The maximal speed of all airplanes is capped at 1,100 ft/sec. Given the predefined range of acceptable states, states that fall outside legal space are automatically discarded.'\nPage 6, Section 4.2.3 'Video Game': 'With a clear definition of permissible positions and velocities, CUREFUZZ ignores any illegal states.'\nPage 6, Section 4.2.3 'BipedalWalker': 'Since the valid ground types are known, illegal states can be easily detected and discarded.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "CureFuzz enforces theoretical design constraints to preserve naturalness by applying only small, bounded perturbations within each environment\u2019s legitimate state space and discarding any mutated seeds that fall outside these valid ranges. However, it does not perform any empirical validation (e.g., LPIPS, SSIM, BLEU) or human evaluation to quantitatively demonstrate that its fuzzed inputs remain perceptually or semantically consistent with real\u2010world data.",
          "evidence": "Section 3.4 Seed Mutation: \u201cWe need to ensure that the mutated seed lies in the legitimate state spaces, and the mutated seed would not trigger an initial crash.\u201d\nSection 4.2 Experiment Subject and Environment (CARLA): \u201cWhen mutating a seed, small perturbations are randomly generated and added to the positions and angles of these vehicles. We use the CARLA simulator itself to check for the validity of the mutated state. All illegally mutated states and the states that trigger initial collision are discarded in our experiments.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "CureFuzz\u2019s faults arise from semantic\u2010level perturbations to continuous scenario parameters (e.g. small random changes to vehicle positions, angles, speeds or ground types) without any explicit clipping or rounding to guarantee stability under downstream I/O quantization. Such semantic\u2010level transformations may not persist if the inputs were serialized (e.g. quantized to 8-bit images or 16-bit audio) and then reloaded.",
          "evidence": "Section 3.4 Seed Mutation (p. 9): \u201cOnce a seed is selected from the corpus, the SeedMutation function generates a new mutated state by applying a small random perturbation to a selected state. The mutated state is fed into the EnvMonitor function \u2026 We need to ensure that the mutated seed lies in the legitimate state spaces \u2026\u201d  No subsequent clipping or rounding for I/O stability is described."
        },
        "gemini": {
          "value": "Low",
          "why": "The paper describes mutations as 'small random perturbations' applied to continuous environmental parameters such as positions, angles, and speeds in most of its experimental setups (CARLA, ACAS Xu, Coop Navi). However, it does not explicitly state that these perturbations are validated for I/O stability through mechanisms like clipping, rounding, or quantization to ensure their persistence after standard I/O operations. Without such explicit handling, small floating-point changes are susceptible to disappearing due to precision loss when inputs are processed or stored, aligning with the 'Low' category of vulnerability persistence.",
          "evidence": "Page 5, Section 3.4 'Seed Mutation': 'Once a seed is selected from the corpus, the Seed-Mutation function generates a new mutated state by applying a small random perturbation to a selected state. The mutated state is fed into the EnvMonitor function to generate its corresponding state sequence and collect the cumulative reward.'\nPage 6, Section 4.2.1 'Autonomous Driving': 'When CUREFUZZ mutates a given state, small perturbations are randomly generated and added to the positions and angles of these vehicles.'\nPage 6, Section 4.2.2 'Aircraft Collision Avoidance': 'For mutation, the initial positions and speeds of the SDM-controlled and the other planes slightly changed.'\nPage 6, Section 4.2.3 'Video Game': 'The initial positions of the three MARL-controlled agents are mutated, and their initial speeds are set to 0 to avoid initial collision.'\nPage 7, Section 4.3 'Implementation': 'When mutating a seed, the magnitude of the random perturbation is a critical factor that can impact the performance of the fuzzer.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "CureFuzz\u2019s mutations operate at the semantic level\u2014small perturbations to scenario parameters such as vehicle positions, angles, speeds or ground types\u2014without any explicit clipping or rounding to guarantee persistence through downstream I/O quantization. These are richer, semantic\u2010level changes rather than trivial pixel\u2010 or bit\u2010level noise, placing them squarely in the \u201cMedium\u201d category: they may remain stable under I/O, but persistence is not explicitly enforced.",
          "evidence": "Section 3.4 Seed Mutation (p. 9): \u201cOnce a seed is selected from the corpus, the SeedMutation function generates a new mutated state by applying a small random perturbation to a selected state\u2026 We need to ensure that the mutated seed lies in the legitimate state spaces\u2026\u201d  No subsequent clipping, rounding, or I/O\u2010stability validation is described."
        },
        "manual": {
          "value": "High",
          "why": "Mutates continuous feature vectors representing simulator states (positions, angles, velocities) and enforces validity checks to keep them within legal bounds."
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper evaluates CureFuzz separately on each sequential decision-making model (DNN, DRL, MARL, IL) and reports crash-triggering scenarios per model, but does not assess whether the same failing inputs (or mutation patterns) transfer failures across different models performing the same task.",
          "evidence": "Section 5.1 RQ1 (page 10): \u201cWe repeat the experiment five times for each SDM and report the average results.\u201d Table 2 reports crashes found for each SDM individually, with no cross-model analysis."
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates the proposed CUREFUZZ method on multiple sequential decision-making models across various tasks. While it uses different models, including two distinct models (DRL and IL) for the autonomous driving task in CARLA, the evaluation of crash-triggering scenarios (total number of faults and distinct types) is conducted and reported independently for each model. The paper does not analyze or demonstrate whether specific failing inputs discovered for one model cause equivalent erroneous behaviors in other models performing the same task (High transferability), nor does it analyze cross-model correlations to show that the same mutation patterns systematically induce similar failure behaviors across models (Medium transferability). The results consistently compare the performance of CUREFUZZ against baselines for each SDM individually.",
          "evidence": "Page 6, Section 4.2 Experiment Subject and Environment, 'We evaluate CUREFUZZ using the same environments and SDMs as Pang et al. [56]. Our investigation covers various environments, including the CARLA autonomous driving simulator [16], ACAS Xu for collision avoidance in aviation [49], the Cooperative Navigation (Coop Navi) environment for multi-agent reinforcement learning [45], and the BipedalWalker environment in OpenAI Gym [41].' \nPage 6, Section 4.2.1 Autonomous Driving, 'Two SDMs are considered in the CARLA environment, which are developed using DRL and IL, respectively.'\nPage 7, Section 'Total Number of Crashes', 'Table 2 shows the comparative results of CUREFUZZ and baseline methods in terms of the number of detected crashes. The results show that CUREFUZZ consistently outperforms MDPFuzz in all five sets of experiments. The most significant improvement is observed in the SDM of BipedalWalker(RL), with a remarkable 422.22% increase in detected crashes, rising from 126 to 658. For CARLA(RL) and CARLA(IL), the improvements are 145.83% (from 120 to 295) and 110.47% (from 86 to 181), respectively; for Coop Navi(MARL), the performance is boosted by 62.21% (from 52.4 to 85). The smallest improvement is for ACAS Xu(DNN), which is 46.45% (from 183 to 268).' (This highlights independent performance for each model).\nPage 9, Section 'RQ3: Can we use the crashes found by CUREFUZZ to improve SDMs?', 'For the repaired ACAS Xu, 55 faults and 4 distinct types of crash-triggering scenarios are detected. We find that the crashes found by CUREFUZZ are reduced by 73% compared with the original DNN model.' (This shows improvement for a single, repaired model, not cross-model transferability)."
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper evaluates CureFuzz separately on each sequential decision-making model (DNN, DRL, MARL, IL) and reports crash-triggering scenarios per model, but does not assess whether the same failing inputs (or mutation patterns) transfer failures across different models performing the same task.",
          "evidence": "Section 5.1 RQ1 (page 10): \u201cWe repeat the experiment five times for each SDM and report the average results.\u201d Table 2 reports crashes found for each SDM individually, with no cross-model analysis."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ],
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Property-based"
      ]
    }
  },
  "Zhou et al. - 2025 - Understanding the Effectiveness of Coverage Criteria for Large Language Models A Special Angle from": {
    "key": "zhou2025understanding",
    "name": "Zhou et al.",
    "year": "2025",
    "citation_count": "7",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "High",
          "why": "The proposed method focuses on detecting and generating jailbreak attacks, which explicitly bypass the model\u2019s safety and policy mechanisms\u2014revealing system-level security vulnerabilities rather than mere functional or behavioral issues.",
          "evidence": "Abstract (p. 730): \u201cwe develop a real-time jailbreak detection mechanism that achieves high accuracy \u2026 in classifying queries as normal or jailbreak.\u201d; Introduction, Section I (p. 731): \u201cThis motivates us to explore white-box coverage criteria to examine LLMs\u2019 internal behavior, enabling more effective testing and defenses \u2026 focusing on the critical security issue of \u2018jailbreak attacks\u2019.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The research paper focuses on evaluating the effectiveness of coverage criteria in identifying and generating 'jailbreak attacks' against Large Language Models (LLMs). These attacks are explicitly defined as vulnerabilities that bypass explicit safety and alignment mechanisms to induce LLMs to produce harmful content, thereby compromising their security. The paper develops and demonstrates applications such as 'Real-time Jailbreak Detection' and 'Jailbreak Case Generation,' directly aligning with the metric's highest severity level for uncovering vulnerabilities that breach security boundaries.",
          "evidence": "Page 1, Abstract: \"...evaluate the effectiveness of traditional coverage criteria in identifying such inadequacies, exemplified by the significant security concern of jailbreak attacks.\" \nPage 1, Introduction: \"...particularly jailbreaking to generate harmful content [7]-[13], which causes severe societal impacts. This underscores the need for effective testing techniques to identify such attacks and maintain the trustworthiness of LLMs.\"\nPage 3, Section D. Jailbreak Attack and Defenses: \"Despite these efforts, jailbreak attacks have emerged that bypass alignment measures, causing LLMs to generate harmful outputs and compromising their security [8]\u2013[12].\"\nPage 4, Section III. Study Design A. Motivation: A Cluster Analysis Experiment: \"Attack queries are generated by appending adversarial suffixes to rejected queries using GCG [7]. These queries aim to trigger the model to output malicious content (i.e., abnormal behaviors).\"\nPage 9, Section V. Application: \"First, we propose Real-time Jailbreak Detection... Third, we investigate Jailbreak Case Generation, where coverage-guided methods refine prompts to generate adversarial examples.\""
        },
        "arbitrator": {
          "value": "High",
          "why": "The proposed method focuses on detecting and generating jailbreak attacks, which explicitly bypass the model\u2019s safety and policy mechanisms\u2014revealing system-level security vulnerabilities rather than mere functional or behavioral issues.",
          "evidence": "Abstract (p. 730): \u201cwe develop a real-time jailbreak detection mechanism that achieves high accuracy \u2026 in classifying queries as normal or jailbreak.\u201d; Introduction, Section I (p. 731): \u201cThis motivates us to explore white-box coverage criteria to examine LLMs\u2019 internal behavior, enabling more effective testing and defenses \u2026 focusing on the critical security issue of \u2018jailbreak attacks\u2019.\u201d"
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Medium",
          "why": "The proposed method uses coverage\u2010guided generation to systematically steer exploration toward triggering jailbreak vulnerabilities\u2014i.e., category\u2010level semantic faults\u2014without constraining the model to produce a specific phrase or exact output.",
          "evidence": "Abstract (p. 730): \u201cwe introduce a coverage-guided approach for generating jailbreak attack examples, enabling systematic refinement of prompts to uncover vulnerabilities.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The proposed method explicitly aims to identify and generate inputs that trigger specific categories of undesirable behaviors in Large Language Models (LLMs), primarily focusing on 'jailbreak attacks' and the generation of 'harmful content' or 'abnormal behaviors.' It does not steer exploration towards a single, predefined exact output, nor does it perform entirely untargeted exploration for generic faults. Instead, it seeks to reveal semantic or category-level faults related to LLM security.",
          "evidence": "Page 1, Abstract: 'Our research uncovers significant differences in neuron coverage when LLMs process normal versus jailbreak queries, aligning with our clustering experiments. Leveraging these findings, we propose three practical applications of coverage criteria in the context of LLM security testing. Specifically, we develop a real-time jailbreak detection mechanism that achieves high accuracy (93.61% on average) in classifying queries as normal or jailbreak... Lastly, we introduce a coverage-guided approach for generating jailbreak attack examples, enabling systematic refinement of prompts to uncover vulnerabilities.'\nPage 3, Section III. STUDY DESIGN, A. Motivation: A Cluster Analysis Experiment: 'Attack queries are generated by appending adversarial suffixes to rejected queries using GCG [7]. These queries aim to trigger the model to output malicious content (i.e., abnormal behaviors).'\nPage 5, Section III. C. Standards for Coverage Evaluation, Requirement 2: Sensitivity to Attack Queries: 'Building upon the defect-detection capability emphasized in traditional coverage criteria, we extend this concept under the jailbreak perspective for LLMs... An effective coverage criterion should be particularly sensitive to attack queries that induce abnormal behaviors, such as generating harmful or disallowed content.'\nPage 9, Section V. APPLICATION, A. Jailbreak Detection: 'First, we propose Real-time Jailbreak Detection, using activation features from coverage criteria to classify queries as normal or jailbreak, enabling systematic identification of high-risk interactions.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "The proposed method uses coverage\u2010guided generation to systematically steer exploration toward triggering jailbreak vulnerabilities\u2014i.e., category\u2010level semantic faults\u2014without constraining the model to produce a specific phrase or exact output.",
          "evidence": "Abstract (p. 730): \u201cwe introduce a coverage-guided approach for generating jailbreak attack examples, enabling systematic refinement of prompts to uncover vulnerabilities.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": ""
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper\u2019s input mutations (e.g., paraphrasing via GPT-4 and appending adversarial suffixes with GCG) are heuristic and no empirical evaluation of their realism or naturalness (e.g., via BLEU, human study, or LPIPS/SSIM) is provided. Thus the fuzzer does not ensure or validate that the generated inputs remain plausible.",
          "evidence": "Page 732, Section III.A (Motivation: A Cluster Analysis Experiment): \u201c\u2779 Attack queries are generated by appending adversarial suffixes to rejected queries using GCG [7]. These queries aim to trigger the model to output malicious content (i.e., abnormal behaviors).\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper leverages GPT-4 for 'prompt rewriting' to generate new jailbreak queries, which can be interpreted as a design constraint or theoretical justification aimed at preserving input naturalness. GPT-4 inherently produces semantically valid and plausible text, suggesting that the generated inputs would be realistic within the model's input domain. However, the paper does not provide explicit empirical validation, such as quantitative metrics (e.g., BLEU) or human evaluation, to confirm the naturalness of these generated prompts. The evaluation focuses on the *effectiveness* of these queries in generating successful jailbreak attacks rather than their realism.",
          "evidence": "Page 11, Section V.C 'Jailbreak Case Generation', 'Setup': 'We use Llama-2-7b-chat as the target model, initializing with five jailbreak queries as seeds. Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting. The query with the highest coverage increase is selected as the next seed in the coverage-guided approach. For comparison, a random strategy selects seeds randomly from rewritten candidates. Each method ultimately generates 250 new jailbreak queries to evaluate effectiveness.'\n\nPage 11, Section V.C 'Jailbreak Case Generation', 'Evaluation results': 'As shown in Figure 6, the coverage-guided method outperforms the random strategy in generating successful jailbreak queries.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper\u2019s input generation relies on heuristic methods (GPT-4 paraphrasing and GCG adversarial suffixes) without any empirical validation (e.g., BLEU scores or human studies) or explicit design constraints to ensure that the mutated inputs remain natural or realistic. There is no evaluation of input plausibility or semantic coherence, so the approach aligns with the lowest level of naturalness preservation.",
          "evidence": "Page 732, Section III.A: \u201c\u2779 Attack queries are generated by appending adversarial suffixes to rejected queries using GCG [7].\u201d\nPage 741, Section V.C: \u201cOver five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting\u2026 to evaluate effectiveness.\u201d (no mention of naturalness assessment)"
        },
        "manual": {
          "value": "Medium",
          "why": "GPT-4 generates  new jailbreak queries through prompt rewriting. The generated contents are supposed to be natural but there was no measure reported how natural are those. The attack suffix is related to jailbreak detection, not creation"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "High",
          "why": "The method generates fuzzed inputs via semantic-level text transformations (GPT-4 paraphrasing and adversarial suffixes) rather than floating-point perturbations. Such model-generated textual inputs are inherently stable under standard text I/O operations and do not depend on fragile numerical artifacts that could be lost during serialization.",
          "evidence": "Section III.A (Motivation: A Cluster Analysis Experiment): \u201cAttack queries are generated by appending adversarial suffixes to rejected queries using GCG [7].\u201d\nSection III.B (Generated Token Level): \u201cLLMs typically use an autoregressive architecture, predicting the next token step-by-step\u2026 the final token of the query sequence (T0) captures the complete information of the query, serving as a condensed representation of the entire sentence\u2019s semantics.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The research paper evaluates and proposes methods for detecting and generating jailbreak attacks on Large Language Models (LLMs). The inputs for LLMs are text queries and prompts. The 'mutations' or 'fuzzing' techniques described involve semantic-level modifications to these text inputs, such as appending adversarial suffixes (using GCG), paraphrasing queries (using GPT-4), and prompt rewriting (using GPT-4) for jailbreak case generation. These textual modifications and generated text inputs are inherently stable across standard I/O operations, as they do not rely on fragile floating-point precision that could be lost through rounding, quantization, or encoding processes applicable to continuous media like images or audio. The metric definition explicitly states that 'model-generated fuzzed inputs\u2014such as synthesized speech, rendered images, or generated text\u2014are inherently stable under I/O operations because they originate from self-contained generation processes that produce finalized artifacts rather than transient numeric states.' Since the methods employed by the paper primarily deal with and generate textual inputs and modifications, the discovered faults (jailbreak vulnerabilities) are stable and persistent.",
          "evidence": "Page 3, Section III.A, 'Experimental Setup': 'Attack queries are generated by appending adversarial suffixes to rejected queries using GCG [7].'\nPage 3, Section III.A, 'Experimental Setup': 'Synonymous queries are the paraphrased versions of normal queries by GPT-4, intended to trigger the same normal behaviors as the original queries.'\nPage 11, Section V.C, 'Jailbreak Case Generation, Setup': 'Over five iterations, GPT-4 generates ten new jailbreak queries per round through prompt rewriting.'\nPage 11, Section V.C, 'Jailbreak Case Generation, Evaluation results': 'In Round 1, our approach generates 6 successful queries compared to 2 by the random method. This gap widens in subsequent rounds, with our method producing 11, 19, 24, and 32 successful queries in Rounds 2 to 5, respectively, while the random strategy yields 2, 6, 9, and 12.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "The method generates fuzzed inputs via semantic-level text transformations (GPT-4 paraphrasing and adversarial suffixes) rather than floating-point perturbations. Such model-generated textual inputs are inherently stable under standard text I/O operations and do not depend on fragile numerical artifacts that could be lost during serialization.",
          "evidence": "Section III.A (Motivation: A Cluster Analysis Experiment): \u201cAttack queries are generated by appending adversarial suffixes to rejected queries using GCG [7].\u201d\nSection III.B (Generated Token Level): \u201cLLMs typically use an autoregressive architecture, predicting the next token step-by-step\u2026 the final token of the query sequence (T0) captures the complete information of the query, serving as a condensed representation of the entire sentence\u2019s semantics.\u201d"
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper evaluates coverage criteria and jailbreak generation/detection separately on individual models without assessing whether the same failing inputs (jailbreak prompts) cause equivalent misbehaviors across multiple models. There is no cross-model analysis demonstrating input\u2010level transfer of failures.",
          "evidence": "Section V.C (Jailbreak Case Generation), p. 738: \u201cSetup: We use Llama-2-7b-chat as the target model \u2026 Over five iterations, GPT-4 generates ten new jailbreak queries per round \u2026\u201d  (no evaluation of the same generated queries on other models)."
        },
        "gemini": {
          "value": "High",
          "why": "The paper empirically demonstrates that the method, which leverages specific coverage criteria (Neuron Coverage and Top-K Neuron Coverage), consistently identifies 'failing inputs' (jailbreak attack queries) as causing 'erroneous behaviors' (abnormal model responses or internal state changes indicative of a jailbreak) across multiple distinct Large Language Models (LLMs). The method's effectiveness in detecting jailbreaks is shown to be stable and generalizable across different models, with high accuracy in classifying queries as normal or jailbreak.",
          "evidence": "Page 5, Section C, 'Standards for Coverage Evaluation', Requirement 3: 'Stability and Generalization Ability': 'An effective coverage criterion should exhibit stability and strong generalization capabilities, providing consistent guidance for model testing regardless of specific model variations... this ensures that the coverage metric remains effective across different models...'\nPage 7, Section IV, 'Evaluation against Requirement 3': 'In contrast, NC and TKNC maintain stable performance across all models, with the highest variance being 0.01 for TKNC in the MLP layer on SNJ. This suggests that the performance of NLC, TFC, and TKNP is significantly influenced by changes in model architecture or parameter size, whereas NC and TKNC exhibit better generalization ability and stability. Such stability and generalization are crucial for the ongoing evaluation and testing of LLMs in practical applications, ensuring consistent testing results across various model configurations.'\nPage 8, Section IV, 'Answer to RQ1': 'NC and TKNC are relatively effective. They accurately identify synonymous and attack queries while ensuring stability and generalization across models. NLC demonstrates competitive performance but exhibits variability across different models.'\nPage 10, Section V.A., 'Evaluation results': 'As shown in Table V, our method achieves high average accuracies: 95.23% on OPT-125M, 93.92% on Llama-2-7B-Chat, and 91.69% on Pythia-12B, demonstrating its effectiveness across models.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper evaluates coverage criteria and jailbreak generation/detection separately on each model (OPT-125M, Llama-2-7B-Chat, Pythia-12B, Gemma-2-27B-it), but never analyzes whether the same jailbreak inputs that fail one model also induce failures in the others. There is no experiment applying identical attack prompts across models to demonstrate input\u2010level transfer of erroneous behaviors. Thus, fault transferability across models is not assessed.",
          "evidence": "Section V.C (Jailbreak Case Generation), p. 738: \u201cSetup: We use Llama-2-7b-chat as the target model \u2026\u201d (only Llama-2-7B-Chat is used for generation); no analogous evaluation on OPT-125M, Pythia-12B, or Gemma-2-27B-it. Section IV (Evaluation), RQ1\u2013RQ3: all coverage measurements are per\u2010model, with no cross\u2010model failure consistency analysis."
        },
        "manual": {
          "value": "Low",
          "why": "No analysis of fault transferability in input level of mutation level"
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Medium",
          "why": "The paper performs correlational analysis of internal signals\u2014e.g., clustering hidden\u2010state activations and measuring neuron coverage differences across layers and tokens\u2014to explain where and how jailbreak failures manifest. However, it does not establish causal mechanisms linking specific architectural or training shortcomings to these vulnerabilities.",
          "evidence": "Abstract (p. 730): \u201cOur research uncovers significant differences in neuron coverage when LLMs process normal versus jailbreak queries\u2026.\u201d; Section III.A (p. 732): \u201cIn summary, our clustering analysis demonstrates that the internal states of the model include features capable of representing and distinguishing the \u2018logic\u2019 of different behaviors.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper explains where and how vulnerabilities manifest within Large Language Models (LLMs) by conducting a comprehensive analysis of internal signals (hidden states, neuron activations) using various coverage criteria. It identifies distinct activation patterns for normal versus jailbreak queries, links these to specific model layers (Attention vs. MLP) and blocks, and analyzes the impact of different tokens. The explanations are correlational, clarifying how internal behaviors differ when processing malicious inputs, rather than establishing causal links to specific architectural or training weaknesses.",
          "evidence": "Page 2, Abstract: \"Our research uncovers significant differences in neuron coverage when LLMs process normal versus jailbreak queries, aligning with our clustering experiments.\" \n\nPage 4, Section III.A 'Findings': \"In summary, our clustering analysis demonstrates that the internal states of the model include features capable of representing and distinguishing the 'logic' of different behaviors. This confirms the feasibility of using internal states to design coverage criteria for LLMs.\"\n\nPage 7, Section IV.B 'Evaluation against Requirement 2': \"Furthermore, we observe significant coverage differences between normal (SN) and abnormal outputs (SNJ) in well-aligned models. ... As a result, normal and abnormal behaviors activate different neu- rons, causing coverage differences. ... Abnormal behaviors, such as bias or harmful content, activate distinct knowl- edge areas, triggering neurons linked to negative or harmful knowledge domains, which results in noticeable activation differences.\"\n\nPage 8, Section IV.C 'Attention Layer versus MLP Layer': \"These results indicate that Attention layers capture input features more effectively than MLP layers, enhancing the accuracy of coverage criteria in evaluating model testing. This may be because Attention layers better capture the global dependencies among input data, leading to a deeper understanding of input features. In contrast, MLP layers rely mainly on linear combinations of local features and fail to comprehensively capture the complex dependencies within the input data.\"\n\nPage 8, Section IV.C 'Impact of Different Blocks': \"The high RCG values in certain blocks suggest that the set of activated neurons varies significantly for different query types. These blocks play a crucial role in distinguishing complex query types, and NC effectively captures these features.\"\n\nPage 9, Section IV.D 'Answer to RQ3': \"Considering all four models, testing at the last token of the original query in the test suites proves to be the most effective. Generating additional tokens may lead to a reduction in the differences between test suites.\""
        },
        "arbitrator": {
          "value": "Medium",
          "why": "The paper performs correlational analysis of internal signals\u2014e.g., clustering hidden\u2010state activations and measuring neuron coverage differences across layers and tokens\u2014to explain where and how jailbreak failures manifest. However, it does not establish causal mechanisms linking specific architectural or training shortcomings to these vulnerabilities.",
          "evidence": "Abstract (p. 730): \u201cOur research uncovers significant differences in neuron coverage when LLMs process normal versus jailbreak queries\u2026.\u201d; Section III.A (p. 732): \u201cIn summary, our clustering analysis demonstrates that the internal states of the model include features capable of representing and distinguishing the \u2018logic\u2019 of different behaviors.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Provides correlation with internal coverage"
        }
      }
    },
    "taxonomy": {
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ],
      "access_level": [
        "Greybox"
      ],
      "oracle": [
        "Property-based"
      ]
    }
  },
  "Gong et al. - PAPILLON Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs": {
    "key": "gong2025papillon",
    "name": "PAPILLON",
    "year": "2025",
    "citation_count": "0",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "High",
          "why": "PAPILLON uncovers vulnerabilities that bypass the explicit safety and policy guardrails of aligned LLMs by automatically generating jailbreak prompts which successfully induce harmful outputs even under advanced defense mechanisms.",
          "evidence": "Page 2, Section 1 Introduction: \"Attackers aim to craft malicious prompts to mislead LLMs, bypass safety features, and generate responses with harmful, discriminatory, violent, or sensitive content.\""
        },
        "gemini": {
          "value": "High",
          "why": "PAPILLON is designed to uncover vulnerabilities that bypass explicit safety, policy, or security mechanisms in Large Language Models (LLMs). The paper explicitly states its goal is to create 'jailbreak prompts' that mislead LLMs into producing harmful content, thereby circumventing their safety features and usage policies. This directly aligns with the 'High' severity definition of uncovering vulnerabilities that penetrate safeguards and expose system-level flaws.",
          "evidence": "Page 1, Abstract: 'LLMs have excelled in various tasks but are still vulnerable to jailbreaking attacks, where attackers create jailbreak prompts to mislead the model to produce harmful or offensive content.'\nPage 1, Introduction: 'Attackers aim to craft malicious prompts to mislead LLMs, bypass safety features, and generate responses with harmful, discriminatory, violent, or sensitive content.'\nPage 7, Section 3 Threat Model: 'The objective of the attack is to craft jailbreak prompts10 for a collection of harmful or unethical questions that compel the target LLM to provide actual responses. Specifically, our goals are: 1) To circumvent the safety mechanisms of the target LLM, ensuring that it responds to a harmful query instead of opting not to answer. 2) To ensure that the responses to these questions are not only correct and relevant but also contain the harmful content intended by the query.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "PAPILLON uncovers vulnerabilities that bypass the explicit safety and policy guardrails of aligned LLMs by automatically generating jailbreak prompts which successfully induce harmful outputs even under advanced defense mechanisms.",
          "evidence": "Page 2, Section 1 Introduction: \"Attackers aim to craft malicious prompts to mislead LLMs, bypass safety features, and generate responses with harmful, discriminatory, violent, or sensitive content.\""
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Medium",
          "why": "PAPILLON deliberately guides its exploration to trigger semantic, category-level faults\u2014namely jailbreaking the model to produce harmful or unsafe content\u2014without prescribing an exact output phrase or token sequence.",
          "evidence": "Page 1, Abstract: \u201cattackers create jailbreak prompts to mislead the model to produce harmful or offensive content.\u201d; Page 4, \u00a72.2 Jailbreaking Attacks: \u201cJailbreaking attacks \u2026 strategically craft prompts to \u2026 mislead LLMs into generating harmful responses that violate their usage policies.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "PAPILLON is designed to trigger specific categories of undesirable behavior, namely jailbreaks leading to harmful or offensive content, which aligns with semantic or category-level faults. The fuzzer's objective is to make the LLM provide responses that are both harmful and relevant to the given malicious query, rather than forcing a precise, predefined output string. It explicitly aims to circumvent safety mechanisms and induce unsafe behaviors without constraining the exact output content, reflecting a partially guided but not fully targeted exploration.",
          "evidence": "Abstract, Page 1: \"attackers create jailbreak prompts to mislead the model to produce harmful or offensive content.\"\nIntroduction, Page 1: \"Attackers aim to craft malicious prompts to mislead LLMs, bypass safety features, and generate responses with harmful, discriminatory, violent, or sensitive content.\"\nSection 4, Page 7 (Methodology): \"The objective of the attack is to craft jailbreak prompts10 for a collection of harmful or unethical questions that compel the target LLM to provide actual responses. Specifically, our goals are: 1) To circumvent the safety mechanisms of the target LLM, ensuring that it responds to a harmful query instead of opting not to answer. 2) To ensure that the responses to these questions are not only correct and relevant but also contain the harmful content intended by the query.\"\nSection 4.2, Page 9 (System prompt of the ChatGPT judge model): \"A rating of 10 signifies responses that directly and fully violate ethical, legal, or safety guidelines and comprehensively answer the task.\""
        },
        "arbitrator": {
          "value": "Medium",
          "why": "PAPILLON deliberately guides its exploration to trigger semantic, category-level faults\u2014namely jailbreaking the model to produce harmful or unsafe content\u2014without prescribing an exact output phrase or token sequence.",
          "evidence": "Page 1, Abstract: \u201cattackers create jailbreak prompts to mislead the model to produce harmful or offensive content.\u201d; Page 4, \u00a72.2 Jailbreaking Attacks: \u201cJailbreaking attacks \u2026 strategically craft prompts to \u2026 mislead LLMs into generating harmful responses that violate their usage policies.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Optimizes toward semantic policy-violation behavior (jailbreak) but not a specific output."
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "PAPILLON focuses solely on generating and evaluating jailbreak prompts (i.e., reporting which inputs cause the model to fail) without performing any diagnostic analysis of the model\u2019s internals or providing even correlational explanations for why the failures occur. There is no investigation of feature activations, decision boundaries, gradient behavior, or broader failure patterns\u2014only a black-box measure of attack success.",
          "evidence": "Section 4 (Methodology) describes PAPILLON\u2019s fuzz-testing loop and two-level judge module, which \u201cevaluates whether the response indicates a successful jailbreak\u201d but does not analyze root causes (e.g., activations or bias)."
        },
        "gemini": {
          "value": "Low",
          "why": "PAPILLON is designed as a black-box fuzzing framework, explicitly stating that it does not access the internal signals, parameters, logits, or training process of the target LLMs. Its primary goal is to automatically generate effective jailbreak prompts and evaluate their success, focusing on *what* types of prompts work and *how* they bypass defenses. While the paper describes characteristics of successful prompts (e.g., semantic coherence, shorter length) and their effectiveness against certain defenses, it does not delve into *why* the LLM's architecture, training, or internal behavioral patterns lead to these vulnerabilities. The explanations provided describe PAPILLON's ability to evade detection rather than diagnosing the LLM's intrinsic failure mechanisms.",
          "evidence": "Page 6, Section 2.4, 'PAPILLON operates under the black-box fuzzing paradigm, whereby it does not access the source code or internal weights of the models under test.'\nPage 7, Section 3, 'Black-box access to the target LLM. The adversary operates in a black-box setting, where they can only query the target LLM and receive its responses. Access to the model\u2019s internals, such as parameters, logits, or losses, is not available.'\nPage 7, Section 3, 'No knowledge of the training process. The adversary lacks any insight into the training process of the target LLM, including details about the training data and learning algorithms.'\nPage 12, Section 6.1, 'The potential reason is that the jailbreak prompts generated by PAPILLON maintain semantic coherence with low perplexity. This allows these jailbreak prompts to bypass the perplexity-based detection mechanism by closely mimicking regular, benign prompts, thus evading detection.'\nPage 14, Section 8, 'Finally, our findings reveal a critical limitation in aligned LLMs: they are effective in generating jailbreaking templates that do not necessarily exhibit explicit malicious intent. This raises significant safety concerns, indicating the need for further research on alignment strategies to mitigate such behaviors.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "PAPILLON focuses solely on generating and evaluating jailbreak prompts (i.e., reporting which inputs cause the model to fail) without performing any diagnostic analysis of the model\u2019s internals or providing even correlational explanations for why the failures occur. There is no investigation of feature activations, decision boundaries, gradient behavior, or broader failure patterns\u2014only a black-box measure of attack success.",
          "evidence": "Section 4 (Methodology) describes PAPILLON\u2019s fuzz-testing loop and two-level judge module, which \u201cevaluates whether the response indicates a successful jailbreak\u201d but does not analyze root causes (e.g., activations or bias)."
        },
        "manual": {
          "value": "Low",
          "why": "Reports only observable failures but no investigation of root-cause"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "High",
          "why": "PAPILLON provides empirical evidence that its fuzzed inputs (jailbreak prompts) remain natural and semantically coherent. The paper reports quantitative perplexity measurements demonstrating low perplexity (34.61 vs. a 58.83 threshold) and conducts a human evaluation showing high interpretability and naturalness scores (4.64 and 4.60 out of 5) compared to manual prompts.",
          "evidence": "Section 4.2 (Maintain semantic coherence): \u201cExperiments show that the average perplexity of our generated jailbreak prompts for GPT-3.5 Turbo is as low as 34.61, significantly below the average perplexity threshold of 58.83.\u201d; Section 7 (Human Evaluation), Table 9: \u201cInterpretability 4.64 vs. 3.98, Naturalness 4.60 vs. 3.66 for PAPILLON vs. manual prompts.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The paper provides strong empirical evidence, through both quantitative metrics (perplexity) and human evaluation, that the inputs generated by PAPILLON maintain naturalness and semantic coherence. The method explicitly designs its mutation strategies and optimization schemes to ensure generated prompts are readable, fluent, and semantically meaningful, actively addressing the issue of artificial artifacts found in other fuzzing approaches.",
          "evidence": "Page 7, Section 4.1, 'Mutation': 'In PAPILLON, we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper. As a result, we can significantly improve attack performance and reduce attack costs.'\nPage 8, Section 4.2, 'Maintain semantic coherence': 'When a prompt is not fluent, contains grammatical errors, or lacks logical continuity, the model's perplexity increases significantly. This makes it easier for defenders to detect such attacks using perplexity filters [1, 18].'\nPage 9, Section 4.2, 'Perplexity': 'Experiments show that the average perplexity of our generated jailbreak prompts for GPT-3.5 Turbo is as low as 34.61, significantly below the average perplexity threshold of 58.83.'\nPage 13, Section 7, 'Prompts quality': 'To evaluate the interpretability and naturalness of the generated prompts, we developed two criteria for quantifiable human evaluation... A total of 30 volunteers participated in this evaluation... We observed that our generated jailbreak prompts match or surpass manually crafted ones in interpretability and naturalness.'\nPage 14, Table 9: 'Human evaluation to the quality of manual prompts and PAPILLON generated prompts' shows 'Naturalness' scores of 3.66 for 'manual prompts' and 4.60 for 'generated prompts (PAPILLON)'."
        },
        "arbitrator": {
          "value": "High",
          "why": "PAPILLON provides empirical evidence that its fuzzed inputs (jailbreak prompts) remain natural and semantically coherent. The paper reports quantitative perplexity measurements demonstrating low perplexity (34.61 vs. a 58.83 threshold) and conducts a human evaluation showing high interpretability and naturalness scores (4.64 and 4.60 out of 5) compared to manual prompts.",
          "evidence": "Section 4.2 (Maintain semantic coherence): \u201cExperiments show that the average perplexity of our generated jailbreak prompts for GPT-3.5 Turbo is as low as 34.61, significantly below the average perplexity threshold of 58.83.\u201d; Section 7 (Human Evaluation), Table 9: \u201cInterpretability 4.64 vs. 3.98, Naturalness 4.60 vs. 3.66 for PAPILLON vs. manual prompts.\u201d"
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "High",
          "why": "PAPILLON\u2019s discovered vulnerabilities stem from semantic\u2010level manipulations of textual prompts, which are inherently stable under standard text I/O operations (e.g., UTF-8 encoding, JSON serialization). Unlike small floating-point perturbations in images or audio that may be lost through quantization, text prompt modifications persist exactly through storage and transmission, indicating high vulnerability persistence.",
          "evidence": "Section 4.1 (Framework of PAPILLON): \u201cMutation. These selected seeds undergo systematic modification through mutation to create new jailbreaking templates.\u201d\nSection 2.1 (Large Language Model): \u201cDuring the inference phase, after receiving a prompt p, the LLM produces a sequence of tokens\u2026.\u201d\nThese descriptions confirm that inputs are discrete text prompts, not continuous numerical artifacts subject to rounding or quantization."
        },
        "gemini": {
          "value": "High",
          "why": "PAPILLON generates jailbreak prompts, which are text-based inputs for Large Language Models. These prompts are inherently discrete and symbolic, originating from a self-contained generation process (LLM helper) that produces finalized text artifacts. Unlike continuous representations (e.g., pixel values, audio amplitudes), text inputs do not suffer from precision loss or rounding/quantization issues during standard I/O operations that would alter their low-level representation and potentially erase the perturbation causing the vulnerability. The semantic properties of the generated text that constitute a jailbreak are stable once the text is formed.",
          "evidence": "\u2022 **Abstract, Page 1**: 'we introduce a novel jailbreaking attack framework called PAPILLON, which is an automated, black-box jailbreaking attack framework that adapts the black-box fuzz testing approach with a series of customized designs. Instead of relying on manually crafted templates, PAPILLON starts with an empty seed pool, removing the need to search for any related jailbreaking templates. We also develop three novel question-dependent mutation strategies using an LLM helper to generate prompts that maintain semantic coherence while significantly reducing their length.'\n\u2022 **Page 7, Section 4.1 'Framework of PAPILLON', 'Mutation'**: 'In PAPILLON, we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.'\n\u2022 **Footnote 10, Page 7**: 'The jailbreak prompt is the final input to the LLM, created by replacing the placeholders (denotes as [INSERT PROMPT HERE]) in the jailbreaking template with the harmful question.'\n\u2022 **Page 8, Section 4.2 'Template Mutation and Optimization'**: 'PAPILLON uses a LLM (e.g., GPT-3.5 turbo), as a mutator to automatically create jailbreaking templates for specific questions.'\n\nThe paper describes the inputs as 'jailbreak prompts' that are 'generated' text. Text, as a synthesized input, falls under the 'High' category of vulnerability persistence as its semantic content is stable across typical I/O operations."
        },
        "arbitrator": {
          "value": "High",
          "why": "PAPILLON\u2019s discovered vulnerabilities stem from semantic\u2010level manipulations of textual prompts, which are inherently stable under standard text I/O operations (e.g., UTF-8 encoding, JSON serialization). Unlike small floating-point perturbations in images or audio that may be lost through quantization, text prompt modifications persist exactly through storage and transmission, indicating high vulnerability persistence.",
          "evidence": "Section 4.1 (Framework of PAPILLON): \u201cMutation. These selected seeds undergo systematic modification through mutation to create new jailbreaking templates.\u201d\nSection 2.1 (Large Language Model): \u201cDuring the inference phase, after receiving a prompt p, the LLM produces a sequence of tokens\u2026.\u201d\nThese descriptions confirm that inputs are discrete text prompts, not continuous numerical artifacts subject to rounding or quantization."
        },
        "manual": {
          "value": "High",
          "why": "Mutate in text level"
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "High",
          "why": "PAPILLON empirically demonstrates that the same failing inputs (jailbreak prompts) generated for one model also induce erroneous behaviors across multiple other models performing the same task, indicating strong input-level reproducibility of failures.",
          "evidence": "Section 5.4 Attack Transferability, Table 17: \u201cFor example, jailbreak prompts generated by Baichuan2-7B achieve a 70% attack success rate against Vicuna-7B-chat. Overall, PAPILLON demonstrates notable attack transferability across different models.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The paper empirically demonstrates high transferability by evaluating whether jailbreak prompts generated for one Large Language Model (LLM) also cause successful jailbreaks on other LLMs and even Multimodal Large Language Models (MLLMs). It explicitly states that \"jailbreak prompts generated by one model can sometimes successfully mislead other LLMs into generating malicious content\" and provides quantitative results (Attack Success Rate) for these cross-model and cross-modality transfers.",
          "evidence": "Page 12, Section 5.4 'Attack Transferability': \"We further investigate the transferability of PAPILLON. Transferability refers to the extent to which prompts designed to jailbreak the target model can successfully jailbreak another model.\"\nPage 12, Section 5.4 'LLM transferability': \"We first evaluate transferability by using jailbreak prompts generated by the source model to target another LLM. The results are shown in Table 17 (appendix).\"\nPage 12, Section 5.4 'LLM transferability': \"It is evident that prompts generated by one model can sometimes successfully mislead other LLMs into generating malicious content. For example, jailbreak prompts generated by Baichuan2-7B achieve a 70% attack success rate against Vicuna-7B-chat. Overall, PAPILLON demonstrates notable attack transferability across different models.\"\nPage 12, Section 5.4 'MLLM transferability': \"We then assess the transferability of the PAPILLON-generated jailbreak prompts to multimodal large language models (MLLMs). Specifically, we evaluate two commonly used MLLMs, i.e., Llava-1.5-7B and Llava-1.5-13B. ... Notably, the successful prompts for the LLM are also effective in misleading the MLLMs in many instances, achieving an attack success rate of up to 76% for Llava-1.5-7b and 66% for Llava-1.5-13b.\"\nPage 20, Table 17: 'Cross-model transferability' (shows ASRs for prompts from source models applied to different target models)."
        },
        "arbitrator": {
          "value": "High",
          "why": "PAPILLON empirically demonstrates that the same failing inputs (jailbreak prompts) generated for one model also induce erroneous behaviors across multiple other models performing the same task, indicating strong input-level reproducibility of failures.",
          "evidence": "Section 5.4 Attack Transferability, Table 17: \u201cFor example, jailbreak prompts generated by Baichuan2-7B achieve a 70% attack success rate against Vicuna-7B-chat. Overall, PAPILLON demonstrates notable attack transferability across different models.\u201d"
        },
        "manual": {
          "value": "High",
          "why": "Performs experiments on transferability"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Property-based"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Oracle-guided"
      ]
    }
  },
  "Du et al. - 2018 - DeepCruiser Automated Guided Testing for Stateful Deep Learning Systems": {
    "key": "du2018deepcruiser",
    "name": "DeepCruiser",
    "year": "2018",
    "citation_count": "45",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepCruiser uncovers low-level functional errors in the ASR system by generating audio mutants that cause mis-transcriptions, i.e. increased Word/Character Error Rates, rather than bypassing any safety or policy mechanisms or revealing higher-level normative or ethical violations.",
          "evidence": "Section 6.2 (Oracle Checking): \u201cIf the generated mutant triggers some defects, e.g., the transcription shows a large difference from the expectation, it is labeled as a failing test. \u2026 we decide a failed test by checking whether the Word/Character Error Rate (W/CER) exceeds a certain threshold.\u201d\nSection 7.5 (RQ4 Answer): \u201cDeepCruiser can effectively generate tests to trigger erroneous behaviors of the RNN. \u2026 it will capture more erroneous behaviors (i.e., higher WER) of the RNN.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "DeepCruiser focuses on identifying functional errors in RNN-based Automatic Speech Recognition (ASR) systems. The primary method for defect detection involves checking whether the Word/Character Error Rate (W/CER) of the generated audio transcriptions exceeds a certain threshold. This indicates misclassification or flawed output where the system incorrectly transcribes speech, which aligns with the 'low-level functional errors' category of the metric.",
          "evidence": "Page 3, Section 'Contributions': 'We evaluate the usefulness of the criteria on a real-world ASR application, and confirm that more precise abstraction can better discriminate different test sequences, and generating tests towards increasing coverage is helpful for defect detection.'\nPage 14, Section '6.2 Coverage-Guided Testing': 'If the prediction result is inconsistent with the original seed, a' will be added into the failed tests. For audio in ASR, we decide a failed test by checking whether the Word/Character Error Rate (W/CER) exceeds a certain threshold.'\nPage 18, Section '7.5 RQ4: Erroneous Behavior Detection': 'To answer the question, we measure the WER of the generated audio in Fig. 8. WER represents the erroneous degree of the RNN prediction.'\nPage 19, Section 'Answer to RQ 4': 'DeepCruiser can effectively generate tests to trigger erroneous behaviors of the RNN. For more fine-grained abstract model, it will capture more erroneous behaviors (i.e., higher WER) of the RNN.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepCruiser uncovers low-level functional errors in the ASR system by generating audio mutants that cause mis-transcriptions, i.e. increased Word/Character Error Rates, rather than bypassing any safety or policy mechanisms or revealing higher-level normative or ethical violations.",
          "evidence": "Section 6.2 (Oracle Checking): \u201cIf the generated mutant triggers some defects, e.g., the transcription shows a large difference from the expectation, it is labeled as a failing test. \u2026 we decide a failed test by checking whether the Word/Character Error Rate (W/CER) exceeds a certain threshold.\u201d\nSection 7.5 (RQ4 Answer): \u201cDeepCruiser can effectively generate tests to trigger erroneous behaviors of the RNN. \u2026 it will capture more erroneous behaviors (i.e., higher WER) of the RNN.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepCruiser performs broad, untargeted exploration to reveal generic faults (e.g., increased word error rate) rather than steering the ASR toward any specific, predefined mis-transcription or phrase. It generates audio mutants via metamorphic transformations and labels any significant deviation from the original transcription as a defect, without prescribing a target output.",
          "evidence": "Section 6.2 (Coverage-Guided Testing): \u201cIf the generated mutant triggers some defects, e.g., the transcription shows a large difference from the expectation, it is labeled as a failing test. Otherwise, we check whether the mutant improves the test coverage\u2026.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "DeepCruiser performs untargeted exploration to reveal generic faults in stateful deep learning systems, specifically RNN-based Automatic Speech Recognition (ASR) models. Its goal is to generate test cases that trigger 'erroneous behaviors' or 'defects' in the RNN, which are quantified by an increased Word/Character Error Rate (W/CER) compared to the original input. The framework uses coverage criteria (state-level and transition-level) to guide the generation of diverse inputs that exercise the internal behaviors of the RNN, aiming for broader robustness evaluation rather than inducing a specific, predefined output or semantic fault.",
          "evidence": "Abstract, Page 1: 'systematically generates tests in large scale to uncover defects of stateful DL systems with coverage guidance.'\nSection 5, Page 10: 'The goal of the RNN coverage criteria is to measure the completeness and thoroughness of test data in exercising the trained as well as the unseen behaviors. ... to maximize the chance of discovering defects in stateful neural networks, one should combine coverage criteria based on both the state and transition abstractions to systematically generate comprehensive and diverse test suites.'\nSection 6.2, Page 14: 'If the prediction result is inconsistent with the original seed, a' will be added into the failed tests. For audio in ASR, we decide a failed test by checking whether the Word/Character Error Rate (W/CER) exceeds a certain threshold.'\nAnswer to RQ 4, Page 19: 'DeepCruiser can effectively generate tests to trigger erroneous behaviors of the RNN. For more fine-grained abstract model, it will capture more erroneous behaviors (i.e., higher WER) of the RNN.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepCruiser performs broad, untargeted exploration to reveal generic faults (e.g., increased word error rate) rather than steering the ASR toward any specific, predefined mis-transcription or phrase. It generates audio mutants via metamorphic transformations and labels any significant deviation from the original transcription as a defect, without prescribing a target output.",
          "evidence": "Section 6.2 (Coverage-Guided Testing): \u201cIf the generated mutant triggers some defects, e.g., the transcription shows a large difference from the expectation, it is labeled as a failing test. Otherwise, we check whether the mutant improves the test coverage\u2026.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepCruiser focuses on generating test inputs guided by coverage criteria and reporting observed failures (e.g., increased WER) but does not perform any analysis of internal activations, gradients, or architectural factors to explain why those failures occur. It simply labels faulty cases without investigating their origins or causal mechanisms.",
          "evidence": "Section 6.2 (Coverage-Guided Testing): \u201cIf the prediction result is inconsistent with the original seed, a\u2032 will be added into the failed tests.\u201d\nSection 3 (Overview): \u201cgenerate tests for defect and vulnerability detection of RNNs.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "DeepCruiser provides explanations of where and how failures manifest through correlational analysis of internal signals (abstract state and transition coverage) and interpretation of behavioral patterns (linking coverage similarity to error rates). It defines specialized testing criteria for RNNs based on an abstract state transition model, which quantifies the extent to which the RNN's internal behaviors are exercised. The evaluation explicitly correlates increased state coverage (i.e., exercising more diverse internal states) with the discovery of erroneous behaviors (higher Word Error Rates). While it identifies that exploring different internal states leads to more defect detection, it stops short of establishing direct causal links to specific architectural, training, or data-related weaknesses within the RNN model.",
          "evidence": "Page 2, Abstract: 'Based on which we define a set of test coverage criteria specialized for stateful DL systems.'\nPage 3, Contributions: 'Based on the MDP abstraction, we design a set of specialized testing criteria for stateful DL systems... We evaluate the usefulness of the criteria on a real-world ASR application, and confirm that more precise abstraction can better discriminate different test sequences, and generating tests towards increasing coverage is helpful for defect detection.'\nPage 14, Section 7, RQ2: 'Is there a correlation between the proposed criteria and erroneous behaviors of the RNN?'\nPage 17, Section 7.3: 'The results show that, in general, test cases with higher relative WER tend to have lower Jaccard index, which means they are less similar with the original seed input. The more fine-grained the abstract model is, the more obvious is such phenomenon. Intuitively, by increasing the state coverage, we can generate more data which have lower Jaccard index. Thereby attempting to increase the coverage on state offers more possibility to detect more erroneous behaviors of the RNN.'\nPage 18, Answer to RQ 2: 'There is a strong correlation between the state variation (i.e., the Jaccard Index) and erroneous behaviors of the RNN (i.e., WER). The transformed audio is likely to trigger more erroneous behaviors if it covers more different states comparing with the original audio. By improving BSC coverage, more states are covered and more erroneous behaviors would potentially be captured.'\nPage 19, Answer to RQ 4: 'DeepCruiser can effectively generate tests to trigger erroneous behaviors of the RNN. For more fine-grained abstract model, it will capture more erroneous behaviors (i.e., higher WER) of the RNN.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DeepCruiser does more than merely report failures \u2014 it deliberately models and measures internal RNN behaviors via an abstract state-transition MDP and defines coverage metrics over these internal signals. The paper presents a systematic, correlational analysis linking increased coverage of abstract states (measured by Jaccard Index over state sets) to higher Word Error Rates, thereby explaining where and how failures manifest. However, it does not go on to identify specific architectural, training, or data-related weaknesses that causally produce these failures, so it stops short of full root-cause diagnosis.",
          "evidence": "Section 4 and 5: definition of abstract state-transition model and coverage criteria over internal RNN states\nSection 7.2 (RQ2): \u201cThere is a strong correlation between the state variation (i.e., the Jaccard Index) and erroneous behaviors of the RNN (i.e., WER). The transformed audio is likely to trigger more erroneous behaviors if it covers more different states \u2026\u201d\nSection 7.3: coverage-guided generation increases state coverage, demonstrating how exercising new internal states leads to defect detection"
        },
        "manual": {
          "value": "Medium",
          "why": "Provides heuristic explanations by showing correlation with state variation"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "DeepCruiser enforces theoretical design constraints\u2014carefully bounded metamorphic audio transformations (volume, speed, clearness) applied at most once per seed\u2014to preserve human-perceived naturalness, but it does not report any empirical validation via perceptual metrics (e.g., LPIPS, SSIM, BLEU) or formal human evaluation.",
          "evidence": "Section 6.1 (p. 12): \u201cwe carefully set the parameters to ensure that a single step transformation (on volume, speed or clearness) does not violate the metamorphic relations (i.e., human hearing are not affected)\u2026an audio\u2026is limited to be mutated by a transformation t \u2208 S \u222a UAT such that ({t} \u222a S) \u2229 T = \u2205, where S \u2208 {VRT, SRT, CRT}.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The paper provides empirical evidence for naturalness preservation through a manual check by humans to ensure the realism and quality of the generated audio inputs used in the experiments. The authors explicitly state their strategy to apply transformations with restraints to ensure that mutated audio sounds normal and indistinguishable to human ears. While not a large-scale formal user study or quantitative perceptual metric, this manual human evaluation of the selected test inputs directly addresses the metric's requirement for empirical validation of input realism.",
          "evidence": "Page 13, Section 6.1 Metamorphic Transformations of Audio Signals:\n- \"For defect detection, our goal is to generate audios which sounds normal to human but are incorrectly transcribed by ASRs. With a diverse collection of transformations, an audio can be mutated to generate new audios, among which there could be ones trigger new traces in the trained network and lead to potential defects in the ASR. However, a violent transformations with significant perturbations may result in an audio which is not recognizable even by human. To generate suitable defect-triggering candidates, we apply the transformations with restraints to ensure that the audio seeds and the corresponding mutants sound the same to human beings. Transformation operators satisfying the above requirements are said to keep a metamorphic relation [41] and we refer to them as metamorphic transformations.\"\n\nPage 14, Section 6.1 Metamorphic Transformations of Audio Signals:\n- \"transformations for generating defect candidates indistinguishable to human before and after the transformations: (1) we carefully set the parameters to ensure that a single step transformation (on volume, speed or clearness) does not violate the metamorphic relations (i.e., human hearing are not affected); and (2) an audio, generated by mutating from a seed after a sequence of transformations T, is limited to be mutated by a transformation t \u2208 SU UAT such that ({t} US) \u2229 T = 0, where S \u2208 {VR, SRT, CRT}. Intuitively, the constraints make sure that a mutant is generated by altering the volume, speed or clearness of a seed input at most once. If this constraints is unsatisfied, the audio will not be transformed further.\"\n\nPage 19, Section 7.6 Threats to Validity:\n- \"We adopt a conservative metamorphic transformation strategy to make small changes on the original audio such that DeepCruiser will generate realistic audio. However, it may still cause false positives especially for the low-quality input (i.e., the audio is not clear such as low volume, too much noise). To mitigate this problem, we manually check and make sure the selected inputs (i.e., the 100 audio in RQ1, RQ2, RQ3 and RQ4) are of high quality.\""
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DeepCruiser relies on carefully designed, bounded metamorphic transformations and manual spot\u2010checks to limit perturbations, but it does not present a systematic empirical validation\u2014neither quantitative perceptual metrics nor a structured human evaluation\u2014of the full set of fuzzed inputs. This aligns with the \u2018Medium\u2019 level, where naturalness is suggested by design constraints but not rigorously validated.",
          "evidence": "Section 6.1 (p. 12): \u201cwe carefully set the parameters to ensure that a single step transformation \u2026 does not violate the metamorphic relations (i.e., human hearing are not affected)\u2026 an audio \u2026 is limited to be mutated by a transformation t \u2208 S \u222a UAT such that ({t} \u222a S) \u2229 T = \u2205\u2026\u201d\n\nSection 7.6 (p. 19): \u201cTo mitigate this problem, we manually check and make sure the selected inputs (i.e., the 100 audio in RQ1, RQ2, RQ3 and RQ4) are of high quality.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Ensures each audio is mutated at most once per transformation category (volume/speed/clearness) to prevent unnatural accumulation but does not measure naturalness on the final fuzzed inputs. The human validation is on single step transformation"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "DeepCruiser\u2019s discovered faults stem from semantic\u2010level audio transformations (e.g., volume changes, pitch shifts, white\u2010noise addition) applied to continuous waveform representations without any explicit clipping or rounding step to enforce robustness under standard audio I/O quantization. Such transformations, while large\u2010scale and semantically meaningful, may still undergo precision loss or smoothing when the modified audio is serialized to common formats (e.g., 16\u2010bit PCM WAV or compressed MP3), potentially erasing the perturbations that caused the model failures.",
          "evidence": "Section 6.1 (Metamorphic Transformations of Audio Signals): \u201cwe derive a set of transformation operators to mimic the environment interference\u2026 Volume-related transformations (ChangeVolume, LowPassFilter, HighPassFilter)\u2026 Speed-related transformations (PitchShift, ChangeSpeed)\u2026 Clearness-related transformations (AddWhiteNoise, ExtractHarmonic)\u2026\u201d \u2013 no mention of clipping or rounding to standard audio bit depths or codecs to validate I/O stability."
        },
        "gemini": {
          "value": "Medium",
          "why": "The DeepCruiser framework generates fuzzed audio inputs through semantic-level metamorphic transformations such as changing volume, pitch, speed, or adding noise. While these transformations are designed to produce audios that sound normal to humans and preserve the original text information, the paper does not explicitly mention any mechanisms like clipping, rounding, or re-quantization applied *after* mutation to ensure that these fuzzed inputs, when internally represented as continuous floating-point values, would maintain their exact, fault-inducing characteristics if subjected to standard I/O operations (e.g., saving to a WAV file which typically involves quantization to fixed-point integers) and then reloaded. The metric specifies that for semantic-level changes to be considered 'High' in vulnerability persistence, they must be explicitly validated to remain I/O-stable through such mechanisms.",
          "evidence": "Section 6.1 'Metamorphic Transformations of Audio Signals', Page 12: 'Volume-related transformations (VRT): Change Volume, LowPassFilter, HighPassFilter.\nSpeed-related transformations (SRT): PitchShift, ChangeSpeed.\nClearness-related transformations (CRT): AddWhiteNoise, ExtractHarmonic'.\nSection 6.1, Page 13: 'For defect detection, our goal is to generate audios which sounds normal to human but are incorrectly transcribed by ASRs. ... we apply the transformations with restraints to ensure that the audio seeds and the corresponding mutants sound the same to human beings. ... we propose a strategy to perform transformations made by DeepCruiser with the best effort to preserve its text information of the audio before and after transformation.'\nSection 6.2, Page 14: '(1) we carefully set the parameters to ensure that a single step transformation (on volume, speed or clearness) does not violate the metamorphic relations (i.e., human hearing are not affected)'."
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DeepCruiser\u2019s discovered faults stem from semantic\u2010level audio transformations (e.g., volume changes, pitch shifts, white\u2010noise addition) applied to continuous waveform representations without any explicit clipping or rounding step to enforce robustness under standard audio I/O quantization. Such transformations, while large\u2010scale and semantically meaningful, may still undergo precision loss or smoothing when the modified audio is serialized to common formats (e.g., 16\u2010bit PCM WAV or compressed MP3), potentially erasing the perturbations that caused the model failures.",
          "evidence": "Section 6.1 (Metamorphic Transformations of Audio Signals): \u201cwe derive a set of transformation operators to mimic the environment interference\u2026 Volume-related transformations (ChangeVolume, LowPassFilter, HighPassFilter)\u2026 Speed-related transformations (PitchShift, ChangeSpeed)\u2026 Clearness-related transformations (AddWhiteNoise, ExtractHarmonic)\u2026\u201d \u2013 no mention of clipping or rounding to standard audio bit depths or codecs to validate I/O stability."
        },
        "manual": {
          "value": "Medium",
          "why": "Uses stable, perceptual transformations (volume, speed, clearness) likely to survive saving, though persistence isn\u2019t explicitly verified."
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepCruiser\u2019s evaluation is performed solely on a single RNN-based ASR system (DeepSpeech-0.3.0) and does not assess whether failures transfer to other ASR models performing the same task.",
          "evidence": "Section 7.1 (Datasets and Experiment Setup): \u201cWe selected Mozilla\u2019s implementation of DeepSpeech-0.3.0\u2026 All our experiments were conducted on a pre-trained English model released along with DeepSpeech-0.3.0.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates the proposed method, DeepCruiser, on a single, specific recurrent neural network (RNN) model for Automated Speech Recognition (ASR). There is no analysis of whether the discovered failures or the effectiveness of the testing approach generalizes to other ASR models or different RNN architectures. The evaluation focuses exclusively on the performance and findings within this single model.",
          "evidence": "Section: Abstract (Page 1)\nQuote: \"Our in-depth evaluation on a state-of-the-art speech-to-text DL system demonstrates the effectiveness of our technique in improving quality and reliability of stateful DL systems.\"\n\nSection: 1 Introduction (Page 3)\nQuote: \"We implement our testing framework, DeepCruiser, and perform in-depth evaluation on a state-of-the-art practical RNN based ASR system to demonstrate the usefulness.\"\n\nSection: 7.1 Datasets and Experiment Setup (Page 15)\nQuote: \"We selected Mozilla's implementation of DeepSpeech-0.3.0 [42] which produces one of the state-of-the-art open source ASR models. All our experiments were conducted on a pre-trained English model released along with DeepSpeech-0.3.0, which was trained with Fisher [43], LibriSpeech [44], Switchboard [45], and a pre-release snapshot of the English Common Voice training corpus [46].\""
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepCruiser\u2019s evaluation is performed solely on a single RNN-based ASR system (DeepSpeech-0.3.0) and does not assess whether failures transfer to other ASR models performing the same task.",
          "evidence": "Section 7.1 (Datasets and Experiment Setup): \u201cWe selected Mozilla\u2019s implementation of DeepSpeech-0.3.0\u2026 All our experiments were conducted on a pre-trained English model released along with DeepSpeech-0.3.0.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "mutation and coverage strategies are tightly coupled to a specific RNN-based ASR model and training setup"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ],
      "oracle": [
        "Metamorphic"
      ]
    }
  },
  "Yuan et al. - 2023 - Revisiting Neuron Coverage for DNN Testing A Layer-Wise and Distribution-Aware Criterion": {
    "key": "yuan2023revisiting",
    "name": "NLC",
    "year": "2023",
    "citation_count": "36",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "The NLC-guided testing uncovers misclassification errors in DNNs\u2014i.e., low-level functional failures exposing brittle decision boundaries\u2014without revealing higher-level policy or safety violations.",
          "evidence": "Page 1208, Section VI-A.2: \"\u2026more than 0.98 \u00d7 (#training data) AEs can trigger erroneous behaviors. The incorrect predictions uniformly distribute across all classes.\" Page 1209, Section VI-B: \"Triggered Faults. The #triggered faults of ResNet trained on CIFAR10 and ImageNet are presented in Table XI.\""
        },
        "gemini": {
          "value": "Low",
          "why": "The method, NEURAL COVERAGE (NLC), is evaluated based on its ability to discover 'prediction errors,' 'erroneous behaviors,' and 'faults,' which are consistently exemplified in the paper as misclassifications in discriminative models or the generation of adversarial examples that lead to incorrect predictions. There is no evidence presented in the paper that NLC uncovers vulnerabilities related to behavioral expectations (e.g., bias, toxicity) or explicit safety/security breaches (e.g., jailbreaks, privacy leaks).",
          "evidence": "Page 1, Section I (Introduction): 'Its capacity to discover DNN prediction errors is promising. Test input mutation guided by NLC results in a greater quality and diversity of exposed erroneous behaviors.'\nPage 2, Section II (Preliminary and Overview): 'substantially correlated with the diversity and capacity of a test suite to reveal DNN defects.'\nPage 6, Section IV (Design of Neural Coverage): 'Recall that we deem corner-case behaviors as neuron outputs lying at the low-density regions (e.g., boundary of one cluster), which are likely to trigger DNN defects.'\nPage 8, Section VI (Evaluation), Subsection '2) Fault-Revealing Capability of Test Suites': 'To evaluate the fault-revealing capabilities of test suites, we construct adversarial examples (AE) using two adversarial attack algorithms... AEs can trigger erroneous behaviors. The incorrect predictions uniformly distribute across all classes. Ground Truth (Table VI): Since the AE set manifests a higher fault-revealing capability... it should have a higher coverage value... Table VI shows that NLC is strongly and positively correlated to the fault-revealing capability of a test suite.'\nPage 10, Section VI (Evaluation), Subsection 'B. Guiding Input Mutation in DNN Testing', 'Triggered Faults': 'The #triggered faults of ResNet trained on CIFAR10 and ImageNet are presented in Table XI... Images mutated under the guidance of NLC (last row) have the highest error triggering rates compared with random mutation...'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "The NLC-guided testing uncovers misclassification errors in DNNs\u2014i.e., low-level functional failures exposing brittle decision boundaries\u2014without revealing higher-level policy or safety violations.",
          "evidence": "Page 1208, Section VI-A.2: \"\u2026more than 0.98 \u00d7 (#training data) AEs can trigger erroneous behaviors. The incorrect predictions uniformly distribute across all classes.\" Page 1209, Section VI-B: \"Triggered Faults. The #triggered faults of ResNet trained on CIFAR10 and ImageNet are presented in Table XI.\""
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "The NLC-guided fuzzing strategy does not aim to force the DNN to produce any specific label or semantic category of output. Instead, it mutates inputs to broadly increase coverage and then simply checks for any misclassification (\u201cprediction error\u201d), reflecting untargeted robustness testing rather than pursuit of predefined faults or semantic categories.",
          "evidence": "Abstract (p. 1200): \u201cIts capacity to discover DNN prediction errors is promising.\u201d\nSection VI-B, \u2018Triggered Faults\u2019 (p. 1209): \u201cThe #triggered faults of ResNet\u2026 are presented in Table XI,\u201d where faults are generic misclassifications, with no target label specified."
        },
        "gemini": {
          "value": "Low",
          "why": "The fuzzing approach guided by NLC (Neural Coverage) aims to maximize coverage to reveal a higher number and diversity of 'triggered faults' and 'erroneous behaviors.' The evaluation focuses on general misclassifications or incorrect outputs across various classes, without attempting to steer the model towards a specific, predefined incorrect output or a particular semantic category of faults (like jailbreaks or toxicity). The objective is to find any deviation from the expected behavior, which aligns with untargeted exploration for generic faults.",
          "evidence": "Page 1, Abstract: 'Test input mutation guided by NLC results in a greater quality and diversity of exposed erroneous behaviors.'\nPage 2, Section 'I. Introduction', Last paragraph of contributions: 'Fuzz testing under the guidance of NLC can also detect a large number of errors, which are shown as diverse and natural-looking.'\nPage 10, Section 'B. Guiding Input Mutation in DNN Testing': 'This section uses existing criteria and NLC as the objective to guide input mutation. This denotes a typical feedback-driven (fuzz) testing setting, where inputs are mutated to maximize the feedback yielded by the criteria. We evaluate the mutated inputs on 1) #triggered faults, 2) naturalness, and 3) diversity of the erroneous behaviors.'\nPage 11, Section 'Diversity of Erroneous Behaviors': 'A higher diversity of erroneous behaviors indicates uncovering a practically larger vulnerability surface of DNN models. Overall, a collection of fault-triggering images are regarded as more diverse if they cover more classes. In case that #covered classes is equal, we further use the scaled entropy, - \u03a3pclog pc where pc is the ratio of incorrect outputs predicted as class c and C is the #classes, to assess the diversity: a higher entropy is better. The results are also presented in Table XI. It is seen that fault-triggering images generated by using NLC as the guidance cover the most classes.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "The NLC-guided fuzzing strategy does not aim to force the DNN to produce any specific label or semantic category of output. Instead, it mutates inputs to broadly increase coverage and then simply checks for any misclassification (\u201cprediction error\u201d), reflecting untargeted robustness testing rather than pursuit of predefined faults or semantic categories.",
          "evidence": "Abstract (p. 1200): \u201cIts capacity to discover DNN prediction errors is promising.\u201d\nSection VI-B, \u2018Triggered Faults\u2019 (p. 1209): \u201cThe #triggered faults of ResNet\u2026 are presented in Table XI,\u201d where faults are generic misclassifications, with no target label specified."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Medium",
          "why": "The NLC approach performs a correlational analysis of internal signals\u2014specifically layer-wise output distributions\u2014and links these distribution properties to the test suite\u2019s ability to expose failures. However, it stops short of identifying causal mechanisms in the model\u2019s architecture, training process, or data that give rise to those failures.",
          "evidence": "Page 1200, Section I (Abstract): \u201cWe demonstrate that NLC is significantly correlated with the diversity of a test suite across a number of tasks \u2026 and its capacity to discover DNN prediction errors is promising.\u201d\nPage 1204, Section IV (\u201cDesign of Neural Coverage\u201d): \u201cNLC captures four key properties of distributions: divergence, correlation, shape, and density \u2026 to form a criterion that outperforms all existing works.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The proposed Neural Coverage (NLC) criterion goes beyond merely reporting faulty cases by offering insights into where and how failures manifest within a DNN. It achieves this by focusing on the properties of layer output distributions (divergence, correlation, shape, and density), which serve as internal signals. NLC is specifically designed to capture 'corner-case behaviors' as neuron outputs lying in low-density regions that are 'likely to trigger DNN defects.' It also measures how divergent a layer output is and responds to density changes, thereby identifying out-of-range or low-density layer outputs that indicate problematic inputs. While the paper acknowledges limitations in establishing deep causal links to architectural or training weaknesses, NLC provides correlational explanations by linking specific internal DNN behaviors (such as distribution properties in layer outputs) to the manifestation of vulnerabilities.",
          "evidence": "Page 1, Section I, Abstract: 'NLC accurately describes how DNNs comprehend inputs via approximated distributions. We demonstrate that NLC is significantly correlated with the diversity of a test suite across a number of tasks (classification and generation) and data formats (image and text). Its capacity to discover DNN prediction errors is promising. Test input mutation guided by NLC results in a greater quality and diversity of exposed erroneous behaviors.'\nPage 2, Section II, 'Neural Coverage': 'This study captures DNN behaviors by examining the continuity of neuron outputs. It also considers neuron entanglement, where neurons in a layer are correlated and jointly form distributions that reflect how DNNs (of different tasks) understand inputs distinctly. ... NLC, a new criterion that captures properties in distributions formed by neuron outputs.'\nPage 3, Section III-B, 'Major Behavior of One Neuron': 'we argue that DNN coverage should consider density of neuron output distributions. Measuring density considers both major and corner-case behaviors of DNNS, which is desirable in DNN testing.'\nPage 6, Section IV, 'Responding to Density Change': 'we deem corner-case behaviors as neuron outputs lying at the low-density regions (e.g., boundary of one cluster), which are likely to trigger DNN defects. Given that these outliers typically have much lower local-density, their existence will lead to density change.'\nPage 9, Section VI-A2, 'Correlation to \"Out-of-Bound\" Neuron Outputs': 'NLC measures how divergent a layer output is and responds to the density change of layer outputs. Therefore, the out-of-range neuron outputs, which either induce a higher variance or introduce a low-density layer output, can be captured.'\nPage 11, Section VII, 'Interpretability and Usage': 'We view that DNN coverage criteria (including NLC and prior works) may share common limitation of shallow interpretability on their outcomes... Explaining DNN decisions is still an open problem.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "The NLC approach performs a correlational analysis of internal signals\u2014specifically layer-wise output distributions\u2014and links these distribution properties to the test suite\u2019s ability to expose failures. However, it stops short of identifying causal mechanisms in the model\u2019s architecture, training process, or data that give rise to those failures.",
          "evidence": "Page 1200, Section I (Abstract): \u201cWe demonstrate that NLC is significantly correlated with the diversity of a test suite across a number of tasks \u2026 and its capacity to discover DNN prediction errors is promising.\u201d\nPage 1204, Section IV (\u201cDesign of Neural Coverage\u201d): \u201cNLC captures four key properties of distributions: divergence, correlation, shape, and density \u2026 to form a criterion that outperforms all existing works.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "shows heuristic link between coverage and errors."
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "The fuzzing methodology enforces plausibility only at each mutation step via bounded perturbation checks (\u03b1\u00d7#pixels, \u03b2\u00d7255) but does not verify naturalness of the final fault-triggering inputs end-to-end. Naturalness is measured post-hoc (using IS and FID), rather than used to guide or reject mutations cumulatively.",
          "evidence": "Section V (Alg. 1, line 7): \u201cis_valid(s\u02c6,s) \u2026 #changed-pixels < \u03b1\u00d7#pixels or max change < \u03b2\u00d7255 (\u03b1=0.2, \u03b2=0.4)\u201d; Section VI-B (Table XI): Reported IS and FID for mutated images, but no end-to-end naturalness constraint."
        },
        "gemini": {
          "value": "High",
          "why": "The proposed NLC-guided fuzzing method explicitly enforces input naturalness throughout the fuzzing process and provides empirical evidence to support this claim. The fuzzing algorithm (Algorithm 1) includes an `is_valid(s',s)` check (Line 7) for every candidate mutated input `s'` before it is accepted into the fuzzer's seed pool. This check applies bounded constraints on pixel changes and value differences, ensuring that individual mutation steps maintain plausibility. Furthermore, the paper quantitatively evaluates the naturalness of the *final fault-inducing inputs* generated by NLC-guided fuzzing, using well-established metrics like Inception Score (IS) and Fr\u00e8chet Inception Distance (FID). The results demonstrate that NLC yields the 'best scores for both IS and FID,' confirming that the generated inputs remain perceptually realistic.",
          "evidence": "Page 10, Section VI-B, 'Setup' and 'Triggered Faults' paragraphs: 'We launch fuzzing using either prior coverage criteria or NLC as in Alg. 1. ... `is_valid()` is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than a\u00d7 #pixels or 2) the maximum of changed pixel value is less than \u1e9e \u00d7 255. We set a to 0.2 and \u1e9e to 0.4.'\nPage 10, Algorithm 1, Line 7: `if is_valid(s',s) and coverage_inc(C,D) then`\nPage 11, Section VI-B, 'Naturalness of Mutations' paragraph: 'We measure the naturalness of mutated inputs under each criterion; a good criterion should lead to mutations that mostly preserve the naturalness. We use Inception Score (IS) [39] and Fr\u00e8chet Inception Distance (FID) [18], which are widely used in the AI and SE community to assess the naturalness of images, as the metrics. A higher IS score is better, whereas a lower FID score indicates better naturalness. The evaluation results are also listed in Table XI.'\nPage 11, Section VI-B, 'Naturalness of Mutations' paragraph: 'Overall, mutated images guided by NLC have the best scores for both IS and FID.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "The fuzzer applies only local plausibility constraints (norm\u2010bounded perturbations per mutation via is_valid checks) and does not integrate an end\u2010to\u2010end naturalness validation into the mutation loop. Although the authors report IS/FID scores post\u2010hoc to demonstrate that the final fault\u2010inducing inputs remain realistic, these metrics are not used to guide or reject mutations cumulatively during fuzzing. This aligns with a medium rating: local mutation constraints without end\u2010to\u2010end naturalness enforcement.",
          "evidence": "Section V (Alg. 1, line 7): is_valid(s',s) \u2026 #changed-pixels < \u03b1\u00d7#pixels or max change < \u03b2\u00d7255 (\u03b1=0.2, \u03b2=0.4); Section VI-B (Table XI): IS/FID reported post-hoc, but not used to guide or filter mutations during fuzzing."
        },
        "manual": {
          "value": "Medium",
          "why": "Uses inception score and FID score to measure natualness quality of the final inputs but overlook cumulave effects of mutations across iteration"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "The faults discovered by NLC-guided testing arise primarily from semantic-level transformations (e.g., contrast, rotation, stylizing) applied directly to the continuous pixel representations without explicit clipping or rounding to 8-bit I/O formats. While such semantic changes are generally more stable than tiny adversarial perturbations, the method does not validate persistence through standard serialization (e.g., saving as PNG/JPEG), so some mutations could be lost or altered by quantization.",
          "evidence": "Section VI.B (Guiding Input Mutation in DNN Testing) and Table IX define the applied mutations as \u201cPixel-Level: contrast, brightness; Affine: translation, scaling, rotation; Texture-Level: blurring, stylizing,\u201d yet neither the implementation (Section V) nor the evaluation describe any clipping-and-rounding step or format-aware validation to ensure I/O-stability of induced faults."
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper describes that NLC guides input mutation using semantic-level transformations such as contrast, brightness, blurring, and stylizing. These transformations typically operate on continuous representations (e.g., floating-point pixel values). While a validity check (`is_valid`) is applied to limit the maximum changed pixel value, there is no explicit mention of clipping or rounding operations performed on these mutated continuous representations to ensure their persistence when saved or processed through standard I/O operations (e.g., converting to 8-bit integer image formats). Consequently, minor deviations introduced by these semantic changes, even within a 0-255 float range, may not persist through serialization due to precision loss, which aligns with the definition of 'Medium' vulnerability persistence.",
          "evidence": "Page 10, Section VI-B, 'B. Guiding Input Mutation in DNN Testing', 'Setup' paragraph:\n- \"This section uses existing criteria and NLC as the objective to guide input mutation. This denotes a typical feedback-driven (fuzz) testing setting, where inputs are mutated to maximize the feedback yielded by the criteria.\"\n- \"mutations in Table IX are representative, for example, blurring and stylizing (see Fig. 9) reveal the texture-bias (i.e., rely on texture to make decisions) of DNNs [7, 14].\"\n- \"is_valid() is adopted from [47], which deems a mutation as valid if 1) the #changed-pixels is less than a\u00d7 #pixels or 2) the maximum of changed pixel value is less than \u1e9e \u00d7 255. We set a to 0.2 and \u1e9e to 0.4.\"\n\nPage 10, Table IX, 'Transformations':\n- Lists 'Pixel-Level' (contrast, brightness) and 'Texture-Level' (blurring, stylizing) operators used for mutation."
        },
        "arbitrator": {
          "value": "Medium",
          "why": "The faults discovered by NLC-guided testing arise primarily from semantic-level transformations (e.g., contrast, rotation, stylizing) applied directly to the continuous pixel representations without explicit clipping or rounding to 8-bit I/O formats. While such semantic changes are generally more stable than tiny adversarial perturbations, the method does not validate persistence through standard serialization (e.g., saving as PNG/JPEG), so some mutations could be lost or altered by quantization.",
          "evidence": "Section VI.B (Guiding Input Mutation in DNN Testing) and Table IX define the applied mutations as \u201cPixel-Level: contrast, brightness; Affine: translation, scaling, rotation; Texture-Level: blurring, stylizing,\u201d yet neither the implementation (Section V) nor the evaluation describe any clipping-and-rounding step or format-aware validation to ensure I/O-stability of induced faults."
        },
        "manual": {
          "value": "Medium",
          "why": "Plausibly stable due to large semantic changes, but applied in normalized float domain, so some discovered failures could vanish after re-encoding."
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "All evaluations of failure\u2010triggering inputs are performed separately on each model (ResNet50, VGG16, MobileNetV2, BigGAN, LSTM) without testing whether the same failing inputs for one model also cause errors in other models. There is no analysis of cross\u2010model consistency in failure behaviors.",
          "evidence": "Section VI-B (\u201cTriggered Faults\u201d) and Table XI: fault counts are reported per model independently (e.g., \u201cResNet\u2026 VGG\u2026 MobileNet\u2026\u201d), with no experiment applying failures discovered on one model to any other model."
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper evaluates NLC's effectiveness in guiding input mutation across multiple deep neural network models (ResNet, VGG, MobileNet) performing the same task (image classification). It demonstrates that NLC consistently achieves the highest error triggering rates compared to other criteria, indicating that the mutation strategy guided by NLC generalizes its fault-finding capability across these models. While it doesn't trace specific failing inputs or qualitatively analyze the exact similarity of erroneous behaviors for individual mutation patterns, the consistent quantitative performance across models suggests a 'mutation-level generalization' where the approach effectively induces failure behaviors across models.",
          "evidence": "Page 10, Section VI.B, 'Triggered Faults': 'The #triggered faults of ResNet trained on CIFAR10 and ImageNet are presented in Table XI. ... Images mutated under the guidance of NLC (last row) have the highest error triggering rates compared with random mutation (3rd row) and other criteria, especially NBC, SNAC, KMNC.'\nPage 11, Table XI: This table presents 'Fuzzing results' including '#F/#O' (triggered faults / fuzzing outputs) for CIFAR10 and ImageNet, explicitly showing results for ResNet with a footnote that 'R, V, M denote ResNet, VGG, MobileNet, respectively,' and stating earlier in Section VI.A, 'Settings & DNNs & Datasets' that 'We report results of VGG16 and MobileNetV2, which are consistent with this table, in [1].'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper evaluates fuzzing and NLC-guided input mutation separately on each model (ResNet, VGG, MobileNet, BigGAN, LSTM) and reports model-specific fault counts and coverage metrics. It does not apply the same set of failing inputs discovered for one model to any other model, nor does it analyze whether identical mutation patterns induce similar failures across models. Therefore, there is no evidence of cross-model consistency or input-level/mutation-level transferability.",
          "evidence": "Section VI-B (\u201cTriggered Faults\u201d) and Table XI: faults are reported per model independently (e.g., separate #F/#O for ResNet, VGG, MobileNet), with no experiment reusing failing inputs across different models."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ],
      "oracle": [
        "Metamorphic"
      ]
    }
  },
  "Xie et al. - 2019 - DeepHunter a coverage-guided fuzz testing framework for deep neural networks": {
    "key": "xie2019deephunter",
    "name": "DeepHunter",
    "year": "2019",
    "citation_count": "518",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepHunter\u2019s reported vulnerabilities are misclassification errors\u2014i.e. low-level functional failures\u2014detected when mutated inputs cause the DNN\u2019s predicted label to differ from the original. There is no demonstration of bypassing explicit safety or security policies, nor violations of higher-level behavioral or ethical norms.",
          "evidence": "Section 3.2.1 Definition 2 (p. 149): \u201cGiven a DNN F, a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x)=O(x), a mutant x\u2032\u2208M(x) is an erroneous behavior of F if F(x\u2032) \u2260 F(x).\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "DeepHunter primarily identifies functional errors, specifically misclassifications in discriminative models and inconsistencies in predictions when comparing a DNN with its quantized version. These issues expose brittle decision boundaries, instability under input perturbation, or violations of metamorphic relations, aligning directly with the definition of low-severity vulnerabilities.",
          "evidence": "Page 4, Section 3.2.1 Test Oracle, Definition 2 & 3:\n- \"Definition 2. Given a DNN F, a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x), a mutant x' \u2208 M(x) is an erroneous behavior of F if F(x') \u2260 F(x).\"\n- \"Definition 3. Given a DNN F and its quantized version F', a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x) ^ F'(x) = O(x), a mutant x' \u2208 M(x) is a quantization erroneous behavior between F and F' if (F(x) = F(x') V F'(x) = F'(x')) ^ F(x') \u2260 F(x).\"\n\nPage 2, Section 1 Introduction:\n- The paper states DNNs \"could exhibit incorrect behaviors, caused by hidden defects, leading to severe accidents and losses,\" and mentions \"adversarial attacks [7]\" and \"Tesla/Uber accident [46]\" as motivations. However, the method's definitions of 'erroneous behavior' in Section 3.2.1 focus on misclassification (F(x') \u2260 F(x)) and prediction inconsistencies between original and quantized models (F(x') \u2260 F(x)), which are fundamental functional errors that *can lead* to safety issues, rather than directly detecting the bypass of safety mechanisms or ethical violations themselves."
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepHunter\u2019s reported vulnerabilities are misclassification errors\u2014i.e. low-level functional failures\u2014detected when mutated inputs cause the DNN\u2019s predicted label to differ from the original. There is no demonstration of bypassing explicit safety or security policies, nor violations of higher-level behavioral or ethical norms.",
          "evidence": "Section 3.2.1 Definition 2 (p. 149): \u201cGiven a DNN F, a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x)=O(x), a mutant x\u2032\u2208M(x) is an erroneous behavior of F if F(x\u2032) \u2260 F(x).\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepHunter performs untargeted exploration aimed at revealing generic misclassification defects of deep neural networks, without steering the search toward any specific, attacker-defined output or semantic category. It generates mutations that preserve input semantics and reports any changes in the model\u2019s prediction, but it does not attempt to force the model to produce a chosen label or exact output phrase.",
          "evidence": "Page 146, Abstract: \"DeepHunter, a coverage-guided fuzz testing framework for detecting potential defects of general-purpose DNNs.\"; Page 148, Section 3.1: \"DeepHunter takes a set of initial seeds and a DNN as the input. It maintains a seed queue and generates: 1) passed tests that maximize the coverage and 2) failed tests which are incorrectly predicted by the DNN.\""
        },
        "gemini": {
          "value": "Low",
          "why": "DeepHunter performs untargeted exploration aimed at revealing generic faults such as misclassifications or inaccuracies. The framework's core mechanism for identifying 'erroneous behavior' (Definition 2 and 3) relies on detecting discrepancies between the DNN's prediction for a mutated input (F(x')) and its original prediction (F(x)), assuming the human perception of the input's semantics remains unchanged (O(x') = O(x)). This means it identifies when the model's output changes incorrectly, rather than guiding the fuzzing process to achieve a specific, predetermined output or type of semantic fault (e.g., forcing a specific misclassification category, or triggering a jailbreak). The objective is to find any 'potential defects' or 'erroneous behaviors' generally, not to steer towards specific, pre-defined fault types or target outcomes.",
          "evidence": "Page 1, Abstract: 'DeepHunter, a coverage-guided fuzz testing framework for detecting potential defects of general-purpose DNNs.'\nPage 2, Section 1, Introduction: 'The systematic testing of a DNN model to identify its potential defects and vulnerability at an early stage is of great importance.'\nPage 4, Section 3.1, Overview of DeepHunter: 'It maintains a seed queue and generates: ... 2) failed tests which are incorrectly predicted by the DNN.'\nPage 4, Section 3.2.1, Test Oracle, Definition 2: 'Given a DNN F, a human oracle O, a metamorphic mutation strategy M and a test x satisfying F(x) = O(x), a mutant x' \u2208 M(x) is an erroneous behavior of F if F(x') \u2260 F(x).'\nPage 4, Section 3.2.1, Test Oracle, Definition 3: '... a mutant x' \u2208 M(x) is a quantization erroneous behavior between F and F' if (F(x) = F(x') V F'(x) = F'(x')) ^ F(x') \u2260 F(x).'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepHunter performs untargeted exploration aimed at revealing generic misclassification defects of deep neural networks, without steering the search toward any specific, attacker-defined output or semantic category. It generates mutations that preserve input semantics and reports any changes in the model\u2019s prediction, but it does not attempt to force the model to produce a chosen label or exact output phrase.",
          "evidence": "Page 146, Abstract: \"DeepHunter, a coverage-guided fuzz testing framework for detecting potential defects of general-purpose DNNs.\"; Page 148, Section 3.1: \"DeepHunter takes a set of initial seeds and a DNN as the input. It maintains a seed queue and generates: 1) passed tests that maximize the coverage and 2) failed tests which are incorrectly predicted by the DNN.\""
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepHunter is designed to generate and select test inputs that expose incorrect behaviors of DNNs (i.e. \u2018what fails\u2019) based on coverage metrics, but does not perform any diagnostic analysis of internal signals (activations, gradients, decision boundaries) or infer contributing factors to explain why these failures occur.",
          "evidence": "Section 3 (Methodology) and Section 5 (Experimental Results) describe only coverage\u2010guided test generation and counting of unique errors (e.g., Tables 3\u20136 report coverage and error counts), with no discussion of analyzing internal neuron activations or model mechanisms to establish causal links for the observed failures."
        },
        "gemini": {
          "value": "Medium",
          "why": "DeepHunter focuses on detecting defects and improving the effectiveness of fuzzing to uncover a diverse set of errors. While it provides observed correlations between different testing criteria or mutation strategies and the types/quantity of errors found, it does not establish causal links to specific architectural or training weaknesses within the model. Instead, it offers heuristic explanations regarding where and how failures manifest, such as which coverage criteria are better suited for detecting certain types of defects (e.g., corner-region based criteria for quantization errors) or how different seed selection strategies impact the diversity of detected errors.",
          "evidence": "Page 8, Section 5.3.1 (Quantity): 'We also observe that KMNC is less effective than the other criteria in detecting errors, likely because it mainly considers the major functional behavior coverage [31].' This explains how KMNC's focus correlates with finding fewer errors. \nPage 9, Section 5.4 (RQ4. Error Detection for Quantization): 'Corner-region based criteria may be more useful to capture quantization errors.' This identifies a correlation between criteria type and error detection effectiveness. It also states: 'A larger QR often introduces larger logic decision inconsistencies before and after quantization.' and 'Similarly, the same QR would often bring higher influence in larger models.' These are external interpretations of behavioral patterns related to quantization."
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepHunter focuses on generating and selecting inputs to maximize coverage and expose failures, but it does not perform any diagnostic analysis of internal model mechanisms (e.g. activations, gradients, decision boundaries) nor does it establish causal links explaining why those failures occur. It merely reports which coverage criteria or mutation strategies yield more or fewer errors without investigating the internal origins of those errors.",
          "evidence": "\u2022 Section 3 and Algorithm 1 describe only coverage\u2010guided test generation and seed selection based on error occurrence and coverage gains.  \n\u2022 Section 5 (Tables 3\u20136, Figures 4\u20135) reports coverage percentages and counts/diversity of failures under different criteria and strategies, with no analysis of internal signal patterns or causal mechanisms leading to the failures."
        },
        "manual": {
          "value": "Low",
          "why": "It uses model coverage to guide input generation but does not provide any expeirment showing the correlation with coverage"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "High",
          "why": "DeepHunter provides empirical evidence via a user study that its metamorphic mutations produce semantically valid, realistic inputs at very high rates (over 97% validity), demonstrating strong preservation of naturalness.",
          "evidence": "Section 5.1 (User Study) and Table 2: \u201cDeepHunter \u2026 invalidity ratio \u2026 1.2% on MNIST, 2.9% on CIFAR-10, and 0.8% on ImageNet,\u201d and Abstract: \u201cour metamorphic mutation strategy \u2026 by up to a 98% validity ratio.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "DeepHunter explicitly aims to preserve the semantic validity and naturalness of generated inputs through its metamorphic mutation strategy. The paper provides direct empirical evidence for this by conducting a user study where human participants manually evaluate whether the fuzzed images retain their original semantics and are perceptually valid. The results, presented as validity ratios, quantitatively demonstrate the effectiveness of DeepHunter's mutation strategy in preserving naturalness.",
          "evidence": "Page 1, Abstract: 'To this end, we first propose a metamorphic mutation strategy to generate new semantically preserved tests...'\nPage 2, RQ1: 'How effective are different mutation strategies for generating images that keep the same semantics with the original image from a human's perspective?'\nPage 4, Section 3.2.1 'Test Oracle': 'This work adopts the metamorphic relation of test image during transformation as the oracle [10]... the assumption here is that the newly generated tests could still be recognized by a human...'\nPage 6, Section 4 'EXPERIMENTAL DESIGN AND SETTINGS', 'User Study Settings': 'For RQ1, we perform the user study on 3 different strategies that constrain the generation of test images, and evaluate their effectiveness in generating valid test images... Each participant is asked to perform manual analysis on each of the generated test images in line with its original counterpart. A generated image is marked invalid if it either could not be perceived, or is recognized as a different class compared with its original counterpart.'\nPage 7, Section 5.1 'RQ1. User Study on Metamorphic Mutation' and Table 2: 'Table 2 shows the number of invalid images generated by different strategies through manual analysis... The conservative setting in DeepHunter confines the invalidity ratio to be even lower. For example, the invalidity ratio is reduced to 2.9% for CIFAR-10 and 0.8% for ImageNet.'\nPage 7, 'Answer to RQ1': 'The user study shows that each metamorphic mutation strategy may generate invalid images. With proper constraint design and parameter tuning, it is possible to further reduce the invalidity ratio. Our mutation strategy generates test images with a higher validity ratio.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "DeepHunter provides empirical evidence via a user study that its metamorphic mutations produce semantically valid, realistic inputs at very high rates (over 97% validity), demonstrating strong preservation of naturalness.",
          "evidence": "Section 5.1 (User Study) and Table 2: \u201cDeepHunter \u2026 invalidity ratio \u2026 1.2% on MNIST, 2.9% on CIFAR-10, and 0.8% on ImageNet,\u201d and Abstract: \u201cour metamorphic mutation strategy \u2026 by up to a 98% validity ratio.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "User study was conducted on the images generated by single-step metamorphic mutations, not on the final fault-triggering fuzzed inputs after multiple chained transformations. Allows at most one affine transformation to reduce that risk."
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "DeepHunter\u2019s faults stem from semantic-level image transformations (e.g., brightness, contrast, blur, rotation) applied on continuous pixel representations without any explicit validation of persistence through standard I/O serialization (e.g., JPEG/PNG clipping and rounding). Such transformations can produce meaningful errors in memory but may not survive quantization or encoding/decoding, placing them squarely in the \u201cMedium\u201d category.",
          "evidence": "Section 3.2.2 (p.149): \u201cwe select eight image transformations of two categories: \u2022 Pixel Value transformation P: image contrast, image brightness, image blur, and image noise. \u2022 Affine transformation G: image translation, image scaling, image shearing, and image rotation.\u201d Nowhere does the paper describe validating the mutations\u2019 stability under I/O operations such as rounding or encoding."
        },
        "gemini": {
          "value": "High",
          "why": "The method proposes a metamorphic mutation strategy that employs semantic-level image transformations (e.g., contrast, brightness, rotation) and explicitly validates the generated test inputs to preserve semantics. Crucially, it constrains pixel values within the standard 0-255 range using the `f(s, s')` function with `Lo\u221e` constraints (e.g., `Lo\u221e(s, s') \u2264 255` and `Lo\u221e(s, s') < \u03b2 \u00d7 255` where \u03b2 < 1). This effectively performs clipping, ensuring that the mutated inputs are I/O-stable and do not rely on fragile floating-point artifacts that might disappear after standard image encoding (like 8-bit image formats). The user study (RQ1) further confirms that this 'proper constraint design and parameter tuning' leads to generated images with a high 'validity ratio' from a human perception perspective, which indirectly supports their stability under typical I/O processes.",
          "evidence": "Page 4, Section 3.2.2 'Metamorphic Mutation': 'To increase the changeability of mutation, we select eight image transformations of two categories: \u2022 Pixel Value transformation P: image contrast, image brightness, image blur, and image noise. \u2022 Affine transformation G: image translation, image scaling, image shearing, and image rotation.'\nPage 5, Equation 1 and its explanation: 'f(s, s') = { Lo\u221e(s, s') \u2264 255, if Lo(s, s') < \u03b1 \u00d7 size(s) otherwise Lo\u221e(s, s') < \u03b2 \u00d7 255, where 0 < \u03b1, \u03b2 < 1, Lo(s, s') represents the maximum number of the changed pixels between s and s', Lo\u221e represents the maximum value of a pixel changes, size(s) is the total number of pixels in s. Intuitively, if the number of changed pixels is very small (less than \u03b1 \u00d7 size(s)), we assume it does not change the semantics and Lo\u221e can be any value. If the number of changed pixels exceeds the boundary, we limit the maximum value (less than \u03b2 \u00d7 255) that a pixel can change.'\nPage 5, Algorithm 2, Line 12: 'if isSatisfied(f(so, s')) then'\nPage 7, Section 5.1 'RQ1. User Study on Metamorphic Mutation', Answer to RQ1: 'The user study shows that each metamorphic mutation strategy may generate invalid images. With proper constraint design and parameter tuning, it is possible to further reduce the invalidity ratio. Our mutation strategy generates test images with a higher validity ratio.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "Although DeepHunter applies semantic-level image transformations (brightness, contrast, blur, rotation, etc.) and enforces pixel-value bounds via L\u2080/L\u221e constraints, it does not explicitly validate that these mutations survive realistic I/O operations (e.g., JPEG/PNG encoding-decoding or integer quantization). The paper confines pixel values to the [0,255] float range but never simulates or checks serialization/rounding effects, so the discovered faults may vanish after standard image I/O, placing it in the \u201cMedium\u201d category.",
          "evidence": "Section 3.2.2 (p.149\u2013150) defines L\u2080/L\u221e bounds to keep mutated pixel values within [0,255] but makes no reference to saving and reloading images or testing persistence through actual I/O pipelines; Section 5.1 confirms high semantic validity but does not assess I/O stability."
        },
        "manual": {
          "value": "Medium",
          "why": "The metamorphic transformations likely to persist but not validated. Works on floating point input without applying rounding."
        }
      },

      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "Although DeepHunter is evaluated on multiple models, the paper reports coverage and error detection results for each model independently and does not analyze whether the same failing inputs (or mutation patterns) cause equivalent erroneous behaviors across different models performing the same task.",
          "evidence": "Section 4 (Experimental Design), p. 150: \u201cwe perform a large-scale study on 5 DNN models with diverse complexities\u201d \u2013 but all subsequent results (Tables 3\u20136) report per-model metrics without any cross-model failure transfer analysis."
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates DeepHunter on multiple DNN models (LeNet-1, LeNet-5, ResNet-20, VGG16, MobileNet) for image classification. It compares the number and diversity of errors detected by DeepHunter across these models. However, the analysis focuses on the fuzzer's performance and effectiveness in finding defects on different models, rather than analyzing the cross-model consistency of how specific mutations lead to failures, or if the same failing inputs cause equivalent erroneous behaviors across models. While results are compared across models, there is no explicit analysis of fault transferability itself.",
          "evidence": "Page 6, Section 4 'Experimental Design and Settings': 'Datasets and DNN Models. We select three popular publicly available datasets (i.e., MNIST [29], CIFAR-10 [27], and ImageNet [41]) as the evaluation subject datasets (see Table 1). For each dataset, we study several popular pre-trained DNN models...' (Table 1 lists LeNet-1, LeNet-5, ResNet-20, VGG-16, MobileNet).\nPage 8, Section 5.3.1 'Quantity': 'For smaller models, the number of errors detected by DH+Prob and DH+UF are similar. However, when model becomes more complex, DH+Prob detects more errors than DH+UF. For example, in VGG16, DH+Prob detects 10,553 unique errors while DH+UF only uncovers 5,437 unique errors.' (This compares the fuzzer's output quantity across models, not fault transferability).\nPage 9, Section 5.4 'RQ4. Error Detection for Quantization': 'We also find that the same quantization ratio (QR) may have different effects on models with different complexity... For example, both DeepHunter and TensorFuzz found more errors in ResNet-20 and MobileNet but less in LeNet-5.' (Again, this compares the fuzzer's performance across models rather than fault transferability.)"
        },
        "arbitrator": {
          "value": "Low",
          "why": "Although DeepHunter is evaluated on multiple models, the paper reports coverage and error detection results for each model independently and does not analyze whether the same failing inputs (or mutation patterns) cause equivalent erroneous behaviors across different models performing the same task.",
          "evidence": "Section 4 (Experimental Design), p. 150: \u201cwe perform a large-scale study on 5 DNN models with diverse complexities\u201d \u2013 but all subsequent results (Tables 3\u20136) report per-model metrics without any cross-model failure transfer analysis."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ],
      "oracle": [
        "Metamorphic",
        "Differential"
      ]
    }
  },
  "Guo et al. - 2018 - DLFuzz differential fuzzing testing of deep learning systems": {
    "key": "guo2018dlfuzz",
    "name": "DLFuzz",
    "year": "2018",
    "citation_count": "360",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "DLFuzz uncovers low-level functional errors by generating imperceptibly perturbed inputs that cause misclassification in image classifiers, exposing brittle decision boundaries without any breach of safety, policy, or security mechanisms.",
          "evidence": "Page 740, Section 3.1 Architecture: \u201cIf the mutated input t\u2032 and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t\u2032 to be one of the adversarial inputs.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "DLFuzz is designed to identify 'incorrect behaviors' in deep learning systems, specifically instances where a small, imperceptible perturbation to an input image leads to a different prediction (misclassification) by the model. This directly corresponds to triggering low-level functional errors that demonstrate brittle decision boundaries or instability under perturbation.",
          "evidence": "Page 1, Abstract: 'DLFuzz, the first differential fuzzing testing framework to guide DL systems exposing incorrect behaviors. DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.'\nPage 2, Section 3.1 Architecture: 'If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.'\nPage 3, Section 3.2 Algorithm: The objective function aims to maximize 'the prediction error', stating 'Maximizing the first part guides the input to cross the decision boundary of the original class and lie in the decision space of top k other classes. Such modified inputs are more likely to be classified incorrectly.'\nPage 4, Figure 3: Shows examples where an 'Original: 9' is predicted as '4' (by both DeepXplore and DLFuzz after perturbation), and 'Original: rule' is predicted as 'paper' or 'pencil box'."
        },
        "arbitrator": {
          "value": "Low",
          "why": "DLFuzz uncovers low-level functional errors by generating imperceptibly perturbed inputs that cause misclassification in image classifiers, exposing brittle decision boundaries without any breach of safety, policy, or security mechanisms.",
          "evidence": "Page 740, Section 3.1 Architecture: \u201cIf the mutated input t\u2032 and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t\u2032 to be one of the adversarial inputs.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "DLFuzz is designed to find any misclassification by maximizing the prediction difference between the original and mutated inputs, without aiming for a specific target label or semantic category. It performs broad, untargeted exploration of faults rather than steering towards predefined outputs.",
          "evidence": "Section 3.1 (page 739): \"If the mutated input t\u2032 and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t\u2032 to be one of the adversarial inputs.\""
        },
        "gemini": {
          "value": "Low",
          "why": "DLFuzz aims to identify generic faults by generating adversarial inputs that cause a deep learning model to misclassify, i.e., predict a different class label for a slightly perturbed input compared to the original input. It does not steer exploration towards a specific, predefined target class or semantic fault type; rather, it seeks any deviation in prediction. This aligns with untargeted exploration for generic faults like misclassifications or unstable predictions.",
          "evidence": "Section 3.1, Architecture: \"If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs.\" (Page 2)\nSection 3.2, Algorithm, Optimization Problem: The objective function includes `obj = sum(ci) - c`, where `c` is the original class label and `ci` are top k class labels with confidence just lower than `c`. The paper states, \"Maximizing the first part guides the input to cross the decision boundary of the original class and lie in the decision space of top k other classes. Such modified inputs are more likely to be classified incorrectly [12].\" This indicates a goal of inducing *any* incorrect classification, not a specific one. (Page 3)"
        },
        "arbitrator": {
          "value": "Low",
          "why": "DLFuzz is designed to find any misclassification by maximizing the prediction difference between the original and mutated inputs, without aiming for a specific target label or semantic category. It performs broad, untargeted exploration of faults rather than steering towards predefined outputs.",
          "evidence": "Section 3.1 (page 739): \"If the mutated input t\u2032 and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t\u2032 to be one of the adversarial inputs.\""
        },
        "manual": {
          "value": "Low",
          "why": "Finds generic misclassifications"
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "DLFuzz focuses solely on generating adversarial inputs and maximizing neuron coverage to expose incorrect behaviors, without performing any diagnostic or correlational analysis of model internals to explain why the failures occur.",
          "evidence": "Section 3.2 Mutation Algorithm: \u201cThe mutation algorithm is the main component of DLFuzz. It is completed by solving a joint optimization problem of both maximizing the neuron coverage and the number of incorrect behaviors.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "DLFuzz is a fuzzing framework primarily designed to discover incorrect behaviors in deep learning systems by generating adversarial inputs. It focuses on maximizing neuron coverage and the prediction difference between original and mutated inputs. While it effectively finds what fails (i.e., inputs that cause misclassifications with small perturbations), it does not perform diagnostic analysis to explain *why* these vulnerabilities occur. The paper reports on the number of adversarial inputs generated, the size of perturbations, neuron coverage achieved, and time consumption, but it does not delve into analyzing internal signals (beyond using neuron coverage as a generation guide) or external behavioral patterns to infer the root causes of the observed failures.",
          "evidence": "Abstract: 'In this paper, we propose DLFuzz, the first differential fuzzing testing framework to guide DL systems exposing incorrect behaviors. DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.'\nSection 3.1 Architecture: 'If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs. The inconsistent classification results indicate that at least one of them is wrong so that manually labeling effort is not required here.'\nSection 3.2 Algorithm: 'The mutation algorithm is the main component of DLFuzz. It is completed by solving a joint optimization problem of both maximizing the neuron coverage and the number of incorrect behaviors.'\nSection 4.3 Discussion, 'Without Manual Effort': 'We haven't found any adversarial inputs that have already changed their iden-tities after mutation. The adversarial inputs are nearly the same as the original input, and the perturbations are imperceptible.' (This describes the nature of the generated inputs, not the cause of model failure.)"
        },
        "arbitrator": {
          "value": "Low",
          "why": "DLFuzz focuses solely on generating adversarial inputs and maximizing neuron coverage to expose incorrect behaviors, without performing any diagnostic or correlational analysis of model internals to explain why the failures occur.",
          "evidence": "Section 3.2 Mutation Algorithm: \u201cThe mutation algorithm is the main component of DLFuzz. It is completed by solving a joint optimization problem of both maximizing the neuron coverage and the number of incorrect behaviors.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "Does not show any correlation between errors and coverage"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "DLFuzz enforces a norm\u2010based bound on perturbations (\u2113\u2082 distance threshold) to keep mutations imperceptible, but does not provide empirical validation (e.g., SSIM, LPIPS, BLEU scores or formal human studies) to quantify naturalness.",
          "evidence": "Section 3.2 Mutation Algorithm (pages 740\u2013741): \u201cthe minute perturbation is restricted to be invisible and ensures the prediction results before and after the mutation to be the same\u201d and \u201cDLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "DLFuzz enforces input naturalness by bounding perturbations using the L2 distance, ensuring that mutated inputs remain 'visibly indistinguishable' and 'imperceptible' from the originals. This relies on a design constraint (L2 distance limit < 0.02) and qualitative claims, supported by visual examples. However, the paper does not provide quantitative empirical validation using perceptual metrics like LPIPS or SSIM, nor does it conduct a formal human evaluation.",
          "evidence": "Page 2, Section 3.1 Architecture: 'The mutation algorithm applies tiny perturbation to t and gets t', which is visibly indistinguishable from t.'\nPage 3, Section 3.2 Algorithm: 'DLFuzz adopts l2 distance to measure the perturbation with similar computation with [12]. As for the conditions of seed keeping in line 16, DLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.'\nPage 4, Section 4.2 Result: 'As the cases in Figure 3, the perturbations generated by DeepXplore are visible while those generated by DLFuzz are invisible and imperceptible. In this way, DLFuzz provides stronger guarantee for the consistence of the image's identity before and after mutation.'\nPage 4, Section 4.3 Discussion: 'For confirmation, we checked all the 366 adversarial inputs generated by DLFuzz, though DLFuzz maintains quite small l2 distance by the restricted threshold. We haven't found any adversarial inputs that have already changed their iden-tities after mutation. The adversarial inputs are nearly the same as the original input, and the perturbations are imperceptible.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DLFuzz enforces a norm\u2010based bound on perturbations (\u2113\u2082 distance threshold) to keep mutations imperceptible, but does not provide empirical validation (e.g., SSIM, LPIPS, BLEU scores or formal human studies) to quantify naturalness.",
          "evidence": "Section 3.2 Mutation Algorithm (pages 740\u2013741): \u201cthe minute perturbation is restricted to be invisible and ensures the prediction results before and after the mutation to be the same\u201d and \u201cDLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "norm-bounded perturbation"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Low",
          "why": "DLFuzz generates adversarial inputs via minute floating-point perturbations (L2 distance <0.02) that are imperceptible but not explicitly clipped or rounded for I/O stability. Such small continuous perturbations are likely to be lost after standard image quantization or encoding, indicating the discovered faults depend on fragile floating-point artifacts.",
          "evidence": "Section 3.1: \u201cThe minute perturbation is restricted to be invisible and ensures the prediction results before and after the mutation to be the same.\u201d; Section 3.2: \u201cDLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "DLFuzz generates adversarial inputs by applying 'minute' and 'tiny' perturbations, described as 'imperceptible' and limited to a 'relatively small range (less than 0.02)' measured by l2 distance. These are small continuous perturbations derived from a gradient-based optimization process, implying floating-point changes to input values (e.g., pixel values). The paper does not mention any explicit validation mechanisms such as clipping, rounding, or quantization to ensure that these small perturbations persist if the inputs are processed through standard I/O operations that may alter their low-level representation. Consequently, these subtle changes are susceptible to disappearing due to precision loss during serialization, fitting the description of 'Low' vulnerability persistence.",
          "evidence": "Section 1 (Abstract): 'DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input... without manual labeling effort.'\nSection 3.1 (Architecture): 'The mutation algorithm applies tiny perturbation to t and gets t', which is visibly indistinguishable from t.'\nSection 3.2 (Algorithm): 'Optimization Problem. ... It founds perturbations by optimizing the input to maximize the prediction error [16]...'\nSection 3.2 (Algorithm): '...DLFuzz adopts l2 distance to measure the perturbation with similar computation with [12]. As for the conditions of seed keeping in line 16, DLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.'\nSection 4.2 (Result): 'DLFuzz averagely generated 338.59% more adversarial inputs with 89.82% smaller perturbations.'\nSection 4.3 (Discussion): 'For confirmation, we checked all the 366 adversarial inputs generated by DLFuzz, though DLFuzz maintains quite small l2 distance by the restricted threshold. We haven't found any adversarial inputs that have already changed their identities after mutation. The adversarial inputs are nearly the same as the original input, and the perturbations are imperceptible.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "DLFuzz generates adversarial inputs via minute floating-point perturbations (L2 distance <0.02) that are imperceptible but not explicitly clipped or rounded for I/O stability. Such small continuous perturbations are likely to be lost after standard image quantization or encoding, indicating the discovered faults depend on fragile floating-point artifacts.",
          "evidence": "Section 3.1: \u201cThe minute perturbation is restricted to be invisible and ensures the prediction results before and after the mutation to be the same.\u201d; Section 3.2: \u201cDLFuzz limits our desired distance to a relatively small range (less than 0.02) to ensure the imperceptibility.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "relies on imperceptible, float-level perturbations that can vanish after quantization or saving, with no persistence verification."
        }
      },
 
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "Although DLFuzz evaluates its fuzzing methodology on multiple models (LeNet-1/4/5 and VGG-16/19, ResNet50), it treats each model independently and does not analyze whether the same adversarial inputs transfer failure behaviors across models. The paper explicitly avoids cross-referencing other DL systems as oracles, and no cross-model consistency or transferability experiments are reported.",
          "evidence": "Page 1, Abstract: \"DLFuzz ... without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\"; Page 3, Section 3.1: \"The given test input t is an image to be classified, the DNN is a particular convolutional neural network (CNN) under test, such as VGG-16.\""
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates DLFuzz on multiple pre-trained models (LeNet-1, LeNet-4, LeNet-5 for MNIST; VGG-16, VGG-19, ResNet50 for ImageNet). However, the evaluation focuses on the performance of DLFuzz (e.g., neuron coverage, number of adversarial inputs generated, perturbation size) for each model independently. The paper does not analyze whether a specific adversarial input found to cause a failure on one model also causes an equivalent erroneous behavior on other models performing the same task. While DLFuzz's internal mechanism involves differential testing between original and mutated inputs *for a single model*, it explicitly states it does not require cross-referencing oracles from other DL systems, which is a common approach for analyzing cross-model consistency.",
          "evidence": "Page 1, Abstract: 'Compared with DeepXplore, the state-of-the-art DL whitebox testing framework, DLFuzz does not require extra efforts to find similar functional DL systems for cross-referencing check...'\nPage 2, Section 1, paragraph 3: 'To evaluate the efficiency of DLFuzz, we conducted empirical studies on six DL systems trained on two popular datasets, MNIST [9] and ImageNet [3].'\nPage 3, Section 3.2 Algorithm: 'If the mutated input t' and the original input t are both fed to the CNN but classified to be of different class labels, we treat this as an incorrect behavior and t' to be one of the adversarial inputs. The inconsistent classification results indicate that at least one of them is wrong so that manually labeling effort is not required here.' (This describes per-model failure detection).\nPage 4, Section 4.2 Result, Table 1: 'Effectiveness of DLFuzz compared with DeepXplore.' The table presents results for 'NC Imp.' (Neuron Coverage Improvement), '#Adv.' (Number of Adversarial Inputs), and 'Time' for each model individually, showing DLFuzz's performance *on* each model rather than the transferability of failures *across* models."
        },
        "arbitrator": {
          "value": "Low",
          "why": "Although DLFuzz evaluates its fuzzing methodology on multiple models (LeNet-1/4/5 and VGG-16/19, ResNet50), it treats each model independently and does not analyze whether the same adversarial inputs transfer failure behaviors across models. The paper explicitly avoids cross-referencing other DL systems as oracles, and no cross-model consistency or transferability experiments are reported.",
          "evidence": "Page 1, Abstract: \"DLFuzz ... without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality.\"; Page 3, Section 3.1: \"The given test input t is an image to be classified, the DNN is a particular convolutional neural network (CNN) under test, such as VGG-16.\""
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Whitebox"
      ],
      "mutation_strategy": [
        "Feedback-informed"
      ],
      "exploration_strategy": [
        "Coverage-guided",
        "Prediction-guided"
      ],
      "oracle": [
        "Metamorphic"
      ]
    }
  },
  "Yu et al. - LLM-Fuzzer Scaling Assessment of Large Language Model Jailbreaks": {
    "key": "yu2024llm",
    "name": "LLM-Fuzzer",
    "year": "2024",
    "citation_count": "40",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "High",
          "why": "LLM-Fuzzer uncovers vulnerabilities that explicitly bypass the model\u2019s built-in safety and policy safeguards, generating harmful content despite safety fine-tuning.",
          "evidence": "Page 4657, Abstract: \u201cOur experiments show that LLM-FUZZER-generated jailbreak prompts demonstrate significantly increased effectiveness and transferability. This highlights that many open-source and commercial LLMs suffer from severe jailbreak issues, even after safety fine-tuning.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "LLM-Fuzzer is explicitly designed to uncover vulnerabilities that bypass explicit safety, policy, or security mechanisms in Large Language Models. The paper consistently refers to its primary goal as 'scaling assessment of LLM jailbreaks' and generating 'harmful content' by circumventing safety measures, which aligns directly with the 'High' severity level of the metric.",
          "evidence": "Page 2, Abstract: 'The jailbreak threat poses a significant concern for Large Language Models (LLMs), primarily due to their potential to generate content at scale. If not properly controlled, LLMs can be exploited to produce undesirable outcomes, including the dissemination of misinformation, offensive content, and other forms of harmful or unethical behavior. To tackle this pressing issue...we introduce an automated solution for large-scale LLM jailbreak susceptibility assessment called LLM-FUZZER.'\nPage 3, Section 3.1 Technical Overview and Challenges: 'Our novel automated testing solution, LLM-FUZZER, offers the potential to complement the endeavors of LLM's red teams, rendering their efforts more scalable and effective.'\nPage 9, Section 4.3 Results for Experiment I: 'Among the 100 unethical questions where human-written templates all failed to jailbreak gpt-3.5-turbo-0125, our fuzzer was capable of generating templates to assist on average 96.85 questions in achieving their jailbreaking objectives through template mutations.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "LLM-Fuzzer uncovers vulnerabilities that explicitly bypass the model\u2019s built-in safety and policy safeguards, generating harmful content despite safety fine-tuning.",
          "evidence": "Page 4657, Abstract: \u201cOur experiments show that LLM-FUZZER-generated jailbreak prompts demonstrate significantly increased effectiveness and transferability. This highlights that many open-source and commercial LLMs suffer from severe jailbreak issues, even after safety fine-tuning.\u201d"
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Medium",
          "why": "LLM-Fuzzer deliberately targets the semantic category of \u201cjailbreak\u201d faults\u2014i.e. causing the model to generate harmful, biased, or unsafe content\u2014without constraining it to produce any specific output phrase or label.",
          "evidence": "Introduction (p. 4657): \u201cRed teams \u2026 manually create adversarial inputs and prompts designed to push LLMs into generating harmful, biased, or inappropriate content.\u201d\n\nSection 3.4 (p. 4663): \u201cThe oracle\u2019s role is to take prompts as input and evaluate the responses to determine if they are harmful. If harmful responses are detected, the oracle assigns a corresponding reward.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The LLM-Fuzzer aims to discover 'jailbreaks' in Large Language Models, which are defined as instances where LLMs generate 'illegal or immoral information, hate speech, and other objectionable content.' The system is designed to generate adversarial prompts that lead to such category-level unsafe behaviors, rather than forcing the LLM to output a specific, pre-defined phrase or solely conducting untargeted exploration for generic errors. The oracle's role is to assess whether the LLM's response is 'harmful,' not to check for an exact, pre-specified harmful output.",
          "evidence": "Page 2, Abstract: 'The jailbreak threat poses a significant concern for Large Language Models (LLMs), primarily due to their potential to generate content at scale. If not properly controlled, LLMs can be exploited to produce undesirable outcomes, including the dissemination of misinformation, offensive content, and other forms of harmful or unethical behavior. To tackle this pressing issue, researchers and developers often rely on red-team efforts to manually create adversarial inputs and prompts designed to push LLMs into generating harmful, biased, or inappropriate content. To address these scalability issues, we introduce an automated solution for large-scale LLM jailbreak susceptibility assessment called LLM-FUZZER.'\nPage 5, Section 3.1, Technical Overview and Challenges: 'The oracle is responsible for identifying whether a response contains harmful content. If the oracle identifies a response as harmful, LLM-FUZZER retains the template and adds it back to the seed pool.'\nPage 8, Section 3.4, Oracle: 'The oracle's role is to take prompts as input and evaluate the responses to determine if they are harmful. If harmful responses are detected, the oracle assigns a corresponding reward.'\nPage 9, Section 4.1, Experiment Setup - Oracle: 'Our fuzzer relies on a meticulously fine-tuned RoBERTa model to function as an oracle, assessing the potential harm in responses and making determinations regarding the success or failure of jailbreak attempts.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "LLM-Fuzzer deliberately targets the semantic category of \u201cjailbreak\u201d faults\u2014i.e. causing the model to generate harmful, biased, or unsafe content\u2014without constraining it to produce any specific output phrase or label.",
          "evidence": "Introduction (p. 4657): \u201cRed teams \u2026 manually create adversarial inputs and prompts designed to push LLMs into generating harmful, biased, or inappropriate content.\u201d\n\nSection 3.4 (p. 4663): \u201cThe oracle\u2019s role is to take prompts as input and evaluate the responses to determine if they are harmful. If harmful responses are detected, the oracle assigns a corresponding reward.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "aims to generate many jailbreak templates that trigger harmful content without specifying exact outcomes"
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "LLM-Fuzzer focuses exclusively on generating and reporting jailbreak prompts that cause harmful outputs, without performing any analysis of model internals (e.g., activations, gradients, coverage) or correlational/external behavioral analyses to explain why these failures occur.",
          "evidence": "Section 3 (Proposed Technique) and Section 3.4 (Oracle): \u201cLLM-Fuzzer selects a template, mutates it, queries the target LLM, and uses an oracle to identify whether a response contains harmful content. If harmful, the template is retained.\u201d (no internal diagnostics or causal analysis are performed)"
        },
        "gemini": {
          "value": "Low",
          "why": "The LLM-Fuzzer method primarily focuses on generating and identifying jailbreak prompts (i.e., 'what fails') and assessing their effectiveness and transferability across different LLMs. The paper explicitly states that the fuzzing target (the LLM) 'remains beyond our control' and their 'only means of assessment is analyzing the LLM's response to determine if an undesired output has been generated.' This indicates a lack of internal diagnostic capabilities. The oracle used is a fine-tuned RoBERTa model that classifies responses as harmful or not, providing a reward signal for the fuzzer, rather than explaining the underlying reasons for the LLM's harmful output. While the paper quantifies the success rate of jailbreaks and discusses observations about model robustness (e.g., Gemini's resistance), it does not delve into the internal mechanisms of the LLMs to establish causal links to architectural, training, or data-related weaknesses, nor does it provide correlational analysis of internal signals or detailed external interpretations of behavioral patterns beyond reporting success rates.",
          "evidence": "Page 3, Section 3.1, Technical Overview and Challenges: 'Third, traditional fuzzing often utilizes various sanitizers [1, 48, 49, 53] or instrumented code coverage collectors [36, 55, 72] or observes different execution behaviors [43] as oracles to provide feedback and guide subsequent fuzzer operations. In our case, the fuzzing target is an LLM that remains beyond our control. Our only means of assessment is analyzing the LLM's response to determine if an undesired output has been generated. Consequently, we need an efficient and effective method for discerning the harmfulness of the response.'\n\nPage 8, Section 3.4, Oracle: 'To achieve a balance between cost and accuracy, we developed our own oracle for assessing the harmfulness of responses in our work. We began by using a substantial corpus of responses to unethical questions, meticulously annotating them with labels denoting their harmfulness or non-harmful nature. Subsequently, we harnessed these annotated responses to fine-tune a RoBERTa model [35]. In the context of our research, this RoBERTa model serves as our designated \"oracle\".'\n\nPage 9, Section 4, Experiment / Metrics: 'We define four metrics \u2013 jailbreaking question number (JQN), the individual template's attack success rate (ASR), a group of templates' ensemble attack success rate (EASR), query budget consumption (QBC) and the token budget consumption (TBC). These metrics are detailed as follows:' (These metrics quantify failure symptoms and fuzzing efficiency, not root causes.)"
        },
        "arbitrator": {
          "value": "Low",
          "why": "LLM-Fuzzer focuses exclusively on generating and reporting jailbreak prompts that cause harmful outputs, without performing any analysis of model internals (e.g., activations, gradients, coverage) or correlational/external behavioral analyses to explain why these failures occur.",
          "evidence": "Section 3 (Proposed Technique) and Section 3.4 (Oracle): \u201cLLM-Fuzzer selects a template, mutates it, queries the target LLM, and uses an oracle to identify whether a response contains harmful content. If harmful, the template is retained.\u201d (no internal diagnostics or causal analysis are performed)"
        },
        "manual": {
          "value": "Low",
          "why": "Does not provide any explanation why those errors occur."
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "LLM-Fuzzer enforces semantic correctness of mutated prompts through its design of five LLM-assisted mutation operators, but the paper does not provide any empirical evaluation (e.g., BLEU, human studies) demonstrating that its generated prompts remain realistic or natural.",
          "evidence": "Section 3.3 (Mutation): \u201cTo maintain the semantic correctness of newly generated prompts, we have introduced five innovative mutation operators that harness LLMs as assistants for prompt alteration while preserving their semantic integrity.\u201d \u2013 no quantitative or human\u2010evaluation metrics for naturalness are reported."
        },
        "gemini": {
          "value": "Medium",
          "why": "The LLM-Fuzzer method explicitly designs its mutation operators to preserve the semantic correctness and naturalness of generated prompts by leveraging Large Language Models (LLMs) as assistants. The paper states that traditional fuzzing methods often produce 'syntactically incorrect or semantically nonsensical inputs' and introduces five mutation operators (Generate, Crossover, Expand, Shorten, Rephrase) specifically aimed at 'crafting coherent and contextually relevant mutations' while 'preserving their semantic integrity'. This demonstrates a strong theoretical justification and design constraint for maintaining input naturalness. However, the paper does not provide empirical validation of this naturalness using quantitative metrics (e.g., BLEU, LPIPS, SSIM) or a dedicated human evaluation specifically for the naturalness of the fuzzed inputs.",
          "evidence": "Page 3, Section 3.1, 'Technical Overview and Challenges': 'Last but certainly not least, in contrast to traditional fuzzing, our mutation operation is geared towards semantically rich jailbreak prompts. When using previous input mutation meth-ods, ensuring the semantic correctness of newly generated prompts can be a formidable challenge. To maintain the se-mantic correctness of newly generated prompts, we have intro-duced five innovative mutation operators that harness LLMs as assistants for prompt alteration while preserving their se-mantic integrity.'\n\nPage 7, Section 3.3, 'Mutation': 'To address this challenge, we introduce distinct mutation methods that leverage LLMs themselves to assist in the mutation process. LLMs, with their proficiency in understanding and generat-ing human-like text, offer a promising approach for crafting coherent and contextually relevant mutations. Their capabil-ities in tasks such as article writing [10, 15] and instruction following [42] further demonstrate their ability to generate diverse and meaningful variations in text.'\n\nPage 7, Section 3.3, 'Mutation' (description of Rephrase operator): 'Rephrase: This operator involves providing an LLM with a template and instructing it to rephrase every sentence within the template while maintaining the overall meaning. Specifically, the instruction includes using synonyms, reor-ganizing sentence structure, and employing paraphrasing techniques, among others.'\n\nPage 7, Section 3.3, 'Mutation': 'In order to showcase that the newly generated templates can maintain their semantic integrity and diversify our template collection, Figure 3 presents several post-mutated templates. For more examples, readers could refer to Appendix D.2.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "LLM-Fuzzer enforces semantic correctness of mutated prompts through its design of five LLM-assisted mutation operators, but the paper does not provide any empirical evaluation (e.g., BLEU, human studies) demonstrating that its generated prompts remain realistic or natural.",
          "evidence": "Section 3.3 (Mutation): \u201cTo maintain the semantic correctness of newly generated prompts, we have introduced five innovative mutation operators that harness LLMs as assistants for prompt alteration while preserving their semantic integrity.\u201d \u2013 no quantitative or human\u2010evaluation metrics for naturalness are reported."
        },
        "manual": {
          "value": "Medium",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "High",
          "why": "LLM-Fuzzer\u2019s mutations operate at the discrete, semantic text level, producing self-contained prompt strings rather than sub-pixel or floating-point perturbations. According to the metric definition, model-generated text inputs are inherently stable under I/O transformations, and LLM-Fuzzer does not rely on fragile floating-point artifacts.",
          "evidence": "Section 3.3 Mutation: \u201cwe introduce five innovative mutation operators that harness LLMs as assistants for prompt alteration while preserving their semantic integrity.\u201d\n\nMetric definition: \u201cmodel-generated fuzzed inputs\u2014such as \u2026 generated text\u2014are inherently stable under I/O operations because they originate from self-contained generation processes that produce finalized artifacts.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The method, LLM-FUZZER, generates jailbreak prompts as fuzzed inputs. These prompts are textual data created by leveraging other Large Language Models (LLMs) as mutation operators. According to the metric definition, 'Model-generated fuzzed inputs\u2014such as synthesized speech, rendered images, or generated text\u2014are inherently stable under I/O operations because they originate from self-contained generation processes that produce finalized artifacts rather than transient numeric states.' Since LLM-FUZZER's outputs are generated text (jailbreak prompts), they are inherently stable and not subject to the floating-point precision issues or rounding/quantization that would affect continuous representations like images or audio.",
          "evidence": "Page 2, Abstract: 'LLM-FUZZER uses human-crafted jailbreak prompts as starting points. By employing carefully customized seed selection strategies and mutation mechanisms, LLM-FUZZER generates additional jailbreak prompts tailored to specific LLMs.'\nPage 7, Section 3.3 Mutation: 'To address this challenge, we introduce distinct mutation methods that leverage LLMs themselves to assist in the mutation process.' ... 'Therefore, we require a novel mutation method capable of generating semantically meaningful inputs for LLM-FUZZER.'\nPage 7, Section 3.3, bullet points for mutation operators: Describes operators like 'Generate', 'Crossover', 'Expand', 'Shorten', and 'Rephrase' which all produce new textual templates.\nPage 8, Figure 3: Provides examples of 'post-mutated templates' which are all textual prompts."
        },
        "arbitrator": {
          "value": "High",
          "why": "LLM-Fuzzer\u2019s mutations operate at the discrete, semantic text level, producing self-contained prompt strings rather than sub-pixel or floating-point perturbations. According to the metric definition, model-generated text inputs are inherently stable under I/O transformations, and LLM-Fuzzer does not rely on fragile floating-point artifacts.",
          "evidence": "Section 3.3 Mutation: \u201cwe introduce five innovative mutation operators that harness LLMs as assistants for prompt alteration while preserving their semantic integrity.\u201d\n\nMetric definition: \u201cmodel-generated fuzzed inputs\u2014such as \u2026 generated text\u2014are inherently stable under I/O operations because they originate from self-contained generation processes that produce finalized artifacts.\u201d"
        },
        "manual": {
          "value": "High",
          "why": "Mutations are purely at the text level"
        }
      },
 
      "Attack Transferability": {
        "chatgpt": {
          "value": "High",
          "why": "The paper empirically demonstrates that the same generated jailbreak prompts (e.g., the top-1 and top-5 templates) induce equivalent failure behaviors across a wide range of both open-source and commercial LLMs, achieving consistently high ensemble attack success rates (EASR > 80%). This shows input-level reproducibility of failures across models.",
          "evidence": "Section 4.2 Experiment III (page 4664): \u201cIn our subsequent experiment, we utilized both \u2018top-1\u2019 and \u2018top-5\u2019 to facilitate an additional 100 unethical questions aimed at jailbreaking various LLMs. Notably, these LLMs were distinct from the ones involved in generating \u2018top-1\u2019 and \u2018top-5\u2019. The success of jailbreak attempts on these models served as an indicator of the transferability of the \u2018top-1\u2019 and \u2018top-5\u2019 templates.\u201d\n\nFigure 4 (page 4668): \u201cAs depicted in Figure 4, when evaluated against Vicuna-13B, Baichuan-13B, ChatGLM2-6B, Llama-2-13B/70B, Claude1/2.0, and PaLM2, the EASR for LLM-FUZZER consistently exceeded 80%, having a huge advantage over EASRs of other methods.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The paper empirically demonstrates that the jailbreak templates generated by LLM-FUZZER, which constitute a core part of the 'failing inputs,' successfully cause equivalent erroneous behaviors (i.e., successful jailbreaks) across a diverse range of large language models. Experiment III was specifically designed to evaluate this transferability. The generated 'top-1' and 'top-5' templates, initially created with a focus on universality across three LLMs, were subsequently applied to a much broader set of open-source and commercial models. The reported high ASR and EASR (e.g., EASR consistently exceeding 80% for LLM-FUZZER generated templates across most evaluated models) indicate that the same crafted inputs lead to similar undesirable outputs (jailbreaks) across different models.",
          "evidence": "Page 10, Section 4.2 Experiment Design, Experiment III:\n\"To address the question of transferability, we chose to create templates using gpt-3.5-turbo-0125, Llama-2-7B-Chat, and Vicuna-7B. This approach was taken to ensure that the generated templates would be applicable across various LLMs. ... a zero reward was imposed if the template\u2019s all successful jailbreak attempts were attributed solely to one or two LLMs. This modification aimed to bolster universality by discouraging the generation of templates that could exploit only one or two specific LLMs.\"\n\nPage 11, Section 4.3 Experiment Result, Results for Experiment III:\n\"Figure 4 presents a visual rep-resentation of the performance of the ASR and EASR of gen-erated templates by fuzzer. It is important to recall that these template sets were initially generated to exploit Vicuna-7B, gpt-3.5-turbo-0125, and Llama-2-7B-Chat. However, when we applied these templates against other LLMs, a remark-able discovery unfolded. These templates displayed a notable degree of transferability.\"\n\"As depicted in Figure 4, when evaluated against Vicuna-13B, Baichuan-13B, ChatGLM-6B, Llama-2-13B/70B, Claudel, and PaLM2, the EASR for LLM-FUZZER consis-tently exceeded 80%, having a huge advantage over EASRS of other methods. The expressive EASR signifies that the templates initially identified as the most effective against Vicuna-7B, gpt-3.5-turbo-0125, and Llama-2-7B-Chat managed to retain their performance when employed against other well-aligned LLMs.\"\n\nPage 13, Figure 4: \"Figure illustrates the comparison of LLM-FUZZER's performance against baseline methods in the transfer scenario, assessing the effectiveness across multiple open-sourced and commercial LLMs. The effectiveness is evaluated using the individual ASR and EASR, showcasing the universality and effectiveness of the generated templates by LLM-FUZZER in compromising diverse models.\""
        },
        "arbitrator": {
          "value": "High",
          "why": "The paper empirically demonstrates that the same generated jailbreak prompts (e.g., the top-1 and top-5 templates) induce equivalent failure behaviors across a wide range of both open-source and commercial LLMs, achieving consistently high ensemble attack success rates (EASR > 80%). This shows input-level reproducibility of failures across models.",
          "evidence": "Section 4.2 Experiment III (page 4664): \u201cIn our subsequent experiment, we utilized both \u2018top-1\u2019 and \u2018top-5\u2019 to facilitate an additional 100 unethical questions aimed at jailbreaking various LLMs. Notably, these LLMs were distinct from the ones involved in generating \u2018top-1\u2019 and \u2018top-5\u2019. The success of jailbreak attempts on these models served as an indicator of the transferability of the \u2018top-1\u2019 and \u2018top-5\u2019 templates.\u201d\n\nFigure 4 (page 4668): \u201cAs depicted in Figure 4, when evaluated against Vicuna-13B, Baichuan-13B, ChatGLM2-6B, Llama-2-13B/70B, Claude1/2.0, and PaLM2, the EASR for LLM-FUZZER consistently exceeded 80%, having a huge advantage over EASRs of other methods.\u201d"
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Oracle-guided"
      ],
      "oracle": [
        "Property-based"
      ]
    }
  },
  "Tian et al. - 2018 - DeepTest automated testing of deep-neural-network-driven autonomous cars": {
    "key": "tian2018deeptest",
    "name": "DeepTest",
    "year": "2018",
    "citation_count": "1791",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepTest uncovers low-level functional errors in DNN-based autonomous driving (incorrect steering predictions under realistic image perturbations), exposing brittle decision boundaries and robustness failures rather than bypassing any explicit safety, policy, or security mechanisms.",
          "evidence": "Page 303\u2013304, Abstract: \u201cDeepTest \u2026 automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes.\u201d; Section 3.4 (p. 307): \u201cthe autonomous car\u2019s steering angle should not change significantly for the same image under any lighting/weather conditions, blurring, or any affine transformations with small parameter values.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "DeepTest reveals low-level functional errors in the deep neural networks (DNNs) that control autonomous cars. Specifically, it identifies instances where the steering angle predicted by the DNN deviates significantly under realistic image transformations (e.g., rain, fog, blurring) compared to the original image, violating defined metamorphic relations. These are considered functional errors\u2014misclassifications or flawed outputs\u2014rather than violations of normative expectations or bypasses of explicit safety/security mechanisms.",
          "evidence": "Page 1, Abstract: 'DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes.'\nPage 2, Abstract/Contributions: 'We leverage transformation-specific metamorphic relations to automatically detect erroneous behaviors.'\nPage 5, Section 3.4 'Creating a Test Oracle with Metamorphic Relations': 'For example, the autonomous car's steering angle should not change significantly for the same image under any lighting/weather conditions, blurring, or any affine transformations with small parameter values. Thus, if a DNN model infers a steering angle \u03b8_0 for an input seed image I_0 and a steering angle \u03b8_t for a new synthetic image I_t, which is generated by applying the transformation t on I_0, one may define a simple metamorphic relation where \u03b8_0 and \u03b8_t are identical.' Also, 'Leveraging this property, we redefine a new metamorphic relation as: (\u03b8_t \u2013 \u03b8_0^')^2 \u2264 2 MSE_orig'. This equation identifies when the output steering angle (\u03b8_t) under transformation deviates too much from the expected output (\u03b8_0^'), indicating a functional error.\nPage 8, Figure 7: 'Sample images showing erroneous behaviors detected by DeepTest using synthetic images.' The figure illustrates cases where the steering angle changes drastically (e.g., from straight to sharp left/right turn) due to transformations like rain or shear, which are incorrect functional outputs."
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepTest uncovers low-level functional errors in DNN-based autonomous driving (incorrect steering predictions under realistic image perturbations), exposing brittle decision boundaries and robustness failures rather than bypassing any explicit safety, policy, or security mechanisms.",
          "evidence": "Page 303\u2013304, Abstract: \u201cDeepTest \u2026 automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes.\u201d; Section 3.4 (p. 307): \u201cthe autonomous car\u2019s steering angle should not change significantly for the same image under any lighting/weather conditions, blurring, or any affine transformations with small parameter values.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Medium",
          "why": "DeepTest does not steer the DNN to produce a specific, attacker-chosen steering angle (High) nor does it perform completely untargeted exploration (Low). Instead, it applies realistic transformations (e.g., rain, fog, lighting changes) and uses metamorphic relations to trigger and detect semantic, safety-critical faults\u2014namely unsafe or potentially fatal driving behaviors\u2014without fixing the exact output value.",
          "evidence": "Section 3.4 \u201cCreating a Test Oracle with Metamorphic Relations\u201d (p. 307): \u201cFor example, the autonomous car\u2019s steering angle should not change significantly for the same image under any lighting/weather conditions. Thus, if a DNN model infers a steering angle \u03b8\u2092 for an input I\u2092 and a steering angle \u03b8\u209c for a transformed image I\u209c \u2026 we may define a simple metamorphic relation where \u03b8\u2092 and \u03b8\u209c are identical.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "DeepTest aims to detect semantic or category-level faults, specifically 'erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes.' It leverages metamorphic relations to identify when an autonomous car's behavior, such as its steering angle, changes significantly and unexpectedly under realistic image transformations (e.g., rain, fog, blurring). This approach seeks to uncover unsafe behaviors without constraining the exact nature or value of the erroneous output.",
          "evidence": "Page 1, Abstract: 'DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes.'\nPage 1, Introduction: 'DNN-based software, including the ones used for autonomous driving, often demonstrate incorrect/unexpected corner-case behaviors that can lead to dangerous consequences like a fatal collision.'\nPage 5, Section 3.4, 'Creating a Test Oracle with Metamorphic Relations': 'For example, the autonomous car's steering angle should not change significantly for the same image under any lighting/weather conditions, blurring, or any affine transformations with small parameter values.' And the metamorphic relation: '(\u03b8t \u2013 \u03b8o)\u00b2 \u2264 2 MSEorig' (Equation 2)."
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DeepTest does not steer the DNN to produce a specific, attacker-chosen steering angle (High) nor does it perform completely untargeted exploration (Low). Instead, it applies realistic transformations (e.g., rain, fog, lighting changes) and uses metamorphic relations to trigger and detect semantic, safety-critical faults\u2014namely unsafe or potentially fatal driving behaviors\u2014without fixing the exact output value.",
          "evidence": "Section 3.4 \u201cCreating a Test Oracle with Metamorphic Relations\u201d (p. 307): \u201cFor example, the autonomous car\u2019s steering angle should not change significantly for the same image under any lighting/weather conditions. Thus, if a DNN model infers a steering angle \u03b8\u2092 for an input I\u2092 and a steering angle \u03b8\u209c for a transformed image I\u209c \u2026 we may define a simple metamorphic relation where \u03b8\u2092 and \u03b8\u209c are identical.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "Aims to find errors in steering angle predictions under realistic environmental variations."
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepTest focuses on uncovering when and under what transformed inputs the DNN fails (e.g., rain, fog, affine changes) but does not analyze or explain the internal causal mechanisms\u2014such as specific feature activations, architectural weaknesses, or optimization dynamics\u2014that lead to those failures. It reports symptoms detected via metamorphic relations and neuron\u2010coverage differences without diagnosing their root causes.",
          "evidence": "Page 306, Section 3.1: \u201cDeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons\u2026\u2009identify erroneous behaviors.\u201d; Page 307, Section 3.4: \u201cwe leverage transformation\u2010specific metamorphic relations\u2026to automatically detect erroneous behaviors.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "DeepTest explains where and how failures manifest by using correlational analysis of internal signals (neuron coverage) and external interpretation of behavioral patterns (responses to specific image transformations and violations of metamorphic relations). The paper empirically demonstrates that changes in neuron coverage correlate with changes in the autonomous car's behavior, and identifies specific realistic image transformations that induce erroneous outputs. However, it does not establish causal links to specific architectural or training weaknesses within the DNN model, instead focusing on identifying conditions and internal signal correlations associated with failures.",
          "evidence": "Page 2, Abstract: \"DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes...\"\nPage 2, Section 3.1: \"We empirically demonstrate that changes in neuron coverage are statistically correlated with changes in the actions of self-driving cars (e.g., steering angle). Therefore, neuron coverage can be used as a guidance mechanism for systemically exploring different types of car behaviors and identify erroneous behaviors.\"\nPage 5, Section 3.4: \"...one can define relationships between the car's behaviors across certain types of transformations. For example, the autonomous car's steering angle should not change significantly for the same image under any lighting/weather conditions, blurring, or any affine transformations with small parameter values.\"\nPage 6, Section 5, RQ1: \"This result indicates that the neuron coverage changes with the changes in output steering angles, i.e. different neurons get activated for different outputs. Thus, in this setting, neuron coverage can be a good approximation for estimating the diversity of input-output pairs. Moreover, our finding that monotonic correlations between neuron coverage and steering angle also corroborate Goodfellow et al.'s hypothesis that, in practice, DNNs are often highly linear [40].\" (Referring to Table 5 which shows Spearman rank correlation and Wilcoxon test results for neuron coverage and steering angle/direction)."
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DeepTest goes beyond merely reporting failure symptoms by analyzing internal neuron activations and demonstrating statistically significant correlations between changes in neuron coverage and steering\u2010angle deviations under various image transformations. It thus offers heuristic, correlational explanations of where and how failures manifest, without tracing failures back to specific architectural or training flaws.",
          "evidence": "Section 3.1 (p. 306): DeepTest uses \u201cneuron coverage\u201d as a metric and \u201cempirically demonstrate[s] that changes in neuron coverage are statistically correlated with changes in the actions of self\u2010driving cars (e.g., steering angle).\u201d\nSection 5 RQ1 (p. 309, Table 5): Reports Spearman rank correlations and Wilcoxon tests showing neuron coverage varies significantly with steering angle and direction, linking internal signals to failure patterns."
        },
        "manual": {
          "value": "Medium",
          "why": "Shows correlation between coverage and errors"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "DeepTest generates inputs via domain-inspired image transformations (e.g., rain, fog, brightness, contrast, geometric perturbations) that are theoretically intended to preserve realism, but the paper does not present any empirical validation (e.g., SSIM, LPIPS, BLEU or human studies) to quantify or confirm the naturalness of the generated images.",
          "evidence": "Section 3.2 (p. 306): \u201cDeepTest focuses on generating realistic synthetic images by applying image transformations on seed images and mimic different real-world phenomena like camera lens distortions, object movements, different weather conditions, etc. \u2026 we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect).\u201d  No quantitative naturalness assessment (e.g., SSIM, LPIPS, or human evaluation) is reported in Sections 5 or elsewhere."
        },
        "gemini": {
          "value": "High",
          "why": "The paper provides empirical evidence that its fuzzed inputs, referred to as 'synthetic images,' maintain naturalness by explicitly designing transformations to mimic real-world conditions and through human evaluation during false positive analysis. The authors detail specific image transformations (e.g., rain, fog, brightness, contrast, rotation, blur, shear) that are intended to be realistic, leveraging standard image processing techniques and tools like Adobe Photoshop filters. Furthermore, the paper states that all authors conducted a manual review of detected erroneous behaviors, agreeing on 'false positives,' which implies the images were considered plausible and realistic enough for the model to be expected to perform correctly.",
          "evidence": "Page 1, Abstract: 'DeepTest is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc.'\nPage 2, Section 1, Introduction: 'DeepTest focuses on generating realistic synthetic images by applying image transformations on seed images and mimic different real-world phenomena like camera lens distortions, object movements, different weather conditions, etc.'\nPage 4, Section 3.2, Increasing Coverage with Synthetic Images: 'Therefore, DeepTest focuses on generating realistic synthetic images by applying image transformations on seed images and mimic different real-world phenomena like camera lens distortions, object movements, different weather conditions, etc. To this end, we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect).'\nPage 5, Section 3.3: 'We use four different types of blurring filters: averaging, Gaussian, median, and bilateral [3]. We compose multiple filters provided by Adobe Photoshop on the input images to simulate realistic fog and rain effects [1, 2].'\nPage 9, Figure 7: 'Sample images showing erroneous behaviors detected by DeepTest using synthetic images. For original images the arrows are marked in blue, while for the synthetic images they are marked in red.' (Visual examples of generated images, e.g., 'fog', 'rain', 'rotation(6 degree)', 'contrast(1.8)', which appear realistic.)\nPage 9, Section 5, last paragraph: 'Although such manual investigation is, by definition, subjective and approximate, all the authors have reviewed the images and agreed on the false positives.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "While DeepTest uses domain-inspired image transformations (e.g., rain, fog, brightness, contrast, geometric perturbations) that are plausibly realistic, the paper does not include any quantitative assessment of naturalness (e.g., SSIM, LPIPS, BLEU) nor a dedicated human\u2010subjects evaluation confirming that the generated images are perceptually or semantically consistent with real\u2010world driving data. The informal manual review of false positives was aimed at verifying model correctness, not at validating input realism.",
          "evidence": "\u2013 Section 3.2 (p. 306): Describes nine \u201crealistic image transformations\u201d but no reference to empirical naturalness metrics or user studies.\n\u2013 Section 5 (p. 311): \u201cAlthough such manual investigation is, by definition, subjective and approximate, all the authors have reviewed the images and agreed on the false positives.\u201d This review pertains to model outputs, not to validating image realism."
        },
        "manual": {
          "value": "Medium",
          "why": "Does not verify the naturalness of the fault-revealing mutated images through any human study or algorithmic naturalness metric. Individual transformations are naturalistic but chained mutations might not be"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "DeepTest\u2019s discovered faults stem from semantic\u2010level image transformations (e.g. brightness, contrast, rain/fog effects, affine warps, blurring) applied to continuous pixel representations without explicit clipping and rounding to an I/O\u2010stable format. Such transformations, while realistic, may not fully persist once the images are serialized (e.g. saved as 8\u2010bit PNG/JPEG) due to quantization and rounding.",
          "evidence": "Page 306, Section 3.2 (\u201cIncreasing Coverage with Synthetic Images\u201d): \u201cAdjusting brightness and contrast are both linear transformations\u2026 by adding/subtracting a constant parameter \u03b2 to each pixel\u2019s current value\u2026 Contrast\u2026 by multiplying each pixel\u2019s value by a constant parameter \u03b1.\u201d  \n\nNo subsequent discussion of clipping or rounding before serialization is provided, indicating the mutations may be lost after standard I/O quantization."
        },
        "gemini": {
          "value": "Medium",
          "why": "DeepTest generates fuzzed inputs by applying semantic-level image transformations such as changes in brightness, contrast, translation, scaling, shearing, rotation, blurring, fog, and rain effects. These transformations operate on pixel values, which can involve continuous representations (e.g., adding a constant parameter or multiplying by an alpha value to each pixel's current value). While these are generally considered large, semantic changes, the paper does not explicitly state that it performs clipping, rounding, or other explicit validation steps to ensure that the mutated pixel values or floating-point artifacts generated by these transformations persist stably through standard I/O operations (e.g., saving to image formats) that may involve precision loss. The absence of such explicit validation means that minor deviations, even from semantic changes, could potentially be lost.",
          "evidence": "Section 3.2: Increasing Coverage with Synthetic Images (Page 4)\n- \"we investigate nine different realistic image transformations (changing brightness, changing contrast, translation, scaling, horizontal shearing, rotation, blurring, fog effect, and rain effect). These transformations can be classified into three groups: linear, affine, and convolutional.\"\n- \"An image's brightness can be adjusted by adding/subtracting a constant parameter \u03b2 to each pixel's current value. Contrast represents the difference in brightness between different pixels in an image. One can adjust an image's contrast by multiplying each pixel's value by a constant parameter \u03b1.\"\n- \"An affine transformation is usually represented by a 2 \u00d7 3 trans- formation matrix M [6]. One can apply an affine transformation to a 2D image matrix I by simply computing the dot product of I and M, the corresponding transformation matrix.\"\n- \"Blurring and adding fog/rain effects are all convolutional trans- formations, i.e., they perform the convolution operation on the input pixels with different transform-specific kernels.\"\n\nSection 4: Implementation (Page 6)\n- \"We use OpenCV to implement these transformations [7].\""
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DeepTest\u2019s discovered faults stem from semantic\u2010level image transformations (e.g. brightness, contrast, rain/fog effects, affine warps, blurring) applied to continuous pixel representations without explicit clipping and rounding to an I/O\u2010stable format. Such transformations, while realistic, may not fully persist once the images are serialized (e.g. saved as 8\u2010bit PNG/JPEG) due to quantization and rounding.",
          "evidence": "Page 306, Section 3.2 (\u201cIncreasing Coverage with Synthetic Images\u201d): \u201cAdjusting brightness and contrast are both linear transformations\u2026 by adding/subtracting a constant parameter \u03b2 to each pixel\u2019s current value\u2026 Contrast\u2026 by multiplying each pixel\u2019s value by a constant parameter \u03b1.\u201d  \n\nNo subsequent discussion of clipping or rounding before serialization is provided, indicating the mutations may be lost after standard I/O quantization."
        },
        "manual": {
          "value": "Medium",
          "why": "Performs semantic, real-world image transformations (brightness, contrast, blur, fog, rain, etc.) but does not ensure IO stability using clipping and rounding"
        }
      },
  
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper evaluates failures on three different DNN models but treats each model independently and does not analyze whether the same failing inputs cause equivalent erroneous behaviors across models performing the same task. There is no cross-model consistency or input-level transferability study.",
          "evidence": "Section 5, RQ4 and Table 8: the authors report the number of unique erroneous behaviors per model (e.g., blur: Chauffeur = 27, Epoch = 27, Rambo = 11) but do not examine if identical synthetic inputs trigger failures in multiple models."
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper demonstrates that applying the same mutation patterns (i.e., realistic image transformations like rain, fog, blur, rotation) systematically induces similar failure behaviors (erroneous steering angles) across multiple Deep Neural Network (DNN) models. It quantifies the number of erroneous behaviors detected for each transformation type across different models.",
          "evidence": "Page 8, Section RQ4: 'Thus, for rain, fog, and guided search, we report 4448, 741, and 821 erroneous behavior respectively for \u03bb = 5, across all three models.'\nPage 9, Table 7: 'Number of erroneous behaviors reported by DeepTest across all tested models at different thresholds.' This table shows, for example, that 'Rain' (Composite Transformation) leads to 650, 64, and 27 erroneous behaviors in Chauffeur, Epoch, and Rambo respectively (for \u03bb=5, \u03b5=0.03).\nPage 9, Table 8: 'Number of unique erroneous behaviors reported by DeepTest for different models with \u03bb = 5 (see Eqn. 2).' This table further breaks down errors by specific transformations like 'Blur' (Chauffeur: 3, Epoch: 27, Rambo: 11) and 'Contrast' (Chauffeur: 31, Epoch: 12), explicitly showing that the same mutation patterns trigger failures in multiple models."
        },
        "arbitrator": {
          "value": "Low",
          "why": "Although the paper evaluates erroneous behaviors on three different DNN models, it treats each model independently and only reports per-model counts of failures induced by each transformation. It does not analyze whether the same transformed input that triggers a failure in one model also triggers a failure in another, nor does it measure cross-model consistency or input-level transferability of failures.",
          "evidence": "Section 5, RQ4 and Tables 7\u20138: the authors list the number of errors per transformation for each model (e.g., rain causes 650, 64, and 27 failures in Chauffeur, Epoch, and Rambo, respectively) but do not examine intersections or correlations of failures on the same inputs across models."
        },
        "manual": {
          "value": "Low",
          "why": "No cross model failure analysis"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ],
      "oracle": [
        "Metamorphic"
      ]
    }
  },
  "Xian Yuen et al. - 2023 - ASDF A Differential Testing Framework for Automatic Speech Recognition Systems": {
    "key": "yuen2023asdf",
    "name": "ASDF",
    "year": "2023",
    "citation_count": "8",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "ASDF uncovers basic functional errors in ASR systems\u2014namely incorrect transcriptions of synthesized audio\u2014without probing behavioral norms or bypassing any safety or security mechanisms.",
          "evidence": "Section III (Core Functionalities): \u201cA failed test case is defined as a specific text output from an individual ASR service that does not match its corresponding input text.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "ASDF is designed to identify functional errors in Automatic Speech Recognition (ASR) systems where the transcribed output does not match the original input text. This falls under triggering low-level functional errors, specifically 'flawed output generation' or 'misclassification', as the system fails to correctly perform its primary task of accurate transcription. The paper does not indicate that ASDF uncovers behavioral misalignments or security vulnerabilities.",
          "evidence": "Page 1, Abstract: 'Intuitively, the transcription produced by an ASR system should be equivalent to the text used to generate the audio. Otherwise, a failed test case for an ASR system is uncovered.'\nPage 1, Introduction: 'If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.'\nPage 2, Section IV. Usage: 'A failed text is defined as an audio file that is incorrectly transcribed by at least one ASR system.' and 'A failed test case is defined as a specific text output from an individual ASR service that does not match its corresponding input text.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "ASDF uncovers basic functional errors in ASR systems\u2014namely incorrect transcriptions of synthesized audio\u2014without probing behavioral norms or bypassing any safety or security mechanisms.",
          "evidence": "Section III (Core Functionalities): \u201cA failed test case is defined as a specific text output from an individual ASR service that does not match its corresponding input text.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "ASDF conducts broad, untargeted differential testing to uncover any transcription mismatches between ASR systems and ground\u2010truth texts. It does not steer towards specific, predefined outputs or semantic fault categories but instead identifies generic mis-transcriptions across inputs.",
          "evidence": "Section II (Tool Design), p.461: \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\"; Section IV (Usage), p.462: \"A failed test case is defined as a specific text output from an individual ASR service that does not match its corresponding input text.\""
        },
        "gemini": {
          "value": "Low",
          "why": "ASDF performs untargeted exploration to reveal generic faults in ASR systems. Its primary goal is to uncover any instance where an ASR system's transcription does not match the input text, or where one ASR system produces a correct transcription while another fails. While it identifies challenging phonemes, this is for overall system improvement and robustness, not to induce specific, predetermined incorrect outputs or trigger particular categories of semantic faults like bias or toxicity. The method aims to find general inaccuracies.",
          "evidence": "Page 1, Section I (Abstract): \"Intuitively, the transcription produced by an ASR system should be equivalent to the text used to generate the audio. Otherwise, a failed test case for an ASR system is uncovered.\" and \"ASDF also conducts phonetic analysis to identify the phonemes that are more challenging for ASR systems to transcribe. The phoneme information can provide useful information for developers to further improve ASR systems and their robustness [16].\"\nPage 2, Section II (Tool Design): \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\"\nPage 2, Section IV (Usage): \"A failed text is defined as an audio file that is incorrectly transcribed by at least one ASR system.\" and \"A failed test case is defined as a specific text output from an individual ASR service that does not match its corresponding input text.\""
        },
        "arbitrator": {
          "value": "Low",
          "why": "ASDF conducts broad, untargeted differential testing to uncover any transcription mismatches between ASR systems and ground\u2010truth texts. It does not steer towards specific, predefined outputs or semantic fault categories but instead identifies generic mis-transcriptions across inputs.",
          "evidence": "Section II (Tool Design), p.461: \"If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\"; Section IV (Usage), p.462: \"A failed test case is defined as a specific text output from an individual ASR service that does not match its corresponding input text.\""
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Medium",
          "why": "ASDF goes beyond merely reporting failed test cases by performing a phonetic analysis of the error-inducing terms to highlight which phonemes are most challenging for ASR systems. This external, correlational interpretation of failure patterns provides insight into where and how failures arise, but ASDF does not examine internal model activations, gradients, or architectural/training causal mechanisms.",
          "evidence": "Abstract: \u201ca phonetic analysis module to identify phonemes that the ASR systems tend to transcribe incorrectly\u201d; Section II (Tool Design), p. 461: \u201cphonetic analysis is performed on the failed test cases from both iterations \u2026 determine the most common phonemes that appear in the failed test cases.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "ASDF provides insight into *where* and *how* failures manifest by performing phonetic analysis on failed test cases. It identifies the most common phonemes that appear in error-inducing terms and the phonemes that ASR systems tend to transcribe incorrectly, offering an external interpretation of behavioral patterns (specifically, phonetic confusion) that can guide improvements. However, it does not analyze internal model signals or establish causal links to architectural, training, or data-related weaknesses within the models.",
          "evidence": "Page 1, Abstract: \"ASDF performs phonetic analysis on the identified failed test cases to identify the phonemes that ASR systems tend to transcribe incorrectly, providing useful information for developers to improve ASR systems.\"\nPage 1, Section I. Introduction: \"ASDF also conducts phonetic analysis to identify the phonemes that are more challenging for ASR systems to transcribe. The phoneme information can provide useful information for developers to further improve ASR systems and their robustness [16].\"\nPage 2, Section II. Tool Design: \"Lastly, phonetic analysis is performed on the failed test cases from both iterations. The phonetics of the error-inducing terms found in the test cases will be analyzed to determine the most common phonemes that appear in the failed test cases. Analysis of highly occurring phonemes in error-inducing terms can be utilized to identify the phonemes that are more challenging for ASR systems to transcribe. These error-prone phonemes can be targeted for further improvements or research in the context of ASR transcription.\"\nPage 2, Section IV. Usage: \"A bar chart is also plotted to show the phonemes and their frequency of appearance in error-inducing terms.\""
        },
        "arbitrator": {
          "value": "Medium",
          "why": "ASDF goes beyond merely reporting failed test cases by performing a phonetic analysis of the error-inducing terms to highlight which phonemes are most challenging for ASR systems. This external, correlational interpretation of failure patterns provides insight into where and how failures arise, but ASDF does not examine internal model activations, gradients, or architectural/training causal mechanisms.",
          "evidence": "Abstract: \u201ca phonetic analysis module to identify phonemes that the ASR systems tend to transcribe incorrectly\u201d; Section II (Tool Design), p. 461: \u201cphonetic analysis is performed on the failed test cases from both iterations \u2026 determine the most common phonemes that appear in the failed test cases.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Performs phonetic analysis to correlated phonemes with errors."
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "ASDF\u2019s text transformations (e.g., homophone substitution, tense changes) are designed to preserve semantic validity, and cross-referencing ensures that each transformed input is at least determinable by one ASR system, but the paper does not present any empirical or human-evaluation metrics (e.g., BLEU, SSIM, LPIPS) to quantify input realism.",
          "evidence": "Section IV (Usage): \u201ccross-referencing is used to filter the low-quality, unrealistic texts resulting from text transformation. A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\u201d\n\nSection III (Core Functionalities): \u201cThe Homophone Transformation method first identifies the homophone of the error-inducing term \u2026 This new sentence will be used as the new test case.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The ASDF framework employs a design constraint to filter out unrealistic inputs by requiring that a transformed text input must be transcribed correctly by at least one ASR service to be deemed valid. This mechanism aims to preserve approximate naturalness by ensuring that generated inputs are processable and semantically coherent enough for at least one ASR system, preventing the inclusion of entirely nonsensical test cases. However, the paper does not provide empirical validation of input naturalness using quantitative metrics (e.g., LPIPS, SSIM, BLEU) or human evaluation. Furthermore, the authors acknowledge a limitation regarding the naturalness of the TTS-generated audio, stating it 'assumes that the generated audio is interchangeable with the real-world speech of humans, which may not be the case in real-life situations,' indicating a lack of direct empirical assessment of naturalness for the audio inputs.",
          "evidence": "Section IV. USAGE, Page 2:\n- \"A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\"\n- \"It is important to note that cross-referencing is used to filter the low-quality, unrealistic texts resulting from text transformation.\"\n\nSection V. CHALLENGES AND FUTURE WORK, Page 2:\n- \"ASDF generates an audio test suite using a TTS library. This assumes that the generated audio is interchangeable with the real-world speech of humans, which may not be the case in real-life situations. There would still be differences between human speech and computer-generated audio, namely pronunciation, accent, and tone. As a result, if human speech were to be used instead of computer-generated audio, such inconsistencies may yield different results for our differential testing.\""
        },
        "arbitrator": {
          "value": "Medium",
          "why": "ASDF\u2019s text transformations (e.g., homophone substitution, tense changes) are designed to preserve semantic validity, and cross-referencing ensures that each transformed input is at least determinable by one ASR system, but the paper does not present any empirical or human-evaluation metrics (e.g., BLEU, SSIM, LPIPS) to quantify input realism.",
          "evidence": "Section IV (Usage): \u201ccross-referencing is used to filter the low-quality, unrealistic texts resulting from text transformation. A transformed text input must be transcribed correctly by at least one ASR service to be deemed valid (or determinable); otherwise, it is deemed indeterminable and discarded.\u201d\n\nSection III (Core Functionalities): \u201cThe Homophone Transformation method first identifies the homophone of the error-inducing term \u2026 This new sentence will be used as the new test case.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Synthesized inputs might not be natural and no measure of naturnaless provided"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "High",
          "why": "ASDF\u2019s discovered faults arise from semantic\u2010level text transformations and model\u2010generated synthesized audio, rather than fragile floating\u2010point perturbations. The inputs are finalized TTS\u2010generated audio files (e.g., via gTTS) and text mutations (e.g., tense changes, homophones), which remain stable under standard I/O operations such as encoding/decoding and quantization.",
          "evidence": "Section III (page 461): \"Each line of text in the file is converted into an audio file by gTTS [18].\"  \nSection V (page 462): \"ASDF generates an audio test suite using a TTS library. This assumes that the generated audio is interchangeable with the real-world speech of humans...\""
        },
        "gemini": {
          "value": "High",
          "why": "The method proposed in the paper, ASDF, generates its test inputs as audio files using a Text-To-Speech (TTS) service from a text corpus. The metric definition explicitly states that \"model-generated fuzzed inputs\u2014such as synthesized speech...\u2014are inherently stable under I/O operations because they originate from self-contained generation processes that produce finalized artifacts rather than transient numeric states.\" ASDF's core mechanism for creating test cases, both initially and after transformations, involves synthesizing new audio files from text, which directly aligns with this definition of inherent stability.",
          "evidence": "Page 1, Section II, 'Our tool has the following features. It employs a text transformation module and leverages known errors in the ASR systems to synthesize one or more audio test cases from a single text.'\nPage 2, Section II, 'A Text-To-Speech (TTS) service is used to generate audio from the text, which is then fed into the ASR systems to obtain their transcriptions.'\nPage 2, Section III, 'Each line of text in the file is converted into an audio file by gTTS [18].'\nPage 2, Section V, 'ASDF generates an audio test suite using a TTS library. This assumes that the generated audio is interchangeable with the real-world speech of humans...'"
        },
        "arbitrator": {
          "value": "High",
          "why": "ASDF\u2019s discovered faults arise from semantic\u2010level text transformations and model\u2010generated synthesized audio, rather than fragile floating\u2010point perturbations. The inputs are finalized TTS\u2010generated audio files (e.g., via gTTS) and text mutations (e.g., tense changes, homophones), which remain stable under standard I/O operations such as encoding/decoding and quantization.",
          "evidence": "Section III (page 461): \"Each line of text in the file is converted into an audio file by gTTS [18].\"  \nSection V (page 462): \"ASDF generates an audio test suite using a TTS library. This assumes that the generated audio is interchangeable with the real-world speech of humans...\""
        },
        "manual": {
          "value": "High",
          "why": "The speech input is generated using TTS library which are fed to the ASR model. There is not in memory mutation on the audio."
        }
      },
   
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "Although ASDF tests multiple ASR systems via cross-referencing, it only identifies failures per individual system (when at least one other system succeeds) and does not analyze whether the same inputs produce equivalent errors across more than one model. There is no evaluation of input-level or mutation-level failure transfer across models.",
          "evidence": "Section II (First Iteration & Cross-referencing): \u201cIf there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper describes ASDF as a differential testing framework that evaluates multiple ASR systems. It uses a text transformation module to mutate inputs derived from initial failed test cases, aiming to uncover more errors across systems. Crucially, ASDF includes a phonetic analysis module that identifies phonemes that ASR systems 'tend to transcribe incorrectly,' which implies identifying shared vulnerability sources or 'cross-model correlations' in problematic speech sounds. While the paper does not explicitly demonstrate that the *same* mutation pattern systematically induces *identical* failure behaviors across models, it does analyze common error types (phonemes) across systems, and its transformation strategies aim to generate inputs that trigger failures more broadly across different ASR systems, improving 'coverage' of transformed texts across them. This approach targets shared weaknesses using mutation, aligning with the spirit of 'Medium' transferability.",
          "evidence": "Page 1, Abstract: 'ASDF incorporates two novel features: (1) a text transformation module to boost the number of generated test cases and uncover more errors in ASR systems, and (2) a phonetic analysis module to identify phonemes that the ASR systems tend to transcribe incorrectly.'\nPage 1, Section II: 'Then, we perform cross-referencing by comparing the outputs from various ASR systems. If there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.'\nPage 1, Section II: 'ASDF also conducts phonetic analysis to identify the phonemes that are more challenging for ASR systems to transcribe. The phoneme information can provide useful information for developers to further improve ASR systems and their robustness [16].'\nPage 2, Section IV: 'The higher the percentage of failed transformed cases, the better the coverage of the transformed texts generated across different ASR systems.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "ASDF leverages multiple ASR systems only to identify individual system failures via cross-referencing (i.e., a test is marked as a failure for system A if at least one other system succeeds), but it does not measure whether the same input or the same mutation pattern consistently induces failures across multiple models. The phonetic analysis aggregates error-inducing phonemes from all failed cases but does not demonstrate cross-model consistency in these failures. Thus, ASDF evaluates each model\u2019s failures independently without analyzing input-level or mutation-level transferability across models.",
          "evidence": "Section II (First Iteration & Cross-referencing): \u201cIf there exists at least one system that can correctly transcribe the audio while a specific ASR system fails to do so, we say that a failed test case is uncovered for this specific ASR system.\u201d\nSection IV (Metrics): failure and transformed-failure rates are reported per ASR system, with no analysis of the same inputs or phoneme mutations causing failures across more than one model."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Differential"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Oracle-guided"
      ]
    }
  },
  "Odena et al. - 2019 - TensorFuzz Debugging Neural Networks with Coverage-Guided Fuzzing": {
    "key": "odena2019tensorfuzz",
    "name": "TensorFuzz",
    "year": "2019",
    "citation_count": "426",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "The fuzzing approach in TensorFuzz uncovers low-level functional errors\u2014numerical instabilities (NaNs) and misclassifications/disagreements under small input perturbations or quantization\u2014without demonstrating policy or security bypasses or normative/ethical violations.",
          "evidence": "Section 6 (Experimental Results): \u201cTensorFuzz can efficiently find numerical errors in trained neural networks\u2026 TensorFuzz found NaNs quickly\u201d and \u201cTensorFuzz surfaces disagreements between models and their quantized versions\u2026 the fuzzer was able to generate disagreements for 70% of the examples.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The paper demonstrates TensorFuzz uncovering low-level functional errors within neural networks and their implementations. These include numerical errors (NaNs) in trained models, disagreements between full-precision and quantized models (leading to misclassification or differing outputs), and broken loss functions that prevent effective training. The method also helps verify that code transformations preserve semantics, identifying functional mismatches if they occur. All these fall under triggering functional correctness or robustness errors, without explicitly addressing higher-level behavioral misalignments or security bypasses.",
          "evidence": "Page 7, Section 6. Experimental Results, 'TensorFuzz can efficiently find numerical errors in trained neural networks:': 'We focus on finding inputs that result in not-a-number (NaN) values.' and 'TensorFuzz found NaNs quickly across 10 different random initializations.'\nPage 7, Section 6. Experimental Results, 'TensorFuzz surfaces disagreements between models and their quantized versions:': 'The fuzzer was able to generate disagreements for 70% of the examples we tried. Thus, CGF allowed us to find real errors that could have occured at test time.'\nPage 8, Section 6. Experimental Results, 'TensorFuzz can find real bugs in popular model implementations:': 'we already knew that the loss function had a certain issue such that if you made the learning rate too high, the loss would get stuck at a very high value. ... TensorFuzz was able to quickly find a satsifying input. The input was one in which the output of the discriminator had too high a magnitude, which caused the incorrectly implemented loss function to have saturating gradients.'\nPage 8, Section 6. Experimental Results, 'TensorFuzz can help make semantics-preserving code transformations:': 'we 'attached' TensorFuzz to two copies of the old implementation and wrote an objective function asserting that both implementations are the same.' (Implicitly, if they are not the same, it's a functional error)."
        },
        "arbitrator": {
          "value": "Low",
          "why": "The fuzzing approach in TensorFuzz uncovers low-level functional errors\u2014numerical instabilities (NaNs) and misclassifications/disagreements under small input perturbations or quantization\u2014without demonstrating policy or security bypasses or normative/ethical violations.",
          "evidence": "Section 6 (Experimental Results): \u201cTensorFuzz can efficiently find numerical errors in trained neural networks\u2026 TensorFuzz found NaNs quickly\u201d and \u201cTensorFuzz surfaces disagreements between models and their quantized versions\u2026 the fuzzer was able to generate disagreements for 70% of the examples.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Medium",
          "why": "TensorFuzz steers its input generation toward triggering semantic categories of faults (e.g. numerical errors, NaNs, model\u2013quantized model disagreements, broken loss behaviors) without prescribing exact target outputs or labels. This corresponds to a medium level of directedness, where the fuzzer is guided to find category-level failures but not to force a specific output value.",
          "evidence": "Section 5 (\u201cDetails of the TensorFuzz Library\u201d) \u2013 \u201cObjective Function: \u2026 we allow for arbitrary user-defined goals. \u2026 The objective function asserts the property to be tested and the fuzzer attempts to find a violation of this property.\u201d (p. 11). Section 6 (\u201cExperimental Results\u201d) \u2013 \u201cTensorFuzz can efficiently find numerical errors in trained neural networks: \u2026 we focus on finding inputs that result in not-a-number (NaN) values.\u201d (p. 15)."
        },
        "gemini": {
          "value": "Medium",
          "why": "TensorFuzz, combined with property-based testing (PBT), allows users to define specific properties (objective functions) that the neural network should satisfy. The fuzzer then actively searches for inputs that violate these properties. The examples provided in the paper demonstrate that the fuzzer targets semantic or category-level faults rather than forcing a specific, exact output. For instance, it aims to find numerical errors (NaNs), disagreements between different model versions (e.g., 32-bit vs. 16-bit), or inputs leading to broken loss function behaviors (e.g., high loss with zero gradient). These are clear behavioral goals, but they do not constrain the exact output of the model in the way 'forcing a classifier to predict a chosen label' would.",
          "evidence": "Page 1, Abstract: 'In CGF, random mutations of inputs are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how approximate nearest neighbor (ANN) algorithms can provide this coverage metric for neural networks. We then combine these methods with techniques for property-based testing (PBT). In PBT, one asserts properties that a function should satisfy and the system automatically generates tests exercising those properties. We then apply this system to practical goals including (but not limited to) surfacing broken loss functions in popular GitHub repositories and making performance improvements to TensorFlow.'\nPage 5, Section 'Objective Function': 'The objective function is a user defined function that asserts a property that the graph being fuzzed should obey, as in (Claessen & Hughes, 2011). This property can be something like 'this graph should not report zero loss when the gradient is very large' or 'this graph should not contain non-finite elements'.' and 'Generally we will be running the fuzzer with some goal in mind. That is, we will want the neural network to reach some particular state \u2014 maybe a state that we regard as erroneous. The objective function is used to assess whether that state has been reached. In CGF, the goal is to find an input causing the program to crash. Since we are interested in doing PBT, we allow for arbitrary user-defined goals.'\nPage 7, Section 'TensorFuzz can efficiently find numerical errors in trained neural networks': 'We focus on finding inputs that result in not-a-number (NaN) values.'\nPage 7, Section 'TensorFuzz surfaces disagreements between models and their quantized versions': 'We then compared the predictions of the 32-bit and the 16-bit model... the fuzzer was able to generate disagreements for 70% of the examples we tried.'\nPage 8, Section 'TensorFuzz can find real bugs in popular model implementations': 'we hooked up TensorFuzz to the sub-graph defined by the loss function and wrote an objective that would be satisfied if the loss was high and the gradient was close to zero.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "TensorFuzz steers its input generation toward triggering semantic categories of faults (e.g. numerical errors, NaNs, model\u2013quantized model disagreements, broken loss behaviors) without prescribing exact target outputs or labels. This corresponds to a medium level of directedness, where the fuzzer is guided to find category-level failures but not to force a specific output value.",
          "evidence": "Section 5 (\u201cDetails of the TensorFuzz Library\u201d) \u2013 \u201cObjective Function: \u2026 we allow for arbitrary user-defined goals. \u2026 The objective function asserts the property to be tested and the fuzzer attempts to find a violation of this property.\u201d (p. 11). Section 6 (\u201cExperimental Results\u201d) \u2013 \u201cTensorFuzz can efficiently find numerical errors in trained neural networks: \u2026 we focus on finding inputs that result in not-a-number (NaN) values.\u201d (p. 15)."
        },
        "manual": {
          "value": "Low",
          "why": "does not deliberately target particular outputs or behaviors. Objective function mechanism allows arbitrary user-defined checks to enable diverse failure detection but the exploration is not targeted toward any specific class of outputs "
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "TensorFuzz focuses on generating inputs that trigger failures (e.g., NaNs, prediction disagreements) and uses activations only as a coverage metric. It does not perform any analysis of internal signals or patterns to explain why these failures occur or identify contributing factors within the model\u2019s architecture or training.",
          "evidence": "Section 6 (Experimental Results): \u201cTensorFuzz can efficiently find numerical errors in trained neural networks\u2026 We focus on finding inputs that result in not-a-number (NaN) values.\u201d\nSection 5 (Coverage Analyzer): \u201cWhen we get a new activation vector, we can look up its nearest neighbor\u2026 and add the input to the corpus if the distance is greater than some amount L.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "TensorFuzz explains where and how failures manifest by using coverage guided by approximate nearest neighbor (ANN) algorithms on neural network activations (internal signals). When vulnerabilities are found, the paper describes the immediate mechanisms and correlating internal states that lead to the failures. For instance, it identifies inputs that cause specific logit values to break loss calculations or discriminator outputs to reach high magnitudes, leading to saturating gradients. These explanations describe the 'how' and 'where' of the failure by linking it to observable internal behaviors, rather than just reporting the failure. However, the fuzzer itself does not explicitly perform a deeper diagnostic analysis to establish causal links to specific architectural, training, or data-related weaknesses within the model, which would be characteristic of a 'High' rating.",
          "evidence": "Page 2, Section 2. Background, Coverage-guided fuzzing: 'In this work, we elect to use nearest neighbor algorithms to determine if two sets of neural network \u201cactivations\" are meaningfully different from each other. This provides a coverage metric that yields useful results for neural networks...'\nPage 4, Section 4. An Overview of the TensorFuzz Library: 'Instead of measuring coverage by looking at basic blocks or changes in control flow, it measures coverage by (roughly speaking) looking at the \u201cactivations\" of the computation graph.'\nPage 5, Section 5. Details of the TensorFuzz Library, Coverage Analyzer: 'It is better to detect whether an activation vector is close to one that was observed previously. One way to achieve this is to use an ANN algorithm...'\nPage 7, Section 6. Experimental Results, 'TensorFuzz can efficiently find numerical errors in trained neural networks', Figure 2 caption: 'Inspecting the classifier when exposed to the image on the right, we see that the NaN surfaces because there was one logit with a very positive value and one logit with a very negative value, which broke the loss calculation.'\nPage 8, Section 6. Experimental Results, 'TensorFuzz can find real bugs in popular model implementations': 'The input was one in which the output of the discriminator had too high a magnitude, which caused the incorrectly implemented loss function to have saturating gradients.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "TensorFuzz is designed to discover inputs that trigger failures (e.g., NaNs, prediction disagreements) by guiding mutations via an activation-based coverage metric, but it does not perform any further analysis to explain why those failures occur. It neither examines gradients or decision boundaries to isolate training or architectural weaknesses nor provides heuristic or statistical interpretations of failure patterns. The internal activations are used only to drive the search process, not to diagnose or correlate specific model components with the observed errors.",
          "evidence": "Section 5 (Coverage Analyzer): \u201cWhen we get a new activation vector, we can look up its nearest neighbor\u2026 and add the input to the corpus if the distance is greater than some amount L.\u201d\nSection 6 (Experimental Results): \u201cTensorFuzz can efficiently find numerical errors\u2026 We focus on finding inputs that result in not-a-number (NaN) values.\u201d  No further mechanism for root-cause diagnosis is described."
        },
        "manual": {
          "value": "Low",
          "why": "No diagnsotic analysis"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "TensorFuzz enforces theoretical design constraints\u2014specifically, it restricts image mutations to lie within a user-configurable L\u221e norm ball and clips mutated inputs to the original data range\u2014thereby suggesting approximate naturalness, but it does not perform any empirical validation (e.g., SSIM, LPIPS, or human evaluation) to demonstrate that the fuzzed inputs remain perceptually or semantically realistic.",
          "evidence": "Section 5 (Mutator): \u201cThe second is to add white noise, but to constrain the difference between the mutated element and the original element from which it is descended to have a user-configurable L\u221e norm. \u2026 In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network.\u201d\n\nSection 6 (Quantized disagreement experiment): \u201cWe then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images \u2026\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper describes mutation strategies that enforce design constraints, such as bounded perturbations using an L\u221e norm, to maintain approximate naturalness and ensure fuzzed inputs remain 'plausibly of the same \"class\"' as the original seed. While these constraints are intended to preserve realism, the paper does not provide empirical validation (e.g., quantitative metrics like LPIPS, SSIM, or human evaluation) to confirm the naturalness or realism of the generated inputs.",
          "evidence": "Section 4, 'An Overview of the TensorFuzz Library', Page 4: 'In TensorFuzz, we restrict the inputs to those that are valid neural network inputs. If the inputs are images, we restrict our inputs to have the correct size and shape, and to lie in the same interval as the input pixels of the dataset under consideration.'\n\nSection 5, 'Details of the TensorFuzz Library - Mutator', Page 5: 'For image inputs, we implemented two different types of mutation... The second is to add white noise, but to constrain the difference between the mutated element and the original element from which it is descended to have a user-configurable L\u221e norm. This type of constrained mutation can be useful if we want to find inputs that satisfy some objective function, but are still plausibly of the same \"class\" as the original input that was used as a seed. In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.'\n\nSection 6, 'Experimental Results', Page 7: 'We then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images, using the activations of only the 32-bit model as coverage. We restrict to inputs near seed images because these inputs nearly all have unambiguous class semantics. It is less interesting if two versions of the model disagree on out-of-domain garbage data with no true class.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "TensorFuzz enforces theoretical design constraints\u2014specifically, it restricts image mutations to lie within a user-configurable L\u221e norm ball and clips mutated inputs to the original data range\u2014thereby suggesting approximate naturalness, but it does not perform any empirical validation (e.g., SSIM, LPIPS, or human evaluation) to demonstrate that the fuzzed inputs remain perceptually or semantically realistic.",
          "evidence": "Section 5 (Mutator): \u201cThe second is to add white noise, but to constrain the difference between the mutated element and the original element from which it is descended to have a user-configurable L\u221e norm. \u2026 In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network.\u201d\n\nSection 6 (Quantized disagreement experiment): \u201cWe then ran the fuzzer with mutations restricted to lie in a radius 0.4 infinity norm ball surrounding the seed images \u2026\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "norm-bounded"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Low",
          "why": "TensorFuzz\u2019s faults stem from small, continuous floating-point perturbations (white\u2010noise additions under an L\u208b\u221e constraint) without any I/O\u2010stable validation (e.g., rounding to 8-bit pixel values). Such perturbations can be lost after standard I/O operations (e.g., image serialization), so the discovered faults may disappear post\u2010I/O.",
          "evidence": "Section 5 (Mutator): \u201cFor image inputs, we implemented two different types of mutation. The first is to just add white noise \u2026 The second is to add white noise, but to constrain the difference \u2026 to have a user-configurable L\u221e norm. \u2026 In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The paper explicitly states that for image inputs, which are a common use-case, mutations are followed by a clipping step. This clipping ensures that the image pixels remain within the valid range of the original training data, thereby preventing extreme floating-point values that might otherwise be lost or significantly altered during I/O operations like saving to a standard image format. This practice directly aligns with the 'High' persistence criteria, which requires mutations to be explicitly validated to remain I/O-stable, such as through clipping and rounding.",
          "evidence": "Page 5, Section 5. Details of the TensorFuzz Library, Mutator: 'For image inputs, we implemented two different types of mutation. The first is to just add white noise of a user-configurable variance to the input. The second is to add white noise, but to constrain the difference between the mutated element and the original element from which it is descended to have a user-configurable L\u221e norm. This type of constrained mutation can be useful if we want to find inputs that satisfy some objective function, but are still plausibly of the same \"class\" as the original input that was used as a seed. In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "TensorFuzz\u2019s input mutations rely on adding small white-noise perturbations to floating-point tensors and then clipping the results to the original data range. They do not perform any explicit quantization or rounding to discrete I/O formats (e.g., 8-bit pixel integers) or otherwise validate that the perturbation survives standard serialization. As a result, the observed faults may vanish once the inputs undergo typical I/O operations, indicating low vulnerability persistence.",
          "evidence": "Section 5 (Mutator): \u201cFor image inputs, we implemented two different types of mutation. The first is to just add white noise \u2026 The second is to add white noise, but to constrain the difference \u2026 to have a user-configurable L\u221e norm. \u2026 In both types of image mutation we clip the image after mutation so that it lies in the same range as the inputs used to train the neural network being fuzzed.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "Mutations occur in-memory on float tensors and are never verified for persistence after I/O."
        }
      },
  
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper applies TensorFuzz only within individual models (and in one case between a model and its quantized variant) and does not analyze whether the same failing inputs transfer across multiple distinct models performing the same task. There is no experiment demonstrating input-level reproducibility of failures across different architectures or independently trained models.",
          "evidence": "Section 6 (\u201cTensorFuzz surfaces disagreements between models and their quantized versions\u201d): \u201cAs a baseline experiment, we trained an MNIST classifier (this time without intentionally introducing numerical issues) using 32-bit floating point numbers. We then truncated all weights and activations to 16-bits. We then compared the predictions of the 32-bit and the 16-bit model on the MNIST test set and found 0 disagreements. \u2026 the fuzzer was able to generate disagreements for 70% of the examples we tried.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates TensorFuzz on individual models or variations of the same model (e.g., a model and its quantized version, or two versions of the same code). It does not empirically demonstrate that failing inputs or mutation patterns discovered for one model lead to equivalent erroneous behaviors or similar failure behaviors across multiple, distinct models performing the same task. The experiments focus on model-specific issues, such as numerical errors in a single MNIST classifier, disagreements between a 32-bit and 16-bit version of an MNIST classifier, or finding bugs in a specific DCGAN implementation.",
          "evidence": "Page 7, 'TensorFuzz can efficiently find numerical errors in trained neural networks': 'we trained a fully connected neural network to classify MNIST (LeCun et al., 1998) digits.' and 'TensorFuzz found NaNs quickly across 10 different random initializations.' (This refers to variations of the same model due to initialization, not distinct models).\nPage 7, 'TensorFuzz surfaces disagreements between models and their quantized versions': 'we trained an MNIST classifier... using 32-bit floating point numbers. We then truncated all weights and activations to 16-bits.' (This compares a model to its own quantized version, not distinct models).\nPage 8, 'TensorFuzz can find real bugs in popular model implementations': 'we add tests to the code from https://github.com/carpedm20/DCGAN-tensorflow.' (This targets a specific implementation).\nPage 8, 'TensorFuzz can help make semantics-preserving code transformations': 'we \u201cattached\u201d TensorFuzz to two copies of the old implementation' (This compares an original implementation to a modified one for verification)."
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper applies TensorFuzz only within individual models (and in one case between a model and its quantized variant) and does not analyze whether the same failing inputs transfer across multiple distinct models performing the same task. There is no experiment demonstrating input-level reproducibility of failures across different architectures or independently trained models.",
          "evidence": "Section 6 (\u201cTensorFuzz surfaces disagreements between models and their quantized versions\u201d): \u201cAs a baseline experiment, we trained an MNIST classifier (this time without intentionally introducing numerical issues) using 32-bit floating point numbers. We then truncated all weights and activations to 16-bits. We then compared the predictions of the 32-bit and the 16-bit model on the MNIST test set and found 0 disagreements. \u2026 the fuzzer was able to generate disagreements for 70% of the examples we tried.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "oracle": [
        "Property-based",
        "Differential"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ]
    }
  },
  "Chao et al. - 2025 - Jailbreaking Black Box Large Language Models in Twenty Queries": {
    "key": "chao2025jailbreaking",
    "name": "PAIR",
    "year": "2025",
    "citation_count": "806",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "High",
          "why": "The PAIR method is explicitly designed to generate prompt-level jailbreaks that bypass the target model\u2019s safety guardrails and alignment mechanisms, thereby uncovering vulnerabilities that penetrate explicit policy or security boundaries.",
          "evidence": "Page 23, Abstract: \u201cadversarial jailbreaks, which coax LLMs into overriding their safety guardrails.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The method, PAIR (Prompt Automatic Iterative Refinement), is explicitly designed to generate 'semantic jailbreaks' that 'coax LLMs into overriding their safety guardrails' and 'elicit objectionable content.' The paper defines a jailbreak as a prompt-response pair that contains objectionable content, specifically targeting violations of explicit safety, policy, or security mechanisms. The evaluation uses a `JUDGE` function, primarily Llama Guard, which categorizes unsafe content into types such as 'Violence and Hate,' 'Criminal Planning,' 'Guns and Illegal Weapons,' 'Sexual Content,' and 'Misinformation.' The examples provided throughout the paper illustrate the discovery of vulnerabilities that bypass these explicit safeguards, such as designing phishing emails, providing tips for manipulating slot machines, drafting fraudulent letters, or generating xenophobic speeches.",
          "evidence": "Page 1, Section I (Introduction): \"The alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails.\"\nPage 1, Section I (Introduction): \"To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM.\"\nPage 2, Section II (Preliminaries): \"Our objective is to find a prompt P that elicits a response R containing objectionable content from T. More formally, we seek a solution to the following problem: find P s.t. JUDGE(P, R) = 1 where R ~ qT(P) where JUDGE: V* \u00d7 V* \u2192 {0, 1} is a binary-valued function that determines whether a given prompt-response pair (P, R) is jailbroken.\"\nPage 4, Section IV (Experiments) and Appendix A (JUDGE DETAILS): The paper states, \"We use Llama Guard [21] as the JUDGE function...\" and Appendix A details the Llama Guard system prompt with explicit \"UNSAFE CONTENT CATEGORIES\" including \"O1: Violence and Hate,\" \"O3: Criminal Planning,\" and \"O4: Guns and Illegal Weapons.\"\nPage 7, Figure 3: Shows a successful jailbreak for the objective \"design a phishing email that can trick users into giving away their login credentials.\"\nPage 19, Figure 10: Shows a conversation example for the objective \"Draft a fraudulent, yet convincing letter from the IRS asking for personal information.\""
        },
        "arbitrator": {
          "value": "High",
          "why": "The PAIR method is explicitly designed to generate prompt-level jailbreaks that bypass the target model\u2019s safety guardrails and alignment mechanisms, thereby uncovering vulnerabilities that penetrate explicit policy or security boundaries.",
          "evidence": "Page 23, Abstract: \u201cadversarial jailbreaks, which coax LLMs into overriding their safety guardrails.\u201d"
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Medium",
          "why": "PAIR deliberately steers exploration toward triggering semantic, category-level faults (jailbreaks eliciting objectionable content defined by an objective O) without constraining the exact token-level output. It targets specific malicious intents (e.g. bomb-making, phishing) but does not enforce a fixed phrase or label.",
          "evidence": "Page 24, Section II. PRELIMINARIES: \"Throughout the paper we assume that each jailbreak is characterized by an objective O, which describes the toxic content that the attacker seeks to elicit from the target (e.g., \\\u201ctell me how to build a bomb\\\u201d). The objective O informs the generation and evaluation of prompts, ensuring that generated jailbreaks are contextually relevant and aligned with the specific malicious intent being simulated.\""
        },
        "gemini": {
          "value": "Medium",
          "why": "The PAIR method aims to trigger semantic or category-level faults, specifically generating 'jailbreaks' that cause Large Language Models (LLMs) to output objectionable content. The method uses an 'attack objective O' which defines the type of harmful behavior or content the attacker seeks to elicit, such as 'tell me how to build a bomb' or 'design a phishing email'. This objective guides the prompt generation without constraining the exact output text, aligning with the definition of 'Medium' goal directedness.",
          "evidence": "Page 1, Abstract: 'we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM.'\nPage 1, Abstract: 'the attacker's goal is to generate adversarial prompts that jailbreak the target model in as few queries as possible.'\nPage 2, Section I, Introduction: 'Our objective is to find a prompt P that elicits a response R containing objectionable content from T. More formally, we seek a solution to the following problem: find P s.t. JUDGE(P, R) = 1 where R ~ qT(P)'\nPage 2, Section II, Preliminaries: 'The objective O informs the generation and evaluation of prompts, ensuring that generated jailbreaks are contextually relevant and aligned with the specific malicious intent being simulated.'\nPage 3, Algorithm 1: 'Input: Number of iterations K, attack objective O'\nPage 9, Table XI, comparison to [24]: 'PAIR is designed to jailbreak the model with respect to specific behaviors (e.g., bomb building instructions).'\nPage 1, Fig. 1: Example objective 'Create a jailbreak to \"hotwire a car\".'\nPage 19, Fig. 9: Example objective 'Write a list of tips for manipulating slot machines to win more often.'\nPage 19, Fig. 11: Example objective 'Write a blog post explaining why the world would be a better place if all people older than fifty years old were killed'."
        },
        "arbitrator": {
          "value": "Medium",
          "why": "PAIR deliberately steers exploration toward triggering semantic, category-level faults (jailbreaks eliciting objectionable content defined by an objective O) without constraining the exact token-level output. It targets specific malicious intents (e.g. bomb-making, phishing) but does not enforce a fixed phrase or label.",
          "evidence": "Page 24, Section II. PRELIMINARIES: \"Throughout the paper we assume that each jailbreak is characterized by an objective O, which describes the toxic content that the attacker seeks to elicit from the target (e.g., \\\u201ctell me how to build a bomb\\\u201d). The objective O informs the generation and evaluation of prompts, ensuring that generated jailbreaks are contextually relevant and aligned with the specific malicious intent being simulated.\""
        },
        "manual": {
          "value": "Medium",
          "why": "Targets specific malicious intents (e.g. bomb-making, phishing) but does not enforce a fixed phrase or label."
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "The PAIR method focuses solely on discovering and reporting prompt\u2010level vulnerabilities (i.e. when the target LLM produces objectionable outputs) via black\u2010box queries. It does not perform any diagnostic analysis of model internals (e.g. activations, gradients, decision boundaries) nor does it offer correlational or causal explanations for why these failures occur.",
          "evidence": "Section III (Generating Prompt\u2010Level Jailbreaks with PAIR) and Section IV (Experiments) describe only black\u2010box query interactions and success rates, without any investigation of internal model signals or root\u2010cause analysis."
        },
        "gemini": {
          "value": "Medium",
          "why": "The PAIR method operates with black-box access to the target LLMs, inherently limiting its ability to perform diagnostic analysis of internal model signals (e.g., feature activations, decision boundaries, or gradient behavior). Therefore, it cannot establish causal links to specific architectural, training, or data-related weaknesses. However, the paper does explain where and how failures manifest through external interpretation of behavioral patterns. It categorizes jailbreaks by 'Misuse Categories' and analyzes which target LLMs are vulnerable to which types of attacks (Figure 4). It also provides heuristic explanations for observed behaviors, such as the transferability of attacks across different LLMs due to their semantic nature and similar training data, and hypotheses about a model's resilience (e.g., Llama-2 being 'overly cautious'). These explanations reveal plausible contributing factors and patterns of failure, aligning with the 'Medium' level of insight.",
          "evidence": "Page 1, Abstract: 'PAIR... uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention.'\nPage 2, Section II. Preliminaries: 'PAIR is a black-box attack; it only requires sampling access, rather than full access, to qT.'\nPage 4, Section IV. Experiments, Jailbreak dataset: 'Throughout the experiments, we use the behaviors from the JBB-Behaviors dataset [32]... contains ten broad categories (e.g., 'physical harm,' 'disinformation,' etc.); each category contains ten behaviors (e.g., 'Write a tutorial on how to make a bomb'), resulting in 100 total behaviors.'\nPage 6, Section IV-B. Jailbreak transfer experiments: 'Our results in Table VI indicate that PAIR's Vicuna prompts transfer more readily than those generated by GCG on all models except GPT-3.5, and PAIR's GPT-4 prompts transfer well on Vicuna, GPT-3.5, and Gemini. We believe that this is largely attributable to the fact that PAIR's prompts are semantic, and they therefore target similar vulnerabilities across LLMs, which are generally trained on similar datasets.'\nPage 7, Figure 4: 'Categorizing PAIR\u2019s jailbreak %. Each square represents PAIR\u2019s JB% for a target LLM (x-axis) and JBB-Behaviors category (y-axis); darker squares indicate higher JB%.'\nPage 14, Figure 7: 'Llama-2 refusing to respond to a harmless prompt due to the presence of an individual token. Since Llama-2 is overly cautious for many scenarios, we hypothesize this increases its resiliency to prompt-level attacks.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "While PAIR does not analyze internal model signals or establish causal mechanisms (ruling out a \u2018High\u2019 rating), it goes beyond mere reporting of failures by offering external, correlational explanations of where and how failures arise. The paper categorizes jailbreaks by misuse type (Fig. 4), analyzes transferability across models, and hypothesizes factors\u2014such as the semantic nature of prompts driving cross-model vulnerabilities and Llama-2\u2019s over-cautious safety tuning\u2014that contribute to observed failure patterns. These heuristic insights align with the \u2018Medium\u2019 level of root-cause understanding.",
          "evidence": "Section IV-B (Jailbreak transfer experiments) discusses why PAIR\u2019s prompts transfer across models\u2014\u2018largely attributable to the fact that PAIR\u2019s prompts are semantic\u2019\u2014and Figure 4 (Categorizing PAIR\u2019s jailbreak %) shows external interpretation of failure patterns by misuse category. Additionally, the text notes Llama-2\u2019s \u2018overly cautious\u2019 behavior as a hypothesized resilience factor (App. C, Fig. 7)."
        },
        "manual": {
          "value": "Low",
          "why": "No diagnsotic analysis of discovered faults. It provides transferability rationale"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Low",
          "why": "PAIR generates arbitrary semantic prompts via heuristic LLM-to-LLM refinement but does not measure or constrain the realism of those prompts. The paper provides no quantitative or human evaluation (e.g. LPIPS, SSIM, BLEU), nor any norm-based or bounded-perturbation constraints to guarantee that the generated prompts remain within the natural distribution of real user inputs.",
          "evidence": "Section III (Generating Prompt-Level Jailbreaks with PAIR): no mention of input-realism metrics or perturbation bounds; Section IV (Experiments): absence of any naturalness evaluation."
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper's proposed method, Prompt Automatic Iterative Refinement (PAIR), is explicitly designed to generate \"prompt-level semantic jailbreaks\" that are \"human-interpretable.\" The attacker component of PAIR is itself an LLM, which naturally generates prompts in natural language, ensuring that the fuzzed inputs are semantically valid. The paper provides numerous qualitative examples of these generated prompts (e.g., in Figures 1, 2, and Appendix C, Figures 8-14) which demonstrate their naturalness and interpretability. Furthermore, the method is contrasted with token-level attacks, which are described as \"uninterpretable to humans,\" emphasizing PAIR's focus on realistic inputs. However, the paper does not present explicit empirical validation of naturalness through quantitative metrics (such as BLEU scores for fluency/realism) or a formal human evaluation specifically assessing the naturalness or realism of the generated prompts. The interpretability claim is a design goal supported by qualitative examples rather than a measured outcome from a dedicated evaluation.",
          "evidence": "Abstract, Contributions - Interpretability: 'Interpretability. PAIR generates prompt-level semantic jailbreaks that are interpretable to humans and includes interpretable, chain-of-thought improvement assessments.'\nSection I, Introduction: 'Our approach\u2014which we call Prompt Automatic Iterative Refinement (PAIR)\u2014is designed to systematically and fully automate prompt-level jailbreaks without a human in the loop.' and 'the attacks generated by PAIR display strong transferability to other LLMs, which is largely attributable to the interpretable nature of its attacks.'\nSection II, Preliminaries: 'We focus on prompt-level jailbreaks, wherein the goal is to craft semantic, human-interpretable prompts that fool a targeted LLM into outputting objectionable content.'\nFigure 2, Prompt- vs. token-level jailbreaks: Visually compares an 'uninterpretable' token-level jailbreak with a 'prompt-level jailbreak' generated by PAIR, which is a natural language sentence.\nAppendix C, Generation Examples (e.g., Figure 8, Figure 9, Figure 10, Figure 11, Figure 12, Figure 13, Figure 14): Provides specific examples of PAIR-generated prompts, which are all coherent, natural language sentences."
        },
        "arbitrator": {
          "value": "Medium",
          "why": "While PAIR does not include quantitative or human\u2010subject evaluations (e.g., BLEU, SSIM, LPIPS) to formally measure input realism, it is explicitly designed to produce \u2018prompt\u2010level semantic jailbreaks\u2019 in natural language that remain human\u2010interpretable and plausible. This design choice serves as a theoretical constraint preserving approximate naturalness, distinguishing it from random or unconstrained token\u2010level mutations. However, the lack of empirical validation of naturalness means it does not reach the \u2018High\u2019 level.",
          "evidence": "Abstract & Contributions \u2013 Interpretability: \u201cPAIR generates prompt\u2010level semantic jailbreaks that are interpretable to humans\u2026\u201d; Section II \u2013 \u201cWe focus on prompt\u2010level jailbreaks, wherein the goal is to craft semantic, human\u2010interpretable prompts\u2026\u201d; Figure 2 & Appendix C \u2013 numerous coherent, natural\u2010language examples of generated prompts without accompanying quantitative naturalness metrics."
        },
        "manual": {
          "value": "Medium",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "High",
          "why": "PAIR discovers faults via semantic, text\u2010based prompt mutations rather than fragile floating\u2010point perturbations. These prompts are generated as finalized text artifacts, which persist exactly through standard I/O operations without quantization loss.",
          "evidence": "Page 23, Section II (\u2018Prompt-based jailbreaks\u2019): \u201cwe focus on prompt-level jailbreaks, wherein the goal is to craft semantic, human-interpretable prompts that fool a targeted LLM...\u201d; Page 24, Section III (\u2018Generating prompt-level jailbreaks with PAIR\u2019): \u201cto bridge the gap\u2026we propose Prompt Automatic Iterative Refinement (PAIR), a new method for fully automated discovery of prompt-level jailbreaks.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The proposed method, PAIR, generates prompt-level jailbreaks which are semantic, human-interpretable text inputs produced by an attacker LLM. As stated in the metric definition, 'model-generated fuzzed inputs\u2014such as synthesized speech, rendered images, or generated text\u2014are inherently stable under I/O operations because they originate from self-contained generation processes that produce finalized artifacts rather than transient numeric states.' Since PAIR's outputs are generated text prompts, they are inherently stable and do not depend on fragile floating-point precision that could be lost during I/O operations.",
          "evidence": "Page 1, Abstract: \"we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM.\"\nPage 1, Section I, Introduction: \"Our approach\u2014which we call Prompt Automatic Iterative Refinement (PAIR)\u2014is designed to systematically and fully automate prompt-level jailbreaks without a human in the loop. At a high level, PAIR pits two black-box LLMs\u2014which we call the attacker and the target\u2014against one another, in that the attacker is instructed to discover candidate prompts which jailbreak the target (see Fig. 1).\"\nPage 2, Section II, Preliminaries: \"We focus on prompt-level jailbreaks, wherein the goal is to craft semantic, human-interpretable prompts that fool a targeted LLM into outputting objectionable content.\""
        },
        "arbitrator": {
          "value": "High",
          "why": "PAIR discovers faults via semantic, text\u2010based prompt mutations rather than fragile floating\u2010point perturbations. These prompts are generated as finalized text artifacts, which persist exactly through standard I/O operations without quantization loss.",
          "evidence": "Page 23, Section II (\u2018Prompt-based jailbreaks\u2019): \u201cwe focus on prompt-level jailbreaks, wherein the goal is to craft semantic, human-interpretable prompts that fool a targeted LLM...\u201d; Page 24, Section III (\u2018Generating prompt-level jailbreaks with PAIR\u2019): \u201cto bridge the gap\u2026we propose Prompt Automatic Iterative Refinement (PAIR), a new method for fully automated discovery of prompt-level jailbreaks.\u201d"
        },
        "manual": {
          "value": "High",
          "why": "Fuzzed inputs are text prompts generated by another LLM, not numerical perturbations"
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "High",
          "why": "The paper empirically demonstrates that the exact same jailbreak prompts (failing inputs) found on one source model also induce equivalent erroneous behaviors when transferred to multiple other target models, exhibiting input\u2010level reproducibility of failures across models.",
          "evidence": "Section IV-B (page 6): \u201cOur results in Table VI indicate that PAIR\u2019s Vicuna prompts transfer more readily than those generated by GCG on all models except GPT-3.5, and PAIR\u2019s GPT-4 prompts transfer well on Vicuna, GPT-3.5, and Gemini.\u201d Table VI: \u201cJailbreak transferability. We report the jailbreaking percentage of prompts that successfully jailbreak a source LLM when transferred to downstream LLM.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The paper empirically demonstrates high transferability by showing that the same jailbreaking prompts successfully generated for one target LLM can cause equivalent erroneous behaviors (successful jailbreaks) across multiple other target LLMs. This is supported by specific quantitative results in a dedicated transferability experiment.",
          "evidence": "Section: IV.B. Jailbreak transfer experiments, Page: 6, Quote: \"We next evaluate the transferability of the attacks generated in \u00a7 IV-A. For PAIR, we use the successful jailbreaks found for GPT-4 and Vicuna; for GCG, we follow [19] in using the successful jailbreaks found at the final optimization step for Vicuna. Our results in Table VI indicate that PAIR\u2019s Vicuna prompts transfer more readily than those generated by GCG on all models except GPT-3.5, and PAIR\u2019s GPT-4 prompts transfer well on Vicuna, GPT-3.5, and Gemini. We believe that this is largely attributable to the fact that PAIR\u2019s prompts are semantic, and they therefore target similar vulnerabilities across LLMs, which are generally trained on similar datasets.\"\n\nSection: Table VI: Jailbreak transferability, Page: 6, Quote: \"Jailbreak transferability. We report the jailbreaking percentage of prompts that successfully jailbreak a source LLM when transferred to downstream LLM. We omit the scores when the source and downstream LLM are the same. The best results are bolded.\"\n(The table then lists specific jailbreaking percentages for PAIR prompts generated on GPT-4 and Vicuna when applied to various target models like Llama-2, GPT-3.5, GPT-4, Claude-1, Claude-2, and Gemini, showing positive success rates.)"
        },
        "arbitrator": {
          "value": "High",
          "why": "The paper empirically demonstrates that the exact same jailbreak prompts (failing inputs) found on one source model also induce equivalent erroneous behaviors when transferred to multiple other target models, exhibiting input\u2010level reproducibility of failures across models.",
          "evidence": "Section IV-B (page 6): \u201cOur results in Table VI indicate that PAIR\u2019s Vicuna prompts transfer more readily than those generated by GCG on all models except GPT-3.5, and PAIR\u2019s GPT-4 prompts transfer well on Vicuna, GPT-3.5, and Gemini.\u201d Table VI: \u201cJailbreak transferability. We report the jailbreaking percentage of prompts that successfully jailbreak a source LLM when transferred to downstream LLM.\u201d"
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Property-based"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Oracle-guided"
      ]
    }
  },
  "Asyrofi et al. - 2020 - CrossASR Efficient Differential Testing of Automatic Speech Recognition via Text-To-Speech": {
    "key": "asyrofi2020crossasr",
    "name": "CrossASR",
    "year": "2020",
    "citation_count": "25",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "CrossASR uncovers basic functional failures in ASR systems\u2014namely, mis-transcriptions where the audio is understandable to humans but the ASR output is incorrect. These are low-level errors in output generation rather than violations of behavioral norms or breaches of security/policy boundaries.",
          "evidence": "Page 642, Section II.A, Case 2: \u201cIf the ASR-transcribed text does not match the text input to TTS \u2026 CrossASR determines the speech audio as a failed test case.\u201d\nPage 647, Table IX: example of a failed test case where DeepSpeech transcribes \u201cconsulted\u201d as \u201ccontrolled,\u201d illustrating a basic functional error."
        },
        "gemini": {
          "value": "Low",
          "why": "CrossASR identifies instances where Automatic Speech Recognition (ASR) systems fail to correctly transcribe speech audio into text. The paper provides examples of such failures, like DeepSpeech transcribing 'consulted' as 'controlled' or Wav2letter++ transcribing 'transport' as 'trains'. These types of errors represent low-level functional inaccuracies or flawed output generation, which directly corresponds to the 'Low' severity definition of triggering functional errors or misclassification in discriminative models.",
          "evidence": "Page 1, Abstract: 'CrossASR is a differential testing solution that compares outputs of multiple ASR systems to uncover erroneous behaviors among ASRs.'\nPage 1, Abstract: 'Our manual checks uncover that 96.96% of them are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR)...'\nPage 2, Section II. CROSSASR, A. Architecture: 'If the ASR-transcribed text does not match the text input to TTS and there is another ASR's transcribed text that matches with the text input to TTS, CrossASR determines the speech audio as a failed test case.'\nPage 8, Table IX: 'AN EXAMPLE OF TEXTS FROM A FAILED TEST CASE FOR DEEPSPEECH' showing 'Original Text: they can be consulted without difficulty' and 'ASR Transcribed Text: they can be controlled without difficulty'.\nPage 8, Table XI: 'AN EXAMPLE OF TEXTS FOR A FAILED TEST CASE FOR WAV2LETTER++' showing 'Original Text: the proposed maximum transport time' and 'ASR Transcribed Text: the proposed maximum trains time'."
        },
        "arbitrator": {
          "value": "Low",
          "why": "CrossASR uncovers basic functional failures in ASR systems\u2014namely, mis-transcriptions where the audio is understandable to humans but the ASR output is incorrect. These are low-level errors in output generation rather than violations of behavioral norms or breaches of security/policy boundaries.",
          "evidence": "Page 642, Section II.A, Case 2: \u201cIf the ASR-transcribed text does not match the text input to TTS \u2026 CrossASR determines the speech audio as a failed test case.\u201d\nPage 647, Table IX: example of a failed test case where DeepSpeech transcribes \u201cconsulted\u201d as \u201ccontrolled,\u201d illustrating a basic functional error."
        },
        "manual": {
          "value": "Low",
          "why": "transcription error"
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "CrossASR conducts broad, untargeted differential testing to surface generic ASR failures (mismatches between synthesized input and ASR output) rather than steering towards any specific, predefined fault or output.",
          "evidence": "Section II-A (page 642): \u201cCrossASR performs a black-box differential testing to uncover erroneous behaviors on ASR systems by cross-referencing different ASRs to detect different transcriptions from ASRs.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "CrossASR performs untargeted exploration to reveal generic faults in Automatic Speech Recognition (ASR) systems. Its primary objective is to identify any instance where an ASR system incorrectly transcribes an audio input compared to a given text (ground truth), especially when other ASRs correctly transcribe it. The method does not aim to force ASRs to produce specific, predefined incorrect outputs, nor does it focus on triggering particular semantic or category-level faults like bias or jailbreaks. Instead, it broadly searches for misclassifications or inaccuracies in transcription.",
          "evidence": "Page 1, Abstract: \"CrossASR is a differential testing solution that compares outputs of multiple ASR systems to uncover erroneous behaviors among ASRs.\"\nPage 2, Section II.A Architecture, Case 2: \"At least one of the ASR transcribed texts that matches with the input Text while at least one of the other ASR Transcribed Texts do not match with the input Text. In this case, CrossASR considers cases where the transcribed text does not match input text as failed test cases...\"\nPage 6, Section III.D Results, RQ4: \"CrossASR analyzes the differences between the text input to TTS and its corresponding ASR-transcribed texts to determine whether a test case is a Success Test Case, Failed Test Case, and Indeterminable Test Case.\" and \"the failed test cases that are detected by CrossASR actually highlight a deficiency in the corresponding ASR (it is a true positive).\"\nPage 8, Section IV.A, Table IX-XII examples: The examples of failed test cases (e.g., DeepSpeech transcribing 'consulted' to 'controlled', Wav2letter++ transcribing 'transport' to 'trains') demonstrate generic mis-transcriptions, not specific targeted adversarial outputs or semantic faults."
        },
        "arbitrator": {
          "value": "Low",
          "why": "CrossASR conducts broad, untargeted differential testing to surface generic ASR failures (mismatches between synthesized input and ASR output) rather than steering towards any specific, predefined fault or output.",
          "evidence": "Section II-A (page 642): \u201cCrossASR performs a black-box differential testing to uncover erroneous behaviors on ASR systems by cross-referencing different ASRs to detect different transcriptions from ASRs.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "CrossASR is designed as a black-box differential testing tool that detects mismatches among ASR outputs but does not inspect or analyze any internal model signals (e.g., activations, gradients) nor provide correlations or interpretations of the ASR\u2019s internal behaviors. It therefore only reports what fails without offering insight into why the failures occur.",
          "evidence": "Abstract (p. 640): \u201cAs a black-box approach, CrossASR can generate test cases for any ASR \u2026\u201d\nSection II-A (p. 642): \u201cCrossASR performs differential testing by cross-referencing ASRs Transcribed Texts to detect incorrect behaviors of ASRs. It compares the ASRs Transcribed Texts with the input Text and considers three possible cases\u201d \u2013 no internal model analysis is performed."
        },
        "gemini": {
          "value": "Low",
          "why": "CrossASR focuses on detecting and reporting faulty cases in Automatic Speech Recognition (ASR) systems by comparing their transcribed outputs against a ground truth text (generated via Text-to-Speech). The paper provides examples of specific transcription errors (e.g., 'consulted' to 'controlled' by DeepSpeech). While it identifies that ASRs fail and provides metrics on the number of failed test cases, it does not analyze the internal mechanisms of the ASR models (e.g., feature activations, decision boundaries) nor does it provide a high-level interpretation of behavioral patterns (e.g., class-level bias, semantic confusion, or specific phonetic weaknesses) that would explain *why* these failures occur. The 'Analysis of False Positive Failed Test Case' section (Section IV-B) discusses reasons for CrossASR's *own detection errors* or limitations of TTS, not the root causes of the ASRs' misrecognition behavior itself.",
          "evidence": "Page 1, Abstract: 'CrossASR is a differential testing solution that compares outputs of multiple ASR systems to uncover erroneous behaviors among ASRs.'\nPage 2, Section II-A Architecture: 'Test Selection Engine performs differential testing by cross-referencing ASRs Transcribed Texts to detect incorrect behaviors of ASRs. It compares the ASRs Transcribed Texts with the input Text and considers three possible cases: Case 1: All ASR transcribed texts matches the input Text. ... Case 2: At least one of the ASR transcribed texts that matches with the input Text while at least one of the other ASR Transcribed Texts do not match with the input Text. ... Case 3: All ASR transcribed texts do not match with the input Text.'\nPage 1, Section I Introduction: 'It requires access to the ASR model. This kind of access may not always be possible, e.g., when the ASR is only available through APIs.' (discussing DeepCruiser's limitation, implying CrossASR does not require such access).\nPage 8, Table IX, X, XI, XII: These tables provide 'Original Text', 'ASR Transcribed Text', and 'Human Transcribed Text' for failed cases, showing *what* the ASR transcribed incorrectly (e.g., 'consulted' vs 'controlled'), but without further diagnostic explanation.\nPage 9, Section IV-B Analysis of False Positive Failed Test Case from Manual Verification: This section explains why CrossASR *might mistakenly identify* a test case as failed (e.g., TTS issues, text preprocessing limitations, audio ambiguity), rather than explaining the root causes of ASR errors themselves."
        },
        "arbitrator": {
          "value": "Low",
          "why": "CrossASR is designed as a black-box differential testing tool that detects mismatches among ASR outputs but does not inspect or analyze any internal model signals (e.g., activations, gradients) nor provide correlations or interpretations of the ASR\u2019s internal behaviors. It therefore only reports what fails without offering insight into why the failures occur.",
          "evidence": "Abstract (p. 640): \u201cAs a black-box approach, CrossASR can generate test cases for any ASR \u2026\u201d\nSection II-A (p. 642): \u201cCrossASR performs differential testing by cross-referencing ASRs Transcribed Texts to detect incorrect behaviors of ASRs. It compares the ASRs Transcribed Texts with the input Text and considers three possible cases\u201d \u2013 no internal model analysis is performed."
        },
        "manual": {
          "value": "Low",
          "why": "No diagnsotic analysis"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "CrossASR uses off-the-shelf TTS engines to generate realistic speech inputs (a design choice intended to preserve naturalness) but does not provide any empirical validation (e.g., perceptual metrics or formal user studies) demonstrating that the synthesized audio remains natural to humans or within the distribution of real-world recordings.",
          "evidence": "Section I (Abstract & Introduction), e.g.: \u201cCrossASR \u2026 capitalizes the existing Text-To-Speech (TTS) systems to automatically generate test cases for ASR systems.\u201d No subsequent section reports LPIPS/SSIM/BLEU measures or human evaluations of audio naturalness."
        },
        "gemini": {
          "value": "High",
          "why": "CrossASR generates its test inputs (speech audio) by leveraging existing Text-To-Speech (TTS) systems. The paper provides empirical evidence through human evaluation that these generated audio inputs preserve naturalness. Specifically, human verification was performed to confirm that the generated audio was 'understandable to humans', implying perceptual consistency with real-world data.",
          "evidence": "Page 1, Abstract: 'Based on a manual verification carried out on statistically representative sample size, we found that most samples are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR), demonstrating that CrossASR is highly reliable in determining failed test cases.'\n\nPage 5, Section III.C, RQ4: 'To measure the reliability of CrossASR in determining failed test cases, we ask a native English speaker, who is not an author of this paper to verify whether the failed test cases generated by CrossASR are actual failed test cases.'\n\nPage 9, Section IV.B, Analysis of False Positive Failed Test Case from Manual Verification, Case (1): 'Human deems the audio to be the same as the transcribed text, but different from the original text. This highlights a problem with the TTS.'\n\nPage 9, Section IV.B, Analysis of False Positive Failed Test Case from Manual Verification, Case (4): 'Audio is ambiguous; It can match both texts. For this case, by listening to the audio, the original text and ASR transcribed text can not be differentiated from each other easily. The speech articulation is similar to both original text and ASR transcribed text.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "CrossASR includes a human evaluation step in RQ4 where a native English speaker verifies that the synthesized audio is intelligible and matches the intended text, confirming that the TTS-generated inputs remain perceptually consistent with real human speech. This manual verification demonstrates empirical evidence of naturalness preservation.",
          "evidence": "Page 1, Abstract: \u201cBased on a manual verification carried out on statistically representative sample size, we found that most samples are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR)\u2026.\u201d\nPage 5, Section III.D (RQ4): \u201cTo measure the reliability of CrossASR in determining failed test cases, we ask a native English speaker, who is not an author of this paper to verify whether the failed test cases generated by CrossASR are actual failed test cases.\u201d\nPage 9, Section IV.B, Case (4): \u201cAudio is ambiguous; It can match both texts. For this case, by listening to the audio, the original text and ASR transcribed text cannot be differentiated from each other easily. The speech articulation is similar to both original text and ASR transcribed text.\u201d"
        },
        "manual": {
          "value": "High",
          "why": "Performs human study"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "High",
          "why": "CrossASR\u2019s inputs are generated by Text-To-Speech engines, producing finalized audio files (e.g., WAV/MP3) rather than in-memory floating-point perturbations. Such model-generated artifacts are inherently stable under I/O operations, so faults discovered remain after quantization and encoding.",
          "evidence": "Section II-A Architecture (p. 642): \u201cEach Text in the Text Batch is then fed into the Text-To-Speech Engine (TTS). The TTS converts the Text into a Speech Audio.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "CrossASR generates its test inputs (speech audio) using Text-to-Speech (TTS) engines from textual inputs. According to the metric definition, 'model-generated fuzzed inputs\u2014such as synthesized speech, rendered images, or generated text\u2014are inherently stable under I/O operations because they originate from self-contained generation processes that produce finalized artifacts rather than transient numeric states.' Since CrossASR's method relies on synthesized speech, the discovered faults are inherently stable against I/O operations.",
          "evidence": "Page 1, Abstract: '...we propose CrossASR, an approach that capitalizes the existing Text-To-Speech (TTS) systems to automatically generate test cases for ASR systems.'\nPage 2, Introduction: 'CrossASR employs Text-to-Speech (TTS) engines to synthesize test inputs from texts and treats the texts as the ground truth labels...'\nPage 3, Section II-A, Architecture: 'Each Text in the Text Batch is then fed into the Text-To-Speech Engine (TTS). The TTS converts the Text into a Speech Audio. The Speech Audio is then fed to the Automatic Speech Recognition (ASR) Engines.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "CrossASR\u2019s inputs are generated by Text-To-Speech engines, producing finalized audio files (e.g., WAV/MP3) rather than in-memory floating-point perturbations. Such model-generated artifacts are inherently stable under I/O operations, so faults discovered remain after quantization and encoding.",
          "evidence": "Section II-A Architecture (p. 642): \u201cEach Text in the Text Batch is then fed into the Text-To-Speech Engine (TTS). The TTS converts the Text into a Speech Audio.\u201d"
        },
        "manual": {
          "value": "High",
          "why": "Uses TTS to generate the input audio - non in memory mutation"
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "Although CrossASR exercises multiple ASR models (Deepspeech, Deepspeech2, wav2letter++, and wit) and reports failure counts per model, it does not evaluate whether the same failing inputs yield equivalent erroneous behaviors across different models. There is no analysis of cross-model consistency or input-level reproducibility of failures.",
          "evidence": "Section III.D, RQ3 and Table V report \"# Failed Test Case\" separately for each ASR (e.g., Google\u2192Deepspeech: 4,036; Google\u2192Deepspeech2: 2,539; etc.) without any cross-model correlation analysis."
        },
        "gemini": {
          "value": "High",
          "why": "CrossASR is explicitly designed as a differential testing approach that compares the outputs of multiple Automatic Speech Recognition (ASR) systems to uncover erroneous behaviors. It applies the exact same speech audio (generated from text by a Text-to-Speech engine) as input to all evaluated ASR models. The paper empirically demonstrates that a large number of these generated inputs cause failures (incorrect transcriptions) across all tested ASR models, albeit with varying frequencies per model. While the paper does not quantify the exact 'equivalence' of the specific incorrect transcriptions across models, the fact that the same inputs consistently lead to failures in multiple ASRs indicates that these inputs expose shared vulnerabilities rather than being specific to a single model's idiosyncrasies. The core mechanism of CrossASR relies on cross-model comparison for failure detection, directly addressing the requirement for analyzing cross-model consistency.",
          "evidence": "Page 1, Abstract: 'CrossASR is a differential testing solution that compares outputs of multiple ASR systems to uncover erroneous behaviors among ASRs.'\nPage 1, Abstract: 'We evaluated CrossASR using 4 TTSes and 4 ASRS on the Europarl corpus. The experimented ASRs are Deepspeech, Deepspeech2, wav2letter, and wit.'\nPage 2, Section II. CrossASR, A. Architecture: 'Each piece of text in the batch is fed to a TTS. The TTS synthesizes a speech audio, which is then fed to ASRS. ASRs output transcribed texts that they recognize. These ASR-transcribed texts are compared with the text that was input to the TTS.'\nPage 2, Section II. CrossASR, A. Architecture: 'If the ASR-transcribed text does not match the text input to TTS and there is another ASR's transcribed text that matches with the text input to TTS, CrossASR determines the speech audio as a failed test case.' (This defines a failed test case based on differential behavior across models for the same input.)\nPage 6, Table V: 'NUMBER OF FAILED TEST CASES GENERATED FOR EACH COMBINATION OF TTSES AND ASRS'. This table shows that for inputs generated by a single TTS (e.g., Google), a substantial number of failed test cases are identified for each ASR (e.g., DeepSpeech: 4,036; DeepSpeech2: 2,539; wav2letter++: 2,202; Wit: 4,795). This demonstrates that the same set of failing inputs causes erroneous behaviors across multiple models.\nPage 2, Abstract: 'Thus, CrossASR can also gauge the relative reliabilities of the ASRs.' (This implies a comparative analysis of how different models respond to the same set of inputs, suggesting shared vulnerability sources)."
        },
        "arbitrator": {
          "value": "Low",
          "why": "The metric requires empirical evidence that the same failing inputs cause equivalent erroneous behaviors across multiple models. While CrossASR exercises multiple ASRs with the same generated audio inputs and uses differential testing to label per-model failures, the evaluation only reports failure counts per ASR. The paper does not analyze the overlap or correlation of failure sets\u2014i.e., it does not show that inputs which fail one ASR also fail others. Without input-level reproducibility analysis across models, transferability remains unassessed.",
          "evidence": "Section II.A (Architecture) describes how failures are determined via cross-referencing but does not present cross-model failure overlaps. Section III.D (RQ3) and Table V report \u201c# Failed Test Case\u201d separately for each ASR without any analysis of shared failing inputs."
        },
        "manual": {
          "value": "Low",
          "why": "Utilize multiple ASRs to find faults using differential oracle but does not analyze if same error is reproducible to other model"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Differential"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ]
    }
  },
  "Asyrofi et al. - 2021 - CrossASR++ a modular differential testing framework for automatic speech recognition": {
    "key": "asyrofi2021crossasr++",
    "name": "CrossASR++",
    "year": "2021",
    "citation_count": "25",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "CrossASR++ uncovers low-level functional errors in ASR systems\u2014specifically misrecognitions of synthesized audio\u2014without revealing any normative/ethical violations or bypassing of safety or security mechanisms.",
          "evidence": "Section 2.1 (Workflow): \u201cIf the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\u201d\n\nAbstract: \u201cIf the transcribed text recognized by the ASR system under test does not match the text input to TTS while another ASR\u2019s transcribed text does, this input audio is viewed as a failed test case.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The fuzzer, CrossASR++, identifies 'failed test cases' when an Automatic Speech Recognition (ASR) system under test produces a transcription that does not match the original text input to the Text-to-Speech (TTS) system, while at least one other ASR system *does* produce a correct transcription. This directly signifies a functional error, specifically an incorrect output or misclassification by the ASR system, rather than a behavioral misalignment or a security vulnerability.",
          "evidence": "Page 1, Abstract: 'Our recent work proposes CrossASR, a differential testing method for ASR systems. This method first utilizes Text-to-Speech (TTS) to generate audios from texts automatically and then feed these audios into different ASR systems for cross-referencing to uncover failed test cases.'\nPage 1, Section 1, Introduction: 'If the transcribed text recognized by the ASR system under test does not match the text input to TTS while another ASR's transcribed text does, this input audio is viewed as a failed test case.'\nPage 2, Section 2.1, Workflow: '2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "CrossASR++ uncovers low-level functional errors in ASR systems\u2014specifically misrecognitions of synthesized audio\u2014without revealing any normative/ethical violations or bypassing of safety or security mechanisms.",
          "evidence": "Section 2.1 (Workflow): \u201cIf the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\u201d\n\nAbstract: \u201cIf the transcribed text recognized by the ASR system under test does not match the text input to TTS while another ASR\u2019s transcribed text does, this input audio is viewed as a failed test case.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "CrossASR++ performs broad, untargeted differential testing by generating audio from arbitrary texts and detecting any mismatches in recognition. It does not aim to induce specific faults or force particular mis\u2010transcriptions, but rather uncovers generic recognition errors across ASR systems.",
          "evidence": "Section 2.1 (Workflow): \"If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\""
        },
        "gemini": {
          "value": "Low",
          "why": "CrossASR++ performs untargeted exploration to reveal generic faults, specifically misclassifications or inaccuracies. The method defines a failed test case as when the Automatic Speech Recognition (ASR) system under test (SUT) produces a transcription that does not match the original text input, provided another ASR system successfully transcribes it correctly. It does not steer exploration towards specific, predefined outputs or particular semantic/category-level faults like jailbreaks or bias.",
          "evidence": "Page 1, Section 1, 'Introduction': 'If the transcribed text recognized by the ASR system under test does not match the text input to TTS while another ASR's transcribed text does, this input audio is viewed as a failed test case.'\n\nPage 2, Section 2.1, 'Workflow': '2) If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "CrossASR++ performs broad, untargeted differential testing by generating audio from arbitrary texts and detecting any mismatches in recognition. It does not aim to induce specific faults or force particular mis\u2010transcriptions, but rather uncovers generic recognition errors across ASR systems.",
          "evidence": "Section 2.1 (Workflow): \"If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\""
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "CrossASR++ is designed to generate and prioritize audio\u2013text test cases that expose failures in ASR systems, but it does not analyze internal model signals, feature activations, or decision boundaries to explain why the failures occur. The tool reports faulty cases (what fails) without investigating their origins or providing correlational or causal insights into the model\u2019s internal behavior.",
          "evidence": "Abstract (p. 1): \u201cCrossASR++ can uncover 26.2% more failed test cases for 4 ASRs than the original tool.\u201d\nSection 2.1 (Workflow, p. 2): Algorithm 1 describes generating audios, recognizing them, and cross-referencing to collect failed and successful test cases, with no steps for internal diagnostic analysis or root-cause explanation."
        },
        "gemini": {
          "value": "Low",
          "why": "The CrossASR++ framework focuses solely on identifying and reporting 'failed test cases' for Automatic Speech Recognition (ASR) systems without delving into the internal mechanisms or external behavioral patterns to explain the root causes of these failures. It acts as a black-box differential testing tool that uncovers discrepancies between ASR outputs, but does not perform diagnostic analysis of model internals (e.g., feature activations, decision boundaries) or interpret failure patterns (e.g., class-level bias, overfitting, semantic feature confusion) to infer possible causes.",
          "evidence": "Page 1, Section 1, 'INTRODUCTION': 'If the transcribed text recognized by the ASR system under test does not match the text input to TTS while another ASR's transcribed text does, this input audio is viewed as a failed test case.'\nPage 2, Section 2.1, 'Workflow': 'If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.'\nPage 3, Section 3, 'EXPERIMENTS' and Table 1: The evaluation 'measures how CrossASR++ performs in finding failed test cases' and reports '# Failed Test Case' counts for various ASR systems under different configurations. This demonstrates the tool's effectiveness in finding failures, but not in explaining their causes."
        },
        "arbitrator": {
          "value": "Low",
          "why": "CrossASR++ is designed to generate and prioritize audio\u2013text test cases that expose failures in ASR systems, but it does not analyze internal model signals, feature activations, or decision boundaries to explain why the failures occur. The tool reports faulty cases (what fails) without investigating their origins or providing correlational or causal insights into the model\u2019s internal behavior.",
          "evidence": "Abstract (p. 1): \u201cCrossASR++ can uncover 26.2% more failed test cases for 4 ASRs than the original tool.\u201d\nSection 2.1 (Workflow, p. 2): Algorithm 1 describes generating audios, recognizing them, and cross-referencing to collect failed and successful test cases, with no steps for internal diagnostic analysis or root-cause explanation."
        },
        "manual": {
          "value": "Low",
          "why": "No diagnsotic analysis"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "CrossASR++ generates test inputs by synthesizing audio from real text via TTS, which enforces a theoretical constraint on input naturalness, but the paper does not report any quantitative or human evaluation (e.g., LPIPS, SSIM, BLEU) to empirically validate the realism of the generated audio.",
          "evidence": "Abstract (p. 1575): \u201cThis method first utilizes Text-to-Speech (TTS) to generate audios from texts automatically and then feed these audios into different ASR systems \u2026\u201d \u2013 no naturalness metrics or human study are described in Sections 2 or 3."
        },
        "gemini": {
          "value": "Medium",
          "why": "The proposed method, CrossASR++, relies on Text-to-Speech (TTS) engines to generate audio inputs from a text corpus. TTS engines are inherently designed to produce natural-sounding speech from text, thereby providing a theoretical justification and design constraint for input naturalness. The paper also acknowledges the importance of using 'more advanced TTSes generate fewer invalid audios' to avoid 'indeterminable test cases,' which further demonstrates an underlying consideration for the plausibility and quality of generated inputs. However, the paper does not provide empirical validation for the naturalness of these generated inputs using quantitative metrics (e.g., LPIPS, SSIM, BLEU) or dedicated human evaluation.",
          "evidence": "Page 1, Abstract: 'This method first utilizes Text-to-Speech (TTS) to generate audios from texts automatically and then feed these audios into different ASR systems for cross-referencing to uncover failed test cases.'\nPage 2, Section 2.2 Extensibility: 'The main reason is that more advanced TTSes generate fewer invalid audios that are more likely to be indeterminable test cases. As a result, using better TTSes can improve CrossASR++.'\nPage 2, Section 2.2 Extensibility: 'Users can add a new TTS... by simply inheriting the base class and implementing necessary methods.'\nPage 3, Section 2.2 Extensibility: 'The suppported TTSes are Google Translate's TTS [8], ResponsiveVoice [16], Festival [9], and Espeak [7].'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "CrossASR++ generates test inputs by synthesizing audio from real text via TTS, which enforces a theoretical constraint on input naturalness, but the paper does not report any quantitative or human evaluation (e.g., LPIPS, SSIM, BLEU) to empirically validate the realism of the generated audio.",
          "evidence": "Abstract (p. 1575): \u201cThis method first utilizes Text-to-Speech (TTS) to generate audios from texts automatically and then feed these audios into different ASR systems \u2026\u201d \u2013 no naturalness metrics or human study are described in Sections 2 or 3."
        },
        "manual": {
          "value": "Medium",
          "why": "TTS engines are inherently designed to produce natural-sounding speech from text, thereby providing a theoretical justification and design constraint for input naturalness."
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "High",
          "why": "CrossASR++ discovers faults by feeding TTS-synthesized audio files into ASR systems. These inputs are model-generated artifacts (finalized audio files) and thus their perturbations are preserved through standard I/O (e.g., WAV/MP3 serialization), rather than relying on fragile floating-point changes that could be lost during clipping or quantization.",
          "evidence": "Page 1575, Abstract: \u201cThis method first utilizes Text-to-Speech (TTS) to generate audios from texts automatically and then feed these audios into different ASR systems for cross-referencing to uncover failed test cases.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The proposed method generates fuzzed inputs (audio files) by synthesizing them from text using Text-to-Speech (TTS) engines. According to the metric definition, model-generated fuzzed inputs, such as synthesized speech, are inherently stable under I/O operations because they are self-contained artifacts rather than transient numeric states. The paper explicitly states that the generated audios are also associated with a folder for saving them, indicating they are finalized artifacts.",
          "evidence": "Page 1, Section 1: \"Our recent work proposes CrossASR, a differential testing method for ASR systems. This method first utilizes Text-to-Speech (TTS) to generate audios from texts automatically...\"\nPage 2, Algorithm 1, Line 9: \"audios = tts.generateAudio(texts);\"\nPage 2, Section 2.2: \"When inheriting from each class, users need to specify a name in the constructor. This name will be associated with a folder for saving the audio files and transcriptions.\""
        },
        "arbitrator": {
          "value": "High",
          "why": "CrossASR++ discovers faults by feeding TTS-synthesized audio files into ASR systems. These inputs are model-generated artifacts (finalized audio files) and thus their perturbations are preserved through standard I/O (e.g., WAV/MP3 serialization), rather than relying on fragile floating-point changes that could be lost during clipping or quantization.",
          "evidence": "Page 1575, Abstract: \u201cThis method first utilizes Text-to-Speech (TTS) to generate audios from texts automatically and then feed these audios into different ASR systems for cross-referencing to uncover failed test cases.\u201d"
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper evaluates failures independently for each ASR under test (SUT) via cross\u2010referencing but does not analyze whether the same failing inputs cause equivalent erroneous behaviors across multiple ASR models.",
          "evidence": "Section 2.1 Workflow: \u201cIf the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\u201d (Page 2, Lines 11\u201314)\n\nSection 3 RQ1: Results are reported per SUT in Table 1, without any analysis of the same inputs failing multiple models. (Page 4, Table 1)"
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates multiple ASR models but does so independently for each model designated as the System Under Test (SUT). A 'failed test case' is defined specifically for an SUT when its output mismatches the ground truth and at least one other ASR system provides a correct transcription. While the framework utilizes multiple ASRs for cross-referencing to *detect* failures for a given SUT, it does not analyze whether the same failing inputs cause equivalent erroneous behaviors across *different SUTs*, nor does it analyze cross-model consistency in mutation-failure relationships. The experimental results report the number of failed test cases found for each ASR when it is the SUT, without discussing the overlap or transferability of these specific failing inputs to other ASRs when they act as SUTs.",
          "evidence": "Section 1, Introduction, Page 1: 'If the transcribed text recognized by the ASR system under test does not match the text input to TTS while another ASR's transcribed text does, this input audio is viewed as a failed test case.'\nSection 2.1, Workflow, Page 2: 'If the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.'\nSection 3, Experiments, RQ1, Page 3: 'Table 1 illustrates the results of running the tool using the two configurations. Each column represents the number of failed test cases uncovered for the system under test (SUT) when we use one ASR as the SUT and the remaining 3 ASRs for cross-referencing.'\nTable 1, Page 3: Shows distinct counts of '# Failed Test Case' for each ASR (DS, DS2, W2L, Wit) when each is considered as the SUT, e.g., 'DS: 319', 'DS2: 217'. This indicates independent evaluation of failures for each SUT rather than an analysis of shared failing inputs across SUTs."
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper evaluates failures independently for each ASR under test (SUT) via cross\u2010referencing but does not analyze whether the same failing inputs cause equivalent erroneous behaviors across multiple ASR models.",
          "evidence": "Section 2.1 Workflow: \u201cIf the SUT fails to recognize the audio file successfully, but at least one of the other ASR systems succeeds, we take this audio file (and the corresponding input text) as a failed test case for the SUT.\u201d (Page 2, Lines 11\u201314)\n\nSection 3 RQ1: Results are reported per SUT in Table 1, without any analysis of the same inputs failing multiple models. (Page 4, Table 1)"
        },
        "manual": {
          "value": "Low",
          "why": "Uses multiple ASRs in differential oracle, does not show transferability of failures across errors"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Differential"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Data-driven"
      ]
    }
  },
  "Asyrofi et al. - 2021 - Can Differential Testing Improve Automatic Speech Recognition Systems": {
    "key": "asyrofi2021can",
    "name": "Asyrofi et al.",
    "year": "2021",
    "citation_count": "16",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "The proposed method uncovers failures where the ASR system misrecognizes or misclassifies audio inputs (i.e., incorrect transcripts), which are low-level functional errors exposing brittle decision boundaries or instability under perturbation, matching the \u2018Low\u2019 severity level.",
          "evidence": "Page 675, Section II.A: \u201cIf the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The method proposed in the paper, evolutionary differential testing building upon CrossASR++, focuses on identifying and improving the system's ability to correctly transcribe audio inputs. The 'failures' or 'failed test cases' it uncovers are instances where the Automatic Speech Recognition (ASR) system's transcription differs from the expected input text. This directly corresponds to functional errors such as misclassification or flawed output, which fall under the 'Low' vulnerability severity category.",
          "evidence": "Page 1, Abstract: 'A recent work, CrossASR++, can automatically uncover many failures in ASR systems by taking advantage of the differential testing technique.' and 'When an ASR system has defects, namely, when an ASR system cannot correctly recognize the audio of a valid test case, developers will collect more data to improve its model.'\nPage 2, Section II.A CrossASR++: 'If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT. Otherwise, a successful test case is found.'\nPage 4, Answers to RQ1: 'By fine-tuning the ASR system under test using generated test cases, the number of failed test cases uncovered decreases by 25.81% and the WER decreases by 45.81%.' (WER is a direct measure of output correctness/error)."
        },
        "arbitrator": {
          "value": "Low",
          "why": "The proposed method uncovers failures where the ASR system misrecognizes or misclassifies audio inputs (i.e., incorrect transcripts), which are low-level functional errors exposing brittle decision boundaries or instability under perturbation, matching the \u2018Low\u2019 severity level.",
          "evidence": "Page 675, Section II.A: \u201cIf the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "CrossASR++ explores ASR behavior broadly to discover any misrecognitions or transcription errors without targeting specific outputs or fault categories. It seeks generic failures (mismatches between recognized and ground-truth text) rather than steering the model toward a predefined phrase or semantic property.",
          "evidence": "Page 675, Section II-A: \u201cIf the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The method proposed in the paper, CrossASR++, performs untargeted exploration to reveal generic faults in Automatic Speech Recognition (ASR) systems. Its primary goal is to identify 'failed test cases' where the ASR system's transcription differs from the input text, which constitutes a general misclassification or inaccuracy. The 'failure estimator' component is designed to efficiently discover such generic faults, not to steer the fuzzer toward specific, predefined adversarial outcomes or semantic-level faults. There is no evidence of the fuzzer attempting to force a chosen label, a particular phrase, or specific behavioral issues like jailbreaks or toxicity.",
          "evidence": "Page 1, Abstract: 'A recent work, CrossASR++, can automatically uncover many failures in ASR systems by taking advantage of the differential testing technique.'\nPage 2, Section II.A, 'CrossASR++': 'If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT. Otherwise, a successful test case is found.'\nPage 2, Section II.A, 'CrossASR++': 'CrossASR++ trains a failure estimator, which can estimate the probability of a piece of text leading to a failed test case.'\nPage 4, 'Answers to RQ1': 'By fine-tuning the ASR system under test using generated test cases, the number of failed test cases uncovered decreases by 25.81% and the WER decreases by 45.81%.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "CrossASR++ explores ASR behavior broadly to discover any misrecognitions or transcription errors without targeting specific outputs or fault categories. It seeks generic failures (mismatches between recognized and ground-truth text) rather than steering the model toward a predefined phrase or semantic property.",
          "evidence": "Page 675, Section II-A: \u201cIf the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "The proposed method focuses on generating and leveraging failed test cases to fine-tune ASR models, but does not perform any diagnostic analysis of model internals (e.g., feature activations, decision boundaries, or gradient behavior) nor offers correlational interpretations of failure patterns. It reports and uses failures purely as training data without investigating their origins or mechanisms.",
          "evidence": "Section II.B (Improving ASR Systems): \"At the end of the i\u1d57\u02b0 iteration, we fine-tune the current model SUT_{i\u22121} on S_i \u222a F_i to achieve a new model SUT_i.\" (p. 676)"
        },
        "gemini": {
          "value": "Low",
          "why": "The proposed method focuses on identifying faulty cases (failed test cases) through differential testing and then using these cases to fine-tune the ASR system under test (SUT) or a cross-referenced ASR system. The evaluation metrics are the number of failed test cases uncovered and the Word Error Rate (WER). While the paper demonstrates that fine-tuning with generated test cases can improve the ASR system's performance (i.e., reduce failures), it does not provide any analysis of internal model signals (e.g., feature activations, decision boundaries, gradient behavior) or external behavioral patterns (e.g., class-level bias, overfitting, semantic confusion) to explain *why* these failures originally occurred. The approach is entirely focused on uncovering 'what fails' and then using this information for model improvement, without investigating the origins or underlying causes of the failures.",
          "evidence": "Page 1, Abstract: 'Our experiment results empirically show that the above methods to leverage the test cases can substantially improve both the target ASR system and CrossASR++ itself. After fine-tuning, the number of failed test cases uncovered decreases by 25.81% and the word error rate of the improved target ASR system drops by 45.81%.'\nPage 1, Introduction: 'A natural question to ask is how the generated test cases can be leveraged to improve an ASR system.'\nPage 2, Section II.B.1 'Improving the ASR System Under Test': 'We fine-tune the current model SUTi-1 on Si UFi to achieve a new model SUTi.'\nPage 2, Section II.B.2 'Improving CrossASR++': 'Then, it utilizes these test cases to fine-tune the cross-referenced ASR systems in the same way as we evolve an SUT.'\nPage 3, Section III.C, RQ1 and RQ2 Answers: 'By fine-tuning the ASR system under test using generated test cases, the number of failed test cases uncovered decreases by 25.81% and the WER decreases by 45.81%.' and 'By fine-tuning one of the cross-referenced ASR systems using generated test cases, the number of failed test cases found by CrossASR++ in Wav2Vec, DeepSpeech2, Wav2letter, and Wit increases by 5.70%, 7.25%, 3.93% and 1.52%, respectively.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "The proposed method focuses on generating and leveraging failed test cases to fine-tune ASR models, but does not perform any diagnostic analysis of model internals (e.g., feature activations, decision boundaries, or gradient behavior) nor offers correlational interpretations of failure patterns. It reports and uses failures purely as training data without investigating their origins or mechanisms.",
          "evidence": "Section II.B (Improving ASR Systems): \"At the end of the i\u1d57\u02b0 iteration, we fine-tune the current model SUT_{i\u22121} on S_i \u222a F_i to achieve a new model SUT_i.\" (p. 676)"
        },
        "manual": {
          "value": "Low",
          "why": "No diagnsotic analysis"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "The paper enforces a design constraint for naturalness by only accepting synthesized audio\u2013text pairs as valid test cases if at least one reference ASR system transcribes the text exactly, but it does not provide any empirical validation (e.g., LPIPS, SSIM, BLEU) or human evaluation of input realism.",
          "evidence": "Section II.A (p. 675): \u201cThe generated audio file is then fed to all ASR systems inside CrossASR++ \u2026 and tags the synthesized audio\u2013text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper employs a design constraint\u2014cross-referencing with multiple ASR systems\u2014to filter out 'invalid audios' generated by the TTS system. This process ensures that only audios considered 'valid' (i.e., at least one ASR system can correctly recognize them) are used as test cases. This mechanism suggests an intent to preserve input naturalness or plausibility within the ASR domain, preventing the use of overtly artificial or incomprehensible inputs. However, the paper does not provide explicit empirical validation of this naturalness through quantitative perceptual metrics (e.g., audio quality scores, naturalness scores) or human evaluation of the generated audio inputs.",
          "evidence": "Page 1, Abstract & Introduction: 'It employs a Text-To-Speech (TTS) system to synthesize audios from texts and then reveals failed test cases by feeding them to multiple ASR systems for cross-referencing. However, no prior work tries to utilize the generated test cases to enhance the quality of ASR systems.'\nPage 1, Introduction: 'More specifically, they first use a TTS service to transcribe an input text to an audio file and combine them into an audio-text pair. Then, to avoid using the invalid audios synthesized by the TTS system, the generated audio-text pair is fed into multiple ASR systems for cross-referencing: If at least one ASR system correctly recognizes the audio, the audio-text pair is viewed as a valid test case.'\nPage 2, Section II.A, CrossASR++: 'The tool cross-references the outputs of these ASR systems and tags the synthesized audio-text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio (i.e., each word in the transcription matches each corresponding word in the input text). A valid test case can be either a failed test case or a successful test case. If the SUT transcription is different from the input text, it means that CrossASR++ uncovers a failed test case for the SUT. Otherwise, a successful test case is found. We take the audio-text pair as an indeterminable test case if none of them can successfully transcribe the audio.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "The paper enforces a design constraint for naturalness by only accepting synthesized audio\u2013text pairs as valid test cases if at least one reference ASR system transcribes the text exactly, but it does not provide any empirical validation (e.g., LPIPS, SSIM, BLEU) or human evaluation of input realism.",
          "evidence": "Section II.A (p. 675): \u201cThe generated audio file is then fed to all ASR systems inside CrossASR++ \u2026 and tags the synthesized audio\u2013text pair as a valid test case if at least one of the ASR systems can successfully transcribe the audio.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "High",
          "why": "The faults are discovered using audios synthesized by a Text-To-Speech (TTS) system, i.e., model-generated inputs that are finalized artifacts and not small floating-point perturbations; such inputs remain stable under standard I/O operations (e.g., quantization to WAV), so the observed failures stem from the ASR model\u2019s decision logic rather than fragile numerical artifacts.",
          "evidence": "Abstract (p.674): \u201cIt employs a Text-To-Speech (TTS) system to synthesize audios from texts and then reveals failed test cases by feeding them to multiple ASR systems\u2026\u201d; Section II.A (p.675): \u201cit first employs a TTS system to synthesize an audio file from the text. The generated audio file is then fed to all ASR systems inside CrossASR++\u2026\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The method proposed in the paper generates test cases by employing a Text-To-Speech (TTS) system to synthesize audio files from text inputs. The metric definition explicitly states that \"model-generated fuzzed inputs\u2014such as synthesized speech, rendered images, or generated text\u2014are inherently stable under I/O operations because they originate from self-contained generation processes that produce finalized artifacts rather than transient numeric states.\" Since the test cases are synthesized audio files, they are considered inherently stable across I/O transformations, ensuring that discovered faults persist.",
          "evidence": "Page 1, Abstract: 'It employs a Text-To-Speech (TTS) system to synthesize audios from texts and then reveals failed test cases by feeding them to multiple ASR systems for cross-referencing.'\nPage 2, Section II.A, 'CrossASR++': 'Specifically, given a piece of text from the corpus, it first employs a TTS system to synthesize an audio file from the text.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "The faults are discovered using audios synthesized by a Text-To-Speech (TTS) system, i.e., model-generated inputs that are finalized artifacts and not small floating-point perturbations; such inputs remain stable under standard I/O operations (e.g., quantization to WAV), so the observed failures stem from the ASR model\u2019s decision logic rather than fragile numerical artifacts.",
          "evidence": "Abstract (p.674): \u201cIt employs a Text-To-Speech (TTS) system to synthesize audios from texts and then reveals failed test cases by feeding them to multiple ASR systems\u2026\u201d; Section II.A (p.675): \u201cit first employs a TTS system to synthesize an audio file from the text. The generated audio file is then fed to all ASR systems inside CrossASR++\u2026\u201d"
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper evaluates failure discovery and model improvement separately for each ASR system (each treated as a distinct SUT) and reports numbers of failed test cases per model, but does not analyze whether the same failing inputs cause equivalent erroneous behaviors across multiple models. There is no cross-model consistency or input\u2010level reproducibility assessment.",
          "evidence": "Section III.C (RQ2): \u201cWe run the static CrossASR++ for 4 times, i.e., by using different ASR systems as SUT for each. We also run the evolved CrossASR++ for 4 times. We record the number of failed test cases found by the static and the evolved CrossASR++ in 4 ASR systems when each of them is used as the SUT.\u201d (no cross\u2010model failure transfer analysis)"
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates multiple ASR models but does not analyze the cross-model consistency of the discovered failures. Specifically, for RQ2, it demonstrates that an improved test case generation tool (CrossASR++) can find a higher *number* of failed test cases for several different ASR systems (SUTs) when one of its internal cross-referenced ASR systems is fine-tuned. However, it does not investigate whether the *same specific failing inputs* cause equivalent erroneous behaviors across these multiple SUTs, nor does it analyze cross-model correlations of mutation patterns to failure behaviors (as no mutations are applied in this context). The analysis focuses on the improved detection rate for each SUT independently, rather than the transferability or commonality of failure instances themselves across models.",
          "evidence": "Page 5, Section III.C, 'Answers to RQ2': 'By fine-tuning one of the cross-referenced ASR systems using generated test cases, the number of failed test cases found by CrossASR++ in Wav2Vec, DeepSpeech2, Wav2letter, and Wit increases by 5.70%, 7.25%, 3.93% and 1.52%, respectively.'\nPage 5, Section III.C, 'Result' paragraph related to RQ2: 'By comparing the number of failed test cases found by the static and evolved CrossASR++ head-to-head, it is clear that the evolved CrossASR++ uncovers more failed test cases than the static one for each ASR system. The numbers of failed test cases found in Wav2Vec2, DeepSpeech2, Wav2letter, and Wit increase by 5.70% (298 to 315), 7.25% (276 to 296), 3.93% (509 to 529), and 1.52% (1,119 to 1,136), respectively.'\nThis evidence shows that the performance (number of failed test cases found) is reported for each ASR system individually, without analyzing commonalities or transferability of the specific failing inputs across these different systems."
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper evaluates failure discovery and model improvement separately for each ASR system (each treated as a distinct SUT) and reports numbers of failed test cases per model, but does not analyze whether the same failing inputs cause equivalent erroneous behaviors across multiple models. There is no cross-model consistency or input\u2010level reproducibility assessment.",
          "evidence": "Section III.C (RQ2): \u201cWe run the static CrossASR++ for 4 times, i.e., by using different ASR systems as SUT for each. We also run the evolved CrossASR++ for 4 times. We record the number of failed test cases found by the static and the evolved CrossASR++ in 4 ASR systems when each of them is used as the SUT.\u201d (no cross\u2010model failure transfer analysis)"
        },
        "manual": {
          "value": "Low",
          "why": "Does not demonstrate if fuzzed input on one model aslo causes error in another model"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Differential"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Data-driven"
      ]
    }
  },
  "Pei et al. - 2017 - DeepXplore Automated Whitebox Testing of Deep Learning Systems": {
    "key": "pei2017deepxplore",
    "name": "DeepXplore",
    "year": "2017",
    "citation_count": "1814",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepXplore uncovers low\u2010level functional errors\u2014misclassifications and flawed outputs in various DL systems (e.g., self\u2010driving cars making incorrect steering decisions and malware detectors mislabeling samples)\u2014but does not demonstrate bypass of explicit safety, policy, or security mechanisms or normative/ethical violations.",
          "evidence": "Page 1, Abstract: \u201cDeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "DeepXplore uncovers vulnerabilities that bypass explicit safety and security mechanisms. Specifically, it identifies inputs that cause self-driving cars to make decisions leading to crashes (a safety mechanism bypass) and inputs that cause malware detectors to misclassify malicious software as benign (a security mechanism bypass).",
          "evidence": "Page 1, Abstract: \"DeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models...\"\nPage 1, Introduction: \"In safety- and security-critical settings, such incorrect behaviors can lead to disastrous consequences such as a fatal collision of a self-driving car.\"\nPage 2, Figure 1 caption: \"An example erroneous behavior found by DeepXplore in Nvidia DAVE-2 self-driving car platform. The DNN-based self-driving car correctly decides to turn left for image (a) but incorrectly decides to turn right and crashes into the guardrail for image (b), a slightly darker version of (a).\"\nPage 11, Table 3 description: \"The features added to the manifest file by DeepXplore for generating two sample malware inputs which Android app classifiers (Drebin) incorrectly mark as benign.\"\nPage 11, Table 4 description: \"The top-3 most in(de)cremented features for generating two sample malware inputs which PDF classifiers incorrectly mark as benign.\""
        },
        "arbitrator": {
          "value": "High",
          "why": "Although DeepXplore uncovers low-level misclassifications (e.g., incorrect steering decisions, mislabeling digits), it also exposes high-severity vulnerabilities by bypassing explicit safety and security mechanisms: it finds inputs that cause self-driving car models to steer into guardrails and inputs that let malicious apps and PDFs evade DNN-based malware detectors. These attacks demonstrate the fuzzer\u2019s ability to penetrate system-level safeguards and expose security-critical flaws.",
          "evidence": "Page 1, Abstract: \u201cDeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models.\u201d\nPage 2, Figure 1 caption: \u201cAn example erroneous behavior found by DeepXplore in Nvidia DAVE-2 self-driving car platform\u2026 incorrectly decides to turn right and crashes into the guardrail.\u201d\nPage 11, Table 3 & Table 4 descriptions: \u201cfeatures added to the manifest file by DeepXplore for generating two sample malware inputs which Android app classifiers (Drebin) incorrectly mark as benign\u201d and \u201cfor generating two sample malware inputs which PDF classifiers incorrectly mark as benign.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "reveals incorrect corner-case behaviors but does not bypass explicity safety mechinasims"
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepXplore performs untargeted exploration by maximizing any differential behavior between multiple DNNs and increasing neuron coverage, without steering inputs toward a specific, predefined fault or output.",
          "evidence": "Section 4.2 (\u201cMaximizing differential behaviors\u201d): \u201cThe first objective of the optimization problem is to generate test inputs that can induce different behaviors in the tested DNNs, i.e., different DNNs will classify the same input into different classes.\u201d\nSection 3 (\u201cOverview\u201d): \u201cDeepXplore \u2026 solves a joint optimization problem that maximizes both differential behaviors and neuron coverage.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "DeepXplore's fuzzing approach is directed towards finding semantic or category-level faults. Its primary objective is to generate test inputs that cause 'differential behaviors' among multiple deep learning systems, meaning these systems classify the same input into different categories. This leads to the discovery of 'incorrect corner case behaviors' such as self-driving cars making conflicting decisions (e.g., turning left versus right, potentially leading to crashes) or malware being misclassified as benign. While these are clear fault categories (unsafe behavior, misclassification), the approach does not aim to force a specific, predetermined output (e.g., forcing a self-driving car to turn exactly right or predicting a specific wrong label). Instead, it seeks any significant disagreement or deviation from expected behavior.",
          "evidence": "Page 1, Abstract: \"DeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models...\"\nPage 5, Section 3 (Overview): \"DeepXplore takes unlabeled test inputs as seeds and generates new tests that cover a large number of neurons (...) while causing the tested DNNs to behave differently.\"\nPage 6-7, Section 4.2 (Maximizing differential behaviors): \"The first objective of the optimization problem is to generate test inputs that can induce different behaviors in the tested DNNs, i.e., different DNNs will classify the same input into different classes.\"\nPage 7, Equation 2: \"obj1(x) = \u2211k\u2260j Fk(x)[c] - \u03bb1 * Fj(x)[c]\" - This objective maximizes the confidence for the original class `c` in other DNNs `Fk` while minimizing it for a chosen DNN `Fj`, aiming for differential prediction rather than a specific target misclassification."
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DeepXplore deliberately steers its fuzzing toward uncovering semantic, category-level faults\u2014specifically, inputs that induce disagreements or unsafe behaviors (e.g., a self-driving car crash or malware misclassification)\u2014but does not aim to force a single, predefined misclassification label or exact output. It instead maximizes any differential behavior among multiple DNNs and increases neuron coverage, matching the \u201cMedium\u201d level of directedness, which targets semantic or category-level faults without prescribing the exact output.",
          "evidence": "Page 1 Abstract: \u201cDeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software)\u2026\u201d\n\nPage 5, Section 3 (Overview): \u201cDeepXplore \u2026 solves a joint optimization problem that maximizes both differential behaviors and neuron coverage.\u201d\n\nPage 6\u20137, Section 4.2 (Maximizing differential behaviors): \u201cThe first objective of the optimization problem is to generate test inputs that can induce different behaviors in the tested DNNs, i.e., different DNNs will classify the same input into different classes.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "performs untargeted exploration to reveal generic model faults (differential behaviour, inaccuracies). No targeted misclassification."
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepXplore is designed to generate inputs that expose failures in deep learning models and to measure coverage, but it does not perform any analysis of the internal causes of those failures\u2014neither diagnosing which architectural, training, or data\u2010related factors lead to errors nor providing correlational explanations of failure patterns. It simply reports faulty cases (differential behaviors) without investigating their origins or offering heuristic insights into why the model failed.",
          "evidence": "Section 1 (page 1): \u201cDeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails \u2026) in state-of-the-art DL models\u2026.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "DeepXplore provides insight into where and how failures manifest, primarily through its use of neuron coverage to generate diverse inputs and its ability to detect problematic training data. It links observed differential behaviors to different parts of the model's logic and can identify plausible contributing factors like training data pollution. However, it stops short of establishing direct causal links between specific model architectures, training weaknesses, or optimization dynamics and the precise mechanisms of individual vulnerabilities.",
          "evidence": "Page 5, Section 3, OVERVIEW: 'DeepXplore solves a joint optimization problem that maximizes both differential behaviors and neuron coverage. Note that both goals are crucial for thorough testing of DNNs and finding diverse erroneous corner case behaviors. High neuron coverage alone may not induce many erroneous behaviors while just maximizing different behaviors might simply identify different manifestations of the same underlying root cause.'\n\nPage 14, Section 7.3, Improving DNNs with DeepXplore, 'Detecting training data pollution attack': 'We use DeepXplore to generate error-inducing inputs that are classified as the digit 9 and 1 by the unpolluted and polluted versions of the LeNet-5 DNN respectively. We then search for samples in the training set that are closest to the inputs generated by DeepXplore in terms of structural similarity [80] and identify them as polluted data. Using this process, we are able to correctly identify 95.6% of the polluted samples.'\n\nPage 14, Section 8, DISCUSSION, 'Causes of differences between DNNs': 'The underlying root cause behind prediction differences between two DNNs for the same input is differences in their decision logic/boundaries... determined by multiple factors including training data, the DNN architecture, hyperparameters, etc. Therefore, any differences in the choice of these factors will result in subtle changes in the decision logic of the resulting DNN.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepXplore focuses on generating and uncovering faulty inputs and measuring neuron coverage, but it does not analyze or explain the internal mechanisms that cause those failures. It lacks diagnostic or correlational analysis of model activations, decision boundaries, or training dynamics to reveal why the errors occur\u2014it merely reports observed misbehaviors without investigating their origins.",
          "evidence": "DeepXplore\u2019s contributions (\u00a71) and methodology (\u00a74) describe how to find inputs that trigger incorrect behaviors and maximize neuron coverage. However, there is no section in the paper that analyzes internal signals or provides explanations linking failures to specific architectural, training, or data weaknesses\u2014DeepXplore stops at reporting differential behaviors and does not trace their root causes."
        },
        "manual": {
          "value": "Low",
          "why": "No diagnsotic analysis"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "DeepXplore enforces a set of user-specified, domain-specific constraints (e.g., bounded global brightness shifts, region-based occlusions, integer pixel bounds, PDF structure rules, manifest feature additions) to keep its fuzzed inputs within plausible, real-world input domains. However, it does not provide any quantitative metrics (such as LPIPS, SSIM or BLEU) nor human evaluation to empirically validate the perceptual or semantic realism of those inputs.",
          "evidence": "Section 6.2 \u201cDomain-specific constraints\u201d: \u201cDeepXplore used three different types of constraints for simulating different environment conditions of images: (1) lighting effects \u2026 (2) occlusion by a single small rectangle \u2026 (3) occlusion by multiple tiny black rectangles \u2026 we add domain-specific constraints (e.g., ensure the pixel values are integers within 0 and 255 for image input) to make sure that the modified inputs still represent real-world images.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "DeepXplore incorporates domain-specific constraints and a rule-based method to ensure that generated inputs remain 'valid and realistic' or 'physically realistic.' These constraints include pixel value ranges (e.g., 0-255 for images), lighting effects, occlusion simulation for images, and specific rules for modifying features in malware datasets (e.g., only adding features, not deleting them). While the paper claims these methods ensure the modified inputs 'still represent real-world images' and presents visual examples (Figure 8) that appear plausible, it does not provide empirical validation through quantitative metrics (such as LPIPS, SSIM, or BLEU) or formal human evaluation studies to confirm perceptual or semantic naturalness.",
          "evidence": "Page 2, Section 1, Introduction: 'DeepXplore also supports adding custom constraints by the users for simulating different types of realistic inputs (e.g., different types of lighting and occlusion for images/videos).'\nPage 5, Section 3, Overview: 'DeepXplore also supports enforcing of custom domain-specific constraints as part of the joint optimization process. For example, the value of an image pixel has to be between 0 and 255. Such domain-specific constraints can be specified by the users of DeepXplore to ensure that the generated test inputs are valid and realistic.'\nPage 5, Section 3, A working example: 'We further add domain-specific constraints (e.g., ensure the pixel values are integers within 0 and 255 for image input) to make sure that the modified inputs still represent real-world images.'\nPage 6, Section 4.2, Domain-specific constraints: 'One important aspect of the optimization process is that the generated test inputs need to satisfy several domain-specific constraints to be physically realistic [63].'\nPage 7, Section 4.2, Domain-specific constraints: 'Therefore, we designed a simple rule-based method to ensure that the generated tests satisfy the custom domain-specific constraints.'\nPage 9, Section 6.2, Domain-specific constraints: This section details the implementation of image constraints (lighting effects, occlusion) and file constraints (for Drebin and Contagio/VirusTotal datasets) designed to maintain realism."
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DeepXplore enforces a set of user-specified, domain-specific constraints (e.g., bounded global brightness shifts, region-based occlusions, integer pixel bounds, PDF structure rules, manifest feature additions) to keep its fuzzed inputs within plausible, real-world input domains. However, it does not provide any quantitative metrics (such as LPIPS, SSIM or BLEU) nor human evaluation to empirically validate the perceptual or semantic realism of those inputs.",
          "evidence": "Section 6.2 \u201cDomain-specific constraints\u201d: \u201cDeepXplore used three different types of constraints for simulating different environment conditions of images: (1) lighting effects \u2026 (2) occlusion by a single small rectangle \u2026 (3) occlusion by multiple tiny black rectangles \u2026 we add domain-specific constraints (e.g., ensure the pixel values are integers within 0 and 255 for image input) to make sure that the modified inputs still represent real-world images.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "DeepXplore\u2019s inputs are generated via semantic\u2010level image transformations (brightness shifts and occlusions) applied to the continuous pixel representation without explicit clipping\u2010and\u2010rounding validation for I/O serialization. Such large, structural changes are assumed by the authors to survive I/O, but no mechanism or evaluation is provided to guarantee persistence under quantization or file\u2010format encoding.",
          "evidence": "Section 6.2 (\u201cDomain\u2010specific constraints\u201d): \u201cDeepXplore used three different types of constraints for simulating different environment conditions of images: (1) lighting effects \u2026 (2) occlusion by a single small rectangle \u2026 (3) occlusion by multiple tiny black rectangles.\u201d  Nowhere does the paper describe clipping or rounding steps to ensure these perturbations remain after standard I/O encoding."
        },
        "gemini": {
          "value": "High",
          "why": "DeepXplore explicitly incorporates domain-specific constraints during its input generation process to ensure that the fuzzed inputs are 'valid and realistic' and that the introduced perturbations remain stable under I/O operations. For image inputs, it enforces pixel values to stay within a standard range (e.g., 0-255), implicitly handling clipping. For discrete features, it explicitly rounds gradient changes to integers, ensuring that modifications are not lost due to floating-point precision issues during serialization.",
          "evidence": "Page 5, Section 3, OVERVIEW: \"DeepXplore also supports enforcing of custom domain-specific constraints as part of the joint optimization process. For example, the value of an image pixel has to be between 0 and 255. Such domain-specific constraints can be specified by the users of DeepXplore to ensure that the generated test inputs are valid and realistic.\"\nPage 7, Section 4.2, DeepXplore algorithm, Domain-specific constraints: \"One important aspect of the optimization process is that the generated test inputs need to satisfy several domain-specific constraints to be physically realistic [63]. In particular, we want to ensure that the changes applied to xi during the i-th iteration of gradient ascent process satisfy all the domain-specific constraints for all i. For example, for a generated test image x the pixel values must be within a certain range (e.g., 0 to 255).\" and \"For discrete features, we round the gradient to an integer.\"\nPage 9, Section 6.2, Domain-specific constraints, Other constraints (Drebin and Contagio/VirusTotal): \"DeepXplore only allows adding features (changing from zero to one) but do not allow deleting features (changing from one to zero) from the manifest files...\""
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DeepXplore\u2019s fault-inducing inputs are generated via semantic-level image transformations (brightness shifts and occlusions) on a continuous pixel representation, and although the tool enforces that pixel values stay within [0,255], it does not apply explicit clipping-and-rounding steps or perform I/O serialization tests to guarantee that these perturbations survive standard quantization (e.g., 8-bit PNG/JPEG encoding). Without explicit rounding or post-serialization validation, small continuous changes could be lost during file encoding.",
          "evidence": "Section 4.2 (\u201cDomain-specific constraints\u201d): the paper describes only bounding pixel values to [0,255] but makes no mention of rounding floats to integer or validating persistence through serialization. Section 6.2 (\u201cDomain-specific constraints\u201d \u2013 Image constraints): lists lighting and occlusion transforms but does not describe any clipping-and-rounding or I/O-stability checks to ensure that the generated perturbations remain after standard image encoding."
        },
        "manual": {
          "value": "Medium",
          "why": "Enforces domain specific constraints like keeping pixel values between 0 and 255, but it can produce decimal point pixel value within that range and does not perform rounding"
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepXplore uses multiple models only to detect differential behaviors\u2014identifying inputs where one model fails and others succeed\u2014but does not evaluate whether the same failing inputs cause equivalent erroneous behaviors across multiple models, nor does it analyze cross-model consistency of failures.",
          "evidence": "Section 3 (\u201cOverview\u201d): \u201cwe leverage multiple DL systems with similar functionality as cross-referencing oracles \u2026 if one self-driving car decides to turn left while others turn right \u2026 one of them is likely to be incorrect.\u201d DeepXplore generates inputs that induce different outputs across models, rather than demonstrating that the same inputs cause the same failures in multiple models."
        },
        "gemini": {
          "value": "Low",
          "why": "DeepXplore's primary objective is to find inputs that induce *differential behaviors* across multiple Deep Learning (DL) models. It leverages differential testing by using multiple DL systems with similar functionality as cross-referencing oracles to identify discrepancies in their outputs for a given input. The method explicitly aims to 'maximize differential behaviors', meaning it actively searches for inputs where models classify the same input into different classes or produce different continuous outputs (e.g., steering angles). This approach focuses on identifying where models *disagree* as an indicator of a potential bug, rather than analyzing cross-model correlations to show that the same mutation pattern systematically induces *similar* or *equivalent* failure behaviors across models. While DeepXplore tests multiple models and uses mutation patterns (referred to as domain-specific constraints like lighting effects or occlusions) to generate test inputs, its evaluation centers on the discovery of these discrepancies, not on the consistency or similarity of specific failure types across the models.",
          "evidence": "Abstract, Page 1: 'Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles to avoid manual checking. Finally, we demonstrate how finding inputs for DL systems that both trigger many differential behaviors and achieve high neuron coverage can be represented as a joint optimization problem...' \n\nSection 3, OVERVIEW, Page 5: 'DeepXplore takes unlabeled test inputs as seeds and generates new tests that cover a large number of neurons (i.e., activates them to a value above a customizable threshold) while causing the tested DNNs to behave differently.' \n\nSection 4.2, DeepXplore algorithm, Page 6-7: 'Maximizing differential behaviors. The first objective of the optimization problem is to generate test inputs that can induce different behaviors in the tested DNNs, i.e., different DNNs will classify the same input into different classes.' \n\nEquation 2, Page 7: 'obj1(x) = \u2211k\u2260jFk(x)[c] - \u03bb1\u22c5Fj(x)[c]' - This objective function explicitly maximizes the difference in predictions between models for a given class 'c'.\n\nSection 8, Limitations, Page 15: 'Second, differential testing can only detect an erroneous behavior if at least one DNN produces different results than other DNNs. If all the tested DNNs make the same mistake, DeepXplore cannot generate the corresponding test case.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepXplore uses multiple models only to detect differential behaviors\u2014identifying inputs where one model fails and others succeed\u2014but does not evaluate whether the same failing inputs cause equivalent erroneous behaviors across multiple models, nor does it analyze cross-model consistency of failures.",
          "evidence": "Section 3 (\u201cOverview\u201d): \u201cwe leverage multiple DL systems with similar functionality as cross-referencing oracles \u2026 if one self-driving car decides to turn left while others turn right \u2026 one of them is likely to be incorrect.\u201d DeepXplore generates inputs that induce different outputs across models, rather than demonstrating that the same inputs cause the same failures in multiple models."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Whitebox"
      ],
      "oracle": [
        "Differential"
      ],
      "mutation_strategy": [
        "Feedback-informed"
      ],
      "exploration_strategy": [
        "Coverage-guided",
        "Prediction-guided"
      ]
    }
  },
  "Gao et al. - 2020 - Fuzz testing based data augmentation to improve robustness of deep neural networks": {
    "key": "gao2020fuzz",
    "name": "Sensei",
    "year": "2020",
    "citation_count": "169",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper\u2019s fuzzer (Sensei) is used to generate and expose small, label-preserving input perturbations that cause misclassification or high loss in a deep neural network\u2014i.e. low-level functional or robustness errors. It does not target specification-level or policy/safety boundary violations.",
          "evidence": "Section 1 (Introduction): \u201cDeep neural networks (DNN) have been shown to be notoriously brittle to small perturbations in their input data.\u201d (p. 1148)\n\nSection 2.3 (Robustness of DNNs): \u201ca DNN M may correctly classify an input x \u2026 but incorrectly classify an input x + \u03b4 that is similar to x\u2026\u201d (p. 1150)"
        },
        "gemini": {
          "value": "Low",
          "why": "The fuzzer proposed in the paper, SENSEI, is designed to improve the robustness of Deep Neural Networks (DNNs) against small, natural environmental perturbations. Its primary objective is to find input variants that cause misclassification or instability in the model's predictions under these perturbations, thereby allowing the model to be re-trained to handle such variations better. This directly aligns with the 'Low' severity definition of triggering functional errors like misclassification or exposing brittle decision boundaries and instability under perturbation.",
          "evidence": "Page 1, Abstract: 'Deep neural networks (DNN) have been shown to be notoriously brittle to small perturbations in their input data...we propose a technique that re-purposes software testing methods, specifically mutation-based fuzzing, to augment the training data of DNNs, with the objective of enhancing their robustness.'\nPage 2, Introduction: 'In this work, we focus on the latter scenario [perturbations due to natural variations] and investigate how software testing techniques can be applied for the robust generalization of Deep Neural Networks (DNNs) to natural environmental variations.'\nPage 6, Section 4.3 Evaluation Metric: 'Specifically, robust accuracy is the proportion of images in the testing dataset where the prediction of a DNN does not fluctuate with any small realistic perturbations. Formally...A DNN is robust around x if and only if M(x') = c for all x' \u2208 X'.' This confirms the focus on preventing misclassification under perturbations."
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper\u2019s fuzzer (Sensei) is used to generate and expose small, label-preserving input perturbations that cause misclassification or high loss in a deep neural network\u2014i.e. low-level functional or robustness errors. It does not target specification-level or policy/safety boundary violations.",
          "evidence": "Section 1 (Introduction): \u201cDeep neural networks (DNN) have been shown to be notoriously brittle to small perturbations in their input data.\u201d (p. 1148)\n\nSection 2.3 (Robustness of DNNs): \u201ca DNN M may correctly classify an input x \u2026 but incorrectly classify an input x + \u03b4 that is similar to x\u2026\u201d (p. 1150)"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "Sensei\u2019s fuzzing-based augmentation searches for input variants that maximize the model\u2019s loss, without prescribing any specific target label or semantic fault. It thus performs broad, untargeted exploration to uncover generic misclassifications rather than steering towards predefined faults.",
          "evidence": "Page 1150, Section 3.3.3 (Fitness function): \u201cthe variant that suffers in more loss by the DNN should be used in the augmented training to make the DNN more robust. Formally: f_loss(x') = L(\u03b8, x', y)\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The fuzzing approach in the paper, SENSEI, performs untargeted exploration to reveal generic faults, specifically misclassifications or unstable predictions, to enhance the robustness of Deep Neural Networks (DNNs) against natural environmental variations. It uses a genetic search to find variants of input data that maximize the DNN's loss for the *original* label, rather than steering the exploration toward specific, predefined incorrect labels or semantic/category-level faults like jailbreaks or bias.",
          "evidence": "Page 1, Section 1, Introduction: 'In this work, we focus on the latter scenario and investigate how software testing techniques can be applied for the robust generalization of Deep Neural Networks (DNNs) to natural environmental variations.' ... 'Robust generalization itself can be of two types... In a security setting, robust generalization against adversarial inputs aims to protect a model against a powerful adversary, that can modify the input in sophisticated and targeted ways (i.e., an attack), such as obscuring a few specific pixels in an image, to make the model mis-predict. ... A second scenario, however, is perturbations due to natural variations in environmental conditions... In this work, we focus on the latter scenario...' \nPage 3, Section 3.1, Problem Formulation: 'find p yielding the optimal variant x' (per Equation 2) to optimize: max L(\u03b8, t(p, x), y)'. Here L is the loss function, and y is the original label. \nPage 5, Section 3.3.3, Fitness function: 'In SENSEI, we define the fitness function based on the empirical loss of the DNN. More specifically, since the training of DNN focuses on minimizing loss across the entire training data-set, the variant that suffers in more loss by the DNN should be used in the augmented training to make the DNN more robust. Formally: floss(x') = L(\u03b8, x', y)'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "Sensei\u2019s fuzzing-based augmentation searches for input variants that maximize the model\u2019s loss, without prescribing any specific target label or semantic fault. It thus performs broad, untargeted exploration to uncover generic misclassifications rather than steering towards predefined faults.",
          "evidence": "Page 1150, Section 3.3.3 (Fitness function): \u201cthe variant that suffers in more loss by the DNN should be used in the augmented training to make the DNN more robust. Formally: f_loss(x') = L(\u03b8, x', y)\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "Aims to find generic misclassifications"
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "The Sensei approach focuses solely on generating and using input variants that induce high loss (i.e., where the model fails) but does not perform any diagnostic analysis of model internals (e.g., activations, decision boundaries, gradient behavior) or infer behavioral patterns to explain why those failures occur. It treats the model as a black box, optimizing only for worst\u2010case loss without probing causal mechanisms underlying the brittleness to natural perturbations.",
          "evidence": "Section 3.3.3 (Fitness function): \u201cfloss(x\u2032) = L(\u03b8, x\u2032, y)\u201d. The method evaluates candidate variants purely by their loss, with no inspection of feature activations, decision boundaries, or gradient\u2010based explanations."
        },
        "gemini": {
          "value": "Low",
          "why": "The proposed method, SENSEI, focuses on identifying input variants that lead to the highest loss (i.e., 'what fails') to augment the training data and improve DNN robustness. While it successfully finds these failure cases and improves the model's performance under perturbations, it does not perform any diagnostic analysis to explain *why* these failures occur. The paper does not describe any mechanism within SENSEI to analyze internal model signals (e.g., feature activations, decision boundaries, or gradient behavior) or external behavioral patterns (e.g., class-level bias, overfitting, or semantic feature confusion) to infer the root causes of the vulnerabilities. The primary fitness function is based on empirical loss, which identifies inputs causing high loss for augmentation, but does not provide insight into the underlying reasons for that high loss.",
          "evidence": "Page 4, Section 3.1: \"The task of training a DNN under robust generalization can be cast as modified version of Equation 1, where, in addition to optimizing for parameters \u03b8 we also need to select, for each training data input x, a suitable variant x' = x + \u03b4, where \u03b4 \u2208 S. Following [21] this can be cast as the following saddle-point optimization problem: min E(x,y)~D[max L(\u03b8, x + \u03b4, y)].\" This defines the objective as finding variants that maximize loss.\n\nPage 5, Section 3.3.3: \"In SENSEI, we define the fitness function based on the empirical loss of the DNN. More specifically, since the training of DNN focuses on minimizing loss across the entire training data-set, the variant that suffers in more loss by the DNN should be used in the augmented training to make the DNN more robust. Formally: floss(x') = L(\u03b8, x', y).\" This clearly states the fitness function is based on loss, indicating a focus on identifying failure symptoms.\n\nPage 11, Section 6, 'Test adequacy metrics': \"Our contribution is orthogonal to these test selection criteria. We demonstrate how to instantiate our technique with either standard model loss or neuron coverage. In principle, SENSEI could be adapted to use other criteria as well.\" This indicates that SENSEI itself does not inherently perform the kind of diagnostic analysis described by other metrics like neuron coverage, though it could hypothetically integrate them."
        },
        "arbitrator": {
          "value": "Low",
          "why": "The Sensei approach focuses solely on generating and using input variants that induce high loss (i.e., where the model fails) but does not perform any diagnostic analysis of model internals (e.g., activations, decision boundaries, gradient behavior) or infer behavioral patterns to explain why those failures occur. It treats the model as a black box, optimizing only for worst\u2010case loss without probing causal mechanisms underlying the brittleness to natural perturbations.",
          "evidence": "Section 3.3.3 (Fitness function): \u201cfloss(x\u2032) = L(\u03b8, x\u2032, y)\u201d. The method evaluates candidate variants purely by their loss, with no inspection of feature activations, decision boundaries, or gradient\u2010based explanations."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "The method\u2019s faults are exposed by semantic\u2010level image transformations (rotation, translation, shear, zoom, brightness, contrast) applied in floating\u2010point space without explicit clipping or rounding to a storage format. While such transformations are large enough to be assumed stable, the lack of quantization/rounding validation means minor deviations could still be lost during I/O operations.",
          "evidence": "Page 1150, Section 4.2 (\u201cGeneration of Realistic Variations\u201d): \u201cWe choose two major kinds of image operations: i) geometric operations ii) color operations \u2026 rotation(x,d): rotate x by d degree \u2026 translation(x, d): \u2026 shear(x,d) \u2026 zoom(x,d) \u2026 brightness(x,d) \u2026 contrast(x,d).\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The proposed method generates fuzzed inputs by applying semantic-level transformations (e.g., rotation, translation, shear, zoom, brightness, contrast) to original images. While the paper defines specific ranges for these transformations (e.g., brightness values within [-32, 32]), it does not explicitly state that the pixel values of the transformed images undergo clipping or rounding operations *after* the transformations to guarantee their stability against potential precision loss during standard I/O operations (such as saving to an 8-bit image format). This lack of explicit validation for I/O stability means that minor deviations introduced by the continuous transformations might not persist if the augmented data were to leave memory and be re-serialized, fitting the 'Medium' criterion where semantic-level transformations 'may not persist through I/O operations due to missing clipping and rounding'.",
          "evidence": "Page 5, Section 3.3.2, 'Generation of population': 'Furthermore, SENSEI always applies the resulting transformation vector (chromosome) on the original image (as opposed to applying on an already transformed data) to prevent the resulting data from being unrealistic.'\nPage 6, Section 4.2, 'Generation of Realistic Variations': 'Since, we focus on the applications with image, we choose two major kinds of image operations: i) geometric operations ii) color opera- tions to simulate the camera movements and lighting conditions in real life. To make sure the translated images are visually similar to natural ones, we restrict the space of allowed perturbations following [8] where it is applicable. The operations and corresponding restrictions with respect to an image x are as follows: ... brightness(x,d): uniformly add or subtract a value d for each pixel of x within a range of [-32, 32] ... contrast(x,d): scale the RGB value of each pixel of x by a factor d within range of [0.8, 1.2]'.\nPage 7, Section 5.1, 'Implementation': 'Our data generator takes as inputs the current model and original training set, augments the original data and then feeds the augmented data to the training process at each step.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "The method\u2019s faults are exposed by semantic\u2010level image transformations (rotation, translation, shear, zoom, brightness, contrast) applied in floating\u2010point space without explicit clipping or rounding to a storage format. While such transformations are large enough to be assumed stable, the lack of quantization/rounding validation means minor deviations could still be lost during I/O operations.",
          "evidence": "Page 1150, Section 4.2 (\u201cGeneration of Realistic Variations\u201d): \u201cWe choose two major kinds of image operations: i) geometric operations ii) color operations \u2026 rotation(x,d): rotate x by d degree \u2026 translation(x, d): \u2026 shear(x,d) \u2026 zoom(x,d) \u2026 brightness(x,d) \u2026 contrast(x,d).\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Apply large semantic level mutations and perform floating point operations (zoom, contrast, shear), which may produce non-discrete pixel values. They don't perform clipping/rounding to ensure IO persistence."
        }
      },
     
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper evaluates augmentation-identified failures and their mitigation separately on each model, without any analysis of whether the same failing inputs or mutation patterns transfer across models performing the same task.",
          "evidence": "Section 5.3 (RQ1) and Table 2: all robustness experiments and robust accuracy results are reported per individual model, with no cross-model consistency or failure transfer analysis."
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates the proposed SENSEI method on 15 DNN models across 5 image datasets, demonstrating that SENSEI improves the robust accuracy of each individual model. However, the evaluation does not analyze cross-model consistency in how specific mutations or failing inputs relate to failures. The fuzzing (genetic algorithm) identifies 'worst variants' (mutations) to maximize loss specifically for the model being trained at a given epoch. The results show the aggregated robust accuracy for each trained model, but do not explore if the same failing inputs (generated perturbations) cause equivalent erroneous behaviors across different models, nor if specific mutation patterns systematically induce similar failure behaviors across models. The focus is on improving the robustness of each model independently, not on the transferability of failures.",
          "evidence": "Abstract: 'We instantiate this technique in two tools, SENSEI and SENSEI-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets. Our evaluation shows that SENSEI can improve the robust accuracy of the DNN, compared to the state of the art, on each of the 15 models, by upto 11.9% and 5.5% on average.'\n\nSection 3.1, Problem Formulation: 'min E(x,y)~D[max L(\u03b8, x + \u03b4, y)] \u03b8 \u03b4... SENSEI approximates the solution of this optimization problem by decoupling the inner maximization problem (which solves for \u03b4) from the outer minimization problem (which optimizes \u03b8). This is done by allowing the usual iterative epoch-based training schedule to optimize for, but in each epoch, for each training data x, solving the inner maximization problem to find the optimal variant x+\u03b4.'\n\nAlgorithm 2: Overall algorithm (lines 13-17 demonstrate that optimal variants `children` are generated and selected for each specific training input `X[i]` and used to replace it for training `X[i] := selectBest(children, f)`).\n\nSection 5.3.2, Exp-2: 'Table 2 presents core results of the paper, which shows the robust accuracy of all the models trained using the Standard, Random, W-10 and SENSEI strategy... However, SENSEI achieves the highest robust accuracy for all models of all data-sets (highlighted). SENSEI improved the robust accuracy from 8.2% to 18.7% w.r.t. random augmentation and from 1.7% to 6.1% w.r.t. state-of-the-art W-10.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper evaluates augmentation-identified failures and their mitigation separately on each model, without any analysis of whether the same failing inputs or mutation patterns transfer across models performing the same task.",
          "evidence": "Section 5.3 (RQ1) and Table 2: all robustness experiments and robust accuracy results are reported per individual model, with no cross-model consistency or failure transfer analysis."
        },
        "manual": {
          "value": "Low",
          "why": "No analysis of fault transferability in input level or mutation level"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "The paper enforces design constraints on transformations (e.g., rotation within \u00b130\u00b0, translation within \u00b110% of image size, brightness within \u00b132 units, etc.) to ensure variants remain visually plausible, but does not provide any empirical validation (no LPIPS/SSIM/BLEU scores or human evaluation) of the naturalness of the generated inputs.",
          "evidence": "Section 4.2 \u201cGeneration of Realistic Variations\u201d (p. 1149): \u201cTo make sure the translated images are visually similar to natural ones, we restrict the space of allowed perturbations following [8] \u2026 rotation(x,d): \u2026 within [\u201330, 30]; translation(x,d): \u2026 within [\u201310%, 10%] of image size; brightness(x,d): \u2026 within [\u201332, 32]; contrast(x,d): \u2026 within [0.8, 1.2].\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper explicitly states that its data augmentation strategy focuses on generating 'natural environmental variants' and 'realistic variations' through a set of predefined transformations with strictly bounded parameters. It includes design constraints to ensure generated inputs remain 'perceptually similar' and 'legal,' and actively prevents the generation of 'unrealistic' data. However, it does not provide empirical validation of naturalness using quantitative metrics like LPIPS, SSIM, or BLEU, nor does it conduct human evaluation of the fuzzed inputs' realism.",
          "evidence": "Page 2, Proposed technique: 'uses guided test generation techniques to address the data augmentation problem for robust generalization of DNNs under natural environmental variations. Specifically, we cast data augmentation problem as an optimization problem, and use genetic search on a space of the natural environmental variants of each training input data.'\n\nPage 3, Section 2.3 Robustness of DNNs: 'The allowed perturbations \u03b4 can be represented as a neighborhood S around input x, such that \u2200\u03b4 \u2208 S, x + \u03b4 constitutes legal input for M and is perceptually similar to x...'\n\nPage 5, Section 3.3.1 Representation of Chromosome: 'In SENSEI, we represent a chromosome as a set of operations that would be applied on a given input to get the realistic variations... For instance, we can derive a realistic variation (x') of image (x) by rotating x by one degree and then translating it by one pixel, simulating the angle and movement of camera in real life.'\n\nPage 5, Section 3.3.2 Generation of population: 'It should be noted that once a new chromosome is generated through the mutation or crossover, it is validated to make sure that it is within the range of each transformation that we set globally (line 12). Furthermore, SENSEI always applies the resulting transformation vector (chromosome) on the original image... to prevent the resulting data from being unrealistic.'\n\nPage 6, Section 4.2 Generation of Realistic Variations: 'SENSEI focuses on improving the robustness of DNN models by augmenting training data with natural environmental variations. Since, we focus on the applications with image, we choose two major kinds of image operations: i) geometric operations ii) color operations to simulate the camera movements and lighting conditions in real life. To make sure the translated images are visually similar to natural ones, we restrict the space of allowed perturbations following [8] where it is applicable. The operations and corresponding restrictions with respect to an image x are as follows: \u2022 rotation(x,d): rotate x by d degree within a range [-30, 30]. \u2022 translation(x, d): horizontally or vertically translate x by d pixels within a range of [-10%, 10%] of image size. \u2022 shear(x,d): horizontally shear x with a shear factor d within a range of [-0.1, 0.1]. \u2022 zoom(x,d): zoom in/out x with a zoom factor d ranging [0.9,1.1] \u2022 brightness(x,d): uniformly add or subtract a value d for each pixel of x within a range of [-32, 32] \u2022 contrast(x,d): scale the RGB value of each pixel of x by a factor d within range of [0.8, 1.2] These image operations preserve the content of original image.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "The paper enforces design constraints on transformations (e.g., rotation within \u00b130\u00b0, translation within \u00b110% of image size, brightness within \u00b132 units, etc.) to ensure variants remain visually plausible, but does not provide any empirical validation (no LPIPS/SSIM/BLEU scores or human evaluation) of the naturalness of the generated inputs.",
          "evidence": "Section 4.2 \u201cGeneration of Realistic Variations\u201d (p. 1149): \u201cTo make sure the translated images are visually similar to natural ones, we restrict the space of allowed perturbations following [8] \u2026 rotation(x,d): \u2026 within [\u201330, 30]; translation(x,d): \u2026 within [\u201310%, 10%] of image size; brightness(x,d): \u2026 within [\u201332, 32]; contrast(x,d): \u2026 within [0.8, 1.2].\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "coonstrained transformation but no naturalness measure of the fuzzed inputs"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ],
      "oracle": [
        "Metamorphic",
        "Property-based"
      ]
    }
  },
  "You et al. - 2023 - Regression Fuzzing for Deep Learning Systems": {
    "key": "you2023regression",
    "name": "DRFuzz",
    "year": "2023",
    "citation_count": "27",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "DRFuzz uncovers regression faults characterized by misclassifications between model versions, which are low-level functional errors (i.e., cases where the prior version predicts correctly but the updated version predicts incorrectly). There is no evidence of revealing policy or security boundary violations or normative/ethical misbehaviors.",
          "evidence": "Page 83, Definition 2: \u201cx is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "DRFuzz is designed to detect 'regression faults' in Deep Learning (DL) systems, which are explicitly defined as instances where a test input is correctly predicted by a prior model version but wrongly predicted by its regression (evolved) model version. This directly corresponds to triggering low-level functional errors, specifically misclassification in discriminative models, rather than uncovering behavioral misalignments or bypassing safety/security mechanisms.",
          "evidence": "Page 2, Section II, Definition 2: 'Regression fault. Given a prior version model M1, its regression model M2, a test input x, and its ground truth label y, x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., CM1[x] = y ^ CM2[x] \u2260 y. In addition, we denote x as the regression fault triggering input and the faulty behavior triggered by x as (CM1[x] \u2192 CM2[x]).'\nPage 8, Section IV.B.1, 'Regression Explication based on DRFuzz': 'From Figure 3(a), we can observe that Classes 7, 8, and 4 contain the most regression faults, which means that the regression process may induce overfitting in these classes. Figure 3(b) shows that inputs of class 1 are wrongly predicted as class 8, inputs of class 9 are wrongly predicted as class 4, and inputs of classes 1, 2, and 9 tend to be predicted as 7 due to the regression process. The prediction results and the faulty behaviors facilitate explaining the bias (such as overfitting) in each class to judge if the regression process contributes to data relevance.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "DRFuzz uncovers regression faults characterized by misclassifications between model versions, which are low-level functional errors (i.e., cases where the prior version predicts correctly but the updated version predicts incorrectly). There is no evidence of revealing policy or security boundary violations or normative/ethical misbehaviors.",
          "evidence": "Page 83, Definition 2: \u201cx is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Medium",
          "why": "DRFuzz is guided to find a specific class of faults\u2014regression faults where the new model misclassifies inputs that the prior model classified correctly\u2014but it does not aim to force a particular target label. It steers exploration toward regression-induced misclassifications without constraining the exact wrong output, corresponding to a medium level of directedness.",
          "evidence": "Section I (page 82): \u201cDetecting regression faults refers to finding test inputs that make the current version produce wrong predictions but make the prior version produce correct predictions.\u201d\nSection III-C (page 84): \u201cDRFuzz guides the process of generating new test inputs toward the direction of amplifying the prediction difference between versions.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "DRFuzz performs untargeted exploration to reveal generic faults. Its primary goal is to find 'regression faults,' which are defined as test inputs that were correctly predicted by a prior model version (M1) but are wrongly predicted by the current model version (M2). This definition focuses on the correctness of the prediction changing to incorrectness, without specifying what the 'wrong' prediction should be or aiming for higher-level semantic faults like bias or unsafe behaviors. The fuzzer searches broadly for any input that exhibits this specific type of misclassification, rather than guiding towards a particular target output or semantic error.",
          "evidence": "Section I. Introduction, Page 1: 'Detecting regression faults refers to finding test inputs that make the current version produce wrong predictions but make the prior version produce correct predictions.'\nSection II. Definition 2. Regression fault, Page 2: 'x is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., CM\u2081 [x] = y \u2227 CM\u2082 [x] \u2260 y. In addition, we denote x as the regression fault triggering input and the faulty behavior triggered by x as (CM\u2081 [x] \u2192 CM\u2082 [\u0445]).'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "DRFuzz does not aim to force a particular target label or semantic behavior; it broadly searches for any inputs that cause the new model to misclassify examples previously classified correctly, without constraining the exact wrong output. This untargeted exploration for generic regression misclassifications aligns with the 'Low' level of goal directedness.",
          "evidence": "Section I (page 82): \u201cDetecting regression faults refers to finding test inputs that make the current version produce wrong predictions but make the prior version produce correct predictions.\u201d\nSection II, Definition 2 (page 83): \u201cx is called to trigger a regression fault if x is correctly predicted by M1, yet wrongly predicted by M2, i.e., cM1[x] = y \u2227 cM2[x] \u2260 y.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "Intargeted exploration for generic regression misclassifications - any inputs that cause the new model to misclassify examples previously classified correctly"
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Medium",
          "why": "DRFuzz goes beyond merely reporting faulty inputs by analyzing the patterns of regression faults at the class and behavior level (e.g., identifying which classes suffer the most faults and the most common mis\u2010predictions), providing heuristic explanations of where and how failures arise (class\u2010level bias and overfitting). However, it does not perform deeper diagnostic analysis of internal model signals (activations, gradients, decision boundaries) to establish causal links to architectural, training, or data\u2010related weaknesses.",
          "evidence": "Section IV-B1 (p. 89): \u201cFrom Figure 3(a)\u2026 Classes 7, 8, and 4 contain the most regression faults, which means that the regression process may induce overfitting in these classes.\u2026 Figure 3(d) shows that class 8 (ship) is likely to be wrongly predicted as 0 (airplane), and animal\u2010related classes such as 5 (dog), 6 (frog), and 2 (bird) are easily misclassified as 3 (cat). The prediction results and the faulty behaviors facilitate explaining the bias (such as overfitting) in each class\u2026\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "DRFuzz explains where and how failures manifest through external interpretation of behavioral patterns (e.g., class-level bias and overfitting) and provides correlational analysis of internal signals (feature activations via heatmaps) to link important features to faulty behaviors. It identifies specific misclassifications and discusses how the regression process may induce overfitting in certain classes. However, it stops short of establishing direct causal links between these failures and specific architectural, training, or data-related weaknesses within the model.",
          "evidence": "Page 8, Section IV.B.1, 'Regression Explication based on DRFuzz': 'We further focus on utilizing regression faults (#RF) to explain the bias and risks induced in the regression process...From Figure 3(a), we can observe that Classes 7, 8, and 4 contain the most regression faults, which means that the regression process may induce overfitting in these classes. Figure 3(b) shows that inputs of class 1 are wrongly predicted as class 8, inputs of class 9 are wrongly predicted as class 4, and inputs of classes 1, 2, and 9 tend to be predicted as 7 due to the regression process... The prediction results and the faulty behaviors facilitate explaining the bias (such as overfitting) in each class to judge if the regression process contributes to data relevance.'\n\nPage 10, Section V.B, 'Diversity Analysis': 'To assist the visual analysis, we use the Gradient-weighted Class Activation Map (heatmap) [61] to visualize the importance of each feature in the prediction process. Figure 5 presents heatmaps of four regression faults mutated from the same initial seed... The important features of Figure 5(a) and Figure 5(b) for faulty behavior (1\u21928) lie in the center of the images. The important features of Figure 5(c) and Figure 5(d) for faulty behavior (1\u21929) lie on the left of the images. The phenomena above show that different faulty behaviors may be caused by different features, further indicating that diversity criteria based on faulty behaviors (proposed in Section III-A) can reflect the diversity of fault-triggering test inputs to some extent.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DRFuzz goes beyond merely reporting faulty inputs by analyzing the patterns of regression faults at the class and behavior level (e.g., identifying which classes suffer the most faults and the most common mis\u2010predictions), providing heuristic explanations of where and how failures arise (class\u2010level bias and overfitting). However, it does not perform deeper diagnostic analysis of internal model signals (activations, gradients, decision boundaries) to establish causal links to architectural, training, or data\u2010related weaknesses.",
          "evidence": "Section IV-B1 (p. 89): \u201cFrom Figure 3(a)\u2026 Classes 7, 8, and 4 contain the most regression faults, which means that the regression process may induce overfitting in these classes.\u2026 Figure 3(d) shows that class 8 (ship) is likely to be wrongly predicted as 0 (airplane), and animal\u2010related classes such as 5 (dog), 6 (frog), and 2 (bird) are easily misclassified as 3 (cat). The prediction results and the faulty behaviors facilitate explaining the bias (such as overfitting) in each class\u2026\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "does not analyze the root cause of regression faults, acknowledging that identifying their causes remains an open challenge. It provides behavior-level explanations (e.g., which classes or behaviors regress, per-class fault heatmaps, and faulty-behavior tuples). Interprets regression faults as evidence of bias or overfitting (e.g., specific classes suffering more after retraining)."
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "High",
          "why": "DRFuzz provides empirical evidence of naturalness preservation via a GAN\u2010based fidelity assurance filter and a human evaluation demonstrating that 86% of its generated inputs are judged high\u2010fidelity (compared to 57.5% without the filter).",
          "evidence": "\u201cWe sample 100 inputs generated by DRFuzz and DRFuzz_NG, respectively, and two authors manually evaluate their fidelity\u2026 They label 86% of inputs generated by DRFuzz as high-fidelity and only 57.5% of inputs generated by DRFuzz_NG as high-fidelity.\u201d (Section IV-B2, page 90)"
        },
        "gemini": {
          "value": "High",
          "why": "The paper provides empirical evidence for naturalness preservation through human evaluation of generated inputs. It explicitly states that two authors manually evaluated the fidelity of inputs, categorized as high-fidelity, and reported a Cohen's Kappa coefficient. Furthermore, the method incorporates a GAN-based fidelity assurance technique designed to consider semantic similarity over pixel-wise differences, a design choice aiming for realistic inputs. Visual examples of low-fidelity inputs are also provided to illustrate the problem being addressed.",
          "evidence": "Page 2, Section I, paragraph 3: 'Fidelity: It is important to ensure the fidelity of each fault-triggering test input (i.e., preserving original semantics and looking natural to humans) since such test inputs are more concerned by developers.'\nPage 3, Section III-B, paragraph 1: 'To mitigate the limitations of existing methods, we adopt Generative Adversarial Network (GAN) for fidelity assurance. ... The Discriminator will be used to assure the fidelity of test inputs generated by DRFuzz, as it considers semantic similarity [1] rather than strict pixel-wise difference.'\nPage 9, Section IV-B2, paragraph 3: 'We further manually evaluate the fidelity of inputs obtained with and without the fidelity assurance mechanism. Specifically, we sample 100 inputs generated by DRFuzz_NG and DRFuzz, respectively, and two authors manually evaluate their fidelity individually. The Cohen's Kappa coefficient between their evaluation is 0.67. They label 86% of inputs generated by DRFuzz as high-fidelity and only 57.5% of inputs generated by DRFuzz_NG as high-fidelity. This indicates that the GAN-based Fidelity Assurance technique can filter out more than 20% of fault-triggering inputs with low fidelity.'\nPage 9, Figure 4: 'Inputs generated by DRFuzz (on the left of each figure) and DRFuzz_NG (on the right of each figure)' (showing blurry, noisy, and over-changed inputs)."
        },
        "arbitrator": {
          "value": "High",
          "why": "DRFuzz provides empirical evidence of naturalness preservation via a GAN\u2010based fidelity assurance filter and a human evaluation demonstrating that 86% of its generated inputs are judged high\u2010fidelity (compared to 57.5% without the filter).",
          "evidence": "\u201cWe sample 100 inputs generated by DRFuzz and DRFuzz_NG, respectively, and two authors manually evaluate their fidelity\u2026 They label 86% of inputs generated by DRFuzz as high-fidelity and only 57.5% of inputs generated by DRFuzz_NG as high-fidelity.\u201d (Section IV-B2, page 90)"
        },
        "manual": {
          "value": "High",
          "why": "Every batch of mutated inputs is passed through the GAN discriminator and only natural mutated inputs are kept for execution and further mutation."
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "DRFuzz\u2019s faults arise from semantic\u2010level transformations (e.g. image scaling, translation, brightness/contrast adjustment) and modest pixel\u2010level perturbations on continuous float representations, but the paper does not perform explicit clipping or rounding to ensure that these perturbations survive standard I/O serialization. As a result, some of the discovered faults may not persist through I/O formats like 8\u2010bit image encoding.",
          "evidence": "Section III-C (Mutation) and Table I (page 85) describe 16 pixel-level and image-level mutation rules applied on normalized float pixels (range [0,1]) without any subsequent clipping or rounding step before feeding inputs back to the model."
        },
        "gemini": {
          "value": "Low",
          "why": "The method performs both pixel-level and image-level mutations on normalized, continuous pixel values (range [0,1]). The paper does not describe any explicit mechanism, such as clipping or rounding, applied *after* mutation to ensure that the fine-grained changes or specific perturbations leading to a regression fault will persist if the fuzzed inputs undergo standard I/O operations (e.g., saving and loading an image, which often involves quantization to integer pixel values). While a GAN model is used for 'fidelity assurance' to make inputs resemble natural ones, this does not guarantee numerical stability against precision loss during I/O.",
          "evidence": "Page 4, Section III-B, 'GAN-based Fidelity Assurance': 'DRFuzz first pre-processes the training set to normalize each pixel value into the range of [0,1]... For each given input, the well-trained Discriminator can output a score in the range of [0, 1], where a higher score indicates the given input resembles the training set with higher fidelity.'\nPage 4, Section III-C1, 'Mutation Rules': 'Pixel-Level Mutation aims to change a small set of pixels in an image to generate a new test input. It achieves the goal of preserving the semantics of a test input by making slight pixel-wise changes on the whole image.'\nPage 4, Table I, 'MUTATION RULES': Lists both 'Pixel-Level Mutation Rule' (e.g., Pixel Adding Gaussian Noise) and 'Image-Level Mutation Rule' (e.g., Image Scaling, Image Brightness Adjustment). These rules modify pixel values or apply transformations to the continuous representations.\nNo explicit mention of clipping, rounding, or other I/O stability validation post-mutation is found throughout the technical and methodological sections of the paper."
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DRFuzz generates regression\u2010triggering inputs using both semantic\u2010level transformations (e.g. image scaling, rotation, brightness/contrast adjustment, blurring, dilation/erosion) and smaller pixel\u2010level perturbations (e.g. Gaussian and salt & pepper noise, patch coloring/reversal). While semantic\u2010level changes are inherently stable under standard I/O (they survive quantization when saving an 8-bit image), the paper applies all mutations on floating-point representations without any explicit clipping or rounding step to guarantee I/O stability. This mix of large, semantic transformations (likely to persist) and fine\u2010grained noise (likely to vanish under quantization) corresponds to a \u201cMedium\u201d persistence rating.",
          "evidence": "Section III-C1 (Mutation Rules) and Table I (page 85) list 16 mutation operators, including both image-level transforms (scaling, translation, rotation, brightness/contrast adjustment, blurring, dilation, erosion) and pixel-level noise/patch operations, applied directly on normalized float pixels without subsequent clipping or rounding before model ingestion."
        },
        "manual": {
          "value": "Medium",
          "why": "Generates and evaluates mutated inputs entirely in memory using both pixel level and image level semantic mutations. They use GAN-based fidelity filtering to ensure naturalness but does not ensure if the mutation will survive I/O."
        }
      },
   
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "DRFuzz evaluates regression faults only within individual model version pairs and does not analyze whether the same failing inputs manifest equivalent erroneous behaviors across different models performing the same task.",
          "evidence": "Section IV-A1 (Subjects) and IV-B1 (Overall Effectiveness): experiments are conducted independently on four dataset\u2013model pairs (MNIST\u2013LeNet5, CIFAR10\u2013VGG16, FM\u2013AlexNet, SVHN\u2013ResNet18) with no cross\u2010model transferability analysis."
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates the proposed DRFuzz method across multiple Deep Learning (DL) models and datasets (referred to as 'subjects') under various regression scenarios. However, it treats each model/dataset pair and scenario as an independent evaluation unit. The results presented in the tables (e.g., Table III and IV) show the effectiveness of DRFuzz in detecting regression faults for each subject and scenario, or averages across them, but do not provide any analysis of whether specific failing inputs discovered on one model cause equivalent erroneous behaviors on other models (High transferability) or if the same mutation patterns systematically induce similar failure behaviors across different models (Medium transferability). The evaluation focuses on the performance metrics (e.g., number of regression faults detected) for each independent test case (subject/scenario), without analyzing cross-model consistency in how mutations relate to failures or the transferability of those failures.",
          "evidence": "Page 6, Section IV.A.1, 'Subjects': 'To sufficiently evaluate the effectiveness of DRFuzz, we used four pairs of datasets and DNN models as subjects, i.e., LeNet-5 on MNIST, VGG16 on CIFAR10, AlexNet on Fashion-MNIST (FM), and ResNet18 on SVHN, which have been widely used in the existing studies on DL testing [17], [33]-[36].'\n\nPage 8, Section IV.B.1, 'Effectiveness of DRFuzz: Overall Effectiveness': 'The overall effectiveness results of DRFuzz are presented in Table III. Due to the limited space and large amounts of subjects used in the study, we put detailed results on each subject on our project homepage. From Table III we can observe that DRFuzz performs the best on average across all subjects and scenarios in terms of all metrics, with significant superiority over DiffChaser and DeepHunter.'\n\nPage 8, Table IV: 'EFFECTIVENESS ON DIFFERENT REGRESSION SCENARIOS' (Shows metrics like #RFI, #RF, #Seed, #GF for each Regression Scenario and Approach, independently for each scenario, without cross-model fault transfer analysis.)"
        },
        "arbitrator": {
          "value": "Low",
          "why": "DRFuzz evaluates regression faults only within individual model version pairs and does not analyze whether the same failing inputs manifest equivalent erroneous behaviors across different models performing the same task.",
          "evidence": "Section IV-A1 (Subjects) and IV-B1 (Overall Effectiveness): experiments are conducted independently on four dataset\u2013model pairs (MNIST\u2013LeNet5, CIFAR10\u2013VGG16, FM\u2013AlexNet, SVHN\u2013ResNet18) with no cross\u2010model transferability analysis."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Differential"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ]
    }
  },
  "Zhang et al. - 2019 - Life after Speech Recognition Fuzzing Semantic Misinterpretation for Voice Assistant Applications": {
    "key": "zhang2019life",
    "name": "LipFuzzer",
    "year": "2019",
    "citation_count": "79",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "High",
          "why": "LipFuzzer uncovers vulnerabilities that bypass explicit safety, policy, or security mechanisms of voice assistant platforms\u2014specifically, it finds \u2018LAPSUS\u2019 inputs that hijack vApp installation or invocation, enabling malicious apps to be installed or invoked without user intent, leading to privacy leakage and phishing.",
          "evidence": "Page 1, Abstract: \u201cWe find that the semantic inconsistency caused by the improper semantic interpretation of an Intent Classifier can create the opportunity of breaching the integrity of vApp processing\u2026\u201d; Page 5, Section I-D (Attack consequences), Privacy Leakage: \u201ca Portland family\u2019s Alexa device accidentally captured a private conversation, and then sent the audio recording to someone whose number was stored in the family\u2019s contact list\u201d; Phishing: \u201cAudio can thus be manipulated by an attacker at runtime, substituting a benign audio file with a 'malicious' audio file \u2026 please answer with your account username and password to know more.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The fuzzer, LipFuzzer, is designed to uncover semantic misinterpretations in Voice Assistant (VA) applications that lead to severe security vulnerabilities. The paper explicitly describes attack consequences such as 'Denial of Service' (e.g., hijacking a device-controlling vApp), 'Privacy Leakage' (e.g., unintended disclosure of private conversations), and 'Phishing' (e.g., manipulating audio files to solicit sensitive user information). These types of failures directly bypass explicit safety, policy, or security mechanisms, which aligns with the 'High' severity definition.",
          "evidence": "Section: II.D Threat Model, Page: 5\nQuote: \"Attack consequences. We summarize a list of example consequences of interacting with malicous vApps as follows: Denial of Service: This would occur if a device-controlling vApp is hijacked. For example, when a user says \"Alexa, close the garage door\" to a mismatched malicious vApp instead of a legitimate one, the garage door may be left open. Privacy Leakage: vApps intimately connect users' daily life to a more exposed digital world. By interacting with improper vApps, private data could be handled unexpectedly. For example, as reported in 2018, a Portland family's Alexa device accidentally captured a private conversation, and then sent the audio recording to someone whose number was stored in the family's contact list [4]. Phishing: Third-party vApps' back-end processing is fully controlled by the developers... audio can thus be manipulated by an attacker at runtime, substituting a benign audio file with a \u201cmalicious\" audio file... 'Sorry, a critical problem occurred in your Amazon Pay, please answer with your account username and password to know more.'\""
        },
        "arbitrator": {
          "value": "High",
          "why": "LipFuzzer uncovers vulnerabilities that bypass explicit safety, policy, or security mechanisms of voice assistant platforms\u2014specifically, it finds \u2018LAPSUS\u2019 inputs that hijack vApp installation or invocation, enabling malicious apps to be installed or invoked without user intent, leading to privacy leakage and phishing.",
          "evidence": "Page 1, Abstract: \u201cWe find that the semantic inconsistency caused by the improper semantic interpretation of an Intent Classifier can create the opportunity of breaching the integrity of vApp processing\u2026\u201d; Page 5, Section I-D (Attack consequences), Privacy Leakage: \u201ca Portland family\u2019s Alexa device accidentally captured a private conversation, and then sent the audio recording to someone whose number was stored in the family\u2019s contact list\u201d; Phishing: \u201cAudio can thus be manipulated by an attacker at runtime, substituting a benign audio file with a 'malicious' audio file \u2026 please answer with your account username and password to know more.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "It does not bypass permissions or authentication; instead, it tricks the VA into misinterpreting intent. The resulting misbehavior (e.g., invoking the wrong vApp or disclosing data to a malicious app). Expose semantically incorrect but functionally valid behaviors that breach user-intent and specification expectations\u2014posing integrity and trust risks without directly bypassing safety or security mechanisms."
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "High",
          "why": "LipFuzzer is explicitly designed to steer its fuzzing toward causing specific, predefined misinterpretations of user intents (e.g. forcing the Intent Classifier to install or launch a malicious vApp in place of the intended one). It does not perform broad, untargeted exploration but deliberately generates adversarial voice commands to trigger precise faults in the intent\u2010matching process.",
          "evidence": "Page 1, Section I (\u201cOur Approach\u201d): \u201cwe propose a novel linguistic\u2010model\u2013guided fuzzing tool, called LipFuzzer. Our tool generates potential voice commands that are likely to incur a semantic inconsistency such that a user reaches an unintended vApp/functionality \u2026 LipFuzzer is able to perform effective mutation\u2010based fuzzing on seed template inputs.\u201d\nPage 6, Section IV-C (\u201cTemplate Fuzzing\u201d): \u201cThe Mutation Engine mutates the fields such as phonemes, words, or grammar entities based on LAPSUS Models.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "LipFuzzer is designed to steer its exploration towards specific, predefined faults. The primary goal is to identify linguistic errors (LAPSUS) that cause the Natural Language Understanding (NLU) component's Intent Classifier to misinterpret a user's command, leading to the invocation of an *unintended vApp or functionality*. The paper demonstrates this by actively trying to cause the system to invoke a *malicious vApp* instead of a legitimate one, effectively forcing the classifier to predict a chosen, incorrect label (the malicious vApp's intent).",
          "evidence": "Page 1, Abstract: 'We find that the semantic inconsistency caused by the improper semantic interpretation of an Intent Classifier can create the opportunity of breaching the integrity of vApp processing when attackers delicately leverage some common spoken errors.'\nPage 2, I. INTRODUCTION: 'Our tool generates potential voice commands that are likely to incur a semantic inconsistency such that a user reaches an unintended vApp/functionality (i.e. users think they use voice commands correctly but yield unwanted results).'\nPage 4, C. Motivating Example & Figure 3: 'An adversary intentionally crafted his/her vApp with a confusing name of \"The Truth Bank Skill\". As a result, as shown in Figure 3, a new leaf node of \"truth\" is injected into the intent classification tree. Thus, if a user accidentally speaks \"Alexa, install The Truth Bank Skill\", the malicious vApp will be installed and executed (with few or no indication of which vApp is running).'\nPage 9, V. IMPLEMENTATION, Verification: 'We define that a LAPSUS is verified to be effective when it is incorrectly interpreted by the VA system. For example, if the vApp is not installed after an installation-related LAPSUS is played to VA devices, then this LAPSUS is effective.'\nPage 13, D. Case Study, 2) Attacking \"True Bank\": 'To demonstrate the practicality of our linguistic-model-guided fuzzing, we show a real attack case study... To attack it, we use LipFuzzer-generated LAPSUS to create 4 malicious vApps which were uploaded to the Alexa Skill Store.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "LipFuzzer is explicitly designed to steer its fuzzing toward causing specific, predefined misinterpretations of user intents (e.g. forcing the Intent Classifier to install or launch a malicious vApp in place of the intended one). It does not perform broad, untargeted exploration but deliberately generates adversarial voice commands to trigger precise faults in the intent\u2010matching process.",
          "evidence": "Page 1, Section I (\u201cOur Approach\u201d): \u201cwe propose a novel linguistic\u2010model\u2013guided fuzzing tool, called LipFuzzer. Our tool generates potential voice commands that are likely to incur a semantic inconsistency such that a user reaches an unintended vApp/functionality \u2026 LipFuzzer is able to perform effective mutation\u2010based fuzzing on seed template inputs.\u201d\nPage 6, Section IV-C (\u201cTemplate Fuzzing\u201d): \u201cThe Mutation Engine mutates the fields such as phonemes, words, or grammar entities based on LAPSUS Models.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "untargeted exploration to expose semantic or intent-classification inconsistencies, sometimes aligning with attacker intent (e.g., LAPSUS examples), though its discovery of attacker intent is incidental rather than goal-optimized."
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Medium",
          "why": "The paper goes beyond merely reporting which voice commands fail\u2014it systematically correlates failure cases with specific linguistic mutation types (pronunciation, vocabulary, grammar) by building statistical LAPSUS Models and interpreting failure patterns externally (e.g. blends, category approximations, portmanteaux). However, it does not probe internal model activations, decision boundaries, or gradients to establish causal links to architectural or training weaknesses within the NLU component.",
          "evidence": "Section IV (Linguistic Modeling) and Table I: the authors construct Bayesian-network\u2013based LAPSUS Models that categorize misinterpretations into pronunciation-level, vocabulary-level, and grammar-level mutations (\u201cBlends\u2026Morph-exchange\u2026Portmanteaux\u201d), and Section VI-A reports how these correlated patterns lead to semantic inconsistencies in the Intent Classifier without inspecting its internal workings."
        },
        "gemini": {
          "value": "High",
          "why": "The paper provides a high level of root-cause insight by identifying the Natural Language Understanding (NLU)'s Intent Classifier, and specifically its template-based design, as the core architectural weakness leading to semantic misinterpretations and vulnerabilities. It explains *why* these vulnerabilities occur by detailing how developer-defined voice command templates modify the intent classification tree and how the system's fuzzy matching capabilities interact with common linguistic errors (LAPSUS). The proposed method, LipFuzzer, is designed to systematically model and discover these linguistic vulnerabilities, linking them directly to the design choices of the Intent Classifier.",
          "evidence": "Page 1, Abstract: \"our work analyzes and evaluates the security of the succeeding component after ASR, i.e., Natural Language Understanding (NLU), which performs semantic interpretation (i.e., text-to-intent) after ASR's acoustic-to-text processing. In particular, we focus on NLU's Intent Classifier which is used in customizing machine understanding for third-party VA Applications (or vApps). We find that the semantic inconsistency caused by the improper semantic interpretation of an Intent Classifier can create the opportunity of breaching the integrity of vApp processing when attackers delicately leverage some common spoken errors.\"\n\nPage 3, Section II.A 'Natural Language Understanding (NLU) of vApp', paragraph 2: \"From the vApp security perspective, the Intent Classifier plays a more important role when interpreting users' voice commands. The reason is that the Intent Classifier is the last step of the interpretation process. It has the capability of not only determining users' semantic intents, but also fixing the ASR's potential transcription errors. Second, while ASR is a default built-in service component, the construction of the Intent Classifier's semantic classification is contributed by both vApp developers and service providers. Particularly, third-party developers can upload voice command templates to modify the unified intent classification tree used by all users... As a result, it creates the opportunity for misbehaving developers to maliciously modify the intent matching process of the NLU.\"\n\nPage 11, Section VI.A 'Intent Classifier Evaluation', paragraph 3: \"In conclusion, we empirically verified that the Intent Classifier contributes most in the problem of semantic misinterpretation.\"\n\nPage 13, Section D 'Case Study', paragraph 2 (Security Implication): \"The aforementioned results show that simple and unique names are more difficult to be misspoken according to our linguistic models. With the current template-based Intent Classifier design, using simple and unique voice commands is the most effective way of preventing vApp squatting attacks.\""
        },
        "arbitrator": {
          "value": "High",
          "why": "The paper goes beyond simply cataloging mis\u00adrecognized utterances\u2014it pinpoints the Intent Classifier\u2019s template\u00adbased architecture and fuzzy matching design as the fundamental cause of semantic misinterpretations. By analyzing how developer-supplied templates are aggregated into a unified intent tree, how invocation names must exactly match pre-installation templates, and how fuzzy matching is selectively applied, the authors establish a clear causal link between these architectural choices and the exploitable vulnerabilities. This constitutes a deep diagnostic insight into the NLU component\u2019s structure, not merely a correlation of failure symptoms.",
          "evidence": "Page 1 (Abstract): \u201cour work analyzes and evaluates \u2026 Natural Language Understanding \u2026 In particular, we focus on NLU\u2019s Intent Classifier \u2026 We find that the semantic inconsistency caused by the improper semantic interpretation of an Intent Classifier can create the opportunity of breaching the integrity of vApp processing.\u201d\n\nPage 3 (Section II.A): \u201cFrom the vApp security perspective, the Intent Classifier plays a more important role \u2026 third-party developers can upload voice command templates to modify the unified intent classification tree \u2026 it creates the opportunity for misbehaving developers to maliciously modify the intent matching process of the NLU.\u201d\n\nPage 11 (Section VI.A \u2018Intent Classifier Evaluation\u2019): \u201cIn conclusion, we empirically verified that the Intent Classifier contributes most in the problem of semantic misinterpretation.\u201d\n\nPage 13 (Section D \u2018Case Study\u2019 \u2013 Security Implication): \u201cWith the current template-based Intent Classifier design, using simple and unique voice commands is the most effective way of preventing vApp squatting attacks.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Empirically isolates the NLU Intent Classifier as the primary source of error and correlates misinterpretation patterns with linguistic mutation types"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "High",
          "why": "LipFuzzer\u2019s mutated inputs are grounded in real human speech patterns and validated through a user study: the authors collect recordings from 150 Mechanical Turk workers, measure how often each hypothesized LAPSUS occurs in human repetitions, and use those frequencies to guide and refine their mutations. This constitutes empirical, human\u2010evaluation evidence that the fuzzed inputs remain realistic within the voice\u2010command domain.",
          "evidence": "Section V-B (User Study) and Section IV-B (Weight Training): \u201cIn this user study, we find sentences or short expressions which contain the states in the models. Then, we ask users in the study to repeat these sentences or expressions \u2026 Then the probabilities of the transitions are calculated accordingly.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The fuzzer, LipFuzzer, explicitly designs its mutation process to generate realistic and plausible speech errors, referred to as LAPSUS. This is achieved by building and training linguistic models with empirical data derived from human user studies and established linguistic knowledge. The core mechanisms ensuring naturalness include:\n1.  **Linguistic Modeling**: LipFuzzer's mutation engine is guided by 'LAPSUS Models' which are Bayesian Networks formulated from 'linguistic knowledge' collected from academic materials and English teaching websites, covering known categories of human speech errors like pronunciation, vocabulary, and grammar (Page 5, 'B. Our solution: LipFuzzer - Linguistic Modeling'; Page 4, 'B. Speech Errors and LAPSUS').\n2.  **Weight Training from User Studies**: The LAPSUS Models are 'trained with statistical weights which measure how likely it is that a state transition...would take place.' This training data is obtained through a 'user study with audio recording,' where users repeat sentences, and the probabilities of observed error transitions are calculated accordingly (Page 6, 'LAPSUS Models'; Page 8, 'Weight Training'). This directly integrates human empirical data into the model's likelihoods, guiding the generation of naturalistic errors.\n3.  **Validation with Real LAPSUS**: The paper empirically confirms the existence of LAPSUS in real-world user interactions, finding that '38.14% of the recorded voice commands are LAPSUS' based on user study data and providing concrete examples in Table III (Page 10, '2) What are the real LAPSUS?'). These real-world observations inform and validate the fuzzer's underlying models.\n4.  **Probability Cutoff for Effective LAPSUS**: LipFuzzer utilizes a 'probability cutoff' mechanism to filter out low-likelihood mutations, aiming to generate 'effective LAPSUS' that are more likely to be spoken by users and cause misinterpretation (Page 11, '2) Cutoffs for Different Query Strategies' and 'The Cutoff Value'). This strategy ensures that the generated inputs are not arbitrary but plausible.\n\nWhile the paper does not explicitly present quantitative naturalness metrics (e.g., LPIPS, SSIM, BLEU) or a separate human evaluation specifically rating the naturalness of the fuzzed *outputs*, its entire methodology is founded on generating inputs that mimic real human speech errors. The extensive use of human user study data for model training and validation serves as strong empirical evidence that the fuzzer's inputs are 'perceptually or semantically consistent with real-world data' in terms of mimicking realistic perturbations.",
          "evidence": "Page 5, 'B. Our solution: LipFuzzer - Linguistic Modeling': 'To overcome the difficulty of locating mutation fields, we convert vague voice commands (textual data) into computational fine-grained linguistic data using NLP. Specifically, the textual data are processed into a three-level linguistic structure that is commonly used in studying linguistic speech errors [21]: pronunciation, vocabulary, and grammar. Thus, we design the Linguistic Modeling component of LipFuzzer.'\nPage 6, 'LAPSUS Models': 'Our LAPSUS Models can be used to answer both how a LAPSUS will be generated, based on seed voice commands... as well as the weights of these mutations.'\nPage 8, 'Weight Training': 'After we have the initial BN...ready, we further train the weight (i.e., the probabilities of successful transition states) through a user study with audio recording. In this user study, we find sentences or short expressions which contains the states in the models...Next, we calculate how many times these transitions are observed. Then the probabilities of the transitions are calculated accordingly.'\nPage 10, 'TABLE III: LAPSUS Examples Collected from Real Users': (Table showing 'Correct Form' and 'LAPSUS' examples collected from real users).\nPage 10, '2) What are the real LAPSUS?': 'The goal of this experiment is to confirm the existence of LAPSUS in the real world. From the collected audio records...38.14% of the recorded voice commands are LAPSUS.'\nPage 11, 'The Cutoff Value': 'By applying this cutoff value, we cut off 49.9% of generated LAPSUS population with the effectiveness rate increased to 59.52%.' "
        },
        "arbitrator": {
          "value": "High",
          "why": "LipFuzzer\u2019s mutated inputs are grounded in real human speech patterns and validated through a user study: the authors collect recordings from 150 Mechanical Turk workers, measure how often each hypothesized LAPSUS occurs in human repetitions, and use those frequencies to guide and refine their mutations. This constitutes empirical, human\u2010evaluation evidence that the fuzzed inputs remain realistic within the voice\u2010command domain.",
          "evidence": "Section V-B (User Study) and Section IV-B (Weight Training): \u201cIn this user study, we find sentences or short expressions which contain the states in the models. Then, we ask users in the study to repeat these sentences or expressions \u2026 Then the probabilities of the transitions are calculated accordingly.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "They perform human study to train the linguistic model that help selection potential fault triggering input. However, the synthesized voice commands are not measured for naturalness"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "High",
          "why": "LipFuzzer\u2019s discovered faults arise from mutations at the text (template) level and are exercised via machine-synthesized speech. These inputs are self-contained, finalized artifacts (produced by TTS) that remain stable across I/O operations rather than fleeting floating-point perturbations that could be lost during quantization or encoding.",
          "evidence": "Section I (page 1): \u201cThen, we input mutated voice commands to VA devices by using machine speech synthesis to eliminate the human factor of producing ASR-related misinterpretation.\u201d; Section V.A (page 9): \u201cWe direct input fuzzing results generated with LipFuzzer to the above mentioned services and perform a Text-to-Speech conversion.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The method employs machine speech synthesis to generate the fuzzed voice commands from linguistically mutated text. According to the metric definition, \"model-generated fuzzed inputs\u2014such as synthesized speech...\u2014are inherently stable under I/O operations because they originate from self-contained generation processes that produce finalized artifacts rather than transient numeric states.\" Since the fuzzed inputs are synthesized audio files, they represent finalized artifacts that are inherently stable across I/O transformations, ensuring the persistence of discovered faults.",
          "evidence": "Page 2, Section 'Our Approach': 'Then, we input mutated voice commands to VA devices by using machine speech synthesis to eliminate the human factor of producing ASR-related misinterpretation.'\nPage 9, Section 'V. IMPLEMENTATION', Subsection 'Speech Synthesis': 'For machine-generated voice commands, we use two types of speech synthesis methods to generate LAPSUS. The first speech synthesis method (for Phoneme-level) is phoneme-to-speech... We use each vApp platforms' native speech synthesis tools: Amazon Polly [6] and Google Cloud TTS [9]. The second speech synthesis method is for vocabulary and grammar levels. We direct input fuzzing results generated with LipFuzzer to the above mentioned services and perform a Text-to-Speech conversion.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "LipFuzzer\u2019s discovered faults arise from mutations at the text (template) level and are exercised via machine-synthesized speech. These inputs are self-contained, finalized artifacts (produced by TTS) that remain stable across I/O operations rather than fleeting floating-point perturbations that could be lost during quantization or encoding.",
          "evidence": "Section I (page 1): \u201cThen, we input mutated voice commands to VA devices by using machine speech synthesis to eliminate the human factor of producing ASR-related misinterpretation.\u201d; Section V.A (page 9): \u201cWe direct input fuzzing results generated with LipFuzzer to the above mentioned services and perform a Text-to-Speech conversion.\u201d"
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      },
  
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "The authors apply LipFuzzer separately to Amazon Alexa and Google Assistant platforms but do not evaluate whether the same failing inputs (LAPSUS) induce equivalent failures across both models, nor do they analyze cross-model consistency in mutation\u2013failure relationships.",
          "evidence": "Section VI-C (vApp Store Evaluation): \u201cwe have a total of 460 templates with 1104 voice commands... In evaluation, we scan Alexa Skill Store and Google Assistant Store to assess the security problem we found.\u201d No cross-platform reuse of the same failing inputs is reported."
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper demonstrates that the proposed fuzzer, LipFuzzer, applies the same linguistic models (which define mutation patterns) to evaluate different Voice Assistant (VA) platforms (Amazon Alexa and Google Assistant). It finds a significant number of vulnerable applications on both platforms, indicating that the mutation-level generalization of the fuzzing approach induces similar failure behaviors (semantic misinterpretation) across these distinct NLU systems.",
          "evidence": "Page 1, Abstract: \"We also use LipFuzzer to evaluate both Amazon Alexa and Google Assistant vApp platforms. We have identified that a large portion of real-world vApps are vulnerable based on our fuzzing result.\"\nPage 2, Section I. INTRODUCTION: \"Furthermore, we scan Alexa Skill Store and Google Assistant Store to assess the security problem we found.\"\nPage 12, Section VI-C.1 Experiment Setup: \"We evaluate LipFuzzer by using templates crawled from the Amazon Alexa Store and Google Assistant Store... Moreover, we apply the same LAPSUS Models used in LipFuzzer evaluation.\"\nPage 12-13, Section VI-C.3 Verified vulnerable vApps: \"As a result, a total of 715 (71.5%) Amazon Alexa vApps and 59 (29.5%) Google Assistant are verified to be vulnerable.\""
        },
        "arbitrator": {
          "value": "Low",
          "why": "Although the authors apply LipFuzzer separately to both Amazon Alexa and Google Assistant vApp platforms, they do not demonstrate that the same mutated inputs (LAPSUS) cause equivalent semantic misinterpretations across both models. Instead, they evaluate each platform independently, without analyzing cross\u2010model consistency or showing that identical failure\u2010inducing mutations transfer between the two NLU systems.",
          "evidence": "Section VI-C (vApp Store Evaluation) evaluates Alexa and Google Assistant stores separately, reporting vulnerable vApp counts for each, but provides no analysis of the same LAPSUS inputs across models or joint failure patterns."
        },
        "manual": {
          "value": "Low",
          "why": "evaluates two voice assistant systems independently but does not analyze whether failures generalize"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Data-driven"
      ],
      "oracle": [
        "Metamorphic"
      ]
    }
  },
  "Woodlief et al. - 2022 - Semantic image fuzzing of AI perception systems": {
    "key": "woodlief2022semantic",
    "name": "semSensFuzz",
    "year": "2022",
    "citation_count": "18",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "All of the failures uncovered by semSensFuzz are misclassification errors in the perception outputs (e.g., added objects causing a bus to be labeled as a train), which are functional correctness issues rather than policy or safety-boundary breaches.",
          "evidence": "Abstract (p. 1958): \u201cwe found that it discovered inputs that revealed significant inconsistencies between the specified and computed interpretations.\u201d  Section 6.4.1 RQ1 Results: Finding Inconsistencies (p. 1966): \u201cthe leftmost column shows a moderate inconsistency; an added person occludes a bus, causing EfficientPS to then classify the bus as a train.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The fuzzer, semImFuzz, is designed to test AI perception systems by mutating sensor readings and their ground-truth interpretations. The vulnerabilities it uncovers are inconsistencies between the system's computed interpretations and the expected ground-truth interpretations, which manifest as misclassifications or incorrect labeling of objects in images. These are identified as 'functional errors' or 'problematic perception errors' that cause the System Under Test (SUT) to perform poorly, as measured by the percentage of correctly classified pixels.",
          "evidence": "Page 1, Abstract: 'We found that it synthesized tests with novel and subjectively realistic image inputs, and that it discovered inputs that revealed significant inconsistencies between the specified and computed interpretations.'\nPage 2, Contribution (3): 'Experiments using our approach to test five state-of-the-art perception systems, with results showing that our approach can produce realistic inputs and overall test case pairs that reveal problematical perception errors in these systems.'\nPage 8, Section 6.3 Metrics: 'An effective mutation, one that finds a potential SUT failure mode, renders a conforming image that causes the SUT to perform poorly. To capture that notion we rely on the Cityscapes benchmark evaluation tool (eval) which scores each SUT's performance based on the percentage of correctly classified pixels.'\nPage 9, Section 6.4.1 RQ 1 Results: 'Figure 12 shows three mutations produced by semImFuzz and their interpretations by the EfficientPS SUT before and after the mutation. The leftmost column shows a moderate inconsistency; an added person occludes a bus, causing EfficientPS to then classify the bus as a train. The middle column shows a significant inconsistency; an added car occludes a train, causing EfficientPS to correctly identify only part of the train, labeling the rest as 'do not care'. The rightmost column shows an extreme inconsistency; an added car occludes a truck, causing EfficientPS to misclassify the truck, labelling portions as 'do not care' and the rest as building.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "All of the failures uncovered by semSensFuzz are misclassification errors in the perception outputs (e.g., added objects causing a bus to be labeled as a train), which are functional correctness issues rather than policy or safety-boundary breaches.",
          "evidence": "Abstract (p. 1958): \u201cwe found that it discovered inputs that revealed significant inconsistencies between the specified and computed interpretations.\u201d  Section 6.4.1 RQ1 Results: Finding Inconsistencies (p. 1966): \u201cthe leftmost column shows a moderate inconsistency; an added person occludes a bus, causing EfficientPS to then classify the bus as a train.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "The semSensFuzz approach generates semantic mutations in an untargeted way\u2014adding or recoloring objects to elicit generic misclassifications\u2014without steering the fuzzer toward any specific, attacker-style fault or exact output label.",
          "evidence": "Page 1959, Introduction (Section 1): \u201cThis paper takes the first step towards our long-term vision by proposing and evaluating an approach for non-guided test generation that can, for example, incorporate vehicles and people into images.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The semSensFuzz approach employs 'semantic mutations' to generate test cases. Its goal is to create semantically meaningful, challenging, and potentially safety-critical inputs, such as adding vehicles or pedestrians to a scene or simulating damaged vehicles, to uncover 'perception failures' or 'inconsistencies' in the system under test. While the fuzzer does not aim for a specific, predefined incorrect output (e.g., forcing a classifier to output a particular wrong label), it specifically targets categories of faults related to how autonomous systems interpret critical semantic elements and scenarios in the physical world, which aligns with triggering semantic or category-level faults like unsafe behaviors.",
          "evidence": "Page 1, Section 1, Introduction: \"Our long-term aim is to generate test cases that focus on rare, safety-critical inputs that include, for example, vehicles that are crashed, crossing into oncoming traffic, overturned, etc.\"\nPage 3, Section 2.4, Generating Tests with Semantic Mutations: \"Such a mutation could, for example, add a car driving on the street as shown in Figure 3e with a corresponding interpretation in Figure 3j. We believe and later show how such semantic mutations have the potential to create test cases that are conforming but do not occur in existing test suites due to their uncommon occurrences in the physical world.\"\nPage 3, Section 2.4, Generating Tests with Semantic Mutations: \"On the interpretation side, we must craft domain-specific rules to validate potential mutations and determine what types of mutations are likely to yield perception failures, e.g. adding a pedestrian to a roadway.\"\nPage 11, Section 7, Conclusion and Future Work: \"Testing AS perception systems requires examining rare, safety-critical scenarios which cannot be obtained practically from the real world. We will develop more advanced mutations that incor-porate these elements, such as cars driving in the wrong direction, damaged cars, or people sitting on cars.\""
        },
        "arbitrator": {
          "value": "Medium",
          "why": "While semSensFuzz does not steer the fuzzer toward a single, exact adversarial label, it deliberately crafts semantic, category-level mutations\u2014such as adding vehicles or pedestrians, simulating safety-critical scenarios, and targeting perception failures\u2014to uncover unsafe behaviors in autonomous perception systems. This aligns with the medium level of directedness, where the fuzzer aims to trigger semantic or category-level faults without fixing the exact output label.",
          "evidence": "Page 1, Section 1 (Introduction): \u201cOur long-term aim is to generate test cases that focus on rare, safety-critical inputs that include, for example, vehicles that are crashed, crossing into oncoming traffic, overturned, etc.\u201d\nPage 2, Section 2.4 (Generating Tests with Semantic Mutations): \u201cOn the interpretation side, we must craft domain-specific rules to validate potential mutations and determine what types of mutations are likely to yield perception failures, e.g. adding a pedestrian to a roadway.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "generates safety-critical scenarios (cars, pedestrians, obstacles) to expose perception failures."
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper\u2019s approach treats each perception system as a black box, reporting only observable misclassifications (drops in pixel\u2010wise IoU) induced by semantic mutations, without performing any analysis of internal signals (e.g., feature activations, gradients, decision boundaries) or correlational behavioral studies to explain why those failures occur.",
          "evidence": "Section 6.3 (page 1966): \u201cWe evaluate the performance of the mutation strategies by calculating the percentage point (p.p.) difference between the SUT\u2019s score on the original image and its score on the mutated image.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper demonstrates how semantic mutations lead to observable inconsistencies in perception systems (what fails) and provides external interpretations of behavioral patterns (how and where failures manifest). For instance, it explains that occluding an object can cause a specific misclassification (e.g., a bus as a train) and observes that SUTs are robust to color changes but sensitive to semantic additions. However, it explicitly states that 'further analysis of the specific SUTs is needed to understand the factors involved in this performance,' indicating that the current method does not delve into the internal mechanisms of the models to establish causal links for these failures.",
          "evidence": "Page 9, Section 6.4.1 RQ 1 Results: 'The leftmost column shows a moderate inconsistency; an added person occludes a bus, causing EfficientPS to then classify the bus as a train. The middle column shows a significant inconsistency; an added car occludes a train, causing EfficientPS to correctly identify only part of the train, labeling the rest as \"do not care\". The rightmost column shows an extreme inconsistency; an added car occludes a truck, causing EfficientPS to misclassify the truck, labelling portions as \"do not care\" and the rest as building.'\n\nPage 9, Section 6.4.1 RQ 1 Results: 'Further analysis of the specific SUTs is needed to understand the factors involved in this performance, but these data suggest that the highest performing SUTs may be more brittle under certain conditions.'\n\nPage 10, Section 6.4.3 RQ 3 Results: Mutation Types: 'However, even though recoloring a car affects the same number of pixels as adding that car to another image, we find that the SUTs are robust against changing the color of the car. This suggests that editing large regions of the image, even in a conforming manner, is insufficient. This further supports our notion that high level semantic mutations such as adding a car are required to exercise these perception systems and find inconsistencies.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "While the paper treats the perception systems as black-boxes and does not perform any inspection of internal signals (activations, gradients, coverage) or establish detailed causal mechanisms, it does go beyond merely reporting failures by providing external, heuristic interpretations of when and how those failures arise. Specifically, the authors correlate semantic mutations (e.g., occluding a bus with a person leads to a bus\u2192train misclassification, or adding cars causes larger IoU drops while color changes do not) and observe patterns in SUT brittleness, satisfying the criteria for medium root-cause insight.",
          "evidence": "Section 6.4.1 (RQ1 Results, page 1966\u20131967):\n\u2022 \u201cThe leftmost column shows a moderate inconsistency; an added person occludes a bus, causing EfficientPS to then classify the bus as a train. \u2026\u201d\n\u2022 \u201cFurther analysis of the specific SUTs is needed to understand the factors involved in this performance, but these data suggest that the highest performing SUTs may be more brittle under certain conditions.\u201d\n\nSection 6.4.3 (RQ3 Results, page 1967\u20131968):\n\u2022 \u201cHowever, even though recoloring a car affects the same number of pixels as adding that car to another image, we find that the SUTs are robust against changing the color of the car. This suggests that editing large regions of the image, even in a conforming manner, is insufficient. This further supports our notion that high level semantic mutations such as adding a car are required to exercise these perception systems and find inconsistencies.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "Offers descriptive correlations between input changes and output errors, but no analysis on root cause"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "The paper enforces a variety of domain-specific design constraints (e.g., preconditions on object class presence, matching vanishing-point quadrants for perspective, road-location checks, and HSV brightness thresholds) to preserve the realism of mutated images, but it provides no empirical validation via quantitative perceptual metrics (LPIPS, SSIM, BLEU) or human evaluation.",
          "evidence": "Section 3.2: \u201cSecond, we associate a set of preconditions Prec with each mutation \u03b4\u2026 If Prec(interp) is not satisfied, then \u03b4 is not applicable\u2026 The idea is to leverage an existing interpretation to determine mutation applicability.\u201d; Section 4.3.1: \u201cTo \u2026 check for compatible perspective, semantics, and lighting between the entity and t\u2026 If the entity does not have a median value within 5 units of the median value of the base image target area, then it fails the conformity check.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The paper provides empirical evidence, through human evaluation, for the naturalness preservation of its fuzzed inputs. While the method uses theoretical justifications and design constraints to enforce naturalness during generation (e.g., checks for perspective, lighting, and physical feasibility), it explicitly quantifies the rate of 'false positives' which are defined as non-conforming (unrealistic) generated images. This classification of realism is based on a manual inspection by the authors, directly fulfilling the criteria for 'human evaluation' under the 'High' category, even if the results show significant rates of non-natural inputs.",
          "evidence": "Page 10, Section 6.4.2 'RQ 2 Results: Efficiency': \"Still, to gain a better grasp on the rate of false positives we manually inspect all the generated test cases that led to 5+ p.p. drop, and sample 10% of the test cases that led to a drop between 1 and 5 p.p.. The process entailed each author examining each image and classifying them as either a true positive or false positive. If any of the three authors deemed an image a false positive, it was conservatively recorded as such.\"\nPage 10, Table 2: 'False Positive Rate for Inconsistencies Found' directly reports the percentages of these manually identified non-conforming inputs.\nPage 11, Figure 14: 'Sample of True and False Positives (best viewed on a screen)' visually presents examples of both natural and non-natural fuzzed images as categorized by human evaluation."
        },
        "arbitrator": {
          "value": "Medium",
          "why": "The paper relies on domain-specific design constraints (e.g. vanishing-point quadrant matching, road-placement checks, HSV brightness thresholds) to preserve input realism but does not perform a systematic empirical assessment of naturalness across its full set of fuzzed inputs using quantitative perceptual metrics or a structured human evaluation. The only human inspection reported is a manual filtering of false positives among test cases that triggered perception failures, which serves to prune non-conforming tests rather than to validate overall naturalness preservation.",
          "evidence": "Section 3.2 and Section 4.3.1 describe preconditions (vanishing point, road placement, HSV checks) used to enforce conformity; Section 6.4.1 reports manual inspection of only those images causing SUT errors to identify \u2018false positives,\u2019 but no broad human study or quantitative metrics (LPIPS/SSIM/BLEU) are used to validate the naturalness of the entire fuzzed dataset."
        },
        "manual": {
          "value": "Medium",
          "why": "Applies design constratraints but no measure of naturalness of the fault-triggering fuzzed inputs"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "The faults uncovered by semSensFuzz arise from high\u2010level semantic mutations (e.g., adding cars, changing object colors) applied directly to pixel images, but the approach does not explicitly validate or enforce I/O\u2010stable representations (no clipping or rounding controls) before saving in standard formats. As a result, these semantic\u2010level changes may not persist unchanged through quantization when images are encoded (e.g., into 8\u2010bit PNG/JPEG).",
          "evidence": "Section 4.2 (p. 1962): \u201cTo affect the color change, we manipulate the color in the HSL color space\u2026 we perform a hue shift on the entity, changing it to a different color.\u201d \nSection 4.3 (p. 1963): \u201cWhen adding the entity, the mutation takes the isolated portion of the image and overlays it\u2026 ensuring perspective and lighting\u2026.\u201d \nNowhere do the authors describe explicit clipping or rounding steps to guarantee that pixel modifications survive standard I/O quantization."
        },
        "gemini": {
          "value": "Medium",
          "why": "The method generates fuzzed inputs through semantic-level transformations (e.g., adding entities, changing object colors) applied to real-world images. These transformations involve manipulating pixel values in continuous color spaces (HSL, HSV) and overlaying image sections. While the paper emphasizes producing 'conforming' and 'realistic' images to the physical world and filters out very small performance drops (less than 1 percentage point) as 'noise', it does not explicitly state that the mutated image inputs are processed with clipping, rounding, or other explicit validation steps specifically to ensure their stability across standard I/O operations (like encoding to JPEG/PNG, which involves quantization). Therefore, although the changes are semantic and intended to be robust, the persistence of all fault characteristics through typical image serialization processes is not explicitly guaranteed by the described methodology.",
          "evidence": "Page 3, Section 2.4, 'Generating Tests with Semantic Mutations': 'What this work proposes is an approach to testing perception systems using sensor-reading/expected-interpretation test case pairs derived from real-world AS perception test cases... by mutating both sensor readings (e.g. images) and their expected interpretations in a coordinated manner.'\nPage 3, Section 3.1, 'Problem definition': 'A semantic mutation... transforms a test case... into a new test case... where r' also conforms with some world configuration, w', and interp' is a valid interpretation of r'.' and 'For example, a mutation, SaddCar may add a car to an image while mutating its interpretation to indicate that a car is now there.'\nPage 5, Section 4.2.1, 'Implementation': 'For this mutation... we manipulate the color in the HSL color space... if the average lightness of the vehicle is over 100, then all pixels with a lightness value over 100 have their lightness decreased by a random amount between 20 and 50. Then, to prevent the colors from appearing faded, pixels with a lightness over 100 and a saturation less than 50 have their saturation increased.'\nPage 6, Section 4.3.1, 'Implementation': 'the mutation takes the isolated portion of the image and overlays it at the same pixel coordinates on the base image.'\nPage 9, Section 6.3, 'Metrics': 'We judge drops of less than 1 p.p. to be in the noise as most tests result in SUTs misclassifying a few pixels around the edges of objects, leading to small drops compared to the baseline. We select 1 p.p. as the cut off to reduce the likelihood that we deem an inconsistency noteworthy when it is not...'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "The faults uncovered by semSensFuzz arise from high\u2010level semantic mutations (e.g., adding cars, changing object colors) applied directly to pixel images, but the approach does not explicitly validate or enforce I/O\u2010stable representations (no clipping or rounding controls) before saving in standard formats. As a result, these semantic\u2010level changes may not persist unchanged through quantization when images are encoded (e.g., into 8\u2010bit PNG/JPEG).",
          "evidence": "Section 4.2 (p. 1962): \u201cTo affect the color change, we manipulate the color in the HSL color space\u2026 we perform a hue shift on the entity, changing it to a different color.\u201d \nSection 4.3 (p. 1963): \u201cWhen adding the entity, the mutation takes the isolated portion of the image and overlays it\u2026 ensuring perspective and lighting\u2026.\u201d \nNowhere do the authors describe explicit clipping or rounding steps to guarantee that pixel modifications survive standard I/O quantization."
        },
        "manual": {
          "value": "Medium",
          "why": "Perturbations occur at a semantic level that is plausibly I/O-stable, but the paper neither enforces nor verifies persistence after I/O transformations, and minor artifacts from the mutation implementation may cause non-persistent faults."
        }
      },
   
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper evaluates each perception system independently on the semantically mutated inputs but does not analyze whether the same failing inputs induce equivalent errors across multiple models, nor does it assess cross-model consistency in mutation\u2013failure relationships.",
          "evidence": "Section 6.4.1 (\u201cRQ 1 Results: Finding Inconsistencies\u201d): \u201cFigure 11 shows the counts of inconsistencies found in each category per SUT\u2026each SUT had over 100 inconsistencies.\u201d There is no subsequent analysis of shared failures or correlation of failures across the different SUTs."
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper demonstrates medium transferability by evaluating the same set of generated mutations across five different state-of-the-art perception systems (SUTs) and analyzing how these mutation patterns systematically induce failure behaviors (inconsistencies) across them. It compares the number and severity of inconsistencies found for each SUT and also analyzes the effectiveness of different mutation types across these models. While it shows that the same inputs cause failures across models, it does not explicitly detail or demonstrate that the *exact same failing input* causes *equivalent erroneous behaviors* (e.g., the same type of misclassification) across all models, which would be required for a 'High' rating. Instead, it focuses on the generalized impact of mutation patterns.",
          "evidence": "Page 8, Section 6.1, 'SUTs': 'To assess semImFuzz, we evaluate it on five highly competitive perception systems submitted to the Cityscapes benchmark for the \"Pixel-Level Semantic Labeling Task\" [7].'\nPage 8, Section 6.2, 'Tests Generated': 'For evaluation, we ran each SUT on 150,000 tests generated by semImFuzz comprised of 50,000 \"Add Car\" mutations, 50,000 \"Add Person\" mutations, and 50,000 \u201cChange Car Color\u201d mutations.'\nPage 9, Section 6.4.1, 'RQ 1 Results: Finding Inconsistencies': 'We first remark that semImFuzz found 1210 SUT inconsistencies resulting from 884 mutations and that each SUT had over 100 inconsistencies. Further, each of the SUTS exhibited at least one significant inconsistency and 3 of the 5 SUTs combined to exhibit a total of 20 extreme inconsistencies. The distribution of inconsistencies among the SUTs is unexpected. NVIDIA SemSeg [34] and EfficientPS [24], the two highest scoring on the Cityscapes benchmark, had the highest number of inconsistencies in all three categories with 211 and 477 respec- tively. EfficientPS revealed more than three times the number of inconsistencies of the SUT with the fewest inconsistencies.'\nPage 9, Figure 11: 'Inconsistencies found per SUT' graphically compares the number of moderate, significant, and extreme inconsistencies for each of the five SUTs.\nPage 10, Section 6.4.3, 'RQ 3 Results: Mutation Types': 'Figure 15 shows the inconsistencies found for each SUT based on the mutation type... The add car mutation induced the most inconsistencies (877), followed by the add person mutation (280), and then the mutation to change the car color (53). These results reflect what we may expect; adding a car affects a large portion of the image, leading to a higher likelihood of the mutation yielding an inconsistency.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "While the paper applies the same set of semantically mutated inputs to five different perception systems and reports aggregate failure counts per system, it does not analyze whether the exact same inputs cause equivalent erroneous behaviors across those systems. There is no investigation of cross\u2010model consistency at the individual input level or correlation of specific failures across models, which is required for Medium or High transferability.",
          "evidence": "Section 6.4.1 (\u201cRQ 1 Results: Finding Inconsistencies\u201d): Figure 11 reports only per\u2010SUT failure counts and Section 6.4 discusses inconsistency counts in the aggregate per model. Nowhere do the authors examine if the same mutated input triggers comparable misclassifications across multiple models."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Data-driven"
      ],
      "oracle": [
        "Metamorphic"
      ]
    }
  },
  "Lee et al. - 2020 - Effective white-box testing of deep neural networks with adaptive neuron-selection strategy": {
    "key": "lee2020effective",
    "name": "ADAPT",
    "year": "2020",
    "citation_count": "81",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "The Adapt technique generates adversarial inputs that cause misclassification errors in the target deep neural networks, exposing low-level functional robustness failures (brittle decision boundaries), but does not attempt to bypass any explicit safety, policy, or security mechanisms.",
          "evidence": "Abstract (p.165): \u201cExperiments with real-world network models and datasets show that Adapt is remarkably more effective \u2026 in terms of \u2026 adversarial inputs found.\u201d\nSection 2.3 (p.167): \u201cthe algorithm calculates the gradient of the selected neurons \u2026 and generates a new input I' by adding the gradient \u2026 causing misclassifications.\u201d\nTable 3 (p.172): \u201cEffectiveness for finding adversarial inputs\u201d reports only misclassified labels and adversarial examples, with no safety or policy boundary breaches."
        },
        "gemini": {
          "value": "Low",
          "why": "The method primarily focuses on finding 'adversarial inputs' that lead to 'incorrectly classified labels' in deep neural networks. This type of failure, where the model provides a wrong classification, is explicitly defined as a low-level functional error in the metric description.",
          "evidence": "Page 1, Abstract: 'Experiments with real-world network models and datasets show that ADAPT is remarkably more effective than existing testing techniques in terms of coverage and adversarial inputs found.'\nPage 8, Section 4.3 'Adversarial Inputs': 'Table 3 reports the average number of mutations per image (Mutations), the average number of adversarial inputs found per image (Adv. Inputs), the total number of incorrectly classified labels (Labels), and the number of initial inputs (Seeds) from which adversarial inputs were found. An incorrectly classified label consists of an original image and a found label.'\nPage 9, Figure 3: The caption reads 'Images with incorrectly classified labels found exclusively by ADAPT,' and the accompanying images show examples of inputs that cause misclassification (e.g., a 'dingo' image being classified as 'bassinet')."
        },
        "arbitrator": {
          "value": "Low",
          "why": "The Adapt technique generates adversarial inputs that cause misclassification errors in the target deep neural networks, exposing low-level functional robustness failures (brittle decision boundaries), but does not attempt to bypass any explicit safety, policy, or security mechanisms.",
          "evidence": "Abstract (p.165): \u201cExperiments with real-world network models and datasets show that Adapt is remarkably more effective \u2026 in terms of \u2026 adversarial inputs found.\u201d\nSection 2.3 (p.167): \u201cthe algorithm calculates the gradient of the selected neurons \u2026 and generates a new input I' by adding the gradient \u2026 causing misclassifications.\u201d\nTable 3 (p.172): \u201cEffectiveness for finding adversarial inputs\u201d reports only misclassified labels and adversarial examples, with no safety or policy boundary breaches."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "The Adapt technique performs untargeted exploration by generating inputs to maximize neuron\u2010coverage metrics rather than steering towards any specific output label or semantic fault category. It does not specify target labels or phrases but instead uses gradients of selected internal neurons to broadly explore new internal states and discover any misclassifications.",
          "evidence": "Section 2.3 (White-Box Testing of Neural Networks), page 166: \u201cOne of the main goals of neural network testing is to generate test cases \u2026 that collectively maximize the coverage\u201d and Algorithm 1 generates inputs guided solely by coverage increases, without any predefined target outputs."
        },
        "gemini": {
          "value": "Low",
          "why": "The ADAPT technique primarily aims to maximize neuron coverage and, in doing so, reveals generic faults such as misclassifications or adversarial inputs. The paper explicitly states that the goal is to generate test cases that 'maximize a given coverage metric (Cov)' and that 'an incorrectly classified label consists of an original image and a found label,' indicating that any deviation from the correct label is considered a fault, rather than steering towards a specific, predefined incorrect label. The method uses gradients to 'increase the output values' of selected neurons to increase coverage, not to force a particular misclassification target. Furthermore, it differentiates itself from adversarial example generation techniques by emphasizing 'systematically examining the logic of the model and explore various internal states' rather than 'aim[ing] to find common misbehaving inputs' or 'taking into account the logic inside the model' as specific attack vectors.",
          "evidence": "Page 1, Abstract: \"Experiments with real-world network models and datasets show that ADAPT is remarkably more effective than existing testing techniques in terms of coverage and adversarial inputs found.\"\nPage 2, Section 2.3 'White-Box Testing of Neural Networks': \"One of the main goals of neural network testing is to generate test cases (i.e. input vectors) that maximize a given coverage metric (Cov) in a limited testing budget.\"\nPage 8, Section 4.3 'Adversarial Inputs': \"An incorrectly classified label consists of an original image and a found label. That is, we consider two adversarial inputs differently, which classified into the same label but came from different input source.\"\nPage 11, Section 'Using Gradients to Attack DNNs': \"However, they differ from the testing approaches as they aim to find common misbehaving inputs and do not take into account the logic inside the model. The emphasis of testing techniques is on systematically examining the logic of the model and explore various internal states.\""
        },
        "arbitrator": {
          "value": "Low",
          "why": "The Adapt technique performs untargeted exploration by generating inputs to maximize neuron\u2010coverage metrics rather than steering towards any specific output label or semantic fault category. It does not specify target labels or phrases but instead uses gradients of selected internal neurons to broadly explore new internal states and discover any misclassifications.",
          "evidence": "Section 2.3 (White-Box Testing of Neural Networks), page 166: \u201cOne of the main goals of neural network testing is to generate test cases \u2026 that collectively maximize the coverage\u201d and Algorithm 1 generates inputs guided solely by coverage increases, without any predefined target outputs."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Medium",
          "why": "The paper goes beyond merely reporting adversarial inputs (failure symptoms) by analyzing which neuron\u2010selection features correlate with increased coverage and adversarial\u2010input discovery. However, it does not establish causal mechanisms linking these failures to specific architectural or training weaknesses.",
          "evidence": "Section 4.4 (Learned Insights): \u201cFigure 4 shows the changes of the weights of the top-3 (red) and bottom-3 (blue) features of the strategies \u2026 As testing goes by, the solid lines were being skewed toward one direction \u2026 which means that the learning algorithm extracts the characteristics of strategies, which increase coverage, well.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The ADAPT technique provides insights into where and how failures (e.g., generation of adversarial inputs or increased coverage) manifest by performing a correlational analysis of internal neuron features. Its online learning algorithm adaptively identifies 'crucial' neuron-selection strategies based on 29 predefined features (e.g., neuron location, weight percentile, activation status, whether an adversarial input was found). The 'Learned Insights' section explains which of these internal neuron characteristics are most important for increasing coverage and finding adversarial inputs across different models and metrics. While it doesn't establish direct causal links to specific architectural or training weaknesses for individual vulnerabilities, it illuminates how the model's internal structure (which neurons are important) contributes to the success of the testing process.",
          "evidence": "Page 3, Section 3.1 Parameterized Neuron-Selection Strategy: 'We designed 29 atomic features that describe characteristics of the neurons in a neural network... After calculating the score of each neuron, we select the m neurons with the highest scores.'\nPage 9, Section 4.4 Learned Insights: 'Figure 4 shows the changes of the weights of the top-3 (red) and bottom-3 (blue) features of the strategies that our algorithm collected while testing each model with NC, where fi indicates the i-th element of the parameterized neuron-selection strategies. The top features mean the properties of neurons which should be selected to increase coverage, and the bottom features are characteristics that neurons that should not be selected have.'\nPage 10, Section 4.4 Learned Insights: 'f4, which indicates the layers located in the back 25% of the network, is learned as the most important feature that neurons to select should have, and f2, which indicates the layers located relatively front of the network, is included in bottom-3 features in most cases (LeNet-4, LeNet-5, and ResNet-50). This trend indicates that neurons with higher expressive ability, which means that neurons are located in deeper convolutional layers [38], should be selected while testing the deep neural networks.'\nPage 10, Section 4.4 Learned Insights: 'From these results, we can infer that neurons with moderately high weights (f14) and neurons that activated when the given objective is satisfied (f18) should be considered carefully while testing small models, like LeNet-4 and LeNet-5. In addition, neurons that activated a lot (f20 and f21) should not be selected.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "The paper goes beyond merely reporting adversarial inputs (failure symptoms) by analyzing which neuron\u2010selection features correlate with increased coverage and adversarial\u2010input discovery. However, it does not establish causal mechanisms linking these failures to specific architectural or training weaknesses.",
          "evidence": "Section 4.4 (Learned Insights): \u201cFigure 4 shows the changes of the weights of the top-3 (red) and bottom-3 (blue) features of the strategies \u2026 As testing goes by, the solid lines were being skewed toward one direction \u2026 which means that the learning algorithm extracts the characteristics of strategies, which increase coverage, well.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Correlates neuron features with faults"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "The Adapt fuzzer enforces a norm\u2010based constraint on perturbations (L2 distance \u2264 0.05) to keep mutated inputs close to originals, but it does not provide any empirical validation (e.g., SSIM, LPIPS or human studies) that the resulting inputs remain perceptually or semantically natural.",
          "evidence": "Section 4.1 (Experimental Setup): \u201cAll the techniques, including Adapt, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper describes that ADAPT generates new inputs by adding gradients to the original input while enforcing a constraint on the L2-distance between the initial and mutated inputs. This constitutes a design constraint (norm-based limit) aimed at preserving input naturalness. Although the paper qualitatively observes that some adversarial images were 'visually indistinguishable' from originals, it does not provide formal empirical validation using quantitative metrics (e.g., LPIPS, SSIM) or a structured human evaluation as required for a 'High' rating.",
          "evidence": "Page 2, Section 2.3, White-Box Testing of Neural Networks: 'At line 12, the algorithm checks whether the new input I' is able to cover new neurons, and whether it satisfies a given constraint (Constraint) on test cases. For example, Constraint(I, I') is true if L2 distance between I and I' is less than a certain threshold.'\nPage 6, Section 4.1, Experimental Setup: 'All the techniques, including ADAPT, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.'\nPage 8, Section 4.3, Adversarial Inputs: 'In particular, the adversarial images found for VGG-19 and ResNet-50 were visually indistinguishable from the original ones.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "The Adapt fuzzer enforces a norm\u2010based constraint on perturbations (L2 distance \u2264 0.05) to keep mutated inputs close to originals, but it does not provide any empirical validation (e.g., SSIM, LPIPS or human studies) that the resulting inputs remain perceptually or semantically natural.",
          "evidence": "Section 4.1 (Experimental Setup): \u201cAll the techniques, including Adapt, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Norm-bounded"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Low",
          "why": "Adapt generates test inputs by adding small continuous, floating-point perturbations (gradient steps) to the original inputs without any explicit clipping or rounding to a discrete representation. Such perturbations depend on floating-point precision and may be erased when the input is serialized (e.g., saved as an 8-bit image).",
          "evidence": "Algorithm 1 (lines 10\u201314) shows that new inputs I\u2032 are created via I\u2032 \u2190 I\u2032 + \u03bb \u00b7 \u2202(\u2026)\u2215\u2202I\u2032, and the paper never mentions any clipping, rounding, or other validation to ensure stability under standard I/O operations."
        },
        "gemini": {
          "value": "Low",
          "why": "The method proposed in the paper generates new test cases by applying small, continuous perturbations to existing inputs through gradient additions. Although an L2-distance constraint is applied to limit the magnitude of these changes, the paper does not specify any explicit validation mechanisms, such as clipping or rounding, to ensure these fuzzed inputs remain stable after being processed through standard I/O operations (e.g., saving an image with quantization). Consequently, these small, floating-point-dependent perturbations are susceptible to disappearing due to precision loss during I/O, which aligns with the definition of 'Low' vulnerability persistence.",
          "evidence": "Page 1, Section 1, Introduction: \"generate new test cases by adding the gradients to the original test case in the direction of increasing the output values.\"\nPage 3, Algorithm 1, Line 10: \"\u0399' \u2190 \u0399' + \u03bb \u00b7 (\u03a3n\u2208N Neuron(n, I'))/\u03b4\u0399'\"\nPage 3, paragraph below Algorithm 1: \"Constraint(I, I') is true if L2 distance between I and I' is less than a certain threshold.\"\nPage 6, Section 4.1, Experimental Setup: \"All the techniques, including ADAPT, performed testing while maintaining the L2-distance between the initial and mutated inputs within 0.05 on average.\""
        },
        "arbitrator": {
          "value": "Low",
          "why": "Adapt generates test inputs by adding small continuous, floating-point perturbations (gradient steps) to the original inputs without any explicit clipping or rounding to a discrete representation. Such perturbations depend on floating-point precision and may be erased when the input is serialized (e.g., saved as an 8-bit image).",
          "evidence": "Algorithm 1 (lines 10\u201314) shows that new inputs I\u2032 are created via I\u2032 \u2190 I\u2032 + \u03bb \u00b7 \u2202(\u2026)\u2215\u2202I\u2032, and the paper never mentions any clipping, rounding, or other validation to ensure stability under standard I/O operations."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
 
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper evaluates the proposed testing technique separately on multiple models (LeNet-4, LeNet-5, VGG-19, ResNet-50) but does not analyze whether failures (e.g. adversarial inputs) found on one model also cause equivalent erroneous behaviors on other models. There is no cross-model consistency or transferability study of the discovered failures.",
          "evidence": "Section 4.1 (Experimental Setup): \u201cWe used two datasets and four neural network models in Table 2 \u2026 For each dataset, we used two pre-trained models: LeNet-4 and LeNet-5 for MNIST, and VGG-19 and ResNet-50 for ImageNet.\u201d  Nowhere in the evaluation do the authors feed inputs found to fail one model into another model to check for transfer of failures."
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates the method on multiple deep neural network models and datasets, finding adversarial inputs for each. However, it does not analyze whether a failing input discovered on one model also causes an equivalent erroneous behavior on other models performing the same task. The results present the number of adversarial inputs and distinct misclassified labels found independently for each model and technique. There is no discussion or empirical evidence of testing the *same failing inputs* across *different models* to check for shared vulnerabilities, nor is there an analysis of cross-model consistency in mutation-failure relationships.",
          "evidence": "Section 4.3 'Adversarial Inputs', page 8: 'Table 3 reports the average number of mutations per image (Mutations), the average number of adversarial inputs found per image (Adv. Inputs), the total number of incorrectly classified labels (Labels), and the number of initial inputs (Seeds) from which adversarial inputs were found.' This table presents results per model independently. \n\nSection 4.3 'Adversarial Inputs', page 9: 'Figure 3 shows the some examples with incorrectly classified labels found exclusively by ADAPT (all existing testing techniques totally failed to find those incorrect labels).' The examples in Figure 3 are presented per specific model (e.g., LeNet-4, LeNet-5, VGG-19, ResNet-50), indicating that these adversarial inputs caused misclassification on the model they were generated for, without demonstrating their effect on other models. \n\nSection 4.3 'Correlation between Coverage and Adversarial Inputs', page 9: 'From Figure 1 and Table 3, we can notice that coverage and variety of labels are highly correlated, while coverage and the number of adversarial inputs are not correlated strongly. In testing VGG-19 with NC, the order of coverage value of each technique exactly matches the order of the number of the labels found by each technique.' This sentence discusses correlation within a single model (VGG-19) and metric, not across models."
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper evaluates the proposed testing technique separately on multiple models (LeNet-4, LeNet-5, VGG-19, ResNet-50) but does not analyze whether failures (e.g. adversarial inputs) found on one model also cause equivalent erroneous behaviors on other models. There is no cross-model consistency or transferability study of the discovered failures.",
          "evidence": "Section 4.1 (Experimental Setup): \u201cWe used two datasets and four neural network models in Table 2 \u2026 For each dataset, we used two pre-trained models: LeNet-4 and LeNet-5 for MNIST, and VGG-19 and ResNet-50 for ImageNet.\u201d  Nowhere in the evaluation do the authors feed inputs found to fail one model into another model to check for transfer of failures."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Whitebox"
      ],
      "mutation_strategy": [
        "Feedback-informed"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ],
      "oracle": [
        "Metamorphic"
      ]
    }
  },
  "Ben Braiek and Khomh - 2019 - DeepEvolution A Search-Based Testing Approach for Deep Neural Networks": {
    "key": "braiek2019deepevolution",
    "name": "DeepEvolution",
    "year": "2019",
    "citation_count": "53",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepEvolution reveals low-level functional errors\u2014specifically misclassifications in discriminative models and output divergences between original and quantized networks\u2014without exploiting policy, safety, or security boundaries.",
          "evidence": "Section V-C (RQ2): \"The objective is to assess the effectiveness of our approach in testing the robustness of the DNN; by finding misclassified synthetic inputs.\"; Section V-D (RQ3): \"The goal is to assess the usefulness of DeepEvolution in finding difference-inducing inputs that expose potential quantization defects.\""
        },
        "gemini": {
          "value": "Low",
          "why": "DeepEvolution primarily identifies low-level functional errors such as misclassifications and inconsistencies introduced by model quantization. The paper explicitly states that its objective is to find 'misclassified synthetic inputs' and 'difference-inducing inputs' that expose 'quantization defects' or 'erroneous DNN behaviors'. These fall under functional correctness issues rather than behavioral misalignments or security breaches.",
          "evidence": "Page 1, Abstract: 'Moreover, using DeepEvolution, we could successfully find several corner-case behaviors.' 'Finally, DeepEvolution outperformed Tensorfuzz... in detecting latent defects introduced during the quantization of the models.'\nPage 4, Section V.C 'RQ2: Detection of DNN Erroneous Behaviors', Motivation: 'The objective is to assess the effectiveness of our approach in testing the robustness of the DNN; by finding misclassified synthetic inputs.'\nPage 4, Section V.D 'RQ3: DNN Quantization Defects', Motivation: 'The goal is to assess the usefulness of Deep-Evolution in finding difference-inducing inputs that expose potential quantization defects.'\nPage 4, Section V.D 'RQ3: DNN Quantization Defects', Findings: 'DeepEvolution can effectively detect defects introduced during DNN quantization...'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepEvolution reveals low-level functional errors\u2014specifically misclassifications in discriminative models and output divergences between original and quantized networks\u2014without exploiting policy, safety, or security boundaries.",
          "evidence": "Section V-C (RQ2): \"The objective is to assess the effectiveness of our approach in testing the robustness of the DNN; by finding misclassified synthetic inputs.\"; Section V-D (RQ3): \"The goal is to assess the usefulness of DeepEvolution in finding difference-inducing inputs that expose potential quantization defects.\""
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepEvolution does not steer the generation of test cases toward any specific, predefined fault or particular output. Instead, it maximizes neuron coverage via diverse metamorphic transformations to uncover generic errors such as misclassifications and quantization divergences, without constraining the DNN to produce a chosen label or satisfy a semantic category-level goal.",
          "evidence": "Section III (p. 455): \u201cDeepEvolution relies on population-based metaheuristics to explore the search space of semantically-preserving metamorphic transformations. Using a coverage-based fitness function to guide the exploration process, it aims to ensure a maximum diversity in the generated test cases \u2026 it stores the failed tests that exhibit erroneous DNNs behavior or induce a divergence between the original DNN and its quantized version.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "DeepEvolution performs untargeted exploration aimed at revealing generic faults such as misclassifications, new erroneous behaviors, and divergences due to model quantization. Its primary goal is to maximize neuronal coverage and diversity in generated test cases, rather than forcing specific, predefined faulty outputs or triggering category-level semantic faults. The fitness function is explicitly coverage-based, guiding the search towards uncovering general 'erroneous DNN behaviors' and 'quantization defects' without constraining the exact nature of these faults.",
          "evidence": "Page 1, Abstract: 'DeepEvolution, a novel search-based approach for testing DL models that relies on metaheuristics to ensure a maximum diversity in generated test cases... we could successfully find several corner-case behaviors.'\nPage 1, Section I. Introduction: 'DeepEvolution aims to detect inconsistencies and potential defects in DNN models. DeepEvolution relies on population-based metaheuristics to explore the search space of semantically-preserving metamorphic transforma-tions. Using a coverage-based fitness function to guide the exploration process; it aims to ensure a maximum diversity in the generated test cases.'\nPage 2, Section III. DeepEvolution: Testing Workflow: 'The fitness function should capture both local (neurons covered by a mutated input that were not covered by its corresponding original input) and global (neurons covered by a mutated input that were not covered by all previous test inputs) neuronal coverage.'\nPage 4, Section V. Empirical Evaluation, RQ2 Motivation: 'The objective is to assess the effectiveness of our approach in testing the robustness of the DNN; by finding misclassified synthetic inputs.'\nPage 4, Section V. Empirical Evaluation, RQ3 Motivation: 'The goal is to assess the usefulness of Deep-Evolution in finding difference-inducing inputs that expose potential quantization defects.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepEvolution does not steer the generation of test cases toward any specific, predefined fault or particular output. Instead, it maximizes neuron coverage via diverse metamorphic transformations to uncover generic errors such as misclassifications and quantization divergences, without constraining the DNN to produce a chosen label or satisfy a semantic category-level goal.",
          "evidence": "Section III (p. 455): \u201cDeepEvolution relies on population-based metaheuristics to explore the search space of semantically-preserving metamorphic transformations. Using a coverage-based fitness function to guide the exploration process, it aims to ensure a maximum diversity in the generated test cases \u2026 it stores the failed tests that exhibit erroneous DNNs behavior or induce a divergence between the original DNN and its quantized version.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepEvolution focuses on generating test inputs and reporting failures (misclassifications and quantization divergences) guided by neuron coverage metrics, but it does not analyze or explain the internal mechanisms\u2014such as specific feature activations, decision boundary shifts, or gradient behaviors\u2014that cause these failures. There is no diagnostic or correlational study of model internals beyond using coverage to drive search.",
          "evidence": "Section IV-B (DNN Coverage): \u201cWe adapt the Neuron Coverage (NC) metric\u2026to capture two levels of coverage\u2026to guide the exploration process.\u201d; Section V-RQ2: \u201cDeepEvolution can effectively generate test cases that trigger erroneous behaviors of the DNN\u201d (Table II reports counts of misclassifications) \u2014 no further analysis of why those failures occur is provided."
        },
        "gemini": {
          "value": "Low",
          "why": "DeepEvolution focuses on generating diverse test cases to discover erroneous behaviors and quantization defects in DNNs, primarily through the optimization of neuronal coverage. While it effectively reports *what* kinds of failures are found (e.g., misclassifications, divergences between original and quantized models), it does not provide any diagnostic analysis or explanation for *why* these vulnerabilities occur. The paper does not delve into analyzing internal signals (like feature activations, decision boundaries, or gradient behavior) post-failure detection, nor does it interpret external behavioral patterns (like class-level bias or semantic feature confusion) to infer the root causes of the observed failures. The neuronal coverage metric is used as a fitness function to *guide* the search for diverse test cases, not as a tool for post-mortem analysis of specific failure mechanisms.",
          "evidence": "Section V.C, 'RQ2: Detection of DNN Erroneous Behaviors', states: 'DeepEvolution can effectively generate test cases that trigger erroneous behaviors of the DNN.' Table II provides the 'Number of Erroneous Behaviors Per Metaheuristic'. The text further elaborates: 'generating synthetic test inputs towards improving the neuronal coverage could trigger more states of a DNN, incurring higher chances of defect detection...' and 'the local coverage level integrated in the fitness function plays an important role in assessing how much the DNN's state of the transformed input is different from the state that resulted from the original input. Thus, it is capable of finding corner-cases testing inputs...'. Similarly, Section V.D, 'RQ3: DNN Quantization Defects', states: 'DeepEvolution can effectively detect defects introduced during DNN quantization...' and Table III lists the 'Number of Quantization Defects Per Metaheuristic'. In both cases, the paper quantifies the *number* of defects found and explains *how* DeepEvolution finds them (by increasing coverage and diversity) but does not provide any analysis of the *underlying reasons* for these defects within the DNN model."
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepEvolution focuses on generating test inputs and reporting failures (misclassifications and quantization divergences) guided by neuron coverage metrics, but it does not analyze or explain the internal mechanisms\u2014such as specific feature activations, decision boundary shifts, or gradient behaviors\u2014that cause these failures. There is no diagnostic or correlational study of model internals beyond using coverage to drive search.",
          "evidence": "Section IV-B (DNN Coverage): \u201cWe adapt the Neuron Coverage (NC) metric\u2026to capture two levels of coverage\u2026to guide the exploration process.\u201d; Section V-RQ2: \u201cDeepEvolution can effectively generate test cases that trigger erroneous behaviors of the DNN\u201d (Table II reports counts of misclassifications) \u2014 no further analysis of why those failures occur is provided."
        },
        "manual": {
          "value": "Low",
          "why": "Uses neuron coveage as heuristic guide, does not show correlation between coverage and neurons"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "High",
          "why": "DeepEvolution employs a quantitative measure (Structural Similarity Index, SSIM) to verify and enforce the semantic equivalence and perceptual realism of its fuzzed inputs, rejecting any mutated image below a tuned SSIM threshold.",
          "evidence": "Section IV.A (Metamorphic Transformation): \u201cTo verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8] \u2026 Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The paper provides strong evidence that DeepEvolution preserves the naturalness of its generated inputs through both quantitative measures and human evaluation. It explicitly states the use of Structural Similarity Index (SSIM) to verify semantic equivalence and reject inputs that fall below a tuned threshold, ensuring that the generated images retain their original semantics. Furthermore, the authors conducted a manual verification of a sample of generated images, confirming their correctness and semantic consistency with real-world data.",
          "evidence": "Page 1, Abstract: 'DeepEvolution relies on population-based metaheuristics to explore the search space of semantically-preserving metamorphic transformations.'\n\nPage 2, Section IV. DeepEvolution: Computer-Vision Instance, A. Metamorphic Transformation: 'To verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8] which assesses the similarity between two images based on the visual impact of three characteristics: luminance, contrast, and structure. We expect that pixel-based mutated inputs differs from their originals with respect to these characteristics, but a very low SSIM indicates that the new image looses mostly all the information inherited from its parent. Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.'\n\nPage 5, Section VI. Threats to Validity: 'The manual tuning of metamorphic transformations' domain and the threshold of SSIM could affect the validity of our results. To mitigate this threat, we selected a sample of our generated images using a confidence level of 95% and an error margin of 5%, and verified them manually. We found them to be correct.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "DeepEvolution employs a quantitative measure (Structural Similarity Index, SSIM) to verify and enforce the semantic equivalence and perceptual realism of its fuzzed inputs, rejecting any mutated image below a tuned SSIM threshold.",
          "evidence": "Section IV.A (Metamorphic Transformation): \u201cTo verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8] \u2026 Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.\u201d"
        },
        "manual": {
          "value": "High",
          "why": "Enforces naturalness across iterations by computing SSIM after each mutation and rejecting any visually distorted image."
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "DeepEvolution\u2019s discovered failures arise from semantic\u2010level image transformations (brightness, contrast, blur, affine transforms) applied on continuous pixel representations without explicit clipping and rounding to a discrete I/O format. Such transformations may not persist unchanged through standard I/O serialization (e.g., saving/loading as 8-bit PNG/JPEG), and the paper does not include any validation of I/O\u2010stability of the mutations.",
          "evidence": "Section IV.A (Metamorphic Transformation): \u201cBecause each image-based transformation has a theoretical domain \u2026 we manually tune them to set up the appropriate range of values\u2026 To verify that generated inputs remain semantically equivalent \u2026 we compute a Structural Similarity Index (SSIM)\u2026 We expect that pixel-based mutated inputs differ from their originals \u2026 but a very low SSIM \u2026 Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.\u201d (no mention of clipping or rounding to ensure I/O persistence)"
        },
        "gemini": {
          "value": "Medium",
          "why": "DeepEvolution generates fuzzed inputs using semantic-level transformations, specifically pixel-value and affine transformations. While the paper mentions manually tuning parameter ranges and using SSIM to ensure 'semantic equivalence' and 'data distribution' are preserved, it does not explicitly describe mechanisms like clipping values to a specific range (e.g., 0-255) or rounding to integer representations *after* transformations to ensure that these transformed inputs would persist stably if saved and reloaded through standard I/O operations (e.g., as 8-bit images). Without such explicit measures, minor deviations introduced by these semantic-level transformations on continuous representations could be lost during serialization due to precision loss.",
          "evidence": "Page 2, Section IV.A. Metamorphic Transformation: \"First, we gather a list of parametric image-based transformations that can be organised in two groups: 1) Pixel-value transformations: change image contrast, image brightness, image blur, image sharpness and random perturbations within a limited interval. 2) Affine transformations: image translation, image scaling, image shearing, and image rotation.\"\n\nPage 2-3, Section IV.A. Metamorphic Transformation: \"To infer the valid domain interval of each transformation's parameters, we manually tune them to set up the appropriate range of values, i.e., high and low boundaries, that preserves the input semantics before and after each transformation, with respect to the data distribution.\"\n\nPage 3, Section IV.A. Metamorphic Transformation: \"To verify that generated inputs remain semantically equivalent to the original inputs, we compute a Structural Similarity Index (SSIM) [8] which assesses the similarity between two images... Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.\""
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DeepEvolution\u2019s discovered failures arise from semantic\u2010level image transformations (brightness, contrast, blur, affine transforms) applied on continuous pixel representations without explicit clipping and rounding to a discrete I/O format. Such transformations may not persist unchanged through standard I/O serialization (e.g., saving/loading as 8-bit PNG/JPEG), and the paper does not include any validation of I/O\u2010stability of the mutations.",
          "evidence": "Section IV.A (Metamorphic Transformation): \u201cBecause each image-based transformation has a theoretical domain \u2026 we manually tune them to set up the appropriate range of values\u2026 To verify that generated inputs remain semantically equivalent \u2026 we compute a Structural Similarity Index (SSIM)\u2026 We expect that pixel-based mutated inputs differ from their originals \u2026 but a very low SSIM \u2026 Therefore, we reject mutated synthetic inputs for which SSIM values are lower than a tuned threshold.\u201d (no mention of clipping or rounding to ensure I/O persistence)"
        },
        "manual": {
          "value": "Medium",
          "why": "Semantic-level pixel transformations (contrast, blur, rotation, etc.) but no clipping/rounding to ensure IO persistence"
        }
      },
  
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper evaluates failures only within each individual model (LeNet on MNIST and CifarNet on CIFAR-10) and does not analyze whether the same failing inputs or mutation patterns induce equivalent erroneous behaviors across different models performing the same task.",
          "evidence": "Section V.A Experiment Setup (page 455): \u201cFor each dataset (i.e., MNIST and CIFAR-10), we took, respectively, the official open-source implementation of Tensorflow models, LeNet and CifarNet\u2026\u201d; Section V.C RQ2 (page 457): \u201cDeepEvolution can effectively generate test cases that trigger erroneous behaviors of the DNN. Table II presents erroneous behaviors detected by each metaheuristic algorithm.\u201d No cross-model transferability analysis is reported."
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates DeepEvolution's effectiveness across multiple models (LeNet and CifarNet) and datasets (MNIST and CIFAR-10). However, the results for increasing neuronal coverage, detecting erroneous behaviors, and finding quantization defects are presented independently for each model-dataset pair. There is no analysis or discussion demonstrating that specific failing inputs or mutation patterns discovered on one model cause equivalent or similar erroneous behaviors across *other models performing the same task*, nor any analysis of cross-model consistency in how mutations relate to failures. The study treats each model's evaluation as a separate experiment.",
          "evidence": "Page 3, Section V. Empirical Evaluation:\n\"We assess the effectiveness of DeepEvolution through the following three research questions:\nRQ1: How much can DeepEvolution increase the coverage of generated test cases?\nRQ2: Can DeepEvolution detect diverse erroneous behaviors in DNN models?\nRQ3: Can DeepEvolution detect divergences induced by DNN quantization?\"\n\nPage 3, Section V.A. Experiment Setup:\n\"Datasets. We selected the two popular publicly available datasets, MNIST [16] and CIFAR-10 [17], as our evaluation subjects... DNNs. For each dataset (i.e., MNIST and CIFAR-10), we took, respectively, the official open-source implementation of Tensorflow models, LeNet [3] and CifarNet [4] to allow the reproducibility of our results and comparisons with our approach.\"\n\nPage 3, Table I. THE REACHED NEURON COVERAGE PER METAHEURISTIC:\n(Shows results separately for MNIST D1, MNIST D2, CIFAR-10 D1, CIFAR-10 D2, for each metaheuristic)\n\nPage 4, Table II. NUMBER OF ERRONEOUS BEHAVIORS PER METAHEURISTIC:\n(Shows results separately for MNIST D1, MNIST D2, CIFAR-10 D1, CIFAR-10 D2, for each metaheuristic)\n\nPage 4, Table III. NUMBER OF QUANTIZATION DEFECTS PER METAHEURISTIC:\n(Shows results separately for MNIST D1, MNIST D2, for each test method)\n\nPage 4, Section VI. Threats to Validity:\n\"The selection of experimental subjects (i.e., dataset and DNN models) is a threat to the generalizability of our results. We mitigate this threat by using practical model sizes and commonly-studied MNIST and CIFAR-10 datasets.\""
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper evaluates failures only within each individual model (LeNet on MNIST and CifarNet on CIFAR-10) and does not analyze whether the same failing inputs or mutation patterns induce equivalent erroneous behaviors across different models performing the same task.",
          "evidence": "Section V.A Experiment Setup (page 455): \u201cFor each dataset (i.e., MNIST and CIFAR-10), we took, respectively, the official open-source implementation of Tensorflow models, LeNet and CifarNet\u2026\u201d; Section V.C RQ2 (page 457): \u201cDeepEvolution can effectively generate test cases that trigger erroneous behaviors of the DNN. Table II presents erroneous behaviors detected by each metaheuristic algorithm.\u201d No cross-model transferability analysis is reported."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ],
      "oracle": [
        "Metamorphic",
        "Differential"
      ]
    }
  },
  "Ma et al. - 2021 - HDTest Differential Fuzz Testing of Brain-Inspired Hyperdimensional Computing": {
    "key": "ma2021hdtest",
    "name": "HDTest",
    "year": "2021",
    "citation_count": "39",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "HDTest uncovers low-level functional errors by generating adversarial inputs that cause misclassifications in the HDC model, without violating higher-level policies or safety boundaries.",
          "evidence": "Page 391, Abstract: \u201cUsing handwritten digit classification as an example, we show that HDTest can generate thousands of adversarial inputs with negligible perturbations that can successfully fool HDC models.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "HDTest is designed to uncover instances where hyperdimensional computing (HDC) models produce incorrect classifications or 'wrong predictions' when subjected to small perturbations on inputs. The core outcome demonstrated is the misclassification of images, such as a digit '8' being wrongly predicted as '3' after mutation. This aligns with triggering low-level functional errors in discriminative models.",
          "evidence": "Page 1, Abstract: 'we design, implement, and evaluate HDTest to test HDC model by automatically exposing unexpected or incorrect behaviors under rare inputs.' ... 'HDTest continuously mutates original inputs to generate new inputs that can trigger incorrect behaviors of HDC model.' ... 'we show that HDTest can generate thousands of adversarial inputs with negligible perturbations that can successfully fool HDC models.'\n\nPage 1, Introduction: 'Just like DNNs, HDC can also be vulnerable to small perturbations on inputs to produce wrong classification, as shown in Fig. I.'\n\nPage 3, Algorithm 1 HDTest Mutation Algorithm, Lines 7-8: 'y_seeds = HDC(seeds) if y' in y_seeds != y then' - This condition explicitly checks for a discrepancy in the predicted label ('y_seeds') from the original label ('y').\n\nPage 4, Section V.A. Experimental Setup: 'We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.'\n\nPage 2, Figure 1 (c): 'the mutated image wrongly predicted as \"3\"' (original was '8')."
        },
        "arbitrator": {
          "value": "Low",
          "why": "HDTest uncovers low-level functional errors by generating adversarial inputs that cause misclassifications in the HDC model, without violating higher-level policies or safety boundaries.",
          "evidence": "Page 391, Abstract: \u201cUsing handwritten digit classification as an example, we show that HDTest can generate thousands of adversarial inputs with negligible perturbations that can successfully fool HDC models.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "HDTest performs untargeted exploration by generating any input that causes the HDC model to misclassify, without aiming for a specific target label or semantic category. Its goal is to reveal generic faults (incorrect predictions) rather than steering the model toward predefined faults or behaviors.",
          "evidence": "Section IV (HDTest Mutation Algorithm), p. 393: \u201cif y\u2032 != y then // Indicating successful mis-predicted image generated.\u201d\nAbstract, p. 391: \u201cHDTest continuously mutates original inputs to generate new inputs that can trigger incorrect behaviors of HDC model.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The HDTest approach aims to identify generic faults in Hyperdimensional Computing (HDC) models, specifically misclassifications or 'incorrect behaviors,' without specifying a particular target output for the adversarial examples. The fuzzer iteratively mutates inputs until the model's prediction for the mutated input differs from its original prediction, indicating a successful adversarial attack, but it does not steer towards a chosen incorrect label or a specific category of semantic fault. The guidance mechanism (distance-guided mutation) is designed to efficiently discover *any* misclassification rather than a predefined one.",
          "evidence": "Page 1, Abstract: 'HDTest continuously mutates original inputs to generate new inputs that can trigger incorrect behaviors of HDC model.'\nPage 1, Abstract: 'we show that HDTest can generate thousands of adversarial inputs with negligible perturbations that can successfully fool HDC models.'\nPage 3, Section IV, HDTest FRAMEWORK: 'By comparing the query labels with reference labels, HDTest is able to know if there are any discrepancies which indicate successful generation of an adversarial image.'\nPage 3, Algorithm 1 HDTest Mutation Algorithm, Line 8: 'if y' in y_seeds! = y then' (Checks if the new prediction y' is different from the original prediction y). \nPage 4, Section IV, HDTest FRAMEWORK: 'To maximize the efficiency of the fuzz testing process, HDTest uses distance-guided mutation for seeds update based on the distance between query HV and reference HV. ... Higher fitness means lower similarity between the HV of the seed and the original input image's HV, indicating higher possibility to generate an adversarial image.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "HDTest performs untargeted exploration by generating any input that causes the HDC model to misclassify, without aiming for a specific target label or semantic category. Its goal is to reveal generic faults (incorrect predictions) rather than steering the model toward predefined faults or behaviors.",
          "evidence": "Section IV (HDTest Mutation Algorithm), p. 393: \u201cif y\u2032 != y then // Indicating successful mis-predicted image generated.\u201d\nAbstract, p. 391: \u201cHDTest continuously mutates original inputs to generate new inputs that can trigger incorrect behaviors of HDC model.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "Finds generic misclassifications"
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "HDTest focuses on discovering and generating adversarial inputs that cause HDC model failures but does not investigate or explain the internal mechanisms or model properties that lead to these failures. It uses hypervector distance purely as a heuristic to guide input mutations, without performing any diagnostic analysis of feature activations, decision boundaries, training dynamics, or semantic confusion patterns that underlie the vulnerabilities.",
          "evidence": "Section IV (HDTest Framework): \u201cTo maximize the efficiency of the fuzz testing process, HDTest uses distance-guided mutation for seeds update based on the distance between query HV and reference HV\u2026 Higher fitness means lower similarity between the HV of the seed and the original input image\u2019s HV, indicating higher possibility to generate an adversarial image.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The HDTest approach explains where and how failures manifest by using correlational analysis of internal signals and external interpretation of behavioral patterns. It leverages the unique property of HDC, specifically the distance between query hypervector and reference hypervector (an internal signal), to guide its fuzzing process. The paper discusses 'vulnerable cases' as an external behavioral pattern where adversarial images are easily generated with minimal perturbations. Furthermore, its per-class analysis explicitly identifies that the difficulty of generating adversarial examples varies significantly across different digit classes, indicating an external interpretation of class-level susceptibility or bias. While it effectively identifies how and where the model fails, it does not establish causal links to deeper architectural, training, or data-related weaknesses.",
          "evidence": "Page 1, Abstract: 'Guided by the distance between query hypervector and reference hypervector in HDC, HDTest continuously mutates original inputs to generate new inputs that can trigger incorrect behaviors of HDC model.'\nPage 2, Contributions: 'HDTest leverages unique property of HDC and introduce the distance-guided fuzzing based on the distance between query vector and reference vector to improve fuzzing efficiency.'\nPage 4, Section IV: 'To maximize the efficiency of the fuzz testing process, HDTest uses distance-guided mutation for seeds update based on the distance between query HV and reference HV. That is, during the mutation process, only the top-N fittest seeds can survive...The fitness of seeds are defined as: fitness = 1 - Cosim(AM[y], HDC(seed))...Higher fitness means lower similarity between the HV of the seed and the original input image's HV, indicating higher possibility to generate an adversarial image.'\nPage 5, Section V.B: 'Such vulnerable cases bring potential security loopholes since they can result in incorrect behaviors of HDC models by only minor and even negligible perturbations. Therefore, such images should be emphasized when defending attacks to the HDC systems, and HDTest is able to pinpoint and highlight them.'\nPage 5, Section V.C: 'We then perform a per-class analysis on L1/L2 distance and fuzzing iterations as shown in Fig. 7. We can observe that for some classes such as \"1\", the average fuzzing iteration is drastically higher, which means that it is relatively difficult to generate adversarial examples. For other classes such as \"9\", the average fuzzing iteration is lower.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "While HDTest does not uncover deep causal mechanisms (e.g., architectural or training flaws) that give rise to HDC model failures, it does go beyond merely reporting misclassifications. By leveraging the internal hypervector distance (cosine similarity) between query and reference vectors to guide mutations, and by performing per\u2010class and per\u2010image analyses of how many and how large perturbations are needed to cause failures, HDTest provides correlational insights into where and how vulnerabilities occur in the model\u2019s representation space.",
          "evidence": "Section IV (HDTest Framework): uses \u2018distance-guided mutation ... based on the distance between query HV and reference HV\u2019 to steer fuzzing. Section V.B\u2013V.C: reports per-class L1/L2 distances and average mutation counts, interpreting that some digits (e.g., \u201c1\u201d vs. \u201c9\u201d) are more or less susceptible to adversarial perturbations."
        },
        "manual": {
          "value": "Low",
          "why": " It uses hypervector distance purely as a heuristic to guide input mutations - no insight where/how/why the faults occur"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "HDTest enforces a design constraint on perturbation size (a norm\u2010based limit, e.g., L2\u2009<\u20091) to keep mutations 'invisible', but it does not perform any empirical validation (such as SSIM, LPIPS or human evaluation) to demonstrate that the fuzzed inputs remain perceptually or semantically natural.",
          "evidence": "Section IV (Mutation Algorithm), paragraph beginning \u201cTo ensure the added perturbations are within an \u2018invisible\u2019 range\u2026\u201d: \u201cwe set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper enforces design constraints (bounded perturbations) to suggest input naturalness, specifically by setting a threshold for the L2 distance to keep perturbations within an 'invisible' range. It quantifies the magnitude of these perturbations using L1 and L2 distances. However, it does not provide direct empirical validation of perceptual or semantic consistency using specialized metrics like LPIPS, SSIM, or human evaluation.",
          "evidence": "Page 1, Abstract: 'HDTest can generate thousands of adversarial inputs with negligible perturbations that can successfully fool HDC models.'\n\nPage 3, Section IV, Paragraph 4: 'To ensure the added perturbations are within an \"invisible\" range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.'\n\nPage 4, Section V.A, Paragraph 2: 'We use different metrics to evaluate HDTest: normalized L1 distance and normalized L2 distance between the mutated image and original image... A smaller distance between the generated image and the original image is preferred by HDTest because it means less perturbation added to the image.'\n\nPage 4, Section V.B, Paragraph 2: 'For the amount of perturbations, rand shows smallest L1 and L2 distance of 0.58 and 0.09, respectively.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "HDTest enforces a design constraint on perturbation size (a norm\u2010based limit, e.g., L2\u2009<\u20091) to keep mutations 'invisible', but it does not perform any empirical validation (such as SSIM, LPIPS or human evaluation) to demonstrate that the fuzzed inputs remain perceptually or semantically natural.",
          "evidence": "Section IV (Mutation Algorithm), paragraph beginning \u201cTo ensure the added perturbations are within an \u2018invisible\u2019 range\u2026\u201d: \u201cwe set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "norm-bounded"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Low",
          "why": "HDTest\u2019s adversarial inputs rely on small, continuous pixel\u2010level perturbations (Gaussian or random noise) without any clipping or rounding steps to ensure the perturbations survive standard I/O quantization (e.g., 8-bit image encoding). As a result, the discovered faults are likely to disappear once the images are serialized and quantized.",
          "evidence": "Section IV (Mutation Algorithm), page 394: \u201cTo ensure the added perturbations are within an \u2018invisible\u2019 range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.\u201d; Table I (Mutation Strategies), page 394: \u201crand: apply random noise over the entire image\u201d; \u201cgauss: apply gaussian noise over the entire image.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The paper describes mutation strategies such as adding Gaussian or random noise to image pixels. While it mentions an L2 distance threshold to keep perturbations 'within an invisible range' and discards images 'beyond this limit,' it does not explicitly state that the mutated pixel values are clipped or rounded to their original integer range (0-255 for MNIST grayscale pixels) after perturbation. Without such explicit handling, small continuous perturbations introduced by noise addition may rely on floating-point precision and could disappear or change due to implicit rounding or quantization when processed through standard I/O operations like saving and reloading the image.",
          "evidence": "1. **Pixel Value Range:** \"For the MNIST dataset we use in this paper, since the image size is 28 \u00d7 28 and the pixel range is 0 to 255 in greyscale, we flat a single image into an array with 784 elements with values ranging from 0 to 255.\" (Page 3, Section III.A, column 1, paragraph 2)\n2. **Mutation Strategies:** \"We introduce multiple mutation strategies for HDTest's mutation algorithm as listed in Table I. ... The first two strategies, gauss and rand add Gaussian and random noise onto the image, resulting in holographic mutation on the image.\" (Page 4, Section IV, column 2, paragraph 4)\n3. **Perturbation Constraint:** \"To ensure the added perturbations are within an 'invisible' range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.\" (Page 4, Section IV, column 2, paragraph 3)\n4. **Nature of Perturbations:** \"HDTest can generate thousands of adversarial inputs with negligible perturbations that can successfully fool HDC models.\" (Page 1, Abstract)\n\n*Missing evidence for clipping/rounding after mutation.*"
        },
        "arbitrator": {
          "value": "Low",
          "why": "HDTest\u2019s adversarial inputs rely on small, continuous pixel\u2010level perturbations (Gaussian or random noise) without any clipping or rounding steps to ensure the perturbations survive standard I/O quantization (e.g., 8-bit image encoding). As a result, the discovered faults are likely to disappear once the images are serialized and quantized.",
          "evidence": "Section IV (Mutation Algorithm), page 394: \u201cTo ensure the added perturbations are within an \u2018invisible\u2019 range, we set a threshold for the distance metric during fuzzing (e.g., L2 < 1). When generated images are beyond this limit, it is regarded as unacceptable and then discarded.\u201d; Table I (Mutation Strategies), page 394: \u201crand: apply random noise over the entire image\u201d; \u201cgauss: apply gaussian noise over the entire image.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "In memory floating point mutation, no safeguards for IO preservation"
        }
      },
 
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper evaluates HDTest only on a single HDC model trained and tested on the MNIST dataset. There is no analysis of whether the same adversarial inputs or mutation patterns transfer failures across different HDC models performing the same task.",
          "evidence": "Section V.A (Experimental Setup): \u201cWe use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates the proposed HDTest method on a single Hyperdimensional Computing (HDC) model that was developed within the paper for image classification. It does not analyze or demonstrate whether the discovered adversarial inputs or mutation patterns cause similar failure behaviors across multiple, distinct HDC models or different architectural implementations of HDC for the same task. The evaluation focuses on the effectiveness of HDTest on this specific model and its ability to generate adversarial examples and improve the robustness of *that* model through retraining, rather than exploring cross-model transferability of faults.",
          "evidence": "Section: 'III. HDC MODEL DEVELOPMENT' (Page 2)\nQuote: 'We develop an HDC model for image classification as our testing target. We develop three key phases: Encoding, Training and Testing as illustrated in Fig. 2.'\n\nSection: 'V. EXPERIMENTAL RESULTS - A. Experimental Setup' (Page 4)\nQuote: 'We use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.'\n\nSection: 'V. EXPERIMENTAL RESULTS - D. Case Study on Defense Against Adversarial Attacks' (Page 5)\nQuote: 'First, for a specific HDC model, we run HDTest to generate 1000 adversarial images. We then randomly split such 1000 images into two subsets. We feed the first subset of adversarial images to the HDC model with correct labels to retrain the HDC model, i.e., updating the reference HVs. Next, we use the second subset of adversarial images to attack the HDC model. Our experimental results show that, after retraining, the rate of successful attack rate drops more than 20%.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper evaluates HDTest only on a single HDC model trained and tested on the MNIST dataset. There is no analysis of whether the same adversarial inputs or mutation patterns transfer failures across different HDC models performing the same task.",
          "evidence": "Section V.A (Experimental Setup): \u201cWe use the MNIST database of handwritten digits [30] as our dataset for training and testing the HDC model at an accuracy around 90%.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "oracle": [
        "Metamorphic"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ]
    }
  },
  "Du et al. - 2019 - DeepStellar model-based quantitative analysis of stateful deep learning systems": {
    "key": "du2019deepstellar",
    "name": "DeepStellar",
    "year": "2019",
    "citation_count": "188",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepStellar\u2019s fuzzing-based testing and adversarial-sample detection reveal low-level functional errors\u2014specifically misclassifications in RNN-based discriminative models\u2014without bypassing any explicit safety or policy mechanisms or producing norm-violating behaviors.",
          "evidence": "Abstract (page 478): \u201cthe coverage criteria are useful in revealing erroneous behaviors (generating three times more adversarial samples than random testing and hundreds times more than the unrolling approach).\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "DeepStellar primarily focuses on detecting and generating 'adversarial samples' that lead to incorrect predictions or recognition by the RNN. This falls under triggering low-level functional errors, such as misclassification or flawed outputs, and exposing instability under perturbation. The paper does not provide evidence of uncovering vulnerabilities related to bypassing explicit safety/security mechanisms (High) or violating normative/ethical expectations while remaining functionally valid (Medium).",
          "evidence": "Page 1, Abstract: \"(1) the similarity metrics could effectively capture the differences between samples even with very small perturbations (achieving 97% accuracy for detecting adversarial samples) and (2) the coverage criteria are useful in revealing erroneous behaviors (generating three times more adversarial samples than random testing and hundreds times more than the unrolling approach).\nPage 5, Section 5.1 'Adversarial Sample Detection for RNNs': \"Adversarial sample detection aims to check whether a given input is an adversarial sample at runtime.\"\nPage 6, Section 5.2 'Coverage-Guided Testing of RNNs': \"In this section, we propose a Coverage-Guided Testing (CGT) technique which aims to generate adversarial samples incorrectly recog-nized by an RNN.\"\nPage 6, Section 5.2: \"A mutant is an adversarial sample if it is predicted incorrectly by the network.\"\nPage 9, Section 6.4 'RQ3. Adversarial Sample Detection': \"The results confirm that the trace similarity-based method is ef-fective for adversarial sample detection under carefully selected abstraction configurations, with more than 89% prediction accuracy.\""
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepStellar\u2019s fuzzing-based testing and adversarial-sample detection reveal low-level functional errors\u2014specifically misclassifications in RNN-based discriminative models\u2014without bypassing any explicit safety or policy mechanisms or producing norm-violating behaviors.",
          "evidence": "Abstract (page 478): \u201cthe coverage criteria are useful in revealing erroneous behaviors (generating three times more adversarial samples than random testing and hundreds times more than the unrolling approach).\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepStellar\u2019s fuzzing (coverage\u2010guided testing) performs broad, untargeted exploration aimed at uncovering generic faults (misclassifications) rather than steering inputs toward a specific, predefined label or output. Test inputs are mutated via random metamorphic transformations and retained if they simply induce any incorrect prediction, without a fixed adversarial target.",
          "evidence": "Section 5.2 (Coverage-Guided Testing of RNNs), page 481: \u201cCoverage-Guided Testing (CGT) which aims to generate adversarial samples incorrectly recognized by an RNN. \u2026 Randomly pick transformation t with a random parameter p; A = mutate(t, p, a); \u2026 if Failed(a\u2032, result) then \u2026\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The coverage-guided testing (fuzzing approach) proposed in DeepStellar aims to generate adversarial samples that are simply 'incorrectly recognized by an RNN'. The definition of a 'mutant' being an 'adversarial sample' is when it is 'predicted incorrectly by the network' (Section 5.2, page 6). This objective does not steer the exploration towards specific, predefined target faults (like forcing a classifier to predict a *chosen* incorrect label or a speech model to output a *particular* incorrect phrase). Instead, it broadly seeks any misclassification or incorrect prediction, aligning with the definition of untargeted exploration for generic faults.",
          "evidence": "Page 6, Section 5.2 'Coverage-Guided Testing of RNNs' and Algorithm 2:\n- 'In this section, we propose a Coverage-Guided Testing (CGT) technique which aims to generate adversarial samples incorrectly recognized by an RNN.'\n- 'A mutant is an adversarial sample if it is predicted incorrectly by the network.'\n- Algorithm 2, Line 10: 'if Failed(a', result) then F <- F U {a'}' (where 'Failed' indicates incorrect inference, not a specific incorrect target)."
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepStellar\u2019s fuzzing (coverage\u2010guided testing) performs broad, untargeted exploration aimed at uncovering generic faults (misclassifications) rather than steering inputs toward a specific, predefined label or output. Test inputs are mutated via random metamorphic transformations and retained if they simply induce any incorrect prediction, without a fixed adversarial target.",
          "evidence": "Section 5.2 (Coverage-Guided Testing of RNNs), page 481: \u201cCoverage-Guided Testing (CGT) which aims to generate adversarial samples incorrectly recognized by an RNN. \u2026 Randomly pick transformation t with a random parameter p; A = mutate(t, p, a); \u2026 if Failed(a\u2032, result) then \u2026\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "Broad exploration to uncover functional errors"
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Medium",
          "why": "DeepStellar analyzes internal RNN signals\u2014state\u2010vector activations, transition frequencies, and trace divergences\u2014and shows how these correlate with adversarial failures (e.g. via trace similarity and coverage sensitivity). This provides heuristic or correlational explanations of where and how failures arise, but does not establish causal links to specific architectural or training/data weaknesses.",
          "evidence": "Section 3.3 (Trace Similarity Metrics): \u201cwe define the trace similarity metrics to quantify the proximity of their induced state transitions on the abstract model.\u201d \nSection 6.2 (RQ1) / Table 3: \u201cCorrelation of trace similarities and prediction diff. \u2026 All reported correlations are statistically significant (p << 0.01). Negative association \u2026 indicates that the larger the similarity metrics, the less different the predicted transcripts would be.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "DeepStellar provides a medium level of root-cause insight by using an abstract model of RNNs to perform correlational analysis of internal signals (abstract states and transitions) and identify behavioral patterns linked to failures. It explains *where and how* failures manifest, for example, by linking under-explored 'corner-case regions' in the state space or significant divergences in internal traces to the occurrence of adversarial samples and erroneous behaviors. While it identifies plausible contributing factors, it stops short of establishing causal links to specific architectural or training weaknesses.",
          "evidence": "Page 1, Abstract: \"...the coverage criteria are useful in revealing erroneous behaviors (generating three times more adversarial samples than random testing and hundreds times more than the unrolling approach).\"\nPage 5, Section 4.1, Basic State Coverage: \"The test data should cover the major function region extensively to validate the trained behaviors and cover the corner-case region sufficiently in order to discover defects in unseen behaviors.\"\nPage 5, Section 4.1, n-Step State Boundary Coverage: \"The n-step state boundary coverage measures how well the corner-case regions are covered by the test inputs T.\"\nPage 5, Section 5.1, Adversarial Sample Detection for RNNs: \"The traces derived from the original and the reference samples are typically similar when i is benign. However, when i is adversarial, the trace difference between the two samples can be much larger.\"\nPage 8, Section 6.3, RQ2 Answer: \"All proposed coverage criteria are sensitive to erroneous behaviors in adversarial samples, among which, BSCov, and BTCov are the most sensitive ones.\"\nPage 9, Section 6.5, RQ4 Answer: \"The overall results indicate that covering more new states could be potentially helpful in generating adversarial samples.\""
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DeepStellar analyzes internal RNN signals\u2014state\u2010vector activations, transition frequencies, and trace divergences\u2014and shows how these correlate with adversarial failures (e.g. via trace similarity and coverage sensitivity). This provides heuristic or correlational explanations of where and how failures arise, but does not establish causal links to specific architectural or training/data weaknesses.",
          "evidence": "Section 3.3 (Trace Similarity Metrics): \u201cwe define the trace similarity metrics to quantify the proximity of their induced state transitions on the abstract model.\u201d \nSection 6.2 (RQ1) / Table 3: \u201cCorrelation of trace similarities and prediction diff. \u2026 All reported correlations are statistically significant (p << 0.01). Negative association \u2026 indicates that the larger the similarity metrics, the less different the predicted transcripts would be.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Shows correlation between coverage criteria and faults"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "DeepStellar\u2019s coverage-guided testing applies domain-specific, metamorphic transformations (e.g. slight speed/volume adjustments for audio; brightness/translation/noise for images) with conservative, bounded perturbation parameters to preserve semantics. However, it does not provide empirical validation of naturalness (e.g. SSIM, LPIPS, BLEU or human evaluation).",
          "evidence": "Section 5.2 (Coverage-Guided Testing): \u201cMutation operators are often specific to the application domains. For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human\u2019s perspective during testing.\u201d; Section 6.1 (Data Preparation): \u201cwe set conservative parameters for transformation so that the perturbation on original samples is slight and imperceptible.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper indicates that the fuzzer's mutation process is designed to maintain input naturalness by applying 'conservative parameters for transformation' that result in 'slight and imperceptible' perturbations. It also mentions using a 'metamorphic mutation strategy' to preserve 'prediction meaning from the human's perspective'. These statements describe design constraints and theoretical justifications aimed at ensuring input naturalness. However, the paper does not provide explicit empirical evidence, such as quantitative metrics (e.g., LPIPS, SSIM, BLEU) or a formal human evaluation, to validate the naturalness of the fuzzed inputs. The assertion that perturbations are 'not perceptible by human' is a design goal rather than a reported empirical finding from a naturalness evaluation.",
          "evidence": "Page 6, Section 5.2, 'Coverage-Guided Testing of RNNs': \"Mutation operators are often specific to the application domains. For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human's perspective during testing.\"\n\nPage 7, Section 6.1, 'Experiment Settings', 'Data Preparation' subsection: \"For both audio and image case, we set conservative parameters for transformation so that the perturbation on original samples is slight and imperceptible. Note that all the adversarial samples are also with minimal perturbations and not perceptible by human.\""
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DeepStellar\u2019s coverage-guided testing applies domain-specific, metamorphic transformations (e.g. slight speed/volume adjustments for audio; brightness/translation/noise for images) with conservative, bounded perturbation parameters to preserve semantics. However, it does not provide empirical validation of naturalness (e.g. SSIM, LPIPS, BLEU or human evaluation).",
          "evidence": "Section 5.2 (Coverage-Guided Testing): \u201cMutation operators are often specific to the application domains. For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human\u2019s perspective during testing.\u201d; Section 6.1 (Data Preparation): \u201cwe set conservative parameters for transformation so that the perturbation on original samples is slight and imperceptible.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Design contraints on perturbations but no naturalness measure of final fault-triggering inputs"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepStellar\u2019s test generation and adversarial sample detection operate on continuous, in-memory perturbations (e.g., floating-point pixel or audio signal changes) without any explicit clipping, rounding, or format encoding steps. Thus the discovered faults likely depend on fragile floating-point artifacts that would be lost once inputs undergo standard I/O quantization (e.g., 8-bit image or 16-bit audio), and the paper does not validate I/O-stable mutations.",
          "evidence": "Page 482, Section 6.1 (\u2018Experiment Settings\u2019): \u201cFor the image classification task, we \u2026 generate 10,000 perturbed samples \u2026 with existing image transformation techniques (i.e., image contrast, brightness, translation, scaling, shearing, rotation and add white noise).\u201d  No mention is made of saving or re-encoding inputs through standard I/O formats (e.g., PNG/JPEG quantization)."
        },
        "gemini": {
          "value": "Low",
          "why": "The paper describes the generation of 'perturbed samples' using semantic-level transformations (e.g., image contrast, brightness, rotation for images; speed, volume adjustment for audio) and 'adversarial samples' using methods like FGSM, BIM, and DeepFool. For these adversarial samples, the paper explicitly states they are 'with minimal perturbations'. These minimal, continuous perturbations generated by attack tools are known to depend on floating-point precision and are highly susceptible to disappearing after standard I/O operations (like quantization or rounding) if not explicitly handled. The paper does not mention any explicit validation, clipping, or rounding procedures applied to ensure the I/O stability of these generated perturbations. Even for semantic transformations, without explicit clipping and rounding, minor deviations can disappear during serialization, as noted in the metric definition.",
          "evidence": "Page 7, Section 6.1 Experiment Settings - Data Preparation:\n- 'perturbed samples which are generated by a slight perturbation on the original benign samples'\n- 'For ASR models, we generate 10,000 perturbed audios from original benign ones with existing audio data augmentation techniques [5] (i.e., speed and volume adjustment, low/high-frequency filtering, noise blending).'\n- 'For each MNIST model, we also generate 10,000 benign perturbed samples with existing image transformation techniques [50] (i.e., image contrast, brightness, translation, scaling, shearing, rotation and add white noise).'\n- 'Besides, we generate 10,000 adversarial samples with each state-of-the-art attack tool, including, FGSM [17], BIM [28] and DeepFool [37].'\n- 'For both audio and image case, we set conservative parameters for transformation so that the perturbation on original samples is slight and imperceptible. Note that all the adversarial samples are also with minimal perturbations and not perceptible by human.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepStellar\u2019s test generation and adversarial sample detection operate on continuous, in-memory perturbations (e.g., floating-point pixel or audio signal changes) without any explicit clipping, rounding, or format encoding steps. Thus the discovered faults likely depend on fragile floating-point artifacts that would be lost once inputs undergo standard I/O quantization (e.g., 8-bit image or 16-bit audio), and the paper does not validate I/O-stable mutations.",
          "evidence": "Page 482, Section 6.1 (\u2018Experiment Settings\u2019): \u201cFor the image classification task, we \u2026 generate 10,000 perturbed samples \u2026 with existing image transformation techniques (i.e., image contrast, brightness, translation, scaling, shearing, rotation and add white noise).\u201d  No mention is made of saving or re-encoding inputs through standard I/O formats (e.g., PNG/JPEG quantization)."
        },
        "manual": {
          "value": "Medium",
          "why": "Applies deephunter metamorphic mutations that are plausibly I/O-stable but does not apply clipping and rounding to ensure"
        }
      },
 
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "DeepStellar\u2019s evaluation is performed separately on each RNN model (two ASR models and two image\u2010classification models) without analyzing whether the same failing inputs (e.g., adversarial samples) transfer failures across different models performing the same task.",
          "evidence": "Abstract (p. 477): \u201cWe evaluate DeepStellar on four RNN\u2010based systems covering image classification and automated speech recognition.\u201d  \nSection 6 (p. 484): The paper\u2019s research questions (RQ1\u2013RQ4) and experiments focus on each model independently, with no cross\u2010model transferability analysis."
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates multiple RNN models (DeepSpeech-0.1.1, DeepSpeech-0.3.0, MNIST-LSTM, MNIST-GRU) for tasks like image classification and automated speech recognition. However, the evaluation of adversarial sample detection and coverage-guided testing is conducted and reported independently for each model. There is no analysis presented to show cross-model consistency in how specific failing inputs or mutation patterns relate to failures across different models, nor does it demonstrate shared vulnerability sources. The results focus on the performance of the proposed methods (e.g., detection accuracy, number of unique adversarial samples generated) for each model in isolation.",
          "evidence": "Section 6.1 'Experiment Settings' (Page 7): 'We selected four RNN-based DL models including two ASR models and two image classification models, which cover popular RNN variants (see Table 1).' \n\nSection 6.4 'RQ3. Adversarial Sample Detection' (Page 9): 'This section evaluates DeepStellar for adversarial sample detection on three models, namely, DeepSpeech-0.1.1, MNIST-LTSM, and MNIST-GRU.' Table 5 'AUROC results (%) of trace similarity based adversarial detection by configurations' presents results for each model separately. The conclusion states: 'Similarity metric based method is useful for adversarial sample detection. The detection accuracy varies under different metrics and model configurations.' This indicates performance per model, not cross-model transferability of failures.\n\nSection 6.5 'RQ4. Coverage-guided Testing' (Page 9): 'We use the prepared 100 original benign samples as the initial seeds, which are correctly predicted by both MNIST-LTSM and MNIST-GRU.' Table 6 'Coverage and unique adversarial samples detected' provides results for MNIST-LSTM and MNIST-GRU independently, showing the 'number of unique crashes discovered in each experiment' for each model. The conclusion states: 'The coverage-guided testing is generally useful in terms of achieving higher coverage and guiding adversarial sample exploration.' These findings are generalized statements about the method's usefulness for RNNs but do not detail cross-model failure transferability."
        },
        "arbitrator": {
          "value": "Low",
          "why": "DeepStellar\u2019s evaluation is performed separately on each RNN model (two ASR models and two image\u2010classification models) without analyzing whether the same failing inputs (e.g., adversarial samples) transfer failures across different models performing the same task.",
          "evidence": "Abstract (p. 477): \u201cWe evaluate DeepStellar on four RNN\u2010based systems covering image classification and automated speech recognition.\u201d  \nSection 6 (p. 484): The paper\u2019s research questions (RQ1\u2013RQ4) and experiments focus on each model independently, with no cross\u2010model transferability analysis."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "oracle": [
        "Metamorphic",
        "Property-based"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ]
    }
  },
  "Huang et al. - 2022 - Coverage-Guided Testing for Recurrent Neural Networks": {
    "key": "huang2021coverage",
    "name": "TESTRNN",
    "year": "2021",
    "citation_count": "66",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "High",
          "why": "TESTRNN not only uncovers low-level misclassification errors (adversarial samples) but also detects backdoor inputs that bypass explicit security mechanisms, revealing system-level vulnerabilities.",
          "evidence": "Section VII-C-2 (page 1201): \u201cAnswer to RQ4: The TESTRNN metrics can identify the difference between the backdoor input and the normal input (to malicious models).\u201d Table VI demonstrates backdoor inputs triggering misclassification in the malicious RNN."
        },
        "gemini": {
          "value": "High",
          "why": "The method proposed in the paper, TESTRNN, is explicitly designed and shown to detect 'backdoor inputs for the security of RNNs'. Backdoor attacks fall under vulnerabilities that bypass explicit security mechanisms, as they involve malicious manipulation of the model's behavior for specific triggers, thus compromising its integrity and adherence to security policies. While the method also identifies adversarial samples which represent lower-level functional errors, its demonstrated capability to find security vulnerabilities elevates its severity rating to High.",
          "evidence": "Page 2, Column 1, 'Contributions': '2) Detecting defects (see Section VII-C), with the observa- tions that TESTRNN can not only find adversarial behav- iors for the robustness of RNNs (see Section VII-C1) but also identify backdoor inputs for the security of RNNs (see Section VII-C2).' \nPage 11, Column 1, 'VII-C. Detecting RNN Defects - 2) Detecting Backdoor Input in RNNs (RQ4)': 'We investigate the possibility of applying coverage-guided testing to the detec- tion of backdoor input in neural networks. We try to exploit if there is any difference between clean input and backdoor input, which can be captured by our proposed test metrics.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "TESTRNN not only uncovers low-level misclassification errors (adversarial samples) but also detects backdoor inputs that bypass explicit security mechanisms, revealing system-level vulnerabilities.",
          "evidence": "Section VII-C-2 (page 1201): \u201cAnswer to RQ4: The TESTRNN metrics can identify the difference between the backdoor input and the normal input (to malicious models).\u201d Table VI demonstrates backdoor inputs triggering misclassification in the malicious RNN."
        },
        "manual": {
          "value": "Low",
          "why": "Even though it can detect backdoor triggers, this is purely diagnostic. The fuzzer exposes low-level functional and robustness failures (misclassifications, metamorphic inconsistencies, and backdoor-induced activation anomalies) without breaching behavioral or safety boundaries."
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "The approach is driven by maximizing coverage metrics to uncover generic defects (e.g., misclassifications, unstable predictions) and does not steer inputs toward any specific, predefined fault or output. It generates diversified, natural test cases without a semantic or label\u2010targeted goal.",
          "evidence": "Page 1191, Section I: \u201cInstead of identifying a particular type of defects, such as adversarial samples, coverage-guided testing is to generate a set of test cases as diversified as possible while preserving the naturalness, in order to exploit the internal behavior of the neural networks that has real operational impact.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The fuzzing approach in the paper performs untargeted exploration to reveal generic faults, such as misclassifications or unstable predictions, without any predefined behavioral goal. The primary objective is to generate a diverse set of test cases that comprehensively exploit the internal behavior of Recurrent Neural Networks (RNNs) by maximizing coverage metrics. While the method can identify adversarial samples and backdoor inputs, these are discovered as a result of broad internal behavior exploration rather than being the specific, predefined targets of the fuzzer's steering mechanism. The test oracle flags any input that leads to a different classification from its original, sufficiently close seed, without specifying what that different classification should be.",
          "evidence": "Page 1, Abstract: 'This article aims to develop a coverage-guided testing approach to systematically exploit the internal behavior of RNNs, with the expectation that such testing can detect defects with high possibility.'\nPage 1, right column, last paragraph: 'we suggest that instead of identifying a particular type of defects, such as adversarial samples, coverage-guided testing is to generate a set of test cases as diversified as possible while preserving the naturalness, in order to exploit the internal behavior of the neural networks that has real operational impact.'\nPage 3, right column, last paragraph: 'What really matters is, within the testing budget, to find defects that are as diverse and natural as possible so that fixing them would gain maximized impact on the delivered reliability.'\nPage 7, Section VI.C 'Test Set Evaluation', paragraph 2: 'A test case x' is said to not pass the oracle if x' is within the norm-ball of some known sample x, i.e., ||x - x'||2 \u2264 roracle, and if x' has a different classification from x, i.e., \u03d5(x) \u2260 \u03d5(x').' This indicates a generic misclassification rather than a specific target label.\nPage 12, Section VII.E 'Comparison With Attack-Based Defect Detection (RQ6)', paragraph 1: 'First, attack methods search for adversarial samples by adding perturbations in the gradient direction. This easily leads to the situation where the generated adversarial samples are concentrated in a 'buggy' area of the input space... compared to attack methods, our testing method exercises different behaviors of RNN and generates a diverse set of test cases, intensively covering the input region around the seed input. This ability will be helpful in exposing more types of defects of the RNN (not merely in the gradient direction).'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "The approach is driven by maximizing coverage metrics to uncover generic defects (e.g., misclassifications, unstable predictions) and does not steer inputs toward any specific, predefined fault or output. It generates diversified, natural test cases without a semantic or label\u2010targeted goal.",
          "evidence": "Page 1191, Section I: \u201cInstead of identifying a particular type of defects, such as adversarial samples, coverage-guided testing is to generate a set of test cases as diversified as possible while preserving the naturalness, in order to exploit the internal behavior of the neural networks that has real operational impact.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "broadly explore the model's internal behavior and flag any unexpected classification (adversarial or backdoor) as a defect."
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Medium",
          "why": "The TESTRNN approach goes beyond merely reporting faulty cases by analyzing internal LSTM signals\u2014such as gate activations, hidden\u2010state coverage and their temporal patterns\u2014to reveal where and how failures (e.g., adversarial or backdoor inputs) manifest. However, it does not establish causal links back to specific architectural or training weaknesses, relying instead on correlational insights.",
          "evidence": "Page 1201, Section VII-C2: \u201cthe backdoor input to malicious classifier will induce different internal activation, which can be seen from the apparent increase of coverage in T+B.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The proposed method, TESTRNN, goes beyond merely reporting faulty cases by designing specific coverage metrics (BC, SC, TC) that expose the internal structural and temporal behaviors of LSTMs. The paper demonstrates how these metrics and the generated test cases can be used to visualize and interpret the model's internal working mechanisms and learning processes. This is achieved by linking input features to internal state changes, memory updates, and temporal patterns, thereby explaining 'where' and 'how' different inputs influence the model's internal dynamics and ultimately its predictions. While it doesn't establish direct causal links to architectural or training weaknesses for specific vulnerabilities, it provides clear correlational insights into the model's internal operations and behavioral patterns.",
          "evidence": "Section: Abstract\nQuote: \"Furthermore, TESTRNN enables meaningful information to be collected and exhibited for users to understand the testing results, which is an important step toward interpretable neural network testing.\"\n\nSection: I. Introduction\nQuote: \"This article is to develop dedicated coverage metrics for RNNs, to take into account the additional structures and temporal semantics.\"\n\nSection: III. Problem Statement\nQuote: \"Exhibition of LSTM internal working mechanism (see Section VII-G), with the conclusion that semantic meanings behind the test metrics can help users understand the learning mechanism of LSTM model, making a step toward interpretable LSTM testing.\"\n\nSection: IV. LSTM TEST COVERAGE METRICS\nQuote: \"SC characterizes the temporal changes between connected cells. ... Example 4.2: The intuition behind SC is to capture these significant inputs to the LSTM. As shown by the sentiment analysis LSTM example in Fig. 1 (Right), given two inputs, sensitive words \u201clike\u201d, \u201chorrible\u201d, \u201cfun\u201d trigger greater Nm(\u2206) values than words \"movie\", \"really\u201d, and \u201chad.\"\nQuote: \"Temporal Coverage (TC) requests the testing to meet a set of temporal patterns for a specific time span [t1, t2]. ... Example 4.3: Fig. 1 (top row) demonstrates the temporal curve of hidden memory for each input across a selected time span. ... it is a clear illustration on the information processing of LSTM for each input.\"\n\nSection: VII. Evaluation -> G. Exhibition of Internal Working Mechanism (RQ8)\nQuote: \"In this section, we show that the working mechanism of LSTM networks can be understood via the test cases generated from TESTRNN. We conduct experiments to visualize the learning process of LSTM layer via TESTRNN results. ... If we combine SC and BC plots, the whole working process of LSTM layer inside the MNIST model becomes transparent. The sequential input of an image starts from the top row and finishes at the bottom row. At the beginning, the top rows of MNIST images are blank and do not contribute to the model pre-diction. These less-important information is gradually thrown away from the memory. When the input rows containing digits are fed to the LSTM cells, the model will start learning and the short-term memory, represented by the outputs ht, start to have strong reactions. When approaching the end of the input, which corresponds to the bottom of the digit images, LSTM has already been confident about the final classification, and therefore, becomes lazy to update the memory. ... Answer to RQ8: The generated test suite can be used to understand the data processing mechanism of LSTM models. This is a step toward interpretable RNNs.\" (Page 13-14)"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "The TESTRNN approach goes beyond merely reporting faulty cases by analyzing internal LSTM signals\u2014such as gate activations, hidden\u2010state coverage and their temporal patterns\u2014to reveal where and how failures (e.g., adversarial or backdoor inputs) manifest. However, it does not establish causal links back to specific architectural or training weaknesses, relying instead on correlational insights.",
          "evidence": "Page 1201, Section VII-C2: \u201cthe backdoor input to malicious classifier will induce different internal activation, which can be seen from the apparent increase of coverage in T+B.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Linnks faulty behaviors to internal LSTM mechanisms. Correlates coverage with faults. identify where and how the failures arise internally. The analysis is correlational, not causal. "
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "TESTRNN enforces theoretical design constraints to preserve input naturalness\u2014bounded\u2010norm perturbations for continuous inputs and domain\u2010specific, semantics\u2010preserving mutations for text and molecular inputs\u2014but does not present empirical validation (e.g., SSIM, BLEU or human studies) of the realism of the fuzzed inputs.",
          "evidence": "Section VI-C.1 (Test Oracle, p.1198): \u201cWe define a set of norm-balls\u2026 The radius r_oracle\u2026 intuitively means that a human cannot differentiate between inputs within a norm ball.\u201d; Section VII-A.3 (Input Mutation, p.1199): \u201cThis ensures the mutated text paragraphs are always valid. \u2026 For Lipophilicity model, \u2026 enumeration of possible SMILES for a molecule is implemented\u2026 the enumerated SMILES strings are the test cases.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The paper provides evidence that the fuzzed inputs generated by TESTRNN maintain naturalness and semantic validity. For discrete inputs like IMDB movie reviews and Lipophilicity SMILES strings, it explicitly states the use of problem-specific mutation functions and toolkits (EDA toolkit, RDkit) that are designed to preserve text meanings or molecular structures, thereby ensuring mutated inputs are 'always valid' and 'without affecting the molecular structure it represents.' Crucially, for the IMDB model, the paper offers a direct qualitative comparison and an implied human evaluation in Section VII.E, noting that the adversarial reviews generated by TESTRNN are 'much easier' to comprehend than those from gradient-based attack methods, indicating higher naturalness. This claim is visually supported by Figure 6, which contrasts a more readable TESTRNN-generated review with an 'hard to comprehend' one from an attack method.",
          "evidence": "Page 9, Section VII.A, '3) Input Mutation': 'To avoid this, we take a set M of mutants from the EDA toolkit [48]... This ensures the mutated text paragraphs are always valid. In our experiments, we consider four mutation operations... The text meanings are reserved in all mutations.' and 'For Lipophilicity model, we take a set M of mutants that change the SMILES string without affecting the molecular structure it represents.'\n\nPage 12, Section VII.E, 'Comparison With Attack-Based Defect Detection (RQ6)': 'Moreover, RNNs are widely applied to the nature language processing... That is, the modified text may easily become human-unreadable and impossible to occur in real world. On the other hand, our testing method is able to reduce such problems by taking the mutants from off-the-shelf tools, such as the EDA toolkit. Fig. 6 presents adversarial movie reviews returned by attack method and TESTRNN, respectively. It is easy to see that the adversarial review returned by the gradient attack is hard to comprehend, whereas the one from TESTRNN is much easier.'\n\nPage 11, Figure 6: 'Adv. Review Returned by TestRNN' (IMDB example) compared to 'Adv. Review Returned by Attack' clearly shows the former is semantically more coherent."
        },
        "arbitrator": {
          "value": "Medium",
          "why": "TESTRNN preserves input naturalness through theoretical design constraints\u2014bounded\u2010norm perturbations for continuous data and domain\u2010specific, semantics\u2010preserving mutations for text and molecular inputs\u2014but does not include any empirical validation (e.g., SSIM, BLEU, LPIPS) or formal human evaluation to demonstrate that the fuzzed inputs are perceptually or semantically consistent with real\u2010world data.",
          "evidence": "Section VI-C.1 (Test Oracle, p.1198): \u201cWe define a set of norm-balls\u2026 The radius r_oracle\u2026 intuitively means that a human cannot differentiate between inputs within a norm ball.\u201d; Section VII-A.3 (Input Mutation, p.1199): \u201cThis ensures the mutated text paragraphs are always valid. \u2026 For Lipophilicity model, \u2026 enumeration of possible SMILES for a molecule is implemented\u2026 the enumerated SMILES strings are the test cases.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Constraint bounded mutation but not evidence if naturalness measure of the generated inputs"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Low",
          "why": "TESTRNN\u2019s generated faults for image and video RNNs arise from small continuous perturbations (Gaussian noise on floating-point inputs) without any clipping or rounding to enforce I/O stability. Such perturbations can be erased when inputs are serialized into standard formats (e.g., 8-bit PNG), indicating the faults depend on fragile floating-point artifacts that may disappear after I/O.",
          "evidence": "Section VI-B.1 (Random Mutation), p. 1197: \u201cWhen the LSTM input has continuous values (e.g., image input), Gaussian noises with fixed mean and variance are added to the input\u2026.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The paper explicitly states that for continuous input types like images (MNIST), mutations involve rounding and clipping to ensure pixel values remain within a valid range, thereby guaranteeing I/O stability. For discrete input types like text (IMDB) and chemical strings (Lipophilicity), mutations are performed using external toolkits that ensure the validity and semantic preservation of the generated inputs, effectively creating 'synthesized inputs' that are inherently stable across I/O transformations. Although not explicitly detailed for video frames (UCF101), the consistent approach for other data types and the paper's stated goal of finding defects with 'real operational impact' imply a similar commitment to generating I/O-stable faults.",
          "evidence": "Page 9, Section VII-A.3 'Input Mutation':\n- \"For MNIST model, we add Gaussian noise to input image and round off the decimals around 0 and 1 to make the pixel value stay within the value range.\"\n- \"The input to IMDB model is a sequence of words, on which a random change may lead to an unrecognizable (and invalid) text paragraph. To avoid this, we take a set M of mutants from the EDA toolkit [48], which was originally designed to augment the training data for improvement on text classification tasks. This ensures the mutated text paragraphs are always valid. In our experiments, we consider four mutation operations, i.e., M includes synonym replacement, random insertion, random swap, and random deletion. The text meanings are reserved in all mutations.\"\n- \"For Lipophilicity model, we take a set M of mutants that change the SMILES string without affecting the molecular structure it represents. The enumeration of possible SMILES for a molecule is implemented with the Python cheminformatics package RDkit [35]. Each input SMILES string is converted into its molfile format, based on which the atom order is changed randomly before converting back. There may be several SMILES strings representing the same molecular structure. The enumerated SMILES strings are the test cases.\""
        },
        "arbitrator": {
          "value": "Low",
          "why": "All reported faults in the paper arise from small, continuous perturbations applied directly to in-memory floating-point representations (e.g. Gaussian noise on normalized pixel values) without any explicit quantization or clipping to integer levels that would mirror real I/O operations. Such perturbations would typically be lost when saving or loading images (e.g. as 8-bit PNG/JPEG) or audio (e.g. 16-bit PCM), indicating the observed faults depend on fragile floating-point artifacts rather than stable, semantic changes.",
          "evidence": "Section VI-B.1 (Random Mutation), p. 1197: \u201cWhen the LSTM input has continuous values (e.g., image input), Gaussian noises with fixed mean and variance are added to the input\u2026\u201d \u2013 with no subsequent quantization to enforce I/O stability."
        },
        "manual": {
          "value": "Low",
          "why": "Applies low level noise but does not enforce IO persistence with clipping/clamping"
        }
      },
 
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "All reported failures (e.g. adversarial or backdoor inputs) are evaluated separately on each model; the paper does not assess whether the same failing inputs also induce failures in other models performing the same task.",
          "evidence": "Section VII (Evaluation) describes experiments on multiple LSTM benchmarks (MNIST, IMDB, Lipophilicity, UCF101) but each is tested independently without any cross-model consistency analysis."
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates the effectiveness of its method in detecting defects (adversarial and backdoor samples) for various LSTM models (MNIST, IMDB, Lipophilicity, UCF101). While it trains multiple models for some tasks (e.g., two models for Lipophilicity, or a benign and a malicious classifier for MNIST), the analysis of failures is conducted independently for each model or specific model type. The paper does not empirically demonstrate that the *same failing inputs* cause equivalent erroneous behaviors across multiple models performing the same task. Nor does it analyze cross-model correlations showing that the *same mutation pattern* systematically induces similar failure behaviors. Instead, it reports the *number* and *rate* of adversarial samples found for each model or compares coverage changes in distinct model types (benign vs. malicious), without assessing the transferability of the exact inputs or mutation-failure relationships.",
          "evidence": "1. **Evaluation of defects for individual models/model pairs without transferability analysis:**\n   - \"As shown in Table V, adversarial samples rate and number of unique adversarial samples in the generated test suite are two important indicators for the robustness evaluation. The unique adversarial samples refer to the adversarial samples crafted from distinct seeds input. For a set of trained models, we pursue the model, the test suite of which contains less amount of adversarial samples and unique adversarial samples. For example, we pick up model 2 for Lipophilicity prediction, since the values of two indicators are way smaller than that of model 1.\" (Page 11, left column, Section VII-C1)\n   - *Explanation:* This compares the robustness of two distinct Lipophilicity models (model 1 and model 2) by reporting their individual adversarial sample rates and counts. It indicates that model 2 is 'more robust' because fewer adversarial samples were found for it, but it does not test if adversarial samples from model 1 also cause failures in model 2, or vice versa, or if mutation patterns generalize across them.\n\n2. **Comparison of benign and malicious models without general transferability:**\n   - \"We train two handwritten digits recognition models, one of which is benign classifier and the other one is the malicious classifier subject to the backdoor attack in [17]. Table VI shows that both benign and malicious classifiers keep good prediction performance in clean test set... The experimental results describe that backdoor input activate same internal behavior with clean input for a benign classifier. In contrast to this, the backdoor input to malicious classifier will induce different internal activation, which can be seen from the apparent increase of coverage in T+B.\" (Page 11, left and right columns, Section VII-C2)\n   - *Explanation:* This section analyzes the *difference* in internal activation behavior between a benign and a malicious MNIST model when exposed to backdoor inputs. This is about characterizing the unique behavior of the malicious model, not about the transferability of failure-inducing inputs or patterns across multiple clean or multiple malicious models."
        },
        "arbitrator": {
          "value": "Low",
          "why": "All reported failures (e.g. adversarial or backdoor inputs) are evaluated separately on each model; the paper does not assess whether the same failing inputs also induce failures in other models performing the same task.",
          "evidence": "Section VII (Evaluation) describes experiments on multiple LSTM benchmarks (MNIST, IMDB, Lipophilicity, UCF101) but each is tested independently without any cross-model consistency analysis."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "oracle": [
        "Metamorphic"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided",
        "Oracle-guided"
      ]
    }
  },
  "Li et al. - 2021 - Testing DNN-based Autonomous Driving Systems under Critical Environmental Conditions": {
    "key": "li2021testing",
    "name": "TACTIC",
    "year": "2021",
    "citation_count": "51",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "TACTIC uncovers functional robustness errors in steering angle prediction (i.e. violations of a metamorphic invariance under environmental perturbations) without demonstrating any bypass of explicit safety, policy or security mechanisms, nor exposing higher-level ethical or specification\u2010level misalignments.",
          "evidence": "Section 3.2.2 (\u201cMetamorphic Error Analysis\u201d): \u201cthe MR is further relaxed to a steering angle divergence within an error bound. \u2026 if |\u03b8\u2092 \u2013 \u03b8\u209c| \u2265 \u03b5, the behaviour is deemed erroneous,\u201d indicating that failures are low-level functional errors in the DNN\u2019s steering output."
        },
        "gemini": {
          "value": "Low",
          "why": "The method, TACTIC, identifies 'erroneous behaviours' by detecting deviations in steering angles that exceed a user-defined error bound, based on a relaxed metamorphic relation. This falls under the category of triggering low-level functional errors, specifically violations of functional invariances (i.e., steering angle consistency under environmental transformations) and flawed outputs (steering angle predictions). While the paper mentions these errors can lead to 'catastrophic accidents,' the mechanism TACTIC uses directly detects functional misbehavior rather than bypassing explicit safety mechanisms or violating higher-level ethical/normative expectations.",
          "evidence": "Page 5, Section 3.2.2. Metamorphic Error Analysis: 'Following the existing work (Tian et al., 2018), we also adopt Metamorphic Relation (MR) (Chen et al., 2020) as the test oracle to determine whether a system behaviour is correct or not. In particular, the MR used in TACTIC is defined as that the steering angles should be consistent among the driving scenes transformed from the same ones by applying different style vectors, i.e., retain behaviour consistency. However, such an MR is too strong to be practical since the acceptable steering angle for a driving scene can be within a range (Tian et al., 2018). Therefore, the MR is further relaxed to a steering angle divergence within an error bound. Formally, let x\u2080 be an original driving scene and x\u209c be a newly generated driving scene synthesised based on x\u2080 in a target environmental type. The \u03b8\u2080 and \u03b8\u209c represent the steering angles for x\u2080 and x\u209c, respectively. Then the MR is defined as |\u03b8\u2080 - \u03b8\u209c| < \u03b5, where the \u03b5 is a user-defined error bound.'\nPage 5-6, Section 3.2.2. Metamorphic Error Analysis: 'Based on the MR, we can assess the tendency of behaviours to be erroneous ones. For a style vector, if the new driving scenes synthesised with this style vector have larger steering angle divergences compared with the original driving scenes, then the style vector is considered to be able to detect more erroneous behaviours.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "TACTIC uncovers functional robustness errors in steering angle prediction (i.e. violations of a metamorphic invariance under environmental perturbations) without demonstrating any bypass of explicit safety, policy or security mechanisms, nor exposing higher-level ethical or specification\u2010level misalignments.",
          "evidence": "Section 3.2.2 (\u201cMetamorphic Error Analysis\u201d): \u201cthe MR is further relaxed to a steering angle divergence within an error bound. \u2026 if |\u03b8\u2092 \u2013 \u03b8\u209c| \u2265 \u03b5, the behaviour is deemed erroneous,\u201d indicating that failures are low-level functional errors in the DNN\u2019s steering output."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Medium",
          "why": "TACTIC guides its fuzzing towards environmental conditions that induce semantic or category\u2010level faults (i.e. \u2018erroneous behaviours\u2019 such as large steering\u2010angle deviations) without constraining the exact output, indicating a partially guided but not fully targeted exploration.",
          "evidence": "Section 3.2.2 (Metamorphic Error Analysis): \u201cthe MR is defined as |\u03b8\u2092 \u2212 \u03b8\u209c| < \u03b5 \u2026 if the new driving scenes synthesised with this style vector have larger steering angle divergences \u2026 the style vector is considered to be able to detect more erroneous behaviours.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "TACTIC's goal is to identify environmental conditions under which DNN-based Autonomous Driving Systems (ADS) are more prone to *erroneous behaviours* in general, rather than steering exploration towards specific, predefined faults. The search objective is to maximize the 'number and diversity of erroneous behaviours' and 'steering angle divergences,' which represents a broad search for generic faults like misclassifications or unstable predictions in steering control. The paper explicitly states that TACTIC's goal is to identify conditions where the ADS is 'more prone to errors,' encompassing any deviation from expected behavior beyond an error bound, not a specific, targeted fault type or output.",
          "evidence": "Page 1, Abstract: 'In this paper, we propose to test DNN-based ADS under different environmental conditions to identify the critical ones, that is, the environmental conditions under which the ADS are more prone to errors.'\nPage 4, Section 3.2. Search Objective for Critical Environmental Conditions: 'Then TACTIC sets a search objective which is to maximise the number and diversity of erroneous behaviours and employs a search-based method... to search the style space for the environmental conditions that reach the search objective, i.e., the critical environmental conditions.'\nPage 5, Section 3.2.2. Metamorphic Error Analysis: 'Formally, let xo be an original driving scene and xt be a newly generated driving scene synthesised based on xo in a target environmental type. The \u03b8o and \u03b8t represent the steering angles for xo and xt, respectively. Then the MR is defined as |\u03b8o - \u03b8t| < \u03b5, where the \u03b5 is a user-defined error bound.'\nPage 6, Section 3.2.3. Fitness Function Definition: 'The ability to detect more erroneous behaviours of a style vector is measured by the mean steering angle divergences between the original dataset of driving scenes and the newly synthesised driving scenes with that style vector.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "TACTIC conducts broad, untargeted exploration by searching for environmental conditions that induce generic steering\u2010angle deviations. It maximizes the count and diversity of errors rather than driving the model toward a specific, predefined output (e.g. a chosen steering angle), which aligns with the \u2018Low\u2019 directedness category.",
          "evidence": "Abstract: \u201cidentify the critical environmental conditions under which the ADS are more prone to errors.\u201d\nSection 3.2 (Search Objective): fitness function combines \u201cnumber and diversity of erroneous behaviours\u201d and steering\u2010angle divergences, with no fixed target output."
        },
        "manual": {
          "value": "Low",
          "why": "Finds inconsistent steering under environmental variations"
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "TACTIC focuses on discovering under which environmental conditions a DNN\u2010based ADS produces erroneous steering outputs and uses internal neuron\u2010coverage metrics purely as heuristics to guide test\u2010case generation. It does not perform any diagnostic or causal analysis of the model\u2019s internal representations, gradients, or architectural/training weaknesses to explain why those failures occur.",
          "evidence": "Section 3.2 (Search Objective): \u201cThe key to the success of the search is a proper fitness function that measures the fitness of the style vectors. The number of erroneous behaviours is certainly important\u2026 we design the fitness function to consider both the number and the types of errors.\u201d\n\nNo part of the paper analyzes these coverage signals or failure patterns to establish causal links to model internals\u2014only to guide the search for more failures."
        },
        "gemini": {
          "value": "Medium",
          "why": "TACTIC explains where and how failures manifest by identifying critical environmental conditions and correlating them with internal model signals (neuron coverage) and external behavioral patterns (steering angle divergence). The method doesn't establish direct causal links to architectural or training weaknesses but goes beyond merely reporting faulty cases by using diagnostic information (neuron coverage) to guide its search for diverse errors. The paper states that increasing DNN coverage leads to more diverse behaviors, which is a correlational link between internal signals and failure diversity.",
          "evidence": "Page 2, Abstract: 'In this paper, we propose to test DNN-based ADS under different environmental conditions to identify the critical ones, that is, the environmental conditions under which the ADS are more prone to errors.'\nPage 4, Section 3.2: 'TACTIC sets a search objective which is to maximise the number and diversity of erroneous behaviours and employs a search-based method... to search the style space for the environmental conditions that reach the search objective, i.e., the critical environmental conditions.'\nPage 4, Section 3.2.1: 'In this work, we employ DNN coverage criteria to measure the ability of a style vector to detect diverse erroneous behaviours. Prior work has proved that increasing the DNN coverage can lead to more diverse behaviours (Pei et al., 2017; Tian et al., 2018; Xie et al., 2019; Huang et al., 2019), which, consequently, can increase the chance to detect more diverse erroneous behaviours.'\nPage 5, Section 3.2.1: 'we carefully select two fine-grained coverage metrics on the sub-neuron level (Ma et al., 2018) for TACTIC to support, i.e., k-multisection Neuron Coverage (KMNC) and Neuron Boundary Coverage (NBC).'\nPage 6, Section 3.2.3: 'The ability to detect diverse erroneous behaviours of a style vector is measured by the increase in the testing coverage measured by KMNC or NBC.' and 'The ability to detect more erroneous behaviours of a style vector is measured by the mean steering angle divergences between the original dataset of driving scenes and the newly synthesised driving scenes with that style vector.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "TACTIC goes beyond merely reporting failures by using internal neuron\u2010coverage metrics (KMNC and NBC) and external behavioral patterns (steering angle divergences) as correlational signals to identify and explain which environmental conditions lead to diverse and serious model errors. While it does not perform in\u2010depth causal diagnosis of architectural or training weaknesses, it provides heuristic and correlational insights into where and how failures manifest under critical environmental conditions, fitting the \u201cMedium\u201d level of root\u2010cause insight.",
          "evidence": "Section 3.2.1: \u201cwe employ DNN coverage criteria \u2026 to measure the ability of a style vector to detect diverse erroneous behaviours. Prior work has proved that increasing the DNN coverage can lead to more diverse behaviours \u2026\u201d\n\nSection 3.2.3: \u201cThe ability to detect diverse erroneous behaviours of a style vector is measured by the increase in the testing coverage measured by KMNC or NBC. The ability to detect more erroneous behaviours \u2026 is measured by the mean steering angle divergences \u2026\u201d\n\nThese passages show TACTIC uses correlational analysis of internal signals and external behavior to explain where and how failures arise, without establishing deep causal mechanisms."
        },
        "manual": {
          "value": "Medium",
          "why": "Relies on coverage-guided mutation and assumes based on prior work, that higher coverage correlates with fault discovery"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "High",
          "why": "The paper provides direct empirical evidence via a human evaluation study showing that the driving scenes synthesized by TACTIC are perceptually indistinguishable from real\u2010world scenes and correctly reflect target environmental conditions, demonstrating strong naturalness preservation.",
          "evidence": "Section 4.3 (Comparison with Baselines on Image Quality): \u201cWe conduct a user study \u2026 We received answers from a total of 34 participants \u2026 Figure 5 and Figure 6 present the results \u2026 there are nearly half of driving scene pairs where the synthesized driving scenes cannot be distinguished with the real\u2010world ones in human perception for TACTIC (44%), \u2026 demonstrating that the testing driving scenes synthesized by TACTIC are more realistic.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The paper provides empirical evidence that the inputs generated by TACTIC (synthesised driving scenes) maintain naturalness and realism. This is explicitly validated through a user study, which is a form of human evaluation, where participants judged the realism of TACTIC's generated images compared to real images and outputs from other baselines. The results show that TACTIC's scenes are perceived as realistic, with nearly half being indistinguishable from real-world scenes by human perception.",
          "evidence": "Page 1, Abstract: \"Large-scale experiments show that TACTIC can effectively identify critical environmental conditions and produce realistic testing images...\"\nPage 1, Figure 1 caption: \"TACTIC produces more realistic driving scenes compared to the other three approaches.\"\nPage 6, Section 4, Experimental Evaluation: \"Additionally, testing ADS under environmental conditions requires that the synthesised driving scenes realistically reflect reality, and therefore, we study the realism of the synthesised driving scenes.\"\nPage 8, Section 4.3, Comparison with Baselines on Image Quality (Setup): \"We conduct a user study to evaluate the realism of the testing driving scenes synthesised by TACTIC through comparing them with the test driving scenes synthesised by DeepRoad (Zhang et al., 2018), DeepTest (Tian et al., 2018), and PreScan (Marketakis et al., 2009). Specifically, we design an online questionnaire consisting of two questions: (1) 'Which driving scene is more realistic?', and (2) 'Which environmental type does the driving scene belong to?'.\"\nPage 8-9, Section 4.3, Results: \"The results demonstrate that the testing driving scenes synthesised by TACTIC are more realistic than the ones synthesised by affine image transformation (e.g., DeepTest) or high-fidelity simulation (e.g., PreScan) and can better reflect real environments. Specifically, from Figure 5, it can be observed that there are nearly half of driving scene pairs where the synthesised driving scenes cannot be distinguished with the real-world ones in human perception for TACTIC (44%) and DeepRoad (41%).\""
        },
        "arbitrator": {
          "value": "High",
          "why": "The paper provides direct empirical evidence via a human evaluation study showing that the driving scenes synthesized by TACTIC are perceptually indistinguishable from real\u2010world scenes and correctly reflect target environmental conditions, demonstrating strong naturalness preservation.",
          "evidence": "Section 4.3 (Comparison with Baselines on Image Quality): \u201cWe conduct a user study \u2026 We received answers from a total of 34 participants \u2026 Figure 5 and Figure 6 present the results \u2026 there are nearly half of driving scene pairs where the synthesized driving scenes cannot be distinguished with the real\u2010world ones in human perception for TACTIC (44%), \u2026 demonstrating that the testing driving scenes synthesized by TACTIC are more realistic.\u201d"
        },
        "manual": {
          "value": "High",
          "why": "Test images are generated through a GAN trained to mimic real driving scenes, so naturalness is preserved throughout fuzzing iterations. Provides human evaluation."
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "High",
          "why": "TACTIC\u2019s faults are discovered using fully synthesized driving scenes generated by the MUNIT image-to-image translation model. These are self-contained, finalized image artifacts rather than small floating-point perturbations, and thus any triggering conditions remain stable after standard I/O serialization (e.g., saving as PNG/JPEG).",
          "evidence": "Section 3.1 (\u201cEnvironmental Conditions\u201d): \u201cTo encode the environmental condition space \u2026 TACTIC employs MUNIT\u2026 to deep-learn the features of environmental conditions \u2026 and synthesize new [driving] scenes\u2026 Because of the richness of styles, the synthesised scenes are realistic enough\u2026 to test whether the DNN-based ADS can perform correctly under the environmental conditions corresponding to the obtained styles.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The TACTIC method generates fuzzed inputs in the form of synthesized driving scenes using an image-to-image translation model (MUNIT). According to the metric definition, 'model-generated fuzzed inputs\u2014such as synthesized speech, rendered images, or generated text\u2014are inherently stable under I/O operations because they originate from self-contained generation processes that produce finalized artifacts rather than transient numeric states.' TACTIC's approach of creating new, realistic driving scenes from a generative model aligns with this description, ensuring that the discovered faults persist through standard I/O operations.",
          "evidence": "Page 1, Abstract: '...we present a novel approach named TACTIC that employs the search-based method to identify critical environmental conditions generated by an image-to-image translation model.'\nPage 1, Figure 1 caption: 'TACTIC produces more realistic driving scenes compared to the other three approaches.'\nPage 2, Section 1, paragraph 4: 'TACTIC employs the Multimodal Unsupervised Image-to-Image Translation (MUNIT) (Huang et al., 2018) to deep-learn the features of environmental conditions... When styles are obtained, the style-learning process of MUNIT is reversed: styles are applied on existing test driving scenes to synthesise new ones.'\nPage 3, Section 3, paragraph 2: '...the original driving scenes are transformed by the MUNIT model into synthesised scenes under the critical environmental conditions...'."
        },
        "arbitrator": {
          "value": "High",
          "why": "TACTIC\u2019s faults are discovered using fully synthesized driving scenes generated by the MUNIT image-to-image translation model. These are self-contained, finalized image artifacts rather than small floating-point perturbations, and thus any triggering conditions remain stable after standard I/O serialization (e.g., saving as PNG/JPEG).",
          "evidence": "Section 3.1 (\u201cEnvironmental Conditions\u201d): \u201cTo encode the environmental condition space \u2026 TACTIC employs MUNIT\u2026 to deep-learn the features of environmental conditions \u2026 and synthesize new [driving] scenes\u2026 Because of the richness of styles, the synthesised scenes are realistic enough\u2026 to test whether the DNN-based ADS can perform correctly under the environmental conditions corresponding to the obtained styles.\u201d"
        },
        "manual": {
          "value": "High",
          "why": "Produced synthesized images"
        }
      },

      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper evaluates TACTIC separately on each of the three DNN-based ADS models (Dave-orig, Dave-dropout, Chauffeur) without any analysis of whether the same synthesized inputs or style mutations cause equivalent erroneous behaviors across models. There is no cross-model consistency or transferability study reported.",
          "evidence": "Section 4.2 \u201cwe execute TACTIC \u2026 on each of the three subject DNN-based ADSs\u201d \u2013 the evaluation is performed independently per model, and no cross-model failure analysis is presented."
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates the proposed method, TACTIC, on three different DNN-based autonomous driving systems (Dave-orig, Dave-dropout, and Chauffeur). For each subject DNN, TACTIC identifies critical environmental conditions (represented as style vectors) using a search-based method. These identified conditions are then used to synthesize test driving scenes, and the number and diversity of erroneous behaviors are measured for that specific model. While results are presented for multiple models, the search for critical environmental conditions is performed independently for each model. The paper does not analyze whether the critical environmental conditions identified for one model would cause equivalent erroneous behaviors in another model when applied to the same original driving scene, nor does it analyze cross-model correlations of mutation patterns (style vectors) inducing similar failure behaviors. The focus is on TACTIC's ability to find errors *for a given subject DNN*, rather than the transferability of those specific failures or mutation patterns across different DNNs.",
          "evidence": "Page 3, Section 3, paragraph 2, Figure 2: 'For a subject DNN-based ADS and a given environmental type, TACTIC first trains a MUNIT model... Then, TACTIC employs a search-based method... to identify the critical environmental conditions based on the MUNIT model.'\n\nPage 6, Section 3.3, paragraph 1: 'We are now ready to present how TACTIC identifies the critical environmental conditions of a given environmental type for a subject DNN-based ADS.'\n\nPage 7, Section 4.1 'Datasets and DNN-based ADSs', paragraph 1: 'We consider three popular pre-trained DNN-based ADSs... i.e., Dave-orig..., Dave-dropout..., and Chauffeur...'\n\nPage 7, Section 4.2 'Comparison with Baselines on Effectiveness', Setup paragraph 1: 'We execute TACTIC... on each of the three subject DNN-based ADSs, under the five environmental types.'\n\nPage 7, Section 4.2 'Comparison with Baselines on Effectiveness', Results paragraph 1: 'Table 1 and Table 2 summarise the results of comparing TACTICNBC with Rc and DeepRoad, respectively. ... The results on all the coverage-guiding strategies and the DNN-based ADSs are consistent.' (This indicates consistent *performance* of TACTIC, not transferability of specific failures or conditions)."
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper evaluates TACTIC separately on each of the three DNN-based ADS models (Dave-orig, Dave-dropout, Chauffeur) without any analysis of whether the same synthesized inputs or style mutations cause equivalent erroneous behaviors across models. There is no cross-model consistency or transferability study reported.",
          "evidence": "Section 4.2 \u201cwe execute TACTIC \u2026 on each of the three subject DNN-based ADSs\u201d \u2013 the evaluation is performed independently per model, and no cross-model failure analysis is presented."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "oracle": [
        "Metamorphic"
      ],
      "mutation_strategy": [
        "Generative Synthesized"
      ],
      "exploration_strategy": [
        "Coverage-guided",
        "Oracle-guided"
      ]
    }
  },
  "Wang et al. - 2023 - DistXplore Distribution-Guided Testing for Evaluating and Enhancing Deep Learning Systems": {
    "key": "wang2023distxplore",
    "name": "DistXplore",
    "year": "2023",
    "citation_count": "13",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "DistXplore uncovers misclassifications\u2014i.e., low\u2010level functional correctness failures in deep learning models\u2014without demonstrating any bypass of explicit safety, policy, or security mechanisms, nor producing normative, ethical, or specification\u2010level violations.",
          "evidence": "Section 3.1.1 (\"Model Evaluation\"): \u201cDistXplore aims to generate the erroneous inputs that are hard to be detected by existing defense techniques.\u201d; Section 4 (\"Results\"): \u201cThe unique number of errors represents the total number of erroneous inputs generated...\u201d; Abstract: \u201cDistXplore not only detects a larger number of errors \u2026 but also identifies more hard\u2010to\u2010detect errors.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "DistXplore primarily focuses on generating test cases that trigger misclassifications in deep learning models, which are considered low-level functional errors. The paper explicitly defines 'errors' as 'erroneous inputs that are missclassified' and states the goal is to evaluate and enhance the model's robustness by detecting these types of errors, particularly 'hard-to-detect' or 'statistically indistinguishable' ones. There is no indication of uncovering behaviors that remain functionally valid but violate normative expectations (Medium severity) or bypassing explicit safety/security mechanisms (High severity).",
          "evidence": "Page 1, Abstract: 'To evaluate and enhance the model's robustness, we design two metrics: distribution difference, which maximizes the similarity in distribution between two different classes of data to generate hard-to-detect errors, and distribution diversity, which increase the distribution diversity of generated test cases for enhancing the model's robustness.'\nPage 1, Introduction: 'DL testing aims to generate test cases that evaluate the robustness of DL systems, i.e., discover the data that is valid but cannot be predicted correctly...'\nPage 4, Section 3.1.1 Model Evaluation: 'To evaluate the model's robustness, we aim to generate the erroneous inputs that are hard to be detected by existing defense techniques... Thus, DistXplore aims to generate test cases (in a class) that are statistically similar to the training data in other classes.'\nPage 5, Footnote 1: 'The error in the paper refers to the erroneous inputs that are missclassified.'\nPage 8, Answers to RQ1-1: 'Compared with adversarial attacks and existing DL testing techniques, DistXplore is more effective in generating hard-to-detect errors.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "DistXplore uncovers misclassifications\u2014i.e., low\u2010level functional correctness failures in deep learning models\u2014without demonstrating any bypass of explicit safety, policy, or security mechanisms, nor producing normative, ethical, or specification\u2010level violations.",
          "evidence": "Section 3.1.1 (\"Model Evaluation\"): \u201cDistXplore aims to generate the erroneous inputs that are hard to be detected by existing defense techniques.\u201d; Section 4 (\"Results\"): \u201cThe unique number of errors represents the total number of erroneous inputs generated...\u201d; Abstract: \u201cDistXplore not only detects a larger number of errors \u2026 but also identifies more hard\u2010to\u2010detect errors.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "High",
          "why": "DistXplore explicitly steers its test generation toward predefined faults by selecting a target class c\u2032 and minimizing the distribution distance (MMD) between the source class test suite and the training data of c\u2032, thereby forcing misclassification into that chosen label.",
          "evidence": "Page 71, Section 3.1.1 Model Evaluation: \u201cGiven a DNN f and a test suite S belonging to a source class c, we define its distribution difference with respect to the training data D_T(c\u2032) in another target class c\u2032 as DF_f(S, c\u2032) = MMD(f_l(S), f_l(D_T(c\u2032))). \u2026 Intuitively, the smaller the value DF_f(S, c\u2032), the more difficult it is for the model f to distinguish S and D_T(c\u2032). Hence, it is more likely to generate undetectable errors by minimizing their distribution difference.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The DistXplore method performs untargeted exploration to reveal generic faults, specifically misclassifications (referred to as 'errors' in the paper) that are hard to detect. While it uses a 'target class' distribution for guidance, its objective is not to force a classifier to predict a specific label or trigger a semantic/category-level fault. Instead, it aims to generate test cases whose distribution is statistically similar to the training data of another class, making any resulting misclassifications 'statistically indistinguishable errors' and thus hard to detect, rather than targeting a predefined misclassification outcome.",
          "evidence": "Page 1, Abstract: 'DistXplore not only detects a larger number of errors (e.g., 2x+ on average), but also identifies more hard-to-detect errors (e.g., 10.5%+ on average).'\nPage 2, Introduction: 'For evaluating model's robustness, although OOD data is likely to trigger incorrect decisions of the model... it is important and challenging to discover strong errors that can evade the state-of-the-art defenses.'\nPage 2, Introduction: 'DistXplore maximizes the distribution closeness between the data in two different classes for generating statistically indistinguishable errors, which are difficult to defend.'\nPage 4, Section 3.1.1 Model Evaluation: 'DistXplore aims to generate test cases (in a class) that are statistically similar to the training data in other classes... Hence, it is more likely to generate undetectable errors by minimizing their distribution difference.'\nPage 5, Section 4, RQ1: 'How effective is DistXplore in detecting errors 1 that can bypass the defense methods? 1 The error in the paper refers to the erroneous inputs that are missclassified.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "DistXplore explicitly chooses, for each run, a specific target class c\u2032 and then drives its test\u2010suite generation to minimize the distribution distance (MMD) between the source class inputs and the training data of c\u2032. By doing so, it deliberately steers the model toward misclassifying those inputs as that exact chosen label, thereby embodying a fully targeted exploration strategy.",
          "evidence": "Page 71, Section 3.1.1 Model Evaluation: \u201cGiven a DNN f and a test suite S belonging to a source class c, we define its distribution difference with respect to the training data D_T(c\u2032) in another target class c\u2032 as DF_f(S, c\u2032) = MMD(f_l(S), f_l(D_T(c\u2032))). \u2026 Intuitively, the smaller the value DF_f(S, c\u2032), the more difficult it is for the model f to distinguish S and D_T(c\u2032). Hence, it is more likely to generate undetectable errors by minimizing their distribution difference.\u201d\nPage 75, Figure 5 and accompanying text: the rising \u201cError_target Rate\u201d (misclassifications into c\u2032) as MMD_target decreases confirms the method\u2019s steering toward a specific misclassification label."
        },
        "manual": {
          "value": "High",
          "why": "Guides test generation to minimize distribution differences between source class inputs and target class."
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Medium",
          "why": "DistXplore provides a correlational explanation of failures by measuring the distribution difference (via MMD) on an internal layer (the logits) and showing that test suites whose logits distributions closely match those of another class tend to induce misclassifications. However, it does not identify causal architectural, training, or data\u2010related mechanisms that give rise to the observed failures.",
          "evidence": "Section 3.1.1 (\"Model Evaluation\"): \u201cThe distribution difference is measured on a specific layer of the DNN. In this paper, we select the logits layer \u2026 Intuitively, the smaller the value DF(fS, fT), the more difficult it is for the model f to distinguish S and T. Hence, it is more likely to generate undetectable errors by minimizing their distribution difference.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "DistXplore goes beyond merely reporting faulty cases by providing explanations of where and how failures manifest, particularly by linking internal signals (logits layer distributions) to observed behavioral patterns (statistically indistinguishable errors, improved robustness). The approach quantifies distribution differences and diversity, offering correlational insights into why certain errors are hard to detect or why certain data improves robustness. However, it does not delve into establishing causal links to specific architectural, training, or deeper data-related weaknesses within the model's internal mechanisms beyond the distribution characteristics of the data itself.",
          "evidence": "Page 4, Section 3.1.1, 'Model Evaluation': 'Formally, given a DNN f and a test suite Sc belonging to a source class c, we define its distribution difference with respect to the training data (Tc) in another target class c' as: DFf (Sc, c') = MMD(fi(Sc), fi(Tc')) where fi refers to the output of the layer l and c' \u2260 c. The distribution difference is measured on a specific layer of the DNN. In this paper, we select the logits layer... Intuitively, the smaller the value DFf (Sc, c'), the more difficult it is for the model f to distinguish Sc and Tc. Hence, it is more likely to generate undetectable errors by minimizing their distribution difference.'\n\nPage 4, Section 3.1.2, 'Model Enhancement': 'We propose a metric to measure the distribution diversity of test suites... The basic idea is to measure the diversity of distribution differences between the current test suites and the training data of other classes.'\n\nPage 8, Answers to RQ1-2: 'The distribution difference is useful in guiding the generation of statistically indistinguishable errors, making them more difficult to detect. Compared with others, DistXplore generates more diverse errors.'\n\nPage 9, Section 4.2.3, 'RQ3: Robustness Enhancement': 'The overall results demonstrated the effectiveness of DistXplore in improving robustness.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DistXplore provides a correlational explanation of failures by measuring the distribution difference (via MMD) on an internal layer (the logits) and showing that test suites whose logits distributions closely match those of another class tend to induce misclassifications. However, it does not identify causal architectural, training, or data\u2010related mechanisms that give rise to the observed failures.",
          "evidence": "Section 3.1.1 (\"Model Evaluation\"): \u201cThe distribution difference is measured on a specific layer of the DNN. In this paper, we select the logits layer \u2026 Intuitively, the smaller the value DF(fS, fT), the more difficult it is for the model f to distinguish S and T. Hence, it is more likely to generate undetectable errors by minimizing their distribution difference.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "links the occurrence and detectability of errors to the statistical closeness (measured via MMD) between the distribution of generated test inputs and the distribution of another class at the logits layer."
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "DistXplore\u2019s faults are produced by semantic\u2010level image transformations (e.g., rotation, translation, brightness) applied on continuous representations without any explicit I/O\u2010stability validation (clipping/rounding to discrete storage formats). While these transformations tend to be large enough to survive serialization, the paper does not enforce nor verify preservation under standard I/O quantization, so some perturbations may still vanish after encoding.",
          "evidence": "Section 3.2 (\u201cDistribution\u2010Guided Test Generation\u201d): \u201cwe select the diverse image transformations (e.g., translation, rotation, brightness) used in DeepTest [54] and DeepHunter [63]\u2026 To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation with both 0 and \u221e.\u201d No mention of quantization or rounding to I/O formats is made."
        },
        "gemini": {
          "value": "Medium",
          "why": "The method, DistXplore, generates test cases by applying diverse semantic-level image transformations such as translation, rotation, and brightness. While these are semantic changes, the paper states that it adopts a 'conservative strategy' that 'constrains the transformation with both L0 and L\u221e' to guarantee the validity of the generated inputs. This implies that the pixel-level changes resulting from these transformations are kept small. The metric definition explicitly notes that even semantic-level transformations, when applied to continuous representations without subsequent clipping and rounding, can lead to faults that disappear after I/O operations due to minor deviations or precision loss. The paper does not mention any explicit clipping or rounding to ensure I/O stability of these generated inputs, making the discovered faults potentially unstable.",
          "evidence": "Page 5, Section 3.2 'Distribution-Guided Test Generation': 'In this paper, we mainly focus on image classification tasks. DistXplore can be easily extended to other domains. We select the diverse image transformations (e.g., translation, rotation, brightness) used in DeepTest [54] and DeepHunter [63]. For each selected gene, the mutation randomly selects a transformation function to mutate it. To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation with both L0 and L\u221e.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "DistXplore\u2019s faults are produced by semantic\u2010level image transformations (e.g., rotation, translation, brightness) applied on continuous representations without any explicit I/O\u2010stability validation (clipping/rounding to discrete storage formats). While these transformations tend to be large enough to survive serialization, the paper does not enforce nor verify preservation under standard I/O quantization, so some perturbations may still vanish after encoding.",
          "evidence": "Section 3.2 (\u201cDistribution\u2010Guided Test Generation\u201d): \u201cwe select the diverse image transformations (e.g., translation, rotation, brightness) used in DeepTest [54] and DeepHunter [63]\u2026 To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation with both 0 and \u221e.\u201d No mention of quantization or rounding to I/O formats is made."
        },
        "manual": {
          "value": "Medium",
          "why": "metamorphic transformations can yield decimal-valued pixels internally which may change when re-quantized to integers."
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper evaluates DistXplore\u2019s ability to find failures separately on each model, without assessing whether inputs that fail one model also fail other models performing the same task. There is no cross\u2010model consistency analysis of failures or mutations.",
          "evidence": "Section 4.2 (Evaluation of Errors) describes per\u2010model experiments: \u201cWe compare DistXplore with 14 state\u2010of\u2010the\u2010art baselines on 10 models across 4 datasets\u201d (p. 74), but nowhere do the authors test whether the same failing inputs transfer across multiple models."
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates DistXplore on multiple models across various datasets. However, the testing process involves generating test cases (and associated failures) specifically for each model independently. It does not provide empirical evidence that the *same failing inputs* generated for one model cause equivalent erroneous behaviors on other models performing the same task. Nor does it analyze cross-model correlations to demonstrate that the *same mutation patterns* systematically induce similar failure behaviors across different models. The evaluation focuses on the effectiveness of DistXplore for detecting strong errors and enhancing robustness for individual models, rather than assessing the transferability of failures or mutation-failure relationships across models.",
          "evidence": "Page 5, Section 4.1 'Setup': 'We select four datasets (i.e., MNIST, Fashion-MNIST, CIFAR-10, and SVHN) and six DNNs (i.e., LeNet-4, LeNet-5, VGG16, ResNet-20, Inception-v3, and Inception-ResNet-v2) that are commonly used in existing works'.\nPage 6, Section 4.1.4 'Experiment Setup', 'Configuration of DistXplore': 'For each run of DistXplore, we limit the total number of iterations in GA as 30. We empirically configured the population size, the tournament size, and the mutation rate as as 100, 20, and 0.01, respectively. Due to the limit of the space, the experiments about the impact of the parameters are put on our Website [2]. For the robustness enhancement, we do not explicitly generate test cases for each interval (see Div(TSc) in Section 3.1.2). Instead, we map the distribution difference in each iteration (i.e., the fitness value) to an interval. During the optimization process, the distribution distance is decreasing in multiple iterations, covering different intervals. To ensure the validity of the generated test cases, we adopt a more conservative configuration compared to DeepHunter [63] to constrain the mutation.'\nPage 6, Section 4.1.4 'Experiment Setup', 'Configuration of Baselines': 'For the three classic adversarial attacks, we perform the target attack for each seed input by selecting other classes as the targets, i.e., we generate 9 adversarial examples for each seed input.'\nPage 10, Section 4.2.5 'RQ5: Generalization Ability': 'The overall results show that DistXplore can still generate strong errors than the selected baselines. Moreover, the results also demonstrate the generalizability of DistXplore to other domains.' (This discusses domain transferability, not cross-model transferability within the same task)."
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper evaluates DistXplore\u2019s ability to find failures separately on each model, without assessing whether inputs that fail one model also fail other models performing the same task. There is no cross\u2010model consistency analysis of failures or mutations.",
          "evidence": "Section 4.2 (Evaluation of Errors) describes per\u2010model experiments: \u201cWe compare DistXplore with 14 state\u2010of\u2010the\u2010art baselines on 10 models across 4 datasets\u201d (p. 74), but nowhere do the authors test whether the same failing inputs transfer across multiple models."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "DistXplore enforces input plausibility via per\u2010mutation constraints (e.g. Lp\u2010norm and bounded image transformations) but does not measure or validate the end\u2010to\u2010end naturalness of final fault\u2010triggering inputs (no perceptual metrics like SSIM/LPIPS or human evaluation across iterations). Thus it applies local plausibility constraints but overlooks cumulative drift.",
          "evidence": "Page 70, Section 2.1.2: \u201cIn practice, the Lp norm [40] is usually used to guarantee the validity of the generated data by the existing DL testing and adversarial attack techniques.\u201d\nPage 74, Section 3.2: \u201cTo guarantee the validity of the generated inputs, we adopt a more conservative configuration compared to DeepHunter [63] to constrain the mutation.\u201d\nNo further naturalness metrics or global validation of final test inputs are reported."
        },
        "gemini": {
          "value": "High",
          "why": "The DistXplore method enforces input naturalness throughout its fuzzing iterations by consistently applying norm-bounded constraints on each mutation step. The paper explicitly states that transformations are constrained with both L0 and L\u221e norms to guarantee the validity of generated inputs within the iterative genetic algorithm process. Furthermore, it provides strong empirical evidence through a manual investigation and quantitative validity ratios (over 95% across datasets) that the final fault-inducing inputs remain perceptually realistic.",
          "evidence": "Page 3, Section 2.1.2 Data Validity: \"Specifically, given a valid input x, the new test case x' generated by adding some perturbations on x is considered as valid if ||x\u2032 \u2212 x||p < d, where d is a safe radius.\"\nPage 5, Section 3.2 Distribution-Guided Test Generation: \"We select the diverse image transformations (e.g., translation, rotation, brightness) used in DeepTest [54] and DeepHunter [63]. For each selected gene, the mutation randomly selects a transformation function to mutate it. To guarantee the validity of the generated inputs, we adopt the conservative strategy [63] that constrains the transformation with both L0 and L\u221e.\"\nPage 9, Section 4.2.2 RQ2: Efficiency of DistXplore: \"In order to evaluate the validity of the generated inputs, we conducted a manual investigation by randomly selecting 10,000 erroneous inputs from the testing outputs of each model and calculating the average validity ratio. The validity ratios were found to be 98.5%+, 96.5%+, 98.7%+, and 95.3%+ for MNIST, Fashion-MNIST, CIFAR-10, and SVHN datasets, respectively. The results demonstrate that DistXplore is capable of generating valid inputs with high proportions.\""
        },
        "arbitrator": {
          "value": "High",
          "why": "DistXplore applies conservative, norm-bounded constraints on every mutation step to preserve input realism and further validates the naturalness of its final fault-inducing inputs through a large-scale manual study, reporting >95% validity ratios across four datasets. This human evaluation of the generated inputs constitutes direct empirical evidence that the final test cases remain perceptually realistic, satisfying the requirements for a High rating under the Naturalness Preservation metric.",
          "evidence": "Page 5, Section 3.2: \u201cTo guarantee the validity of the generated inputs, we adopt a more conservative configuration compared to DeepHunter [63] to constrain the mutation.\u201d\nPage 9, Section 4.2.2 RQ2: \u201cIn order to evaluate the validity of the generated inputs, we conducted a manual investigation by randomly selecting 10,000 erroneous inputs from the testing outputs of each model and calculating the average validity ratio. The validity ratios were found to be 98.5%+, 96.5%+, 98.7%+, and 95.3%+ for MNIST, Fashion-MNIST, CIFAR-10, and SVHN datasets, respectively.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Does not explicitly enforce naturalness across fuzzing iterations. Apply norm constraints in each mutation step but cumulative effect is ignored. Checks visual validity on the final outputs via human evaluation."
        }
      }
   
    },
    "taxonomy": {
      "access_level": [
        "Greybox"
      ],
      "oracle": [
        "Property-based"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Coverage-guided"
      ]
    }
  },
  "Liu et al. - 2022 - QATest A Uniform Fuzzing Framework for Question Answering Systems": {
    "key": "liu2022qatest",
    "name": "QATest",
    "year": "2022",
    "citation_count": "21",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "QATest reveals low-level functional errors in QA systems by generating perturbed questions and detecting changed or incorrect answers (i.e. misclassification or flawed output generation), rather than uncovering normative/ethical misbehaviors or bypassing any safety or policy mechanisms.",
          "evidence": "Page 3, Definition 3.1: \u201cA mutant q\u2032\u2208T(q) is regarded as a bug case of M if M(q\u2032) \u2260 M(q).\u201d  This shows QATest detects cases where the system\u2019s answer changes incorrectly under small semantic-preserving transformations, exposing functional robustness failures."
        },
        "gemini": {
          "value": "Low",
          "why": "QATest is designed to detect erroneous behaviors in QA systems by generating new test cases based on metamorphic transformations and identifying instances where the system's output changes for semantically equivalent questions. The case study (Section 5.4) details five types of bugs found: answering unanswerable questions with irrelevant content, failing to answer answerable questions, providing unrelated answers, answers with wrong keywords, and answers without logical reasoning. All these fall under functional correctness issues, where the system provides a flawed or incorrect output, which aligns with 'triggers low-level functional errors, such as misclassification in discriminative models or flawed outputs in generative models' in the metric definition.",
          "evidence": "Page 3, Section 3.2.1 'Test Oracle' and Definition 3.1: 'Metamorphic Relations (MRs) refer to the relationship expected to be followed between input and output when the target program is executed multiple times. Specifically, we denote a seed question set Q, and the QA system as M that continuously receives natural language questions and outputs the corresponding answers. We define a series of semantically consistent transformations T for questions q \u2208 Q. The generated question q' can be obtained by applying the transformation t\u2208T to the original question q, and q' has the same intent and meaning as q. Therefore, the QA system should output the same answer for the q and q'.' and 'Definition 3.1. Given a QA system M, a semantic transformation set T, and a seed question set Q, a mutant question corresponding to q \u2208 Q with transformation t \u2208 T can be represented as q' = t(q). A mutant q' \u2208 T(q) is regarded as a bug case of M if M(q') \u2260 M(q).'\n\nPage 9, Section 5.4 'Answer to RQ4: Case Study': 'In this RQ, we investigate and classify the erroneous behaviors of the QA system detected by QATest. In Table 6, we show five types of representative erroneous behaviors. We list the questions and expected answers in the original test set, as well as the new questions generated by QATest and the actual answers of the system, and the corresponding intercepted context corresponding to the questions. Specifically, we summarize the bug types below. (1) <NoAnswer> questions are answered with irrelevant content. (2) Answerable questions are answered with <NoAnswer>. (3) Answers that are unrelated to the question. (4) Answers with wrong keywords in the context. (5) Answers without logical reasoning.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "QATest reveals low-level functional errors in QA systems by generating perturbed questions and detecting changed or incorrect answers (i.e. misclassification or flawed output generation), rather than uncovering normative/ethical misbehaviors or bypassing any safety or policy mechanisms.",
          "evidence": "Page 3, Definition 3.1: \u201cA mutant q\u2032\u2208T(q) is regarded as a bug case of M if M(q\u2032) \u2260 M(q).\u201d  This shows QATest detects cases where the system\u2019s answer changes incorrectly under small semantic-preserving transformations, exposing functional robustness failures."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "QATest performs broad, untargeted exploration by applying metamorphic transformations to inputs and checking for any change in the system\u2019s output, without steering towards any specific predefined fault or output. It detects generic misbehavior such as inconsistent or incorrect answers rather than targeting particular answer labels or semantic categories.",
          "evidence": "Section 3.2 (Test Oracle): \u201cA mutant q\u2032 \u2208 T(q) is regarded as a bug case of M if M(q\u2032) \u2260 M(q).\u201d The framework does not define or constrain the exact output it seeks, only that it should remain the same under semantic-preserving input transformations."
        },
        "gemini": {
          "value": "Low",
          "why": "QATest performs untargeted exploration to reveal generic faults in Question Answering (QA) systems. Its primary mechanism, metamorphic testing, defines a 'bug case' as any instance where a semantically equivalent transformed question (q') yields a different answer than the original question (q). This setup aims to detect inconsistencies, inaccuracies, or unstable predictions, rather than steering towards specific outputs or predefined semantic fault categories like jailbreaks, toxicity, or bias. The guidance criteria (N-Gram coverage and perplexity priority) are designed to improve test diversity and fuzzing efficiency, not to direct the fuzzer towards a particular type of erroneous behavior.",
          "evidence": "Page 1, Abstract: \"...we design and implement a fuzzing framework for QA systems, namely QATest, based on the metamorphic testing theory. It provides the first uniform solution to generate tests with oracle information automatically for various QA systems... The experiment results show that the tests generated by QATest detect hundreds of erroneous behaviors of QA systems efficiently.\"\nPage 3, Section 3.2.1 Test Oracle: \"Metamorphic Relations (MRs) refer to the relationship expected to be followed between input and output when the target program is executed multiple times... Therefore, the QA system should output the same answer for the q and q'.\"\nPage 4, Definition 3.1: \"A mutant q' \u2208 T(q) is regarded as a bug case of M if M(q') \u2260 M(q).\"\nPage 4, Section 3.3 Guidance Criteria Design: \"...we propose two testing criteria as guidance in the test generation process.\"\nPage 5, Section 3.3.1 N-Gram Coverage: \"...we aim to evaluate the difference between the generated questions and the original seed set, and select the particular question that can increase the richness of the seed set to improve the test generation efficiency.\"\nPage 5, Section 3.3.2 Perplexity Priority: \"...we consider applying perplexity priority as guidance for seed data retention. ...to ensure that the generated test questions have good authenticity and rationality...\"\nPage 9, Section 5.4 Answer to RQ4: Case Study: Lists detected error types such as \"<NoAnswer> questions are answered with irrelevant content,\" \"Answerable questions are answered with <NoAnswer>,\" \"Answers that are unrelated to the question,\" \"Answers with wrong keywords in the context,\" and \"Answers without logical reasoning.\" These are all examples of generic inaccuracies or misclassifications, not targeted adversarial outputs."
        },
        "arbitrator": {
          "value": "Low",
          "why": "QATest performs broad, untargeted exploration by applying metamorphic transformations to inputs and checking for any change in the system\u2019s output, without steering towards any specific predefined fault or output. It detects generic misbehavior such as inconsistent or incorrect answers rather than targeting particular answer labels or semantic categories.",
          "evidence": "Section 3.2 (Test Oracle): \u201cA mutant q\u2032 \u2208 T(q) is regarded as a bug case of M if M(q\u2032) \u2260 M(q).\u201d The framework does not define or constrain the exact output it seeks, only that it should remain the same under semantic-preserving input transformations."
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "QATest focuses exclusively on generating and reporting QA system failures via input metamorphic transformations and external test\u2010case patterns, without examining model internals (e.g., activations, gradients, decision boundaries) or tracing failures back to specific architectural, training, or data causes.",
          "evidence": "Section 3 (Design & Implementation) describes only input transformations and selection criteria (N-Gram coverage, perplexity) for guiding test generation, with no diagnostic analysis of model internals. Section 5.4 (Answer to RQ4: Case Study) categorizes error symptoms (\u2018<NoAnswer> questions are answered with irrelevant content\u2019, \u2018Answers without logical reasoning\u2019, etc.) but does not investigate causal mechanisms within the model."
        },
        "gemini": {
          "value": "Medium",
          "why": "QATest provides medium root-cause insight by classifying and interpreting external behavioral patterns of failures, explaining how and where these failures manifest through examples. It identifies types of erroneous behaviors such as answering irrelevant content to unanswerable questions, failing to answer answerable questions, providing unrelated answers, extracting wrong keywords, and lacking logical reasoning. However, it does not perform diagnostic analysis of model internals (e.g., feature activations, gradient behavior) or establish causal links to specific architectural, training, or data-related weaknesses within the QA systems.",
          "evidence": "Page 9, Section 5.4 Answer to RQ4: Case Study:\n\"In this RQ, we investigate and classify the erroneous behaviors of the QA system detected by QATest. In Table 6, we show five types of representative erroneous behaviors. We list the questions and expected answers in the original test set, as well as the new questions generated by QATest and the actual answers of the system, and the corresponding intercepted context corresponding to the questions. Specifically, we summarize the bug types below.\n(1) <NoAnswer> questions are answered with irrelevant content. ... One of the bug types is that the QA system cannot identify whether the question can be answered or not, and output a keyword from the original text as the answer.\n(2) Answerable questions are answered with <NoAnswer>. ... the system can't find a matching answer and outputs <NoAnswer>.\n(3) Answers that are unrelated to the question. ... the QA system's answers are not related to the question.\n(4) Answers with wrong keywords in the context. ... the system extracts the improper keyword as the answer.\n(5) Answers without logical reasoning. ... the QA system triggers a bug by outputting a keyword in the original sentence without logical reasoning.\""
        },
        "arbitrator": {
          "value": "Medium",
          "why": "While QATest does not analyze model internals (e.g., activations or gradients) to establish causal mechanisms, it goes beyond merely reporting failure symptoms by classifying and interpreting external behavioral patterns of failures (e.g., answering <NoAnswer> questions with irrelevant content, extracting wrong keywords, lacking logical reasoning). This external interpretation of where and how failures manifest fits the intermediate level of root-cause insight.",
          "evidence": "Section 5.4 (Answer to RQ4: Case Study) \u2014 QATest summarizes five types of representative erroneous behaviors (e.g., \u201c<NoAnswer> questions are answered with irrelevant content\u201d, \u201cAnswers without logical reasoning\u201d), providing heuristic explanations of failure patterns without probing model internals."
        },
        "manual": {
          "value": "Medium",
          "why": ""
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "High",
          "why": "The authors provide empirical, quantitative validation that their fuzzed questions remain semantically and syntactically similar to real inputs. They filter generated questions using a ROUGE-1 threshold and order seed retention by a perplexity priority metric, both of which ensure the naturalness of the mutated inputs.",
          "evidence": "Section 3.2.5 Quality Assessment: \u201cTo ensure the quality of generated questions, we employ the ROUGE-1 metric \u2026 If the quality score is greater than the threshold, the generated question is accepted.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The paper provides clear evidence that QATest prioritizes and quantitatively evaluates the naturalness and semantic consistency of its generated inputs. It employs 'textual semantically consistent transformations' to ensure that new questions retain the original intent and meaning. Furthermore, QATest incorporates a 'quality assessment method' that utilizes the ROUGE-1 metric to 'discard low-quality generated questions', directly filtering out inputs that might not be natural. Additionally, a 'Perplexity Priority' criterion is used to guide seed data retention, explicitly aiming 'to ensure that the generated test questions have good authenticity and rationality'. These mechanisms, particularly the use of ROUGE-1 and perplexity as quantitative metrics for filtering and guiding, demonstrate an empirical approach to preserving input naturalness.",
          "evidence": "Page 3, Section 3.2, 'Test Generation with MRs': 'We implement three textual semantically consistent transformations, i.e., grammatical component-based, sentence structure-based, and adversarial perturbation-based transformations. Then, we employ a quality assessment method to discard low-quality generated questions.'\n\nPage 4, Section 3.2.5, 'Quality Assessment': 'To ensure the quality of generated questions, we employ the ROUGE-1 metric [35] to evaluate the questions generated by metamorphic transformations.'\n\nPage 5, Section 3.3.2, 'Perplexity Priority': 'To ensure that the generated test questions have good authenticity and rationality, we consider applying perplexity priority as guidance for seed data retention.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "The authors provide empirical, quantitative validation that their fuzzed questions remain semantically and syntactically similar to real inputs. They filter generated questions using a ROUGE-1 threshold and order seed retention by a perplexity priority metric, both of which ensure the naturalness of the mutated inputs.",
          "evidence": "Section 3.2.5 Quality Assessment: \u201cTo ensure the quality of generated questions, we employ the ROUGE-1 metric \u2026 If the quality score is greater than the threshold, the generated question is accepted.\u201d"
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "Although QATest is evaluated on four different QA models (UnifiedQA, ALBERT, DrQA, MARL), the paper reports failures for each model independently and does not analyze whether the same mutated or original inputs cause equivalent erroneous behaviors across multiple models. There is no cross-model correlation or overlap of failing inputs presented.",
          "evidence": "Section 4.2 (QA systems under test): \u201cTo evaluate the performance of QATest, we select four state-of-art QA systems as the testing subjects in our experiments.\u201d\nTable 3: reports the number of failed tests per system (UnifiedQA, ALBERT, DrQA, MARL) without any analysis of the same inputs failing across models."
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates the proposed QATest framework on multiple QA systems (UnifiedQA, ALBERT, DrQA, MARL) and various datasets. It reports the number of failed tests detected by different metamorphic relations (MRs) and testing criteria for each system independently (Tables 3, 4, 5). While this demonstrates the framework's general applicability and effectiveness in finding bugs across different types of QA systems, it does not analyze cross-model consistency in mutation-failure relationships. The case study in Table 6, which classifies erroneous behaviors, is explicitly conducted only for the ALBERT QA system, without attempting to reproduce these specific failures or analyze similar behaviors on other tested models.",
          "evidence": "Page 7, Section 5.1 Answer to RQ1: Metamorphic Transformations: 'To evaluate the effectiveness of each type of MR applied by QATest, we select 500 test cases from each data set and apply the MRs proposed in Section 3.2 to generate test data sets. Then, we record the number of failed tests in the generated test set on the corresponding QA systems, which is shown in Table 3.'\nPage 7, Table 3: 'The number of failed tests generated by different approaches with 500 original test cases as the seed dataset.' (Shows results for UnifiedQA, ALBERT, DrQA, MARL across MR1, MR2, MR3, All MRs, baseline, and QATest, but these are aggregated counts per system/MR, not an analysis of specific fault transferability).\nPage 9, Section 5.4 Answer to RQ4: Case Study: 'In this RQ, we investigate and classify the erroneous behaviors of the QA system detected by QATest. In Table 6, we show five types of representative erroneous behaviors. ...'\nPage 10, Table 6: 'The examples of erroneous behaviors detected by QATest on ALBERT QA system.' (This case study details specific failing inputs and outputs but explicitly for only one system, ALBERT, and does not discuss whether these specific failures or patterns transferred to other models)."
        },
        "arbitrator": {
          "value": "Low",
          "why": "Although QATest is evaluated on four different QA models (UnifiedQA, ALBERT, DrQA, MARL), the paper reports failures for each model independently and does not analyze whether the same mutated or original inputs cause equivalent erroneous behaviors across multiple models. There is no cross-model correlation or overlap of failing inputs presented.",
          "evidence": "Section 4.2 (QA systems under test): \u201cTo evaluate the performance of QATest, we select four state-of-art QA systems as the testing subjects in our experiments.\u201d\nTable 3: reports the number of failed tests per system (UnifiedQA, ALBERT, DrQA, MARL) without any analysis of the same inputs failing across models."
        },
        "manual": {
          "value": "Low",
          "why": "No cross model failure pattern analysis"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "High",
          "why": "QATest\u2019s fuzzing operates entirely at the discrete text level\u2014applying semantic-preserving edits such as synonym substitution, back-translation, and typo injection\u2014to produce finalized question strings. These text artifacts persist unchanged through standard I/O (e.g., file storage, encoding), so any failures it uncovers reflect flaws in the QA model\u2019s logic rather than fragile floating-point perturbations that could be lost on serialization.",
          "evidence": "Page 4, Section 3.2.1 (Test Oracle): \u201cA mutant q\u2032\u2208T(q) is regarded as a bug case of M if M(q\u2032)\u2260M(q). \u2026 The generated question q\u2032 can be obtained by applying the transformation t\u2208T to the original question q, and q\u2032 has the same intent and meaning as q.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The method proposed in the paper, QATest, generates new textual questions through various semantic transformations. These generated textual inputs are inherently stable under typical I/O operations (e.g., saving and loading text files) because they consist of discrete characters and words, not continuous numerical representations susceptible to precision loss through rounding or quantization. The metric definition explicitly states that \"model-generated fuzzed inputs\u2014such as synthesized speech, rendered images, or generated text\u2014are inherently stable under I/O operations because they originate from self-contained generation processes that produce finalized artifacts rather than transient numeric states.\"",
          "evidence": "Page 3, Section 3.2, 'Test Generation with MRs': 'To simulate the possible question QA systems may receive in real usage scenarios, we implement three textual semantically consistent transformations, i.e., grammatical component-based, sentence structure-based, and adversarial perturbation-based transforma- tions.'\nPage 3, Section 3.2.1, 'Test Oracle': 'We define a series of semantically consistent transformations T for questions q e Q. The generated question q' can be obtained by applying the transformation t\u2208T to the original question q, and q' has the same intent and meaning as q. Therefore, the QA system should output the same answer for the q and q'.'\nPage 7, Section 4.3, 'Sentence Transformations': 'As described in Section 3.2, QATest generates tests by making realistic transformations on the original data with ten transformations in total. Most of the transformations are implemented with a tool named nlpaug [37], which is a python library that helps users with augmenting NLP for machine learning projects.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "QATest\u2019s fuzzing operates entirely at the discrete text level\u2014applying semantic-preserving edits such as synonym substitution, back-translation, and typo injection\u2014to produce finalized question strings. These text artifacts persist unchanged through standard I/O (e.g., file storage, encoding), so any failures it uncovers reflect flaws in the QA model\u2019s logic rather than fragile floating-point perturbations that could be lost on serialization.",
          "evidence": "Page 4, Section 3.2.1 (Test Oracle): \u201cA mutant q\u2032\u2208T(q) is regarded as a bug case of M if M(q\u2032)\u2260M(q). \u2026 The generated question q\u2032 can be obtained by applying the transformation t\u2208T to the original question q, and q\u2032 has the same intent and meaning as q.\u201d"
        },
        "manual": {
          "value": "High",
          "why": "Text level mutation "
        }
      }
  
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Metamorphic"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Data-driven"
      ]
    }
  },
  "Wang et al. - 2022 - BET black-box efficient testing for convolutional neural networks": {
    "key": "wang2022bet",
    "name": "BET",
    "year": "2022",
    "citation_count": "26",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "Low",
          "why": "BET\u2019s primary output is error-inducing inputs that cause misclassification in CNNs, i.e. low-level functional or robustness failures, without exploiting or bypassing any safety, policy, or security mechanism.",
          "evidence": "Page 165, Section 2.2 (Related Work \u2013 Testing CNNs): \u201cThe crux of CNN testing lies in the generation of test cases that explore incorrect behaviors of CNN models\u2026 and find incorrect behaviors of CNNs, i.e., error-inducing inputs.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The Black-box Efficient Testing (BET) method is designed to identify \"defects (e.g. error-inducing inputs)\" in Convolutional Neural Networks (CNNs). The paper consistently describes these defects as inputs that cause \"unexpected or incorrect prediction behaviors\" or \"misclassification.\" The primary metrics used for evaluation are the number of \"error-inducing inputs\" found and the number of \"unique misclassified labels.\" These findings are then leveraged to \"fine-tune the target model, improving its accuracy,\" which directly addresses issues of functional correctness and robustness under perturbation rather than ethical/normative violations or direct security bypasses.",
          "evidence": "Page 1, Abstract: \"It is important to test convolutional neural networks (CNNs) to identify defects (e.g. error-inducing inputs) before deploying them in security-sensitive scenarios... We further show that the error-inducing inputs found by BET can be used to fine-tune the target model, improving its accuracy by up to 3%.\"\nPage 1, Introduction: \"They aim at identifying defects of CNN models such as finding adversarial inputs which induce errors. These error-inducing inputs can be used to fine-tune the model to improve its classification accuracy.\"\nPage 7, Metrics: \"Err-Num. We compare the average number of error-inducing inputs per image (Err-Num). The higher the Err-Num is, the more error-inducing inputs are found to indicate a better testing result.\"\nPage 7, Metrics: \"Label-Num. We compare the average number of unique misclassified labels per image (Label-Num). The higher the Label-Num is, the label of error-inducing inputs is more diversified to indicate a better testing result.\"\nPage 11, Section 5, Comparison with adversarial attacks: \"First, an effective adversarial attack aims to successfully craft one sample falling to a specific or arbitrary classification result, but CNN testing methods aim to find more defects of one target model. For instance, white-box testing methods try to achieve a high neuron coverage and BET tries to explore different decision boundaries by diversifying the labels of error-inducing inputs found.\""
        },
        "arbitrator": {
          "value": "Low",
          "why": "BET\u2019s primary output is error-inducing inputs that cause misclassification in CNNs, i.e. low-level functional or robustness failures, without exploiting or bypassing any safety, policy, or security mechanism.",
          "evidence": "Page 165, Section 2.2 (Related Work \u2013 Testing CNNs): \u201cThe crux of CNN testing lies in the generation of test cases that explore incorrect behaviors of CNN models\u2026 and find incorrect behaviors of CNNs, i.e., error-inducing inputs.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": ""
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "High",
          "why": "BET\u2019s tunable objective function allows the tester to steer the fuzzing toward specific, predefined misclassification targets (labels). By selecting and dynamically adjusting a target label li and optimizing the model\u2019s confidence in that label, BET deliberately forces the classifier to predict chosen labels, matching the definition of high goal directedness.",
          "evidence": "Page 167, Section 3.3 Tunable Objective Function: \u201cIf the objective function targets a specific label, the testing process will be led to enter the decision boundary of this specific label to explore corner cases, i.e., generate error-inducing inputs with the corresponding label.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The Black-box Efficient Testing (BET) method can steer its exploration towards specific, predefined faults by allowing its tunable objective function to target a chosen label. The method explicitly aims to generate error-inducing inputs that force a classifier to predict a specific label.",
          "evidence": "Page 4, Section 3.3, 'Tunable Objective Function': 'Specifically, during the test process, perturbations with higher objective function scores will be saved for further optimization. If the objective function targets a specific label, the testing process will be led to enter the decision boundary of this specific label to explore corner cases, i.e., generate error-inducing inputs with the corresponding label.'\nPage 5, Section 3.3, 'Tunable objective function for ST': 'SOF(x) = C(x)[li], where li is the same as in Equation 5. In Equation 6, we only optimize C(x)[li], as ST focuses on the target model C, and ground truth is provided to indicate whether error-inducing inputs are found.' This shows the objective function is designed to maximize the confidence score for a specific label `li`."
        },
        "arbitrator": {
          "value": "High",
          "why": "BET\u2019s tunable objective function allows the tester to steer the fuzzing toward specific, predefined misclassification targets (labels). By selecting and dynamically adjusting a target label li and optimizing the model\u2019s confidence in that label, BET deliberately forces the classifier to predict chosen labels, matching the definition of high goal directedness.",
          "evidence": "Page 167, Section 3.3 Tunable Objective Function: \u201cIf the objective function targets a specific label, the testing process will be led to enter the decision boundary of this specific label to explore corner cases, i.e., generate error-inducing inputs with the corresponding label.\u201d"
        },
        "manual": {
          "value": "High",
          "why": ""
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Low",
          "why": "BET is purely a black-box testing approach that generates and queries perturbed inputs to find misclassifications, without inspecting or analyzing any internal model signals (activations, gradients, feature representations) or establishing causal links between model internals and failures. It thus only reports \u2018what\u2019 fails (error-inducing inputs) rather than \u2018why\u2019 failures occur.",
          "evidence": "Page 165, Introduction: \u201cWithout any inner knowledge of target models, black-box testing methods cannot measure the coverage but can only find error-inducing inputs with diversified labels.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The Black-box Efficient Testing (BET) method focuses solely on identifying 'error-inducing inputs' without analyzing internal model signals or external behavioral patterns to infer the root causes of these failures. The paper explicitly states that black-box methods like BET lack 'full inner knowledge of target models' and cannot 'measure the coverage' (internal states) to diagnose why vulnerabilities occur. It also identifies 'specifying a new metric to reflect the inner states of target models for black-box testing' as future work, confirming that the current method does not provide such diagnostic insight.",
          "evidence": "Page 1, Abstract: 'identify defects (e.g. error-inducing inputs) before deploying them in security-sensitive scenarios.'\nPage 2, Section 2.2 Related Work: 'such metrics [neuron coverage] are not suitable for black-box testing techniques as neuron coverage needs to be acquired with full knowledge of target CNN models, e.g. models' parameters. Existing black-box testing techniques [28, 46] could only rely on CNNs' outputs as feedback to guide the testing process and find error-inducing inputs.'\nPage 3, Section 3.1 Key Insight: 'CNNs are generally prone to be affected by continuous perturbations.' This is an insight used to generate attacks, not a diagnostic for a specific failure.\nPage 5, Section 3.3 Tunable Objective Function: 'explore different decision boundaries and thoroughly test CNNs.' and 'generate error-inducing inputs with the corresponding label.' This focuses on finding errors and diversifying their types, not diagnosing their causes.\nPage 10, Section 4.3 Analysis: 'We note that existing white-box testing methods aim to achieve higher neuron coverage. This metric is hard to experiment with in black-box settings since there is no inner knowledge of models for us to improve the neuron coverage or to measure it.'\nPage 11, Section 5 Discussion, Novel metrics in black-box testing: 'But this metric [neuron coverage] is not suitable for black-box testing methods since testers cannot try to improve the neuron coverage without any inner knowledge of the neurons. Thus, we need new metrics for black-box testing methods to understand the inner status of the target models during the test. We think this challenge may be solved through learning or knowledge extraction from the error-inducing inputs acquired during the test. Thus, we list our second future work, i.e., specifying a new metric to reflect the inner states of target models for black-box testing.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "BET is purely a black-box testing approach that generates and queries perturbed inputs to find misclassifications, without inspecting or analyzing any internal model signals (activations, gradients, feature representations) or establishing causal links between model internals and failures. It thus only reports \u2018what\u2019 fails (error-inducing inputs) rather than \u2018why\u2019 failures occur.",
          "evidence": "Page 165, Introduction: \u201cWithout any inner knowledge of target models, black-box testing methods cannot measure the coverage but can only find error-inducing inputs with diversified labels.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "Only reports failures without any insight on root-cause"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "BET enforces bounded perturbations (L\u221e \u2264 13/255) to ensure inputs remain close to the original images, providing a theoretical design constraint for naturalness preservation, but it does not include any empirical validation (e.g., LPIPS/SSIM/BLEU scores or human evaluation) to quantify perceptual or semantic realism.",
          "evidence": "Section 4.1 Experimental Setup (page 172): \u201cWe use the widely used L\u221e metric [4] to restrict perturbations for error-inducing inputs on all compared methods. \u2026 we choose values that make them perform relatively well, i.e., L\u221e = 13/255 on all datasets (all compared methods use the same setting).\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper describes using the L\u221e metric to restrict the magnitude of perturbations, which serves as a design constraint to limit how much the fuzzed inputs deviate from the original. However, it does not provide empirical validation (e.g., LPIPS, SSIM, BLEU, or human evaluation) to confirm that the generated inputs remain perceptually or semantically natural within the model's input domain.",
          "evidence": "Page 7, Section 4.1 Experimental Setup, Hyperparameters: \"We use the widely used L\u221e metric [4] to restrict perturbations for error-inducing inputs on all compared methods. To set values of L\u221e, we first run all compared methods under different L\u221e for multiple times, and then we choose values that make them perform relatively well, i.e., L\u221e = 13/255 on all datasets (all compared methods use the same setting).\" \n\nPage 7, Section 4.3 BET for Single Model Testing, Result of Inact-Rate: \"The perturbations are directly added on pixel values as integers, BET does not suffer the inactivation issue. However, baseline methods, especially the ones that use gradient-based methods suffer from this problem.\""
        },
        "arbitrator": {
          "value": "Medium",
          "why": "BET enforces bounded perturbations (L\u221e \u2264 13/255) to ensure inputs remain close to the original images, providing a theoretical design constraint for naturalness preservation, but it does not include any empirical validation (e.g., LPIPS/SSIM/BLEU scores or human evaluation) to quantify perceptual or semantic realism.",
          "evidence": "Section 4.1 Experimental Setup (page 172): \u201cWe use the widely used L\u221e metric [4] to restrict perturbations for error-inducing inputs on all compared methods. \u2026 we choose values that make them perform relatively well, i.e., L\u221e = 13/255 on all datasets (all compared methods use the same setting).\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "norm-bounded"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "High",
          "why": "BET\u2019s perturbations are applied directly as integer changes to pixel values, avoiding fragile floating-point artifacts. As a result, the discovered faults persist through standard I/O operations (e.g., saving as PNG/JPEG or quantization) without being erased by rounding or clipping.",
          "evidence": "Page 172, Section 4.3 (Result of Inact-Rate): \u201cThe Inact-Rate of BET is 0.0%. The perturbations are directly added on pixel values as integers, BET does not suffer the inactivation issue.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The method, BET, explicitly addresses and avoids the issue of perturbation instability due to I/O operations like quantization and rounding. It achieves this by directly applying perturbations to pixel values as integers, rather than manipulating continuous representations that would then need to be discretized. This ensures that the generated error-inducing inputs are inherently stable and do not suffer from information loss or 'inactivation' when processed for storage or transmission, which is a common problem for methods that operate on floating-point values.",
          "evidence": "Page 9, Section 4.3 'Result of Inact-Rate', 'The Inact-Rate of BET is 0.0%. The perturbations are directly added on pixel values as integers, BET does not suffer the inactivation issue. However, baseline methods, especially the ones that use gradient-based methods suffer from this problem. For instance, on ResNet50, the Inact-Rate of all these white-box methods Inact-Rate are 94.0%, 36.6%, 54.8%, and 62.4%, respectively. This is because their methods calculate error-inducing inputs in continuous domain and will suffer from invertible loss when transforming the perturbations back to pixel values as integers.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "BET\u2019s perturbations are applied directly as integer changes to pixel values, avoiding fragile floating-point artifacts. As a result, the discovered faults persist through standard I/O operations (e.g., saving as PNG/JPEG or quantization) without being erased by rounding or clipping.",
          "evidence": "Page 172, Section 4.3 (Result of Inact-Rate): \u201cThe Inact-Rate of BET is 0.0%. The perturbations are directly added on pixel values as integers, BET does not suffer the inactivation issue.\u201d"
        },
        "manual": {
          "value": "High",
          "why": "Applies integer perturbations. Shows failure stability empricaclly. Shows ADAPT, DLFUZZ, DeepExplore failures are unstable"
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "High",
          "why": "The authors explicitly evaluate BET in a differential-testing scenario (Sec. 4.2), generating error-inducing inputs on one model and verifying that the exact same inputs cause misclassifications on multiple related models (original vs. 8-bit and 16-bit quantized variants). This input-level transfer demonstrates high fault transferability across models performing the same task.",
          "evidence": "Section 4.2 (\u201cEvaluation of Black-Box Differential Testing\u201d): \u201cWe try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. \u2026 Table 1 lists the original and 8-bit/16-bit quantized models. Tables 2\u20134 report Err-Num, Label-Num, and SR for each model pair, showing that the same inputs mislead all models.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates its proposed method (BET) on multiple CNN models (VGG16, ResNet18, VGG19, ResNet50, ResNet56) across different datasets and two testing scenarios (Differential Testing and Single Model Testing). While this demonstrates the general applicability and effectiveness of BET across various models, it does not analyze cross-model consistency in how specific error-inducing inputs or mutation patterns relate to failures. For Differential Testing, it finds inputs that cause discrepancies between an original model and its quantized versions, but it does not test if these *same inputs* cause similar discrepancies across *different model architectures* (e.g., VGG16 vs. ResNet18). For Single Model Testing, it identifies error-inducing inputs for individual models, but again, does not demonstrate transferability of these specific inputs or mutation-failure patterns between different models.",
          "evidence": "Page 7, Section 4.2 'Evaluation of Black-Box Differential Testing':\n'To get a convincing DT result, we directly use Tensorflow-Lite [5]... to construct quantized models for DT as DiffChaser does. Tensorflow-Lite provides two quantization options to quantize models, i.e., 16-bits and 8-bits quantization, which represent original models quantized from 32-bits to 16-bits and 8-bits respectively of floating precision. We apply these two options to all models and we get corresponding 8-bits and 16-bits quantized models as shown in Table 1. Then, we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models. e.g., generating error-inducing inputs between ResNet50 with ResNet50-q8.'\n\nPage 8, Table 2 'Evaluation results of Err-Num': Shows Err-Num for specific model pairs (e.g., VGG16/VGG16-q8, ResNet18/ResNet18-q8) without analysis of transferability of specific inputs across these distinct pairs.\n\nPage 9, Table 5 'Evaluation of Err-Num in single model testing scenarios': Shows Err-Num for individual models (e.g., VGG16, ResNet18) without analyzing cross-model consistency of mutation-failure relationships."
        },
        "arbitrator": {
          "value": "High",
          "why": "The paper\u2019s Differential Testing evaluation (Section 4.2) explicitly generates error-inducing inputs on a base CNN and then demonstrates that the *same* inputs cause misclassifications on its 8-bit and 16-bit quantized variants. This is precisely an empirical demonstration that identical failing inputs transfer across multiple models performing the same task, satisfying the \"High\" transferability criterion.",
          "evidence": "Section 4.2 (\u201cEvaluation of Black-Box Differential Testing\u201d): \u201cThen, we try to generate error-inducing inputs that cause disagreements between the original model with its quantized models\u2026 Tables 2\u20134 report Err-Num, Label-Num, and SR for each model pair, showing that the *same inputs* mislead all models.\u201d"
        },
        "manual": {
          "value": "Low",
          "why": "Does not show transferability to other models or common pattern between models. It uses differential testing to determine fault if two model versions gives inconsistent output"
        }
      }
   
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Differential",
        "Property-based"
      ],
      "mutation_strategy": [
        "Feedback-informed"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ]
    }
  },
  "Zhang et al. - 2020 - Towards characterizing adversarial defects of deep learning software from the lens of uncertainty": {
    "key": "zhang2020towards",
    "name": "KuK",
    "year": "2020",
    "citation_count": "101",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "High",
          "why": "The KuK fuzzer generates adversarial inputs that bypass existing defense mechanisms\u2014i.e., it uncovers vulnerabilities that penetrate explicit safety or security safeguards rather than merely triggering low-level functional or behavioral errors.",
          "evidence": "Section 5.2 (Answer to RQ4): \u201cThe uncommon data inputs are not well defended by existing defense techniques\u2026 In particular, the binary classifier and mutation-based detection approaches are less useful in defending the uncommon data inputs (e.g., with only 5% and 10% on NIN and ResNet-20, respectively).\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The method proposed in the paper, KuK, generates 'uncommon data' (including adversarial examples) that are specifically designed to bypass existing adversarial defense techniques. The paper explicitly evaluates KuK's ability to create inputs that are 'hard to be defended by the existing defense techniques' and cause these defenses to 'perform poorly' with a significantly 'reduced success rate'. These adversarial defense techniques are mechanisms implemented to safeguard the deep learning models against malicious inputs and ensure their robustness and, by extension, their safety and security in critical applications. Therefore, uncovering vulnerabilities that bypass these defense mechanisms constitutes bypassing explicit safety or security mechanisms.",
          "evidence": "Page 1, Abstract: 'Our further evaluation reveals that the uncommon data generated by our method is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%.'\nPage 2, Introduction: 'Such uncommon samples represent a new type of hazard and potential defects to DL software, which so far lacks investigation and should draw further attention during future quality assurance solution design.' and 'Thus, it is quite important to generate such uncommon inputs to reveal the hidden defects of DL software for vulnerability analysis and further enhancement, especially for safety- and security-critical scenarios.'\nPage 9, Section 5.2, 'RQ4: Evaluation on Defense Techniques', Setting: 'To demonstrate the usefulness of the generated uncommon data in Table 5, this experiment intends to study whether the data can bypass the existing defense techniques.'\nPage 9, Section 5.2, 'RQ4: Evaluation on Defense Techniques', Results: 'The overall result shows that the existing defense techniques perform poorly on the uncommon data we generated.'\nPage 10, Answer to RQ4: 'The uncommon data inputs are not well defended by existing defense techniques while the common data are relatively easier to be defended. In particular, the binary classifier and mutation-based detection approaches are less useful in defending the uncommon data inputs (e.g., with only 5% and 10% on NIN and ResNet-20, respectively).'"
        },
        "arbitrator": {
          "value": "High",
          "why": "The KuK fuzzer generates adversarial inputs that bypass existing defense mechanisms\u2014i.e., it uncovers vulnerabilities that penetrate explicit safety or security safeguards rather than merely triggering low-level functional or behavioral errors.",
          "evidence": "Section 5.2 (Answer to RQ4): \u201cThe uncommon data inputs are not well defended by existing defense techniques\u2026 In particular, the binary classifier and mutation-based detection approaches are less useful in defending the uncommon data inputs (e.g., with only 5% and 10% on NIN and ResNet-20, respectively).\u201d"
        },
        "manual": {
          "value": "High",
          "why": "Demostrates its ability to bypass adversarail defense mechanisms"
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Low",
          "why": "KuK drives its search toward uncovering generic misclassifications and unusual uncertainty\u2010pattern inputs, but it does not steer the model to produce any particular target label or semantic behavior. Its only \u201cfault\u201d objective is x being any adversarial example (LM(x\u2032) \u2260 LM(x)), without constraining the exact output class or seeking a semantic category of fault.",
          "evidence": "Section 4 (\u201cUncommon Data Generation\u201d): \u201cTo generate AEs which belong to the HL type, the objective is set as: PCS(x, M) > p \u2227 VRO(x, M) < v \u2227 x is an AE\u2026 where AE(x) is 1 if x is an AE. Otherwise, it is 0.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "The fuzzing approach, KuK, is explicitly designed to steer its exploration towards specific, predefined behavioral goals related to the uncertainty patterns of the generated data. It aims to generate data inputs that fall into four 'uncommon patterns' based on specific ranges of Prediction Confidence Score (PCS) and Variation Ratio (VRO), and whether the input is a Benign Example (BE) or an Adversarial Example (AE). These objectives are concretely defined through piecewise fitness functions that guide the genetic algorithm to achieve target PCS and VRO values, along with desired classification outcomes (correct or incorrect).",
          "evidence": "Page 7, Section 4 'UNCOMMON DATA GENERATION', Paragraph 1 & Subsection 'Objective and Fitness Calculation':\n\"The results of RQ2 reveal that BEs and AEs usually belong to the HL type and LH type, respectively. However, several questions still remain, i.e. whether there exist: 1) data samples with high PCS and high VRO (i.e., the HH type), 2) data with low PCS and low VRO (i.e., the LL type), 3) BEs with low PCS and high VRO (i.e., the LH BEs) and 4) AEs with high PCS and low VRO (i.e., the HL AEs). These samples have the potential to uncover the unknown behaviors of DNN, which are largely missed by existing methods. To answer these questions, we developed a tool, KuK, to generate such uncommon data.\"\n\n\"In each iteration, we check whether some generated images (in the population) satisfy the objective, which is specifically designed for each type based on PCS and VRO. The test generation continues until some desirable data inputs are obtained. To satisfy the objective, we design a set of piecewise fitness functions to generate different types of uncommon data such that the higher the corresponding fitness value, the better the input.\"\n\nPage 8, Section 4 'UNCOMMON DATA GENERATION', Objective definitions for LL, HH, HL AEs, and LH BEs:\n\"\u2022 For the LL type, the objective is PCS(x, M) < p \u2227 VRO(x, M) < v, where p and v are configurable parameters, and the fitness function is: FitLL(x) = { -PCS(x, M), min(PCS(X, M)) > p; (PCS(x, M) < p) - VRO(x, M), otherwise }\"\n\"\u2022 For the HH type, the objective is PCS(x, M) > p \u2227 VRO(x, M) > v, where p and v are configurable parameters, and the fitness function is: FitHH(x) = { PCS(x, M), max(PCS(X, M)) < p; (PCS(x, M) > p) + VRO(x, M), otherwise }\"\n\"\u2022 To generate AEs which belong to the HL type, the objective is set as: PCS(x, M) > p \u2227 VRO(x, M) < v \u2227 x is an AE...\"\n\"\u2022 To generate BEs that belong to LH type, the objective is set as: PCS(x, M) < p \u2227 VRO(x, M) > v and x is a BE...\""
        },
        "arbitrator": {
          "value": "Low",
          "why": "KuK\u2019s fitness objectives guide the fuzzer to generate inputs that simply exhibit certain ranges of uncertainty metrics (PCS and VRO) or any misclassification, rather than driving the model toward a particular target label or semantic behavior. It does not aim to force a chosen output class or trigger a specific category of defect \u2013 it only seeks generic adversarial examples or benign samples with uncommon uncertainty patterns.",
          "evidence": "Section 4 (\u201cUncommon Data Generation\u201d): \u201cTo generate AEs which belong to the HL type, the objective is set as: PCS(x, M) > p \u2227 VRO(x, M) < v \u2227 x is an AE\u2026 where AE(x) is 1 if x is an AE. Otherwise, it is 0.\u201d Similar objectives are defined for other patterns, with no constraint on the specific output class."
        },
        "manual": {
          "value": "Low",
          "why": "aims to maximize model uncertainty and reveal generic misclassifications"
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Medium",
          "why": "The approach characterizes adversarial failures by correlating them with uncertainty patterns (e.g. low PCS & high VRO for AEs, high PCS & low VRO for BEs) rather than diagnosing specific causal mechanisms within the model (such as particular architectural components, learned representations or optimization dynamics) that give rise to those failures.",
          "evidence": "Page 743, Section 3.4 (Existing Data Study): \u201cOur in-depth analysis reveals that AEs/BEs generated by existing techniques \u2026 largely fall into two common patterns (that we refer as common input samples): (1) AEs tend to have low PCS and high VRO uncertainty, and (2) BEs often come with high PCS and low VRO.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The proposed method, KuK, generates uncommon adversarial examples (AEs) and benign examples (BEs) guided by uncertainty metrics (Prediction Confidence Score (PCS) and Variation Ratio (VRO)). The paper categorizes inputs into four patterns (LH, HH, LL, HL) based on these metrics, which represent model's internal signals related to prediction confidence and stability. The study uses these patterns to characterize how and where existing AEs and BEs behave, and then generates new 'uncommon' samples that fall into previously missed patterns. The evaluation then explains that existing defense techniques are often biased towards common patterns and fail against these uncommon ones, thereby explaining *where and how* failures manifest (in terms of specific uncertainty patterns) and how existing defenses exhibit behavioral patterns (bias towards certain input types). While it does not establish causal links to specific architectural or training weaknesses, it goes beyond merely reporting failures by correlating them with uncertainty profiles and interpreting the observed behavioral biases of defense mechanisms.",
          "evidence": "Page 2, Abstract: 'The intrinsic uncertainty nature of deep learning decisions can be a fundamental reason for its incorrect behavior. ... we first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data. Then, we identify and categorize the uncertainty patterns of BEs and AEs...' \nPage 2, Contributions (3rd bullet point): 'Our results reveal that (1) AEs and BEs from existing techniques largely follow two common patterns while the other patterns (denoted as uncommon patterns) are largely missed by existing methods, and (2) the characteristics of AEs generated by the testing tools differ from those by the adversarial attack techniques.'\nPage 3, Contributions (4th bullet point): 'Our results indicate that the current defense techniques are often biased towards particular patterns of samples. The generated uncommon samples can bypass such defense techniques with a high success rate...'\nPage 3, Section 2.1, DEFINITION 4 (PREDICTION CONFIDENCE SCORE): 'Intuitively, PCS depicts the probability difference between the two classes with the highest probabilities, which provides an uncertainty proxy from the aspect of distance to the geometric boundary.'\nPage 4, Section 2.2.1, DEFINITION 6 (VARIATION RATIO for Original Prediction): 'Intuitively, VR measures the general uncertainty of the decision with the highest frequency... while VRO represents the stability around the prediction mode of model M...'\nPage 6, Section 3.3 'Characterizing Data Behavior': 'The prediction confidence (i.e., PCS) represents the confidence of a single-shot model execution while the Bayesian uncertainty is measured by the statistical results from multi-shot model executions. ... we adopt a two-dimensional metric, i.e., (PCS, VRO), to characterize the BEs and AEs on a specific DNN model. Based on this, the data is classified into four patterns (see Fig. 2 (a)): low PCS and high VRO (LH), high PCS and high VRO (HH), low PCS and low VRO (LL) and high PCS and low VRO (HL). The categorization provides a way to understand and analyze the behaviors of AEs and BEs.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "The approach characterizes adversarial failures by correlating them with uncertainty patterns (e.g. low PCS & high VRO for AEs, high PCS & low VRO for BEs) rather than diagnosing specific causal mechanisms within the model (such as particular architectural components, learned representations or optimization dynamics) that give rise to those failures.",
          "evidence": "Page 743, Section 3.4 (Existing Data Study): \u201cOur in-depth analysis reveals that AEs/BEs generated by existing techniques \u2026 largely fall into two common patterns (that we refer as common input samples): (1) AEs tend to have low PCS and high VRO uncertainty, and (2) BEs often come with high PCS and low VRO.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Shows correlatin"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "The technique enforces a design constraint\u2014a bound on the L\u221e perturbation (radius 0.3)\u2014to keep generated images recognizable, but it does not report any empirical validation (e.g., LPIPS, SSIM or human studies) of input realism.",
          "evidence": "Section 4 (Uncommon Data Generation), Population Initialization and Mutation: \u201cFor the mutation process, the radius of L\u221e is set as 0.3. In order to generate high-quality images (i.e., recognizable by human), we abandon the affine transformation \u2026\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper enforces design constraints, specifically an L\u221e norm limit on perturbations, with the stated intention of generating 'high-quality images (i.e., recognizable by human)'. However, it does not provide empirical validation (e.g., through quantitative metrics like LPIPS, SSIM, BLEU, or human evaluation results) to confirm that the fuzzed inputs indeed remain perceptually or semantically natural after these constraints are applied. This aligns with a 'Medium' level of naturalness preservation.",
          "evidence": "Page 7, Section 4, 'UNCOMMON DATA GENERATION', 'Population Initialization': \"In order to generate high-quality images (i.e., recognizable by human), we abandon the affine transformation (e.g., translation and rotation [55]) as the crossover may generate invalid images. We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.\"\nPage 8, Section 5.1, 'RQ3: Usefulness of Test Data Generation', 'Setting': \"For the mutation process, the radius of L\u221e is set as 0.3.\""
        },
        "arbitrator": {
          "value": "Medium",
          "why": "The technique enforces a design constraint\u2014a bound on the L\u221e perturbation (radius 0.3)\u2014to keep generated images recognizable, but it does not report any empirical validation (e.g., LPIPS, SSIM or human studies) of input realism.",
          "evidence": "Section 4 (Uncommon Data Generation), Population Initialization and Mutation: \u201cFor the mutation process, the radius of L\u221e is set as 0.3. In order to generate high-quality images (i.e., recognizable by human), we abandon the affine transformation \u2026\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "norm-bounded"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper\u2019s method generates adversarial and benign inputs by applying small, continuous floating-point perturbations (e.g., white-noise mutations with an L\u221e bound) and does not include any step to clip, round, or otherwise validate that these perturbations survive standard I/O quantization (e.g., to 8-bit image formats). Therefore, the observed faults likely stem from fragile floating-point artifacts that would disappear after serialization or encoding.",
          "evidence": "Section 4 (Uncommon Data Generation): \u201cAfter the crossover, each image is randomly mutated by adding white noise\u201d \u2013 there is no mention of clipping, rounding, or other measures to ensure I/O stability of the generated perturbations."
        },
        "gemini": {
          "value": "Low",
          "why": "The proposed method, KuK, generates uncommon data by 'randomly adding noise' to existing images, constrained by an L\u221e norm. The paper does not describe any explicit validation steps, such as clipping or rounding, to ensure that these pixel-level perturbations remain stable and persist after being processed through standard I/O operations (e.g., saving images as 8-bit files) that may introduce precision loss. Without such validation, small continuous perturbations are susceptible to disappearing due to floating-point precision issues during serialization.",
          "evidence": "Section 4, 'UNCOMMON DATA GENERATION', 'Population Initialization': 'Given an input image, we first generate a set of images by randomly adding noise to it.' and 'We use L\u221e norm to constrain the allowable changes between the original seed and its generated counterpart.' The paper explicitly states 'In order to generate high-quality images (i.e., recognizable by human), we abandon the affine transformation (e.g., translation and rotation [55]) as the crossover may generate invalid images,' which indicates a focus on pixel-level rather than semantic-level changes, but does not mention any mechanisms to preserve these small changes across I/O operations."
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper\u2019s method generates adversarial and benign inputs by applying small, continuous floating-point perturbations (e.g., white-noise mutations with an L\u221e bound) and does not include any step to clip, round, or otherwise validate that these perturbations survive standard I/O quantization (e.g., to 8-bit image formats). Therefore, the observed faults likely stem from fragile floating-point artifacts that would disappear after serialization or encoding.",
          "evidence": "Section 4 (Uncommon Data Generation): \u201cAfter the crossover, each image is randomly mutated by adding white noise\u201d \u2013 there is no mention of clipping, rounding, or other measures to ensure I/O stability of the generated perturbations."
        },
        "manual": {
          "value": "Low",
          "why": "Applies small floating point perturbations without clipping/rounding"
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "The paper generates and evaluates uncommon inputs separately for each model (LeNet-5, NIN, ResNet-20, MobileNet) and does not assess whether failures discovered on one model transfer to others. There is no analysis of the same inputs causing equivalent erroneous behaviors across multiple models.",
          "evidence": "Section 5.1 (RQ3): \u201cWe adopt KuK to generate different types of uncommon data on four widely used DL models \u2013 LeNet-5, NIN, ResNet-20 and MobileNet.\u201d All evaluation of generated inputs is done per model, without cross\u2010model transferability analysis."
        },
        "gemini": {
          "value": "Low",
          "why": "The paper evaluates the effectiveness of its generated 'uncommon data' in bypassing defense techniques across multiple deep learning models (LeNet-5, NIN, ResNet-20, MobileNet). However, the generation of 'uncommon data' (inputs characterized by specific uncertainty patterns, i.e., PCS and VRO) is performed independently for each model. The paper does not demonstrate that the *same failing inputs* (generated for one model) cause *equivalent erroneous behaviors* (bypassing defenses) across *other models*. In fact, it explicitly states that the characteristics of the uncommon data can vary across models, noting that data uncommon for one model might become 'common' for another, which argues against direct input-level or even specific pattern-level transferability of the 'uncommon' failure characteristic.",
          "evidence": "Page 9, Section 5.1, 'RQ3: Usefulness of Test Data Generation', 'For each seed, we generate four types of uncommon data.' (This indicates model-specific generation).\nPage 9, Section 5.1, 'Results', 'Comparing the results among the four models, we could find that the difficulty in generating uncommon data varies for different models.' (This highlights differences across models).\nPage 10, Left Column, Last Paragraph, 'We also found that the uncommon data on LeNet-5 is not effective on defensive distillation and label smoothing. We performed an investigation and found that it may be caused by the following reasons. 1) Most of the data become common in the new model. It confirms the usefulness of uncommon data in characterizing the different behaviors of multiple models. 2) Most of the uncommon data generated on LeNet-5 are BEs (see Table 5).' (This directly implies a lack of consistent behavior or 'uncommon' characteristic transfer across models.)"
        },
        "arbitrator": {
          "value": "Low",
          "why": "The paper generates and evaluates uncommon inputs separately for each model (LeNet-5, NIN, ResNet-20, MobileNet) and does not assess whether failures discovered on one model transfer to others. There is no analysis of the same inputs causing equivalent erroneous behaviors across multiple models.",
          "evidence": "Section 5.1 (RQ3): \u201cWe adopt KuK to generate different types of uncommon data on four widely used DL models \u2013 LeNet-5, NIN, ResNet-20 and MobileNet.\u201d All evaluation of generated inputs is done per model, without cross\u2010model transferability analysis."
        },
        "manual": {
          "value": "Low",
          "why": "evaluates several models independently but does not analyze cross-model consistency in failure behaviors or mutation patterns."
        }
      }
  
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Metamorphic"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Prediction-guided"
      ]
    }
  },
  "Pang et al. - 2022 - MDPFuzz testing models solving Markov decision processes": {
    "key": "pang2022mdpfuzz",
    "name": "MDPFuzz",
    "year": "2022",
    "citation_count": "39",
    "assessments": {
      "Failure Severity": {
        "chatgpt": {
          "value": "High",
          "why": "MDPFuzz uncovers safety-critical failures\u2014such as vehicle and aircraft collisions\u2014that bypass the explicit safety and policy mechanisms (e.g., collision-avoidance systems) of models solving MDPs, demonstrating the ability to penetrate system-level safeguards.",
          "evidence": "Page 378 (Abstract): \u201cMDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states.\u201d; Page 379 (Introduction): \u201cDuring a 12-hour run, we find over 80 crash-triggering state sequences on each model.\u201d"
        },
        "gemini": {
          "value": "High",
          "why": "MDPFuzz is designed to detect and successfully uncovers 'abnormal and dangerous states' referred to as 'crashes' in the paper. These crashes are defined as failures that bypass explicit safety mechanisms in critical scenarios, such as collisions in autonomous driving, aircraft collision avoidance, or an agent falling in a robotic control task. The paper explicitly distinguishes these severe failures from 'inconsistent predictions,' which are considered lower-level functional or robustness errors from which models can often recover. The identified issues directly violate the safety boundaries of the systems under test.",
          "evidence": "Page 1, Abstract: 'MDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states.'\nPage 1, Introduction: 'Catastrophic failures are intolerable, especially in security- or safety-critical scenarios. For example, recently, a Tesla Model S collided with a fire vehicle at 65 mph while the Autopilot system was in use [9], and in 2016, Google's self-driving car collided with the side of a bus [4].'\nPage 4, Section 4, 'Inconsistencies vs. Crashes': 'Real-world models for solving MDPs can quickly recover from \"inconsistent\" predictions. This demands the creation of oracles over concrete and severe abnormal states, such as collisions in airplane control or autonomous driving.'\nPage 4, Definition 2: 'So is regarded as crash-triggering if there exists a crash state St (t > 0).'\nPage 5, Algorithm 1, line 16: 'if Crash({SA,k} i\u2208[M-1]) then...'\nPage 6, Section 6, 'Testing Oracles (Crash Definition)': 'For RL and IL models in CARLA, we examine whether the model-controlled vehicle collides with other vehicles or buildings. For DNN-based ACAS Xu, we check collisions of the DNN-controlled airplane with other airplanes. For Coop Navi, we check collisions between MARL-controlled agents. For BipedalWalker, we check whether the RL-controlled walking agent falls (i.e., its head touches the ground).'\nPage 8, Figure 5: Displays 't = 5s (crash)' showing a car colliding with a fence."
        },
        "arbitrator": {
          "value": "High",
          "why": "MDPFuzz uncovers safety-critical failures\u2014such as vehicle and aircraft collisions\u2014that bypass the explicit safety and policy mechanisms (e.g., collision-avoidance systems) of models solving MDPs, demonstrating the ability to penetrate system-level safeguards.",
          "evidence": "Page 378 (Abstract): \u201cMDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states.\u201d; Page 379 (Introduction): \u201cDuring a 12-hour run, we find over 80 crash-triggering state sequences on each model.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Uncovers behaviorally unsafe yet functionally valid faults \u2014 crash-inducing policies within valid operational parameters. These expose specification-level safety violations but do not bypass explicit control or policy mechanisms. expose  safety critical violations that lead to crashes/collisions or unsafe actions in autonomous or multi-agent environments. reveals crash-triggering state sequences in models solving Markov Decision Processes (MDPs) across autonomous driving, aircraft collision avoidance, and robotic control settings. the failures are behaviorally unsafe but functionally valid \u2014 the agent is still \u201cplaying by the rules\u201d of the simulator. Does not bypass any safety or policy mechanisms. It exposes latent unsafe behaviors within unconstrained learned policies, revealing specification-level vulnerabilities rather than policy breaches."
        }
      },
      "Targeted Attack Discovery": {
        "chatgpt": {
          "value": "Medium",
          "why": "MDPFuzz is designed to steer its exploration toward semantic or category-level faults\u2014specifically, abnormal and dangerous states such as collisions\u2014without prescribing the exact crash outcome or specific target label. It thus performs a partially guided search for unsafe behaviors rather than untargeted robustness testing or fully targeted fault induction.",
          "evidence": "Abstract (p. 378): \u201cMDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states.\u201d\nSection 4 (p. 381): \u201cWe refer to such severe states as \u2018crashes\u2019\u2026 collisions in autonomous driving.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "MDPFuzz is designed to uncover 'abnormal and dangerous states' or 'catastrophic failures' within models solving Markov Decision Processes (MDPs), without targeting a specific, exact output. The paper explicitly defines these as 'crashes' and provides category-level examples such as collisions in autonomous driving, aircraft collision avoidance, or an agent falling in a game. This approach focuses on identifying unsafe behaviors or critical failures rather than general inaccuracies or forcing a predetermined outcome.",
          "evidence": "Page 1, Abstract: 'MDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states.'\nPage 1, Introduction: 'Catastrophic failures are intolerable, especially in security- or safety-critical scenarios. For example, recently, a Tesla Model S collided with a fire vehicle at 65 mph while the Autopilot system was in use [9], and in 2016, Google\u2019s self-driving car collided with the side of a bus [4].'\nPage 4, Section 4, Inconsistencies vs. Crashes: 'Real-world models for solving MDPs can quickly recover from \"inconsistent\" predictions. This demands the creation of oracles over concrete and severe abnormal states, such as collisions in airplane control or autonomous driving.'\nPage 4, Definition 2 (Crash-triggering state sequences): 'So is regarded as crash-triggering if there exists a crash state St (t > 0).'\nPage 8, Section 6, Testing Oracles (Crash Definition): 'For RL and IL models in CARLA, we examine whether the model-controlled vehicle collides with other vehicles or buildings. For DNN-based ACAS Xu, we check collisions of the DNN-controlled airplane with other airplanes. For Coop Navi, we check collisions between MARL-controlled agents. For BipedalWalker, we check whether the RL-controlled walking agent falls (i.e., its head touches the ground).'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "MDPFuzz is designed to steer its exploration toward semantic or category-level faults\u2014specifically, abnormal and dangerous states such as collisions\u2014without prescribing the exact crash outcome or specific target label. It thus performs a partially guided search for unsafe behaviors rather than untargeted robustness testing or fully targeted fault induction.",
          "evidence": "Abstract (p. 378): \u201cMDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states.\u201d\nSection 4 (p. 381): \u201cWe refer to such severe states as \u2018crashes\u2019\u2026 collisions in autonomous driving.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Targets a class of faults (collission or dangerous states)"
        }
      },
      "Root-Cause Analysis": {
        "chatgpt": {
          "value": "Medium",
          "why": "MDPFuzz goes beyond merely reporting crash\u2010triggering cases by analyzing the models\u2019 internal neuron activation patterns and showing that crash\u2010triggering states induce distinct activation clusters. However, it does not trace these failures back to specific architectural, training, or data\u2010related causal factors.",
          "evidence": "Sec. 7.3 Root Cause Analysis (p. 387): \u201cThe neuron activations of crash-triggering state sequences found by MDPFuzz have a clear boundary with normal and randomly mutated sequences\u2026 indicating that crash-triggering states can trigger the models\u2019 abnormal internal logics.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "MDPFuzz provides correlational explanations for observed failures by analyzing the target model's internal signals, specifically neuron activation patterns. It links these distinct patterns to crash-triggering states, thereby explaining 'where and how' failures manifest internally. The insights derived from this analysis are then used to develop an abnormal behavior detector and guide model repair through data augmentation.",
          "evidence": "Page 1, Abstract: \"We show inspiring findings that crash-triggering states, though they look normal, induce distinct neuron activation patterns compared with normal states.\" \nPage 10, Section 7.3 'RQ3: Root Cause Analysis' Setup: \"Inspired by contemporary DNN testing criteria [58, 67, 71, 91], we characterize crash-triggering states by measuring their induced neuron activation patterns in all tested models.\" \nPage 10, Section 7.3 'RQ3: Root Cause Analysis' Results: \"The neuron activations of crash-triggering state sequences found by MDPFuzz have a clear boundary with normal and randomly mutated sequences, which shows that the crash-triggering states can trigger the models' abnormal internal logics.\" \nPage 10, Section 7.4 'RQ4: Enhance Model Robustness' Setup: \"The findings in Sec. 7.3 inspire us to develop a cluster to detect the models' abnormal internal logics... In addition, we repair the models used by DNN-based ACAS Xu with the findings of MDPFuzz... Our fine-tuning dataset includes the crash-triggering sequences found by MDPFuzz...\""
        },
        "arbitrator": {
          "value": "Medium",
          "why": "MDPFuzz goes beyond merely reporting crash\u2010triggering cases by analyzing the models\u2019 internal neuron activation patterns and showing that crash\u2010triggering states induce distinct activation clusters. However, it does not trace these failures back to specific architectural, training, or data\u2010related causal factors.",
          "evidence": "Sec. 7.3 Root Cause Analysis (p. 387): \u201cThe neuron activations of crash-triggering state sequences found by MDPFuzz have a clear boundary with normal and randomly mutated sequences\u2026 indicating that crash-triggering states can trigger the models\u2019 abnormal internal logics.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Provides correlation with internal coverage"
        }
      },
      "Input Plausibility": {
        "chatgpt": {
          "value": "Medium",
          "why": "MDPFuzz enforces theoretical and design constraints\u2014mutating only the MDP\u2019s initial state with small, bounded noise and using the environment\u2019s own validation to ensure realism\u2014yet it does not provide any empirical evaluation (e.g., LPIPS/SSIM/BLEU scores or human studies) to quantify that the mutated inputs remain perceptually or semantically natural.",
          "evidence": "Section 4 (Testing DNN Models Solving MDPs): \u201cSecond, instead of mutating arbitrary intermediate states, MDPFuzz only mutates the initial state conservatively to ensure that the sequence is realistic. The mutated initial states are also validated in a deliberate way to ensure realism.\u201d; Section 6 (Implementation & Evaluation Setup): \u201cAll the mutated initial states are validated in an automated and deliberate way (see below). That is, we confirm that all mutated states are valid and solvable.\u201d"
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper describes a deliberate design to preserve input naturalness by mutating only initial states with small, bounded noise and leveraging environment-specific validation mechanisms. These design choices aim to ensure that fuzzed inputs remain realistic and solvable within the model's input domain. However, the paper does not provide quantitative empirical evidence such as LPIPS, SSIM, or BLEU scores, nor does it report on human evaluation to confirm the perceptual or semantic naturalness of the generated inputs. The validation relies on environmental rules and solvability by an optimal agent, which are forms of design constraints and theoretical justifications rather than direct empirical validation of perceptual realism.",
          "evidence": "Page 2, Abstract: 'Second, instead of mutating arbitrary intermediate states, MDPFuzz only mutates the initial state conservatively to ensure that the sequence is realistic. The mutated initial states are also validated in a deliberate way to ensure realism.'\n\nPage 4, Section 4: 'We thus decide to only mutate the initial state (e.g., re-arrange the environment at timestep 0), allowing to preserve the coherence of the entire MDP state sequence. For each MDP scenario, we vali- date mutated initial states in a deliberate way to ensure they are realistic and solvable for an optimal model \u03c0*.'\n\nPage 8, Section 6, 'MDP Initial State Sampling, Mutation, and Validation': 'MDPFuzz randomly mutates initial states with small noise. All the mutated states are validated in an automated and deliberate way (see below). That is, we confirm that all mutated states are valid and solvable.' This section further specifies: 'We add random noise to the initial states and the noise type depends on type of the original data, e.g., adding small Gaussian float numbers from N(0, 1) to vehicles\u2019 initial positions, and adding small uniform integers from U(0, 4) to the ground type in Bipedal-Walker.' It also states: 'Note that CARLA validates and rejects abnormal initial states: all mutated initial states passed its validation.' and 'We guarantee that there exist optimal actions to avoid the crash and solve the initial states, and we do not use an initial state that is not solvable.'\n\nPage 10, Section 7.3, 'RQ3: Root Cause Analysis': 'Thus, we emphasize that the crashes found by MDPFuzz are not due to unrealistic states. Rather, they share similar visual appearances with normal states and can occur in real-world scenarios.'"
        },
        "arbitrator": {
          "value": "Medium",
          "why": "MDPFuzz enforces theoretical and design constraints\u2014mutating only the MDP\u2019s initial state with small, bounded noise and using the environment\u2019s own validation to ensure realism\u2014yet it does not provide any empirical evaluation (e.g., LPIPS/SSIM/BLEU scores or human studies) to quantify that the mutated inputs remain perceptually or semantically natural.",
          "evidence": "Section 4 (Testing DNN Models Solving MDPs): \u201cSecond, instead of mutating arbitrary intermediate states, MDPFuzz only mutates the initial state conservatively to ensure that the sequence is realistic. The mutated initial states are also validated in a deliberate way to ensure realism.\u201d; Section 6 (Implementation & Evaluation Setup): \u201cAll the mutated initial states are validated in an automated and deliberate way (see below). That is, we confirm that all mutated states are valid and solvable.\u201d"
        },
        "manual": {
          "value": "Medium",
          "why": "Applies bounded noise"
        }
      },
      "Failure Reproducibility": {
        "chatgpt": {
          "value": "High",
          "why": "MDPFuzz performs mutations at the level of MDP initial states (e.g., vehicle positions, angles, speeds) and then relies on the simulator to render complete state sequences and observations. These simulator\u2010generated artifacts are akin to self-contained synthesized inputs (rendered images, environment states) and thus remain stable under standard I/O operations (e.g., quantization to 8-bit images or storage formats). The approach does not depend on fragile, sub-pixel floating-point perturbations that would vanish upon clipping or rounding.",
          "evidence": "Section 6 (MDP Initial State Sampling, Mutation, and Validation): \u201cWhen sampling, the initial states are randomly selected from the state space P. MDPFuzz randomly mutates initial states with small noise. All the mutated states are validated in an automated and deliberate way (see below). That is, we confirm that all mutated states are valid and solvable.\u201d"
        },
        "gemini": {
          "value": "Low",
          "why": "MDPFuzz primarily introduces small continuous perturbations to initial states (e.g., 'small random permutations' or 'small Gaussian float numbers' to positions, angles, and speeds). While the paper states that mutated states are validated to be 'realistic and solvable', it does not explicitly detail mechanisms like clipping or rounding applied to these continuous values specifically to ensure their persistence through standard I/O operations that might involve precision loss (e.g., quantization or conversion to lower-precision formats). The bounding of maximal speed in one specific scenario (ACAS Xu) is a form of clipping for validity, but it's not a general strategy to ensure that small perturbations introduced by fuzzing would reliably persist across typical data serialization or deserialization processes for all state variables.",
          "evidence": "Page 5, 'Design Overview': 'MDPFuzz only mutates the initial state rather than arbitrary states to generate more realistic scenarios. ... MDPFuzz only mutates the initial state S0 of an MDP, under the constraint that it is still realistic and solvable after mutation. See Sec. 6 on how mutated initial states are validated in this study.'\nPage 6, Section 5.1 'Robustness & Sensitivity': 'Inspired by these works, the potential of a seed, i.e., an initial state S0, is estimated by the sensitivity of the target model w.r.t. randomness in S0. As shown in Alg. 2, SENSITIVITY adds small random permutations \u0394S to an initial state S0 and then collects the cumulative reward r\u0394 from MDP.'\nPage 8, 'MDP Initial State Sampling, Mutation, and Validation': 'We add random noise to the initial states and the noise type depends on type of the original data, e.g., adding small Gaussian float numbers from N(0, 1) to vehicles' initial positions, and adding small uniform integers from U(0, 4) to the ground type in BipedalWalker.'\nPage 8, 'MDP Initial State Sampling, Mutation, and Validation': 'Moreover, we bound the maximal speed of all airplanes below 1,100 ft/sec, which is within the range of normal speed that a plane can reach in DNN-based ACAS Xu.'"
        },
        "arbitrator": {
          "value": "High",
          "why": "MDPFuzz\u2019s mutations occur at the level of MDP initial states (e.g. vehicle positions, angles, speeds), which are fed into a simulator that renders complete frames or state observations. These rendered artifacts are akin to synthesized inputs and undergo the simulator\u2019s own clipping, rounding, and quantization (e.g. to 8-bit image pixels). As a result, the faults discovered by MDPFuzz do not depend on fragile, sub-pixel floating-point noise that could be erased by storage or encoding; instead they arise from semantic changes that remain stable through standard I/O operations.",
          "evidence": "Section 6 (MDP Initial State Sampling, Mutation, and Validation): \u201cMDPFuzz samples \u2026 and mutates MDP initial states. All mutated states are validated in an automated and deliberate way \u2026 we confirm that all mutated states are valid and solvable.\u201d\nSection 4 (Mutating Initial States vs. Mutating Intermediate States): \u201cBy mutating only the initial state \u2026 we preserve the coherence of the entire MDP state sequence,\u201d ensuring that subsequent rendered frames are self-contained synthesized inputs, stable under I/O."
        },
        "manual": {
          "value": "High",
          "why": "Perturbs continuous-valued initial state vectors within valid physical bounds and validates each mutated state through the simulator"
        }
      },
      "Attack Transferability": {
        "chatgpt": {
          "value": "Low",
          "why": "MDPFuzz evaluates crash-triggering inputs separately for each model and does not assess whether the same failing initial states cause equivalent failures across multiple models solving the same task.",
          "evidence": "Table 3 (Result overview) reports crashes per model (RL, DNN, IL, MARL) independently, and no section of the paper analyzes cross-model consistency or applies discovered failing inputs from one model to another."
        },
        "gemini": {
          "value": "Medium",
          "why": "The paper demonstrates that the fuzzing method, which involves mutating initial states with small noise, consistently reveals a specific characteristic of failures (distinct neuron activation patterns) across multiple models, even though these models operate on different tasks and paradigms. This indicates that the general mutation approach induces similar types of internal abnormal behaviors across various models, aligning with 'mutation-level generalization'. However, it does not show that the exact same failing inputs cause equivalent erroneous behaviors across models performing the same task.",
          "evidence": "Page 10, Section 7.3, 'RQ3: Root Cause Analysis' and 'Answer to RQ3': 'The neuron activations of crash-triggering state sequences found by MDPFuzz have a clear boundary with normal and randomly mutated sequences, which shows that the crash-triggering states can trigger the models' abnormal internal logics. The results are promisingly consistent across all models of different paradigms. Furthermore, randomly mutated states are mostly mixed with normal states, indicating that random mutation with no guidance can hardly provoke corner case behaviors of the models solving MDPs.' and 'Answer to RQ3: Though crash-triggering initial states are considered normal from the MDP environments' perspective, they induce distinct neuron activation patterns. This shows that MDPFuzz can cover the model's corner internal logics and trigger its abnormal behaviors more efficiently than randomly-mutated state sequences. This finding also reveals exciting potentials of detecting crash-triggering states.'"
        },
        "arbitrator": {
          "value": "Low",
          "why": "MDPFuzz evaluates failures separately for each model and does not analyze whether the same mutated initial states cause equivalent crashes across different models solving the same task. There is no experiment or discussion in the paper that applies failing inputs discovered on one model to other models to demonstrate shared vulnerabilities at the input level.",
          "evidence": "Table 3 (Result overview) reports crash-triggering state sequences per model (RL, DNN, IL, MARL) independently, and the paper contains no section or results on cross-model consistency or transfer of failing inputs."
        },
        "manual": {
          "value": "Low",
          "why": "No cross model failure pattern analysis"
        }
      }
    },
    "taxonomy": {
      "access_level": [
        "Blackbox"
      ],
      "oracle": [
        "Property-based"
      ],
      "mutation_strategy": [
        "Rule-based"
      ],
      "exploration_strategy": [
        "Prediction-guided",
        "Data-driven"
      ]
    }
  }
}