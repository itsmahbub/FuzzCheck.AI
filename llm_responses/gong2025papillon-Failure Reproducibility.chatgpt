```json
{
  "value": "High",
  "why": "PAPILLON generates final test inputs as textual jailbreak prompts (serialized text) produced by its mutators and sent directly to target LLMs; textual prompts are inherently stable to typical numeric I/O effects (quantization/rounding) described in the metric. The paper explicitly defines the 'jailbreak prompt' as the final input string, describes using an LLM mutator to produce these templates (controlling token length via max_new_tokens), and treats those generated texts as the inputs used for both execution and judging. Because PAPILLON produces inputs directly in serialized text form (and controls prompt/token length), the fault-inducing inputs it discovers are reproducible under standard I/O operations per the metric’s allowance for serialized-format inputs.",
  "evidence": "Page 2407, Section 4.1: \"10The jailbreak prompt is the final input to the LLM, created by replacing the placeholders (denotes as [INSERT PROMPT HERE]) in the jailbreaking template with the harmful question.\"; Page 2408, Section 4.2: \"In PAPILLON, we design three novel question-dependent mutation strategies capable of generating mutations with semantic meaning and limited token length by leveraging the power of an LLM helper.\"; Page 2410, Section 5.1 (Experimental Setup): \"The length of the generated prompts is controlled using the max_new_tokens hyperparameter. Specifically, for the ‘role-play' and ‘contextualization' mutations, this value is set to 200 tokens, while an additional 100 tokens are allocated for the ‘expand' mutation.\"; Page 2411, Section 5.3 (Attack Transferability - MLLM transferability): \"successful jailbreaking templates are printed on white-background images and used as image inputs...\" (shows the authors generate serialized text prompts as the primary inputs; when converted to images for MLLMs they do not claim special handling of image quantization, indicating the core method relies on serialized text)."
}
```