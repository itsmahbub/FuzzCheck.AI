```json
{
  "value": "Medium",
  "why": "The paper enforces plausibility at the level of individual mutation steps: it uses domain-specific, metamorphic transformations with conservative parameters (e.g., small speed/volume/noise changes for audio; small brightness/contrast/translation/rotation for images) and states that these perturbations are imperceptible. However, DeepStellar does not describe mechanisms that prevent or measure cumulative drift across multiple fuzzing/mutation iterations, nor does it provide empirical human evaluation or task-specific perceptual metrics (e.g., SSIM/LPIPS for images, PESQ/STOI for audio, or Perplexity for language) to validate that final fault-inducing inputs remain perceptually/semantically realistic. The coverage-guided testing algorithm (Algorithm 2) accepts mutants based on coverage increase alone, with no stated plausibility filter applied to the retained seeds; the paper therefore enforces plausibility in individual mutation choices but overlooks cumulative-effects and lacks quantitative or user-study evidence for final-input plausibility, matching the rubric for “Medium.”",
  "evidence": "Section 5.2 Coverage-Guided Testing (p. 481): \"Mutation operators are often specific to the application domains. For CGT, we mainly focus on image classification and apply the metamorphic mutation strategy [55] for generating new mutants that would keep the prediction meaning from the human’s perspective during testing.\" (Algorithm 2 also shows mutants are retained when they increase coverage: \"Randomly pick transformation t with a random parameter p; A = mutate(t, p, a); ... else if CoverageIncrease(cov, Q) then Q <- Q ∪ a' .\")\n\nSection 6.1 Data Preparation (p. 482–483): \"For each ASR model, we generate 10,000 perturbed audios from original benign ones with existing audio data augmentation techniques [5] (i.e., speed and volume adjustment, low/high-frequency filtering, noise blending).... For each MNIST model, we also generate 10,000 benign perturbed samples with existing image transformation techniques [50] (i.e., image contrast, brightness, translation, scaling, shearing, rotation and add white noise). ... For both audio and image case, we set conservative parameters for transformation so that the perturbation on original samples is slight and imperceptible. Note that all the adversarial samples are also with minimal perturbations and not perceptible by human.\"\n\nEvaluation sections (e.g., RQ3 & RQ4, pp. 484–486) report detection AUROC, coverage numbers and counts of adversarial samples, but the paper does not report any human studies or task-specific perceptual/semantic quality metrics (e.g., LPIPS/SSIM for images, PESQ/STOI for audio, or Perplexity for language) to empirically validate final fault-inducing inputs' realism."
}
```